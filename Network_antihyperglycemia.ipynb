{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8da64548-e7fc-4105-897c-08d7368ff15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3766b840-0384-4fb0-9d4b-bd71a5c24bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.reaad_csv('df_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10356640-4cdc-444f-81d6-20ca6b9bc263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name_Short</th>\n",
       "      <th>Emax_amylase_2.75mcgml</th>\n",
       "      <th>Emax_glucosidase_0.67mcgml</th>\n",
       "      <th>IC50_amylase</th>\n",
       "      <th>IC50_glucosidase</th>\n",
       "      <th>Bioactivity_class_amylase</th>\n",
       "      <th>Bioactivity_class_glucosidase</th>\n",
       "      <th>Alkaloids</th>\n",
       "      <th>Antaquinones</th>\n",
       "      <th>Carotenoids</th>\n",
       "      <th>flavonoids</th>\n",
       "      <th>Reducing_sugars</th>\n",
       "      <th>Saponins</th>\n",
       "      <th>Tannins</th>\n",
       "      <th>Xanthones</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>G_cowa</td>\n",
       "      <td>99.6</td>\n",
       "      <td>51.1</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.63</td>\n",
       "      <td>active</td>\n",
       "      <td>active</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P_debilis</td>\n",
       "      <td>99.5</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.72</td>\n",
       "      <td>active</td>\n",
       "      <td>active</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Name_Short  Emax_amylase_2.75mcgml  Emax_glucosidase_0.67mcgml  \\\n",
       "0     G_cowa                    99.6                        51.1   \n",
       "1  P_debilis                    99.5                        50.0   \n",
       "\n",
       "   IC50_amylase  IC50_glucosidase Bioactivity_class_amylase  \\\n",
       "0          0.13              0.63                    active   \n",
       "1          0.14              0.72                    active   \n",
       "\n",
       "  Bioactivity_class_glucosidase  Alkaloids  Antaquinones  Carotenoids  \\\n",
       "0                        active          0             0            0   \n",
       "1                        active          3             0            0   \n",
       "\n",
       "   flavonoids  Reducing_sugars  Saponins  Tannins  Xanthones  \n",
       "0           0                0         2        2          2  \n",
       "1           5                3         0        0          0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5d8b4c66-5472-4b93-a212-c5982ce74424",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[['Name_Short','Bioactivity_class_amylase','Bioactivity_class_glucosidase', 'Emax_glucosidase_0.67mcgml', 'Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "58e5c085-48e0-4dc3-9bee-a20b609cd504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name_Short</th>\n",
       "      <th>Bioactivity_class_amylase</th>\n",
       "      <th>Bioactivity_class_glucosidase</th>\n",
       "      <th>Emax_glucosidase_0.67mcgml</th>\n",
       "      <th>Alkaloids</th>\n",
       "      <th>Antaquinones</th>\n",
       "      <th>Carotenoids</th>\n",
       "      <th>flavonoids</th>\n",
       "      <th>Reducing_sugars</th>\n",
       "      <th>Saponins</th>\n",
       "      <th>Tannins</th>\n",
       "      <th>Xanthones</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>G_cowa</td>\n",
       "      <td>active</td>\n",
       "      <td>active</td>\n",
       "      <td>51.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P_debilis</td>\n",
       "      <td>active</td>\n",
       "      <td>active</td>\n",
       "      <td>50.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P_evecta_leaf</td>\n",
       "      <td>active</td>\n",
       "      <td>active</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Name_Short Bioactivity_class_amylase Bioactivity_class_glucosidase  \\\n",
       "0         G_cowa                    active                        active   \n",
       "1      P_debilis                    active                        active   \n",
       "2  P_evecta_leaf                    active                        active   \n",
       "\n",
       "   Emax_glucosidase_0.67mcgml  Alkaloids  Antaquinones  Carotenoids  \\\n",
       "0                        51.1          0             0            0   \n",
       "1                        50.0          3             0            0   \n",
       "2                        52.0          0             0            0   \n",
       "\n",
       "   flavonoids  Reducing_sugars  Saponins  Tannins  Xanthones  \n",
       "0           0                0         2        2          2  \n",
       "1           5                3         0        0          0  \n",
       "2           3                0         3        2          0  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "41c8bbad-040c-4327-ad00-7c6bd5b5d9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cd41966e-a070-46c2-a38d-4d203d4c7ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_pandas_edgelist(df1, 'Name_Short', 'Bioactivity_class_glucosidase', edge_attr='flavonoids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2ec2fa0d-2d1e-4fe1-af85-973c2ac63156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nx.draw(G, with_labels=True, font_weight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "90fbd80e-df70-4324-8490-f4edcd8ed88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Res_pearson.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1c811686-eb99-4c7b-a524-759fd8b0f9b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Emax_amylase_2.75mcgml</th>\n",
       "      <th>Emax_glucosidase_0.67mcgml</th>\n",
       "      <th>IC50_amylase</th>\n",
       "      <th>IC50_glucosidase</th>\n",
       "      <th>Alkaloids</th>\n",
       "      <th>Antaquinones</th>\n",
       "      <th>Carotenoids</th>\n",
       "      <th>flavonoids</th>\n",
       "      <th>Reducing_sugars</th>\n",
       "      <th>Saponins</th>\n",
       "      <th>Tannins</th>\n",
       "      <th>Xanthones</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Emax_amylase_2.75mcgml</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.195</td>\n",
       "      <td>-0.618</td>\n",
       "      <td>0.686</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.143</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Emax_glucosidase_0.67mcgml</td>\n",
       "      <td>0.195</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.887</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.363</td>\n",
       "      <td>-0.384</td>\n",
       "      <td>0.454</td>\n",
       "      <td>0.099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IC50_amylase</td>\n",
       "      <td>-0.618</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.440</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.095</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-0.107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IC50_glucosidase</td>\n",
       "      <td>0.686</td>\n",
       "      <td>-0.887</td>\n",
       "      <td>0.312</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.260</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>-0.737</td>\n",
       "      <td>-0.541</td>\n",
       "      <td>0.216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alkaloids</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.499</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.384</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.205</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Antaquinones</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.154</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.179</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.310</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-0.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Carotenoids</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.440</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.384</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>flavonoids</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.353</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>0.260</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.085</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>-0.396</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>0.142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Reducing_sugars</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.310</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Saponins</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>-0.384</td>\n",
       "      <td>0.095</td>\n",
       "      <td>-0.737</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-0.396</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>-0.292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Tannins</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>0.454</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-0.541</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>0.180</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Xanthones</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>0.099</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.074</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Unnamed: 0  Emax_amylase_2.75mcgml  \\\n",
       "0       Emax_amylase_2.75mcgml                   1.000   \n",
       "1   Emax_glucosidase_0.67mcgml                   0.195   \n",
       "2                 IC50_amylase                  -0.618   \n",
       "3             IC50_glucosidase                   0.686   \n",
       "4                    Alkaloids                  -0.108   \n",
       "5                 Antaquinones                   0.174   \n",
       "6                  Carotenoids                   0.135   \n",
       "7                   flavonoids                   0.143   \n",
       "8              Reducing_sugars                  -0.257   \n",
       "9                     Saponins                  -0.221   \n",
       "10                     Tannins                  -0.042   \n",
       "11                   Xanthones                  -0.113   \n",
       "\n",
       "    Emax_glucosidase_0.67mcgml  IC50_amylase  IC50_glucosidase  Alkaloids  \\\n",
       "0                        0.195        -0.618             0.686     -0.108   \n",
       "1                        1.000        -0.404            -0.887      0.248   \n",
       "2                       -0.404         1.000             0.312      0.499   \n",
       "3                       -0.887         0.312             1.000     -0.163   \n",
       "4                        0.248         0.499            -0.163      1.000   \n",
       "5                        0.038         0.154               NaN      0.179   \n",
       "6                        0.049         0.440               NaN      0.384   \n",
       "7                        0.353        -0.269             0.260     -0.015   \n",
       "8                        0.363         0.016            -0.163      0.205   \n",
       "9                       -0.384         0.095            -0.737     -0.136   \n",
       "10                       0.454        -0.156            -0.541      0.109   \n",
       "11                       0.099        -0.107             0.216      0.074   \n",
       "\n",
       "    Antaquinones  Carotenoids  flavonoids  Reducing_sugars  Saponins  Tannins  \\\n",
       "0          0.174        0.135       0.143           -0.257    -0.221   -0.042   \n",
       "1          0.038        0.049       0.353            0.363    -0.384    0.454   \n",
       "2          0.154        0.440      -0.269            0.016     0.095   -0.156   \n",
       "3            NaN          NaN       0.260           -0.163    -0.737   -0.541   \n",
       "4          0.179        0.384      -0.015            0.205    -0.136    0.109   \n",
       "5          1.000       -0.033       0.085            0.310    -0.077    0.158   \n",
       "6         -0.033        1.000      -0.054           -0.108     0.049   -0.168   \n",
       "7          0.085       -0.054       1.000            0.500    -0.396   -0.140   \n",
       "8          0.310       -0.108       0.500            1.000    -0.300    0.180   \n",
       "9         -0.077        0.049      -0.396           -0.300     1.000   -0.111   \n",
       "10         0.158       -0.168      -0.140            0.180    -0.111    1.000   \n",
       "11        -0.075       -0.075       0.142            0.005    -0.292   -0.300   \n",
       "\n",
       "    Xanthones  \n",
       "0      -0.113  \n",
       "1       0.099  \n",
       "2      -0.107  \n",
       "3       0.216  \n",
       "4       0.074  \n",
       "5      -0.075  \n",
       "6      -0.075  \n",
       "7       0.142  \n",
       "8       0.005  \n",
       "9      -0.292  \n",
       "10     -0.300  \n",
       "11      1.000  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "658a7e26-76d3-4055-a425-5153601fc4f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Emax_amylase_2.75mcgml</th>\n",
       "      <th>Emax_glucosidase_0.67mcgml</th>\n",
       "      <th>IC50_amylase</th>\n",
       "      <th>IC50_glucosidase</th>\n",
       "      <th>Alkaloids</th>\n",
       "      <th>Antaquinones</th>\n",
       "      <th>Carotenoids</th>\n",
       "      <th>flavonoids</th>\n",
       "      <th>Reducing_sugars</th>\n",
       "      <th>Saponins</th>\n",
       "      <th>Tannins</th>\n",
       "      <th>Xanthones</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Emax_amylase_2.75mcgml</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.195</td>\n",
       "      <td>-0.618</td>\n",
       "      <td>0.686</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.143</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Emax_glucosidase_0.67mcgml</td>\n",
       "      <td>0.195</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.887</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.363</td>\n",
       "      <td>-0.384</td>\n",
       "      <td>0.454</td>\n",
       "      <td>0.099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IC50_amylase</td>\n",
       "      <td>-0.618</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.440</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.095</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-0.107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IC50_glucosidase</td>\n",
       "      <td>0.686</td>\n",
       "      <td>-0.887</td>\n",
       "      <td>0.312</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.260</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>-0.737</td>\n",
       "      <td>-0.541</td>\n",
       "      <td>0.216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alkaloids</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.499</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.384</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.205</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Antaquinones</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.154</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.179</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.310</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-0.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Carotenoids</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.440</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.384</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>flavonoids</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.353</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>0.260</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.085</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>-0.396</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>0.142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Reducing_sugars</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.310</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Saponins</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>-0.384</td>\n",
       "      <td>0.095</td>\n",
       "      <td>-0.737</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-0.396</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>-0.292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Tannins</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>0.454</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-0.541</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>0.180</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Xanthones</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>0.099</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.074</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Name  Emax_amylase_2.75mcgml  \\\n",
       "0       Emax_amylase_2.75mcgml                   1.000   \n",
       "1   Emax_glucosidase_0.67mcgml                   0.195   \n",
       "2                 IC50_amylase                  -0.618   \n",
       "3             IC50_glucosidase                   0.686   \n",
       "4                    Alkaloids                  -0.108   \n",
       "5                 Antaquinones                   0.174   \n",
       "6                  Carotenoids                   0.135   \n",
       "7                   flavonoids                   0.143   \n",
       "8              Reducing_sugars                  -0.257   \n",
       "9                     Saponins                  -0.221   \n",
       "10                     Tannins                  -0.042   \n",
       "11                   Xanthones                  -0.113   \n",
       "\n",
       "    Emax_glucosidase_0.67mcgml  IC50_amylase  IC50_glucosidase  Alkaloids  \\\n",
       "0                        0.195        -0.618             0.686     -0.108   \n",
       "1                        1.000        -0.404            -0.887      0.248   \n",
       "2                       -0.404         1.000             0.312      0.499   \n",
       "3                       -0.887         0.312             1.000     -0.163   \n",
       "4                        0.248         0.499            -0.163      1.000   \n",
       "5                        0.038         0.154               NaN      0.179   \n",
       "6                        0.049         0.440               NaN      0.384   \n",
       "7                        0.353        -0.269             0.260     -0.015   \n",
       "8                        0.363         0.016            -0.163      0.205   \n",
       "9                       -0.384         0.095            -0.737     -0.136   \n",
       "10                       0.454        -0.156            -0.541      0.109   \n",
       "11                       0.099        -0.107             0.216      0.074   \n",
       "\n",
       "    Antaquinones  Carotenoids  flavonoids  Reducing_sugars  Saponins  Tannins  \\\n",
       "0          0.174        0.135       0.143           -0.257    -0.221   -0.042   \n",
       "1          0.038        0.049       0.353            0.363    -0.384    0.454   \n",
       "2          0.154        0.440      -0.269            0.016     0.095   -0.156   \n",
       "3            NaN          NaN       0.260           -0.163    -0.737   -0.541   \n",
       "4          0.179        0.384      -0.015            0.205    -0.136    0.109   \n",
       "5          1.000       -0.033       0.085            0.310    -0.077    0.158   \n",
       "6         -0.033        1.000      -0.054           -0.108     0.049   -0.168   \n",
       "7          0.085       -0.054       1.000            0.500    -0.396   -0.140   \n",
       "8          0.310       -0.108       0.500            1.000    -0.300    0.180   \n",
       "9         -0.077        0.049      -0.396           -0.300     1.000   -0.111   \n",
       "10         0.158       -0.168      -0.140            0.180    -0.111    1.000   \n",
       "11        -0.075       -0.075       0.142            0.005    -0.292   -0.300   \n",
       "\n",
       "    Xanthones  \n",
       "0      -0.113  \n",
       "1       0.099  \n",
       "2      -0.107  \n",
       "3       0.216  \n",
       "4       0.074  \n",
       "5      -0.075  \n",
       "6      -0.075  \n",
       "7       0.142  \n",
       "8       0.005  \n",
       "9      -0.292  \n",
       "10     -0.300  \n",
       "11      1.000  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rename(columns={'Unnamed: 0' : 'Name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840c3d13-7b09-4917-b32c-fd479db041ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c1b4d2-b363-45be-9559-ccc344a4f6f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f5fa66-b031-4500-a5fb-1156490194fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9209eb02-2926-40ba-b7ba-6d63f108f927",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009ebc84-d45b-44ab-ab6f-89e1b5b5273b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb5e852-9803-4f09-b068-3243f48eda0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93dbe87-4a65-40e2-87ee-5801da014b77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a386408-6ca2-4cdc-80ba-e9b5fb964fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b86b11c-4485-4e09-9925-3a1bcdd20339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dd7c00-f7b0-40c2-99bd-35dea7cc4026",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35e088f-684d-44d0-9287-77158627b2c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5030480d-ac62-4be9-a755-975fa35fa552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288d9082-b2ad-45b7-98e4-4345fe35d346",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0132be20-b553-4d6d-a3e0-3222a572e26f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9913b9e1-7995-4313-9b9b-5543b5f1669d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a20f07-a72c-4fbd-b1f0-4b12b35a5019",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc7f35b-11d8-497c-ab3a-1df12ae8aa4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43430906-62ee-4fa9-b1f8-4ed431b1c9e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317a3aee-b9e0-4233-ab51-1650b875a241",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f379fc31-9bd2-4db1-b9f5-da4608eeef8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f3aef2-b2b5-4e5d-acaa-71945a629329",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ff4ba7-d1db-452d-89c5-2aa91968b455",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e1b388-3a69-4c1d-8216-9430229fbbd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6eca75-aa71-44a3-b0a1-aac97329fcd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b0f4d4-bed0-431a-a647-9cde075b44ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5bf5081-755f-4bfa-8bbe-a129df7fc6ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', \"get_ipython().run_line_magic('history', '-g')\", '_ih[-15:]']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cae04a54-27e8-495d-8f2b-36c17785f5f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'source'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'source'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-8976bb830ba5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pandas_edgelist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/networkx/convert_matrix.py\u001b[0m in \u001b[0;36mfrom_pandas_edgelist\u001b[0;34m(df, source, target, edge_attr, create_using, edge_key)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0medge_attr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m         \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edges_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3024\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3025\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3080\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3082\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3084\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'source'"
     ]
    }
   ],
   "source": [
    "G = nx.from_pandas_edgelist(df, 'Name', 'Bioactivity_class_glucosidase', edge_attr='flavonoids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9763f91-7576-4c15-9afb-1cff1ed17c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/1: print(\"Hello world\")\n",
      " 1/2: import os\n",
      " 1/3:\n",
      "import os\n",
      "import tarfile\n",
      "import urllib\n",
      " 1/4:\n",
      "import os\n",
      "import tarfile\n",
      "import urllib\n",
      " 1/5:\n",
      "import os\n",
      "import tarfile\n",
      "import urllib\n",
      " 1/6:\n",
      "import os\n",
      "import tarfile\n",
      "import urllib\n",
      " 1/7:\n",
      "import os\n",
      "import tarfile\n",
      "import urllib\n",
      " 1/8:\n",
      "import os\n",
      "import tarfile\n",
      "import urllib\n",
      " 1/9: import panda as pd\n",
      "1/10: import pandas as pd\n",
      "1/11: import pandas as pd\n",
      "1/12: import pandas as pd\n",
      "1/13: import pandas as pd\n",
      "1/14: import pandas as pd\n",
      "1/15: import pandas as pd\n",
      "1/16: import pandas as pd\n",
      "1/17:\n",
      "import os\n",
      "import tarfile\n",
      "import urllib\n",
      "1/18:\n",
      "import os\n",
      "import tarfile\n",
      "import urllib\n",
      "1/19:\n",
      "import os\n",
      "import tarfile\n",
      "import urllib\n",
      "1/20: import pandas as pd\n",
      "1/21: import pandas as pd\n",
      "1/22: import pandas as pd\n",
      "1/23:\n",
      "import os\n",
      "import tarfile\n",
      "import urllib\n",
      "\n",
      "Download_root = \"http://raw.githubusercintent.com/ageron/handson-ml2/master\"\n",
      "1/24:\n",
      "import os\n",
      "import tarfile\n",
      "from six.moves import urllib\n",
      "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml/master\"\n",
      "HOUSING_PATH = \"datasets/housing\"\n",
      "HOUSING_URL = DOWNLOAD_ROOT + HOUSING_PATH + \"/housing.tgz\"\n",
      "\n",
      "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
      "    if not os.path.isdir(housing_path):\n",
      "        os.makedirs(housing_path)\n",
      "    tgz_path=os.path.join(housing_path,\"housing.tgz\")\n",
      "    urllib.request.urlretrieve(housing_url,tgz_path)\n",
      "    housing_tgz=tarfile.open(tgz_path)\n",
      "    housing_tgz.extractall(path=housing_path)\n",
      "    housing_tgz.close()\n",
      "1/25:\n",
      "import os\n",
      "import tarfile\n",
      "from six.moves import urllib\n",
      "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml/master\"\n",
      "HOUSING_PATH = \"datasets/housing\"\n",
      "HOUSING_URL = DOWNLOAD_ROOT + HOUSING_PATH + \"/housing.tgz\"\n",
      "\n",
      "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
      "    if not os.path.isdir(housing_path):\n",
      "        os.makedirs(housing_path)\n",
      "    tgz_path=os.path.join(housing_path,\"housing.tgz\")\n",
      "    urllib.request.urlretrieve(housing_url,tgz_path)\n",
      "    housing_tgz=tarfile.open(tgz_path)\n",
      "    housing_tgz.extractall(path=housing_path)\n",
      "    housing_tgz.close()\n",
      "fetch_housing_data()\n",
      "1/26:\n",
      "import os\n",
      "import tarfile\n",
      "from six.moves import urllib\n",
      "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml/master/\"\n",
      "HOUSING_PATH = \"datasets/housing\"\n",
      "HOUSING_URL = DOWNLOAD_ROOT + HOUSING_PATH + \"/housing.tgz\"\n",
      "\n",
      "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
      "    if not os.path.isdir(housing_path):\n",
      "        os.makedirs(housing_path)\n",
      "    tgz_path=os.path.join(housing_path,\"housing.tgz\")\n",
      "    urllib.request.urlretrieve(housing_url,tgz_path)\n",
      "    housing_tgz=tarfile.open(tgz_path)\n",
      "    housing_tgz.extractall(path=housing_path)\n",
      "    housing_tgz.close()\n",
      "fetch_housing_data()\n",
      "1/27:\n",
      "import os\n",
      "import tarfile\n",
      "import urllib\n",
      "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
      "HOUSING_PATH = os.path.join(\"datasets\",\"housing\")\n",
      "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
      "\n",
      "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
      "    os.makedirs(housing_path, exist_ok=True)\n",
      "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
      "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
      "    housing_tgz = tarfile.open(tgz_path)\n",
      "    housing_tgz.extractall(path=housing_path)\n",
      "    housing_tgz.close()\n",
      "fetch_housing_data()\n",
      "1/28:\n",
      "import pandas as pd\n",
      "def load_housing_data(housing_path=HOUSING_PATH):\n",
      "    cvs_path = os.path.join(housing_path, \"housing.csv\")\n",
      "    return pd.read_csv(csv_path)\n",
      "1/29:\n",
      "housing = load_housing_data()\n",
      "housing.head()\n",
      " 2/1:\n",
      "import pandas as pd\n",
      "def load_housing_data(housing_path=HOUSING_PATH):\n",
      "    cvs_path = os.path.join(housing_path, \"housing.csv\")\n",
      "    return pd.read_csv(csv_path)\n",
      " 2/2:\n",
      "import os\n",
      "import tarfile\n",
      "import urllib\n",
      "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
      "HOUSING_PATH = os.path.join(\"datasets\",\"housing\")\n",
      "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
      "\n",
      "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
      "    os.makedirs(housing_path, exist_ok=True)\n",
      "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
      "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
      "    housing_tgz = tarfile.open(tgz_path)\n",
      "    housing_tgz.extractall(path=housing_path)\n",
      "    housing_tgz.close()\n",
      "fetch_housing_data()\n",
      " 2/3:\n",
      "import os\n",
      "import tarfile\n",
      "import urllib\n",
      "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
      "HOUSING_PATH = os.path.join(\"datasets\",\"housing\")\n",
      "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
      "\n",
      "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
      "    os.makedirs(housing_path, exist_ok=True)\n",
      "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
      "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
      "    housing_tgz = tarfile.open(tgz_path)\n",
      "    housing_tgz.extractall(path=housing_path)\n",
      "    housing_tgz.close()\n",
      " 2/4:\n",
      "import os\n",
      "import tarfile\n",
      "import urllib\n",
      "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
      "HOUSING_PATH = os.path.join(\"datasets\",\"housing\")\n",
      "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
      "\n",
      "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
      "    os.makedirs(housing_path, exist_ok=True)\n",
      "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
      "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
      "    housing_tgz = tarfile.open(tgz_path)\n",
      "    housing_tgz.extractall(path=housing_path)\n",
      "    housing_tgz.close()\n",
      " 2/5:\n",
      "import os\n",
      "import tarfile\n",
      "import urllib\n",
      "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
      "HOUSING_PATH = os.path.join(\"datasets\",\"housing\")\n",
      "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
      "\n",
      "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
      "    os.makedirs(housing_path, exist_ok=True)\n",
      "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
      "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
      "    housing_tgz = tarfile.open(tgz_path)\n",
      "    housing_tgz.extractall(path=housing_path)\n",
      "    housing_tgz.close()\n",
      " 2/6: fetch_housing_data()\n",
      " 2/7:\n",
      "import pandas as pd\n",
      "def load_housing_data(housing_path=download):\n",
      "    cvs_path = os.path.join(housing_path, \"housing.csv\")\n",
      "    return pd.read_csv(csv_path)\n",
      " 2/8:\n",
      "import pandas as pd\n",
      "def load_housing_data(housing_path=HOUSING_PATH):\n",
      "    cvs_path = os.path.join(housing_path, \"housing.csv\")\n",
      "    return pd.read_csv(csv_path)\n",
      " 2/9:\n",
      "import pandas as pd\n",
      "def load_housing_data(housing_path=HOUSING_PATH):\n",
      "    cvs_path = os.path.join(housing_path, \"housing.csv\")\n",
      "    return pd.read_csv(csv_path)\n",
      "2/10:\n",
      "housing = load_housing_data()\n",
      "housing.head()\n",
      "2/11:\n",
      "import pandas as pd\n",
      "def load_housing_data(housing_path=HOUSING_PATH):\n",
      "    cvs_path = os.path.join(housing_path, \"housing.csv\")\n",
      "    return pd.read_csv(csv_path)\n",
      "2/12:\n",
      "housing = load_housing_data()\n",
      "housing.head()\n",
      "2/13:\n",
      "housing = load_housing_data()\n",
      "housing.head()\n",
      "2/14:\n",
      "import pandas as pd\n",
      "def load_housing_data(housing_path=HOUSING_PATH):\n",
      "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
      "    return pd.read_csv(csv_path)\n",
      "2/15:\n",
      "housing = load_housing_data()\n",
      "housing.head()\n",
      "2/16:\n",
      "import os\n",
      "import tarfile\n",
      "import urllib\n",
      "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
      "HOUSING_PATH = os.path.join(\"datasets\",\"housing\")\n",
      "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
      "\n",
      "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
      "    os.makedirs(housing_path, exist_ok=True)\n",
      "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
      "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
      "    housing_tgz = tarfile.open(tgz_path)\n",
      "    housing_tgz.extractall(path=housing_path)\n",
      "    housing_tgz.close()\n",
      "2/17: fetch_housing_data()\n",
      "2/18:\n",
      "import pandas as pd\n",
      "def load_housing_data(housing_path=HOUSING_PATH):\n",
      "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
      "    return pd.read_csv(csv_path)\n",
      "2/19:\n",
      "housing = load_housing_data()\n",
      "housing.head()\n",
      "2/20:\n",
      "# To support both python 2 and python 3\n",
      "from __future__ import division, print_function, unicode_literals\n",
      "\n",
      "# Common imports\n",
      "import numpy as np\n",
      "import os\n",
      "\n",
      "# to make this notebook's output stable across runs\n",
      "np.random.seed(42)\n",
      "\n",
      "# To plot pretty figures\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.pyplot as plt\n",
      "mpl.rc('axes', labelsize=14)\n",
      "mpl.rc('xtick', labelsize=12)\n",
      "mpl.rc('ytick', labelsize=12)\n",
      "\n",
      "# Where to save the figures\n",
      "PROJECT_ROOT_DIR = \".\"\n",
      "CHAPTER_ID = \"end_to_end_project\"\n",
      "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
      "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
      "\n",
      "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
      "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
      "    print(\"Saving figure\", fig_id)\n",
      "    if tight_layout:\n",
      "        plt.tight_layout()\n",
      "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
      "2/21:\n",
      "import os\n",
      "import tarfile\n",
      "import urllib\n",
      "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
      "HOUSING_PATH = os.path.join(\"datasets\",\"housing\")\n",
      "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
      "\n",
      "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
      "    os.makedirs(housing_path, exist_ok=True)\n",
      "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
      "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
      "    housing_tgz = tarfile.open(tgz_path)\n",
      "    housing_tgz.extractall(path=housing_path)\n",
      "    housing_tgz.close()\n",
      "2/22: fetch_housing_data()\n",
      "2/23:\n",
      "# Python 3.5 is required\n",
      "import sys\n",
      "assert sys.version_info >= (3, 5)\n",
      "\n",
      "# Scikit-Learn 0.20 is required\n",
      "import sklearn\n",
      "assert sklearn.__version__ >= \"0.20\"\n",
      "\n",
      "# Common imports\n",
      "import numpy as np\n",
      "import os\n",
      "\n",
      "# To plot pretty figures\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.pyplot as plt\n",
      "mpl.rc('axes', labelsize=14)\n",
      "mpl.rc('xtick', labelsize=12)\n",
      "mpl.rc('ytick', labelsize=12)\n",
      "\n",
      "# Where to save the figures\n",
      "PROJECT_ROOT_DIR = \".\"\n",
      "CHAPTER_ID = \"end_to_end_project\"\n",
      "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
      "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
      "\n",
      "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
      "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
      "    print(\"Saving figure\", fig_id)\n",
      "    if tight_layout:\n",
      "        plt.tight_layout()\n",
      "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
      "2/24:\n",
      "import os\n",
      "import tarfile\n",
      "import urllib.request\n",
      "\n",
      "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
      "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
      "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
      "\n",
      "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
      "    if not os.path.isdir(housing_path):\n",
      "        os.makedirs(housing_path)\n",
      "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
      "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
      "    housing_tgz = tarfile.open(tgz_path)\n",
      "    housing_tgz.extractall(path=housing_path)\n",
      "    housing_tgz.close()\n",
      "2/25: fetch_housing_data()\n",
      "2/26:\n",
      "import pandas as pd\n",
      "\n",
      "def load_housing_data(housing_path=HOUSING_PATH):\n",
      "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
      "    return pd.read_csv(csv_path)\n",
      "2/27:\n",
      "housing = load_housing_data()\n",
      "housing.head()\n",
      "2/28:\n",
      "housing = load_housing_data()\n",
      "housing.head()\n",
      " 3/1: import sys\n",
      " 3/2:\n",
      "import sys \n",
      "assert sys.version_info >= (3, 5)\n",
      " 3/3:\n",
      "import sys \n",
      "assert sys.version_info >= (3, 5)\n",
      "\n",
      "import sklearn\n",
      "assert sklearn.__version__>= \"0.20\"\n",
      " 4/1: print(\"hello world!\")\n",
      " 4/2: print(\"hello world!\")\n",
      " 4/3:\n",
      "import os\n",
      "import tarfile\n",
      "import urllib\n",
      "\n",
      "DOWNLOAD_ROOT = https://raw.githubusercontent.com/ageron/handson-ml2/master/\n",
      "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
      "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tagz\"\n",
      "\n",
      "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
      "    os.makedirs(housing_path, exist_ok = True)\n",
      "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
      "    urllib.request.urlretrieve(housing_url,tgz_path)\n",
      "    housing_tgz = tarfile.open(tgz_path)\n",
      "    housing_tgz.extractcall(path=housing_path)\n",
      "    housing_tgz.close()\n",
      " 4/4:\n",
      "import os\n",
      "import tarfile\n",
      "import urllib\n",
      "\n",
      "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
      "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
      "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tagz\"\n",
      "\n",
      "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
      "    os.makedirs(housing_path, exist_ok = True)\n",
      "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
      "    urllib.request.urlretrieve(housing_url,tgz_path)\n",
      "    housing_tgz = tarfile.open(tgz_path)\n",
      "    housing_tgz.extractcall(path=housing_path)\n",
      "    housing_tgz.close()\n",
      " 4/5:\n",
      "import pandas as pd\n",
      "\n",
      "def load_housing_data(housing_path=HOUSING_PATH):csv_path = os.path.join(housing_path, \"housing.csv\")\n",
      "    return pdf.read_csv(csv_path)\n",
      " 4/6:\n",
      "import pandas as pd\n",
      "\n",
      "def load_housing_data(housing_path=HOUSING_PATH):csv_path = os.path.join(housing_path, \"housing.csv\")\n",
      "return pdf.read_csv(csv_path)\n",
      " 4/7:\n",
      "import pandas as pd\n",
      "\n",
      "def load_housing_data(housing_path=HOUSING_PATH):csv_path = os.path.join(housing_path, \"housing.csv\")\n",
      "    return pdf.read_csv(csv_path)\n",
      " 4/8:\n",
      "import pandas as pd\n",
      "\n",
      "def load_housing_data(housing_path=HOUSING_PATH):csv_path = os.path.join(housing_path, \"housing.csv\")\n",
      "   return pdf.read_csv(csv_path)\n",
      " 4/9:\n",
      "import pandas as pd\n",
      "\n",
      "def load_housing_data(housing_path=HOUSING_PATH):csv_path = os.path.join(housing_path, \"housing.csv\")\n",
      "   return pdf.read_csv(csv_path)\n",
      "4/10:\n",
      "import pandas as pd\n",
      "\n",
      "def load_housing_data(housing_path=HOUSING_PATH):csv_path = os.path.join(housing_path, \"housing.csv\") return pdf.read_csv(csv_path)\n",
      "4/11:\n",
      "import pandas as pd\n",
      "\n",
      "def load_housing_data(housing_path=HOUSING_PATH):csv_path = os.path.join(housing_path, \"housing.csv\")\n",
      "    return pdf.read_csv(csv_path)\n",
      "4/12:\n",
      "import pandas as pd\n",
      "\n",
      "def load_housing_data(housing_path=HOUSING_PATH):csv_path = os.path.join(housing_path, \"housing.csv\")\n",
      "    return pdf.read_csv(csv_path)\n",
      "4/13:\n",
      "import pandas as pd\n",
      "\n",
      "def load_housing_data(housing_path=HOUSING_PATH):csv_path = os.path.join(housing_path, \"housing.csv\")\n",
      "    return pd.read_csv(csv_path)\n",
      "4/14:\n",
      "import pandas as pd\n",
      "\n",
      "def load_housing_data(housing_path=HOUSING_PATH):csv_path = os.path.join(housing_path, \"housing.csv\")\n",
      "return pd.read_csv(csv_path)\n",
      "4/15:\n",
      "import pandas as pd\n",
      "\n",
      "def load_housing_data(housing_path=HOUSING_PATH):csv_path = os.path.join(housing_path, \"housing.csv\")\n",
      " return pd.read_csv(csv_path)\n",
      "4/16:\n",
      "import pandas as pd\n",
      "\n",
      "def load_housing_data(housing_path=HOUSING_PATH):csv_path = os.path.join(housing_path, \"housing.csv\")\n",
      "  return pd.read_csv(csv_path)\n",
      "4/17:\n",
      "import pandas as pd\n",
      "\n",
      "def load_housing_data(housing_path=HOUSING_PATH):csv_path = os.path.join(housing_path, \"housing.csv\")\n",
      "return pd.read_csv(csv_path)\n",
      "4/18:\n",
      "import pandas as pd\n",
      "\n",
      "def load_housing_data(housing_path=HOUSING_PATH):csv_path = os.path.join(housing_path, \"housing.csv\")\n",
      "    return pd.read_csv(csv_path)\n",
      "4/19:\n",
      "import pandas as pd\n",
      "\n",
      "def load_housing_data(housing_path=HOUSING_PATH):csv_path = os.path.join(housing_path, \n",
      "                                                                         \"housing.csv\")\n",
      "    return pd.read_csv(csv_path)\n",
      "4/20:\n",
      "import pandas as pd\n",
      "\n",
      "def load_housing_data(housing_path=HOUSING_PATH):csv_path = os.path.join(housing_path, \"housing.csv\")\n",
      "    return pd.read_csv(csv_path)\n",
      "4/21:\n",
      "import pandas as pd\n",
      "\n",
      "def load_housing_data(housing_path=HOUSING_PATH):\n",
      "    csv_path = os.path.join(housing_path,\n",
      "                            \"housing.csv\")\n",
      "    return pd.read_csv(csv_path)\n",
      "4/22: head()\n",
      "4/23:\n",
      "housing = load_housing_data()\n",
      "housing.head()\n",
      "4/24:\n",
      "import pandas as pd\n",
      "\n",
      "def load_housing_data(housing_path=HOUSING_PATH):\n",
      "    csv_path = os.path.join(housing_path,\n",
      "                            \"housing.csv\")\n",
      "    return pd.read_csv(csv_path)\n",
      "4/25: housing.head()\n",
      "4/26:\n",
      "housing = load_housing_data\n",
      "housing.head()\n",
      "4/27:\n",
      "housing = load_housing_data()\n",
      "housing.head()\n",
      "4/28:\n",
      "housing = load_housing_data()\n",
      "housing.head()\n",
      "4/29:\n",
      "import pandas as pd\n",
      "\n",
      "def load_housing_data(housing_path=HOUSING_PATH):\n",
      "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
      "    return pd.read_csv(csv_path)\n",
      "4/30:\n",
      "import os\n",
      "import tarfile\n",
      "import urllib.request\n",
      "\n",
      "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
      "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
      "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tagz\"\n",
      "\n",
      "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
      "    os.makedirs(housing_path, exist_ok = True)\n",
      "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
      "    urllib.request.urlretrieve(housing_url,tgz_path)\n",
      "    housing_tgz = tarfile.open(tgz_path)\n",
      "    housing_tgz.extractcall(path=housing_path)\n",
      "    housing_tgz.close()\n",
      "4/31:\n",
      "import pandas as pd\n",
      "\n",
      "def load_housing_data(housing_path=HOUSING_PATH):\n",
      "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
      "    return pd.read_csv(csv_path)\n",
      "4/32:\n",
      "housing = load_housing_data()\n",
      "housing.head()\n",
      "4/33:\n",
      "import os\n",
      "import tarfile\n",
      "import urllib.request\n",
      "\n",
      "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
      "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
      "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
      "\n",
      "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
      "    os.makedirs(housing_path, exist_ok = True)\n",
      "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
      "    urllib.request.urlretrieve(housing_url,tgz_path)\n",
      "    housing_tgz = tarfile.open(tgz_path)\n",
      "    housing_tgz.extractcall(path=housing_path)\n",
      "    housing_tgz.close()\n",
      "4/34:\n",
      "import pandas as pd\n",
      "\n",
      "def load_housing_data(housing_path=HOUSING_PATH):\n",
      "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
      "    return pd.read_csv(csv_path)\n",
      "4/35:\n",
      "housing = load_housing_data()\n",
      "housing.head()\n",
      "4/36: fetch_housing_data()\n",
      "4/37: fetch_housing_data()\n",
      "4/38:\n",
      "import pandas as pd\n",
      "\n",
      "def load_housing_data(housing_path=HOUSING_PATH):\n",
      "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
      "    return pd.read_csv(csv_path)\n",
      "4/39:\n",
      "import os\n",
      "import tarfile\n",
      "import urllib\n",
      "\n",
      "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
      "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
      "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
      "\n",
      "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
      "    os.makedirs(housing_path)\n",
      "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
      "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
      "    housing_tgz = tarfile.open(tgz_path)\n",
      "    housing_tgz.extractall(path=housing_path)\n",
      "    housing_tgz.close()\n",
      "4/40: fetch_housing_data()\n",
      "4/41:\n",
      "import pandas as pd\n",
      "\n",
      "def load_housing_data(housing_path=HOUSING_PATH):\n",
      "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
      "    return pd.read_csv(csv_path)\n",
      "4/42:\n",
      "housing = load_housing_data()\n",
      "housing.head()\n",
      "4/43:\n",
      "import os\n",
      "import tarfile\n",
      "import urllib\n",
      "\n",
      "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
      "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
      "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
      "\n",
      "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
      "    os.makedirs(housing_path, exist_ok=True)\n",
      "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
      "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
      "    housing_tgz = tarfile.open(tgz_path)\n",
      "    housing_tgz.extractall(path=housing_path)\n",
      "    housing_tgz.close()\n",
      "4/44: fetch_housing_data()\n",
      "4/45:\n",
      "import os\n",
      "import tarfile\n",
      "import urllib.request\n",
      "\n",
      "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
      "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
      "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
      "\n",
      "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
      "    os.makedirs(housing_path, exist_ok=True)\n",
      "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
      "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
      "    housing_tgz = tarfile.open(tgz_path)\n",
      "    housing_tgz.extractall(path=housing_path)\n",
      "    housing_tgz.close()\n",
      "4/46: fetch_housing_data()\n",
      "4/47:\n",
      "import pandas as pd\n",
      "\n",
      "def load_housing_data(housing_path=HOUSING_PATH):\n",
      "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
      "    return pd.read_csv(csv_path)\n",
      "4/48:\n",
      "import os\n",
      "import tarfile\n",
      "import urllib.request\n",
      "\n",
      "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
      "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
      "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
      "\n",
      "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
      "    if not os.path.isdir(housing_path):\n",
      "        os.makedirs(housing_path)\n",
      "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
      "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
      "    housing_tgz = tarfile.open(tgz_path)\n",
      "    housing_tgz.extractall(path=housing_path)\n",
      "    housing_tgz.close()\n",
      "4/49: fetch_housing_data()\n",
      "4/50:\n",
      "\n",
      "\n",
      "fetch_housing_data()\n",
      "4/51: fetch_housing_data()\n",
      "4/52: print(\"hello world!\")\n",
      "4/53:\n",
      "import pandas as pd\n",
      "\n",
      "def load_housing_data(housing_path=HOUSING_PATH):\n",
      "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
      "    return pd.read_csv(csv_path)\n",
      "4/54:\n",
      "housing = load_housing_data()\n",
      "housing.head()\n",
      "4/55:\n",
      "housing = load_housing_data()\n",
      "housing.head()\n",
      "4/56:\n",
      "housing = load_housing_data()\n",
      "housing.head()\n",
      "4/57: housing.info\n",
      "4/58: housing.info()\n",
      "4/59: housing[\"ocean_proximity\"].value_counts()\n",
      "4/60: housing.describe()\n",
      "4/61: %matplotlib inline\n",
      "4/62:\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "housing.hist(bins=50, figsize=(20,15))\n",
      "plt.show()\n",
      " 5/1:\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "housing.hist(bins=100, figsize=(20,15))\n",
      "plt.show()\n",
      " 5/2:\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "housing.hist(bins=50, figsize=(20,15))\n",
      "plt.show()\n",
      " 5/3: print(\"hello world!\")\n",
      " 5/4:\n",
      "import os\n",
      "import tarfile\n",
      "import urllib.request\n",
      "\n",
      "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
      "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
      "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
      "\n",
      "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
      "    if not os.path.isdir(housing_path):\n",
      "        os.makedirs(housing_path)\n",
      "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
      "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
      "    housing_tgz = tarfile.open(tgz_path)\n",
      "    housing_tgz.extractall(path=housing_path)\n",
      "    housing_tgz.close()\n",
      " 5/5: fetch_housing_data()\n",
      " 5/6:\n",
      "import pandas as pd\n",
      "\n",
      "def load_housing_data(housing_path=HOUSING_PATH):\n",
      "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
      "    return pd.read_csv(csv_path)\n",
      " 5/7:\n",
      "housing = load_housing_data()\n",
      "housing.head()\n",
      " 5/8: housing.info()\n",
      " 5/9: housing[\"ocean_proximity\"].value_counts()\n",
      "5/10: housing.describe()\n",
      "5/11:\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "housing.hist(bins=50, figsize=(20,15))\n",
      "plt.show()\n",
      "5/12:\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "housing.hist(bins=100, figsize=(20,15))\n",
      "plt.show()\n",
      "5/13:\n",
      "import numpy as np\n",
      "def split_train_test(data, test_ratio):\n",
      "    shuffled_indices = np.random.permutation(len(data))\n",
      "    test_set_size = int(len(data) * test_ratio)\n",
      "    test_indices = shuffled_indices[:test_set_size]\n",
      "    train_indices = shuffled_indices[test_set_size:]\n",
      "    return data.iloc[train_indices], data.iloc[test_indices]\n",
      "5/14:\n",
      "train_set, test_set = split_train_test(housing, 0.2)\n",
      "len(train_set)\n",
      "5/15: len(test_set)\n",
      "5/16:\n",
      "from zlib import crc32\n",
      "\n",
      "def test_set_check(identifier, test_ratio):\n",
      "    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32\n",
      "\n",
      "def split_train_test_by_id(data, test_ratio, id_column):\n",
      "    ids = data[id_column]\n",
      "    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\n",
      "    return data.loc[~in_test_set], data.loc[in_test_set]\n",
      "5/17:\n",
      "import hashlib\n",
      "\n",
      "def test_set_check(identifier, test_ratio, hash=hashlib.md5):\n",
      "    return hash(np.int64(identifier)).digest()[-1] < 256 * test_ratio\n",
      "5/18:\n",
      "def test_set_check(identifier, test_ratio, hash=hashlib.md5):\n",
      "    return bytearray(hash(np.int64(identifier)).digest())[-1] < 256 * test_ratio\n",
      "5/19:\n",
      "housing_with_id = housing.reset_index()   # adds an `index` column\n",
      "train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"index\")\n",
      "5/20:\n",
      "housing_with_id[\"id\"] = housing[\"longitude\"] * 1000 + housing[\"latitude\"]\n",
      "train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"id\")\n",
      "5/21: test_set.head()\n",
      "5/22: print(housing_with_id[\"id\"])\n",
      "5/23:\n",
      "from sklearn.model_selection import train_test_split\n",
      "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
      "5/24:\n",
      "from sklearn.model_selection import train_test_split\n",
      "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
      "5/25: test_set.head()\n",
      "5/26: housing[\"median_income\"].hist()\n",
      "5/27: housing[\"total_bedrooms\"].hist()\n",
      "5/28: from rdkit import Chem\n",
      "5/29: pip install squarify\n",
      "5/30: import squarify\n",
      "5/31:\n",
      "size = [40, 30, 5, 25]\n",
      "squarify.plot(sizes)\n",
      "plt.show()\n",
      "5/32:\n",
      "sizes = [40, 30, 5, 25]\n",
      "squarify.plot(sizes)\n",
      "plt.show()\n",
      "5/33: cellculture = pd.read_csv('cellculture_isi.csv', encoding = \"utf-8\")\n",
      "5/34: cellculture = pd.read_csv('cellculture_isi', encoding = \"utf-8\")\n",
      "5/35: cellculture = pd.read_csv('/Users/tarapongsrisongkram/OneDrive - Khon Kaen University/Biotech/cellculture_isi.csv')\n",
      "5/36: print(cellculture)\n",
      "5/37: sizes = cellculture.records\n",
      "5/38:\n",
      "sizes = cellculture.records\n",
      "label = cellculture.Web of Science Categories\n",
      "5/39:\n",
      "sizes = cellculture.records\n",
      "label = cellculture.Web-of-Science-Categories\n",
      "5/40:\n",
      "sizes = cellculture.records\n",
      "label = cellculture.WebofScienceCategories\n",
      "5/41:\n",
      "sizes = cellculture.records\n",
      "label = cellculture.Web_of_Science_Categories\n",
      "5/42: cellculture = pd.read_csv('/Users/tarapongsrisongkram/OneDrive - Khon Kaen University/Biotech/cellculture_isi.csv')\n",
      "5/43: print(cellculture)\n",
      "5/44: print(cellculture)\n",
      "5/45: cellculture = pd.read_csv('/Users/tarapongsrisongkram/OneDrive - Khon Kaen University/Biotech/cellculture_isi.csv')\n",
      "5/46: print(cellculture)\n",
      "5/47:\n",
      "sizes = cellculture.records\n",
      "label = cellculture.Categories\n",
      "5/48:\n",
      "squarify.plot(sizes=sizes, labe=label)\n",
      "plt.show()\n",
      "5/49:\n",
      "squarify.plot(sizes=sizes, labe;=label)\n",
      "plt.show()\n",
      "5/50:\n",
      "squarify.plot(sizes=sizes, label=label)\n",
      "plt.show()\n",
      "5/51: print(sizes)\n",
      "5/52:\n",
      "data = int(sizes)\n",
      "print(data)\n",
      "5/53:\n",
      "data = int(sizes)\n",
      "print(sizes)\n",
      "5/54:\n",
      "squarify.plot(sizes=sizes, label=label)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "5/55:\n",
      "squarify.plot(sizes=sizes, label=label, alpha=.8))\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "5/56:\n",
      "squarify.plot(sizes=sizes, label=label, alpha=.8)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "5/57:\n",
      "squarify.plot(sizes=sizes)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "5/58:\n",
      "squarify.plot(sizes=sizes)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "5/59:\n",
      "squarify.plot(sizes=sizes)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "5/60:\n",
      "squarify.plot(sizes=sizes)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "5/61: sizes\n",
      "5/62:\n",
      "sizes = cellculture.records(0-24)\n",
      "label = cellculture.Categories\n",
      "5/63:\n",
      "sizes = cellculture.records\n",
      "label = cellculture.Categories\n",
      "5/64: cellculture = pd.read_csv('/Users/tarapongsrisongkram/OneDrive - Khon Kaen University/Biotech/cellculture_isi.csv')\n",
      "5/65: print(cellculture)\n",
      "5/66:\n",
      "sizes = cellculture.records\n",
      "label = cellculture.Categories\n",
      "5/67: sizes\n",
      "5/68:\n",
      "squarify.plot(sizes=sizes)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "5/69:\n",
      "squarify.plot(sizes=sizes label=label)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "5/70:\n",
      "squarify.plot(sizes=sizes, label=label)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "5/71:\n",
      "squarify.plot(sizes=sizes, label=label, alpha=0.6)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "5/72:\n",
      "squarify.plot(sizes=sizes, label=label, alpha=0.6)\n",
      "plt.axis('off')\n",
      "SMALL_SIZE = 8\n",
      "matplotlib.rc('font', size=SMALL_SIZE)\n",
      "matplotlib.rc('axes', titlesize=SMALL_SIZE)\n",
      "plt.show()\n",
      "5/73:\n",
      "squarify.plot(sizes=sizes, label=label, alpha=0.6)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "5/74:\n",
      "SMALL_SIZE = 8\n",
      "matplotlib.rc('font', size=SMALL_SIZE)\n",
      "matplotlib.rc('axes', titlesize=SMALL_SIZE)\n",
      "5/75:\n",
      "sizes = cellculture.records.iloc[0:9]\n",
      "label = cellculture.Categories\n",
      "5/76: sizes\n",
      "5/77:\n",
      "squarify.plot(sizes=sizes, label=label, alpha=0.6)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "5/78:\n",
      "import matplotlib.pyplot as plt\n",
      "SMALL_SIZE = 8\n",
      "matplotlib.rc('font', size=SMALL_SIZE)\n",
      "matplotlib.rc('axes', titlesize=SMALL_SIZE)\n",
      "5/79:\n",
      "import matplotlib as plt\n",
      "SMALL_SIZE = 8\n",
      "matplotlib.rc('font', size=SMALL_SIZE)\n",
      "matplotlib.rc('axes', titlesize=SMALL_SIZE)\n",
      "5/80:\n",
      "import matplotlib as plt\n",
      "SMALL_SIZE = 8\n",
      "matplotlib.rc('font', size=SMALL_SIZE)\n",
      "matplotlib.rc('axes', titlesize=SMALL_SIZE)\n",
      "5/81:\n",
      "squarify.plot(sizes=sizes, label=label, alpha=0.6, text_kwargs={'fontsize':8})\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "5/82:\n",
      "squarify.plot(sizes=sizes, label=label, alpha=0.6)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "5/83:\n",
      "squarify.plot(sizes=sizes, label=label, alpha=0.6)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "5/84:\n",
      "sizes = cellculture.records.iloc[0:9]\n",
      "label = cellculture.Categories\n",
      "5/85: sizes\n",
      "5/86:\n",
      "squarify.plot(sizes=sizes, label=label, alpha=0.6)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "5/87:\n",
      "sizes = cellculture.records\n",
      "label = cellculture.Categories\n",
      "5/88:\n",
      "squarify.plot(sizes=sizes, label=label, alpha=0.6)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "5/89:\n",
      "squarify.plot(sizes=sizes, label=label, alpha=0.6 )\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "5/90:\n",
      "squarify.plot(sizes=sizes, label=label, alpha=0.6 )\n",
      "\n",
      "plt.show()\n",
      "5/91: squarify.plot(sizes=sizes, label=label, alpha=0.6 )\n",
      "5/92:\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "housing.hist(bins=100, figsize=(20,15))\n",
      "plt.show()\n",
      "5/93:\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "housing.hist(bins=100, figsize=(20,15))\n",
      "plt.show()\n",
      "5/94:\n",
      "squarify.plot(sizes=sizes, label=label, alpha=0.6 )\n",
      "plt.show()\n",
      "5/95:\n",
      "squarify.plot(sizes=sizes, label=label, alpha=0.6)\n",
      "plt.show()\n",
      "5/96: sizes\n",
      "5/97:\n",
      "sizes = cellculture.records\n",
      "label = cellculture.Categories\n",
      "5/98: sizes\n",
      "5/99:\n",
      "squarify.plot(sizes=sizes, label=label, alpha=0.6)\n",
      "plt.show()\n",
      "5/100:\n",
      "squarify.plot(sizes=sizes, label=label, alpha=0.6)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "5/101:\n",
      "sizes = cellculture.records.loc[0:9]\n",
      "label = cellculture.Categories\n",
      "5/102: sizes\n",
      "5/103:\n",
      "squarify.plot(sizes=sizes, label=label, alpha=0.6)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "5/104:\n",
      "squarify.plot(sizes=sizes, alpha=0.6)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "5/105:\n",
      "squarify.plot(sizes=sizes, alpha=0.8)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "5/106:\n",
      "squarify.plot(sizes=sizes, alpha=1)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "5/107:\n",
      "squarify.plot(sizes=sizes, alpha=1)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "5/108:\n",
      "squarify.plot(sizes=sizes, alpha=1)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "5/109:\n",
      "squarify.plot(sizes=sizes, alpha=1)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "5/110:\n",
      "squarify.plot(sizes=sizes, alpha=1)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "5/111:\n",
      "squarify.plot(sizes=sizes, alpha=1)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "5/112:\n",
      "squarify.plot(sizes=sizes, alpha=1)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "5/113:\n",
      "squarify.plot(sizes=sizes, alpha=1)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "5/114:\n",
      "squarify.plot(sizes=sizes, alpha=1)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "5/115:\n",
      "squarify.plot(sizes=sizes, alpha=0.8)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "5/116:\n",
      "squarify.plot(sizes=sizes, alpha=0.8)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "5/117:\n",
      "squarify.plot(sizes=sizes, alpha=0.8)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "5/118:\n",
      "squarify.plot(sizes=sizes, alpha=1)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "5/119:\n",
      "squarify.plot(sizes=sizes, alpha=1)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "5/120:\n",
      "squarify.plot(sizes=sizes, alpha=1)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "5/121:\n",
      "squarify.plot(sizes=sizes, alpha=1)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "5/122:\n",
      "squarify.plot(sizes=sizes, alpha=1)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "f.savefig(\"three.png\", bbox_inches='tight', dpi=600)\n",
      "5/123:\n",
      "squarify.plot(sizes=sizes, alpha=1)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "5/124:\n",
      "squarify.plot(sizes=sizes, alpha=1)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "5/125:\n",
      "f = plt.figure()\n",
      "f.savefig(\"three.png\", bbox_inches='tight', dpi=600)\n",
      "5/126:\n",
      "f = plt.figure()\n",
      "squarify.plot(sizes=sizes, alpha=1)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "f.savefig(\"three.png\", bbox_inches='tight', dpi=600)\n",
      "5/127:\n",
      "f = plt.figure()\n",
      "squarify.plot(sizes=sizes, alpha=1)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "f.savefig(\"three.png\", bbox_inches='tight', dpi=600)\n",
      "5/128:\n",
      "f = plt.figure()\n",
      "squarify.plot(sizes=sizes, alpha=0.8)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "f.savefig(\"three0.8.png\", bbox_inches='tight', dpi=600)\n",
      "5/129:\n",
      "f = plt.figure()\n",
      "squarify.plot(sizes=sizes, alpha=0.8)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "f.savefig(\"three0.8.png\", bbox_inches='tight', dpi=600)\n",
      "5/130:\n",
      "f = plt.figure()\n",
      "squarify.plot(sizes=sizes, alpha=0.8)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "f.savefig(\"three0.8.png\", bbox_inches='tight', dpi=600)\n",
      "5/131:\n",
      "f = plt.figure()\n",
      "squarify.plot(sizes=sizes, alpha=0.8)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "f.savefig(\"three0.8.png\", bbox_inches='tight', dpi=600)\n",
      "5/132:\n",
      "f = plt.figure()\n",
      "squarify.plot(sizes=sizes, alpha=0.8)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "f.savefig(\"three0.8.png\", bbox_inches='tight', dpi=600)\n",
      "5/133:\n",
      "f = plt.figure()\n",
      "squarify.plot(sizes=sizes, alpha=0.8)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "f.savefig(\"three0.8.png\", bbox_inches='tight', dpi=600)\n",
      "5/134:\n",
      "f = plt.figure()\n",
      "squarify.plot(sizes=sizes, alpha=0.8)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "f.savefig(\"three0.8.png\", bbox_inches='tight', dpi=600)\n",
      "5/135:\n",
      "f = plt.figure()\n",
      "squarify.plot(sizes=sizes, alpha=0.8)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "f.savefig(\"three0.8.png\", bbox_inches='tight', dpi=600)\n",
      "5/136:\n",
      "f = plt.figure()\n",
      "squarify.plot(sizes=sizes, alpha=0.8)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "f.savefig(\"three0.8.png\", bbox_inches='tight', dpi=600)\n",
      "5/137:\n",
      "f = plt.figure()\n",
      "squarify.plot(sizes=sizes, alpha=0.8)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "f.savefig(\"three0.8.png\", bbox_inches='tight', dpi=600)\n",
      "5/138:\n",
      "f = plt.figure()\n",
      "squarify.plot(sizes=sizes, alpha=0.8)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "f.savefig(\"three0.8.png\", bbox_inches='tight', dpi=600)\n",
      "5/139:\n",
      "f = plt.figure()\n",
      "squarify.plot(sizes=sizes, alpha=0.8)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "f.savefig(\"three0.8.png\", bbox_inches='tight', dpi=600)\n",
      "5/140:\n",
      "f = plt.figure()\n",
      "squarify.plot(sizes=sizes, alpha=0.8)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "f.savefig(\"three0.8.png\", bbox_inches='tight', dpi=600)\n",
      "5/141:\n",
      "f = plt.figure()\n",
      "squarify.plot(sizes=sizes, alpha=0.8)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "f.savefig(\"three0.8.png\", bbox_inches='tight', dpi=600)\n",
      "5/142:\n",
      "f = plt.figure()\n",
      "squarify.plot(sizes=sizes, alpha=0.8)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "f.savefig(\"three0.8.png\", bbox_inches='tight', dpi=600)\n",
      "5/143:\n",
      "f = plt.figure()\n",
      "squarify.plot(sizes=sizes, alpha=0.8)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "f.savefig(\"three0.8.png\", bbox_inches='tight', dpi=600)\n",
      "5/144:\n",
      "f = plt.figure()\n",
      "squarify.plot(sizes=sizes, alpha=0.8)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "f.savefig(\"three0.8.png\", bbox_inches='tight', dpi=600)\n",
      "5/145:\n",
      "f = plt.figure()\n",
      "squarify.plot(sizes=sizes, alpha=0.8)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "f.savefig(\"three0.8.png\", bbox_inches='tight', dpi=600)\n",
      "5/146:\n",
      "f = plt.figure()\n",
      "squarify.plot(sizes=sizes, alpha=0.8)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "f.savefig(\"three0.8.png\", bbox_inches='tight', dpi=600)\n",
      "5/147:\n",
      "f = plt.figure()\n",
      "squarify.plot(sizes=sizes, alpha=0.8)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "f.savefig(\"three0.8.png\", bbox_inches='tight', dpi=600)\n",
      "5/148:\n",
      "f = plt.figure()\n",
      "squarify.plot(sizes=sizes, alpha=0.8)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "f.savefig(\"three0.8.png\", bbox_inches='tight', dpi=600)\n",
      "5/149:\n",
      "f = plt.figure()\n",
      "squarify.plot(sizes=sizes, alpha=0.8)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "f.savefig(\"three0.8.png\", bbox_inches='tight', dpi=600)\n",
      "5/150:\n",
      "f = plt.figure()\n",
      "squarify.plot(sizes=sizes, alpha=0.8)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "f.savefig(\"three0.8.png\", bbox_inches='tight', dpi=600)\n",
      "5/151:\n",
      "f = plt.figure()\n",
      "squarify.plot(sizes=sizes, alpha=0.8)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "f.savefig(\"three0.8.png\", bbox_inches='tight', dpi=600)\n",
      "5/152:\n",
      "f = plt.figure()\n",
      "squarify.plot(sizes=sizes, alpha=0.8)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "f.savefig(\"three0.8.png\", bbox_inches='tight', dpi=600)\n",
      "5/153:\n",
      "f = plt.figure()\n",
      "squarify.plot(sizes=sizes, alpha=0.8)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "f.savefig(\"three0.8.png\", bbox_inches='tight', dpi=600)\n",
      "5/154:\n",
      "f = plt.figure()\n",
      "squarify.plot(sizes=sizes, alpha=0.8)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "f.savefig(\"three0.8.png\", bbox_inches='tight', dpi=600)\n",
      "5/155:\n",
      "f = plt.figure()\n",
      "squarify.plot(sizes=sizes, alpha=0.8)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "f.savefig(\"three0.8.png\", bbox_inches='tight', dpi=600)\n",
      "5/156:\n",
      "f = plt.figure()\n",
      "squarify.plot(sizes=sizes, alpha=0.8)\n",
      "plt.axis('off')\n",
      "plt.show()\n",
      "f.savefig(\"three0.8.png\", bbox_inches='tight', dpi=600)\n",
      " 8/1: result = c(99, 101, 100, 98,  95, 90)\n",
      " 8/2: result = (99, 101, 100, 98,  95, 90)\n",
      " 8/3:\n",
      "result = (99, 101, 100, 98,  95, 90)\n",
      "time = (0, 3, 6, 12, 24, 48, 96)\n",
      " 8/4:\n",
      "result = (99, 101, 100, 98,  95, 90)\n",
      "time = (0, 3, 6, 12, 24, 48, 96)\n",
      "print(result)\n",
      " 8/5:\n",
      "result = (99, 101, 100, 98,  95, 90)\n",
      "time = (0, 3, 6, 12, 24, 48, 96)\n",
      "test = data.fra,e(time,result)\n",
      " 8/6:\n",
      "result = (99, 101, 100, 98,  95, 90)\n",
      "time = (0, 3, 6, 12, 24, 48, 96)\n",
      "test = data.frame(time,result)\n",
      " 8/7:\n",
      "result = (99, 101, 100, 98,  95, 90)\n",
      "time = (0, 3, 6, 12, 24, 48, 96)\n",
      "test = as.dataframe(time,result)\n",
      " 8/8:\n",
      "result = (99, 101, 100, 98,  95, 90)\n",
      "time = (0, 3, 6, 12, 24, 48, 96)\n",
      "test = dataframe(time,result)\n",
      " 8/9:\n",
      "result = (99, 101, 100, 98,  95, 90)\n",
      "time = (0, 3, 6, 12, 24, 48, 96)\n",
      "test = matrix(time,result)\n",
      "8/10:\n",
      "result = (99, 101, 100, 98,  95, 90)\n",
      "time = (0, 3, 6, 12, 24, 48, 96)\n",
      "plot(result, time)\n",
      "8/11:\n",
      "result = (99, 101, 100, 98,  95, 90)\n",
      "time = (0, 3, 6, 12, 24, 48, 96)\n",
      "plt.plot(result, time)\n",
      "8/12:\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "housing.hist(bins=100, figsize=(20,15))\n",
      "plt.show()\n",
      "8/13: test_set.head()\n",
      "8/14:\n",
      "from sklearn.model_selection import train_test_split\n",
      "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
      "8/15:\n",
      "result = (99, 101, 100, 98,  95, 90)\n",
      "time = (0, 3, 6, 12, 24, 48, 96)\n",
      "plt.plot(result, time)\n",
      "8/16:\n",
      "result = (99, 101, 100, 98,  95, 90)\n",
      "time = (0, 3, 6, 12, 24, 48)\n",
      "plt.plot(result, time)\n",
      "8/17:\n",
      "result = (99, 101, 100, 98,  95, 90)\n",
      "time = (0, 3, 6, 12, 24, 48)\n",
      "plt.plot(result, time)\n",
      "8/18:\n",
      "result = (99, 101, 100, 98,  95, 90)\n",
      "time = (0, 3, 6, 12, 24, 48)\n",
      "plt.plot(time, result)\n",
      "10/1: ### THis is a new notebook for LAT1 analysis ###\n",
      "10/2:\n",
      "# Connect to database \n",
      "pip install chembl_webresource_client\n",
      "10/3:\n",
      "# Connect to database \n",
      "pip install chembl_webresource_client\n",
      "10/4:\n",
      "# Connect to Chembl database \n",
      "pip install chembl_webresource_client\n",
      "10/5:\n",
      "# Connect to Chembl database \n",
      "pipinstall chembl_webresource_client\n",
      "10/6:\n",
      "# Connect to Chembl database \n",
      "pipinstall chembl_webresource_client\n",
      "13/1:\n",
      "# Connect to Chembl database \n",
      "pip install chembl_webresource_client\n",
      "14/1:\n",
      "# Start searching Chembl database\n",
      "from chembl_webresource_clint.new_client import new_client\n",
      "target = new_client.target\n",
      "target_query = target.search('LAT1')\n",
      "14/2: pip install chembl_webresource_clint\n",
      "14/3: pip install chembl_webresource_clint\n",
      "14/4:\n",
      "# Start searching Chembl database\n",
      "from chembl_webresource_clint.new_client import new_client\n",
      "target = new_client.target\n",
      "target_query = target.search('LAT1')\n",
      "14/5: pip install chembl_webresource_client\n",
      "14/6:\n",
      "# Start searching Chembl database\n",
      "from chembl_webresource_clint.new_client import new_client\n",
      "target = new_client.target\n",
      "target_query = target.search('LAT1')\n",
      "14/7:\n",
      "# Start searching Chembl database\n",
      "from chembl_webresource_clint.new_client import new_client\n",
      "target = new_client.target\n",
      "target_query = target.search('LAT1')\n",
      "14/8:\n",
      "# Start searching Chembl database\n",
      "from chembl_webresource_client.new_client import new_client\n",
      "target = new_client.target\n",
      "target_query = target.search('LAT1')\n",
      "14/9:\n",
      "# Start searching Chembl database\n",
      "from chembl_webresource_client.new_client import new_client\n",
      "target = new_client.target\n",
      "target_query = target.search('LAT1')\n",
      "14/10:\n",
      "# Start searching Chembl database\n",
      "import pandas as pd\n",
      "from chembl_webresource_client.new_client import new_client\n",
      "target = new_client.target\n",
      "target_query = target.search('LAT1')\n",
      "16/1:\n",
      "# Start searching Chembl database\n",
      "import pandas as pd\n",
      "from chembl_webresource_client.new_client import new_client\n",
      "molecule = new_client.molecule\n",
      "res = molecule.search('viagra')\n",
      "16/2: pip install chembl_webresource_client\n",
      "16/3:\n",
      "# Start searching Chembl database\n",
      "import pandas as pd\n",
      "from chembl_webresource_client.new_client import new_client\n",
      "molecule = new_client.molecule\n",
      "res = molecule.search('viagra')\n",
      "16/4:\n",
      "# Start searching Chembl database\n",
      "import pandas as pd\n",
      "from chembl_webresource_client.new_client import new_client\n",
      "16/5: pip -update\n",
      "16/6: pip install --upgrade pip\n",
      "17/1: ### THis is a new notebook for LAT1 analysis ###\n",
      "17/2: pip install chembl_webresource_client\n",
      "17/3:\n",
      "# Start searching Chembl database\n",
      "import pandas as pd\n",
      "from chembl_webresource_client.new_client import new_client\n",
      "18/1: pip install chembl_webresource_client\n",
      "18/2:\n",
      "# Start searching Chembl database\n",
      "import pandas as pd\n",
      "from chembl_webresource_client.new_client import new_client\n",
      "18/3: pip install --upgrade pip\n",
      "18/4:\n",
      "#import library\n",
      "import pandas as pd\n",
      "18/5: from chembl_webresource_client.new_client import new_cleint\n",
      "18/6: from chembl_webresource_client.new_client import new_client\n",
      "18/7: from chembl_webresource_client import new_client\n",
      "18/8: pip install Flask-Caching\n",
      "18/9: from chembl_webresource_client import new_client\n",
      "18/10: from chembl_webresource_client import new_client\n",
      "18/11: from chembl_webresource_client import new_client\n",
      "18/12: from chembl_webresource_client import new_client\n",
      "18/13: x\n",
      "18/14: 5+3\n",
      "18/15: x = input(what is your name?)\n",
      "18/16: x = input('what is your name?')\n",
      "18/17: x\n",
      "18/18: print(x)\n",
      "18/19: import tensorflow as tf\n",
      "18/20: import matplot as mp\n",
      "18/21:\n",
      "usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\n",
      "export PATH=\"/usr/local/opt/python/libexec/bin:$PATH\"\n",
      "# if you are on macOS 10.12 (Sierra) use `export PATH=\"/usr/local/bin:/usr/local/sbin:$PATH\"`\n",
      "brew update\n",
      "brew install python  # Python 3\n",
      "18/22:\n",
      "usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\n",
      "export PATH=\"/usr/local/opt/python/libexec/bin:$PATH\"\n",
      "# if you are on macOS 10.12 (Sierra) use `export PATH=\"/usr/local/bin:/usr/local/sbin:$PATH\"`\n",
      "brew update\n",
      "brew install python  # Python 3\n",
      "18/23: pip install chembl_webresource_new.client\n",
      "18/24: pip install chembl_webresource\n",
      "18/25: pip -version\n",
      "18/26: pip -update\n",
      "18/27: pip update\n",
      "18/28: pip --version\n",
      "19/1: ### THis is a new notebook for LAT1 analysis ###\n",
      "19/2:\n",
      "#import library\n",
      "import pandas as pd\n",
      "19/3: 5+3\n",
      "19/4: x = input('what is your name?')\n",
      "19/5: x\n",
      "19/6: print(x)\n",
      "19/7: pip --version\n",
      "19/8: pip install chembl_webresource_client\n",
      "19/9:\n",
      "from chembl_webresource_client.new_client import new_client\n",
      "target = new_client.target\n",
      "gene_name = 'BRD4'\n",
      "res = target.search(gene_name)\n",
      "19/10:\n",
      "from chembl_webresource_client.new_client import new_client\n",
      "target = new_client.target\n",
      "gene_name = 'BRD4'\n",
      "res = target.search(gene_name)\n",
      "19/11:\n",
      "import logging\n",
      "from operator import itemgetter\n",
      "from IPython.display import display, SVG\n",
      "19/12:\n",
      "# Python modules used for API access...\n",
      "from chembl_webresource_client.new_client import new_client\n",
      "19/13:\n",
      "# Python modules used for API access...\n",
      "from chembl_webresource_client import *\n",
      "from chembl_webresource_client.new_client import new_client\n",
      "19/14:\n",
      "# Python modules used for API access...\n",
      "from chembl_webresource_client.new_client import new_client\n",
      "21/1: pip install request_cache-=0.7.0\n",
      "21/2: pip install request_cache==0.7.0\n",
      "21/3: pip install request_cache==0.6.4\n",
      "21/4: pip install requests_cache==0.6.4\n",
      "21/5: pip install chembl_webresource_client\n",
      "21/6:\n",
      "# Python modules used for API access...\n",
      "from chembl_webresource_client.new_client import new_client\n",
      "21/7:\n",
      "#import library\n",
      "import pandas as pd\n",
      "21/8:\n",
      "# Python modules used for API access...\n",
      "from chembl_webresource_client.new_client import new_client\n",
      "target = new_client.target\n",
      "target_query = target.search('LAT1')\n",
      "targets = pd.DataFrame.from_dict(target_query)\n",
      "targets\n",
      "21/9:\n",
      "#Input yourname for this lab notebook\n",
      "labname = input('what is your name?')\n",
      "21/10: print(labname)\n",
      "21/11:\n",
      "#As we can see, we will select H.sapiens LAT1 as atarget\n",
      "selected_target = targets.target_chembl_id[2]\n",
      "selected_target\n",
      "21/12:\n",
      "#Here we will access the activity of compounds in this target, first we will try IC50\n",
      "activity = new_client.activity\n",
      "res = activity.filter(target_chembl_id=selected_target).filter(standard_type=\"IC50\")\n",
      "21/13:\n",
      "#Here we will access the activity of compounds in this target, first we will try IC50\n",
      "activity = new_client.activity\n",
      "result = activity.filter(target_chembl_id=selected_target).filter(standard_type=\"IC50\")\n",
      "21/14: df = pd.DataFrame.from_dict(result)\n",
      "21/15: df\n",
      "21/16:\n",
      "#Here we will access the activity of compounds in this target, first we will try IC50\n",
      "activity = new_client.activity\n",
      "result = activity.filter(target_chembl_id=selected_target).filter(standard_type=\"IC50\")\n",
      "21/17: df = pd.DataFrame.from_dict(result)\n",
      "21/18: df\n",
      "21/19: result\n",
      "21/20:\n",
      "#Here we will access the activity of compounds in this target, first we will try IC50\n",
      "activity = new_client.activity\n",
      "res = activity.filter(target_chembl_id=selected_target).filter(standard_type=\"IC50\")\n",
      "21/21: res\n",
      "21/22:\n",
      "#As we can see, we will select H.sapiens LAT1 as atarget\n",
      "selected_target = targets.target_chembl_id[1]\n",
      "selected_target\n",
      "21/23:\n",
      "#Here we will access the activity of compounds in this target, first we will try IC50\n",
      "activity = new_client.activity\n",
      "res = activity.filter(target_chembl_id=selected_target).filter(standard_type=\"IC50\")\n",
      "21/24: res\n",
      "21/25: df = pd.DataFrame.from_dict(res)\n",
      "21/26:\n",
      "df = pd.DataFrame.from_dict(res)\n",
      "df\n",
      "21/27:\n",
      "#if any compound is missing the IC50 value, then it should drop out\n",
      "df2 = df[df.standard_value.notna()]\n",
      "21/28:\n",
      "#if any compound is missing the IC50 value, then it should drop out\n",
      "df2 = df[df.standard_value.notna()]\n",
      "df2\n",
      "21/29: print(df2.columns)\n",
      "21/30:\n",
      "#we want to see the molecule structure via canonical_smiles\n",
      "df2.canonical_smiles\n",
      "21/31:\n",
      "#We want to know the structure and activity\n",
      "selection = ['molecule_chembl_id','canonical_smiles','standard_units', 'standard_value']\n",
      "21/32:\n",
      "#We want to know the structure and activity\n",
      "selection = ['molecule_chembl_id','canonical_smiles','standard_units', 'standard_value']\n",
      "df3 = df2[selection]\n",
      "21/33:\n",
      "#We want to know the structure and activity\n",
      "selection = ['molecule_chembl_id','canonical_smiles','standard_units', 'standard_value']\n",
      "df3 = df2[selection]\n",
      "df3\n",
      "21/34:\n",
      "#We want to know how many of them is high potency agaisnt LAT1 protein\n",
      "#First we should rearrange from IC50 into pIC50 for a better uniformnity distribution\n",
      "#We will convert IC50 to the negative logarithmic scale which is essentially -log10(IC50) or pIC50()\n",
      "# First Take the IC50 values from the standard_value column and converts it from nM to M by multiplying the value by 109\n",
      "# Take the molar value and apply -log10\n",
      "# Delete the standard_value column and create a new pIC50 column\n",
      "\n",
      "#before we do that ,we need to install nescessery package\n",
      "\n",
      "import numpy as np\n",
      "21/35:\n",
      "#We want to know how many of them is high potency agaisnt LAT1 protein\n",
      "#First we should rearrange from IC50 into pIC50 for a better uniformnity distribution\n",
      "#We will convert IC50 to the negative logarithmic scale which is essentially -log10(IC50) or pIC50()\n",
      "# First Take the IC50 values from the standard_value column and converts it from nM to M by multiplying the value by 109\n",
      "# Take the molar value and apply -log10\n",
      "# Delete the standard_value column and create a new pIC50 column\n",
      "\n",
      "#before we do that ,we need to install nescessery package\n",
      "\n",
      "import numpy as np\n",
      "def pIC50(input):\n",
      "    pIC50 = []\n",
      "\n",
      "    for i in input['standard_value_norm']:\n",
      "        molar = i*(10**-9) # Converts nM to M\n",
      "        pIC50.append(-np.log10(molar))\n",
      "\n",
      "    input['pIC50'] = pIC50\n",
      "    x = input.drop('standard_value_norm', 1)\n",
      "        \n",
      "    return x\n",
      "21/36:\n",
      "#Test\n",
      "def norm_value(input):\n",
      "    norm = []\n",
      "\n",
      "    for i in input['standard_value']:\n",
      "        if i > 100000000:\n",
      "          i = 100000000\n",
      "        norm.append(i)\n",
      "\n",
      "    input['standard_value_norm'] = norm\n",
      "    x = input.drop('standard_value', 1)\n",
      "        \n",
      "    return x\n",
      "21/37: -np.log10(1000)\n",
      "21/38: standard_value_norm\n",
      "21/39: df3.standard_value_norm\n",
      "21/40: df3.standard_value\n",
      "21/41:\n",
      "df_norm = norm_value(df_combined)\n",
      "df_norm\n",
      "21/42:\n",
      "df_norm = norm_value(df3)\n",
      "df_norm\n",
      "21/43:\n",
      "#We want to know how many of them is high potency agaisnt LAT1 protein\n",
      "#First we should rearrange from IC50 into pIC50 for a better uniformnity distribution\n",
      "#We will convert IC50 to the negative logarithmic scale which is essentially -log10(IC50) or pIC50()\n",
      "# First Take the IC50 values from the standard_value column and converts it from nM to M by multiplying the value by 109\n",
      "# Take the molar value and apply -log10\n",
      "# Delete the standard_value column and create a new pIC50 column\n",
      "\n",
      "#before we do that ,we need to install nescessery package\n",
      "\n",
      "import numpy as np\n",
      "def pIC50(input):\n",
      "    pIC50 = []\n",
      "\n",
      "    for i in input['standard_value_norm']:\n",
      "        molar = i*(10**-9) # Converts nM to M\n",
      "        pIC50.append(-np.log10(molar))\n",
      "\n",
      "    input['pIC50'] = pIC50\n",
      "    x = input.drop('standard_value_norm', 1)\n",
      "        \n",
      "    return x\n",
      "21/44:\n",
      "#Test\n",
      "def norm_value(input):\n",
      "    norm = []\n",
      "\n",
      "    for i in input['standard_value']:\n",
      "        if i > 100000000:\n",
      "          i = 100000000\n",
      "        norm.append(i)\n",
      "\n",
      "    input['standard_value_norm'] = norm\n",
      "    x = input.drop('standard_value', 1)\n",
      "        \n",
      "    return x\n",
      "21/45:\n",
      "df_norm = norm_value(df3)\n",
      "df_norm\n",
      "21/46:\n",
      "#We want to know how many of them is high potency agaisnt LAT1 protein\n",
      "#First we should rearrange from IC50 into pIC50 for a better uniformnity distribution\n",
      "#We will convert IC50 to the negative logarithmic scale which is essentially -log10(IC50) or pIC50()\n",
      "# First Take the IC50 values from the standard_value column and converts it from nM to M by multiplying the value by 109\n",
      "# Take the molar value and apply -log10\n",
      "# Delete the standard_value column and create a new pIC50 column\n",
      "\n",
      "#before we do that ,we need to install nescessery package\n",
      "\n",
      "import numpy as np\n",
      "def pIC50(input):\n",
      "    pIC50 = []\n",
      "\n",
      "    for i in input['standard_value']:\n",
      "        molar = i*(10**-9) # Converts nM to M\n",
      "        pIC50.append(-np.log10(molar))\n",
      "\n",
      "    input['pIC50'] = pIC50\n",
      "    x = input.drop('standard_value', 1)\n",
      "        \n",
      "    return x\n",
      "21/47:\n",
      "#Test\n",
      "def norm_value(input):\n",
      "    norm = []\n",
      "\n",
      "    for i in input['standard_value']:\n",
      "        if i > 100000000:\n",
      "          i = 100000000\n",
      "        norm.append(i)\n",
      "\n",
      "    input['standard_value_norm'] = norm\n",
      "    x = input.drop('standard_value', 1)\n",
      "        \n",
      "    return x\n",
      "21/48:\n",
      "df_norm = norm_value(df3)\n",
      "df_norm\n",
      "21/49:\n",
      "#Test\n",
      "molar = d3['standard_value']*(10**-9)\n",
      "21/50:\n",
      "#We want to know the structure and activity\n",
      "selection = ['molecule_chembl_id','canonical_smiles','standard_units', 'standard_value']\n",
      "df3 = df2[selection]\n",
      "df3\n",
      "21/51:\n",
      "#Test\n",
      "molar = df3['standard_value']*(10**-9)\n",
      "21/52:\n",
      "#Test\n",
      "'standard_value'\n",
      "molar = df3['standard_value']*(10**-9)\n",
      "21/53:\n",
      "#Test\n",
      "df3['standard_value']\n",
      "molar = df3['standard_value']*(10**-9)\n",
      "21/54:\n",
      "#Test\n",
      "df3['standard_value']\n",
      "21/55:\n",
      "#Test\n",
      "df3['standard_value']\n",
      "molar = int(df3['standard_value'])*(10**-9)\n",
      "21/56:\n",
      "#Test\n",
      "df3['standard_value']\n",
      "# Calculate logarithm to \n",
      "# base 10 on 'Salary' column\n",
      "data['molar'] = df3['Salary']*10  \n",
      "# Show the dataframe\n",
      "data\n",
      "21/57:\n",
      "#Test\n",
      "df3['standard_value']\n",
      "# Calculate logarithm to \n",
      "# base 10 on 'Salary' column\n",
      "data['molar'] = df3['standard_value']*10  \n",
      "# Show the dataframe\n",
      "data\n",
      "21/58:\n",
      "#Test\n",
      "df3['standard_value']\n",
      "# Calculate logarithm to \n",
      "# base 10 on 'Salary' column\n",
      "data['molar'] = df3['standard_value']*10  \n",
      "# Show the dataframe\n",
      "data\n",
      "21/59:\n",
      "#Test\n",
      "df3['standard_value']\n",
      "\n",
      "df3['molar'] = df3['standard_value']*10  \n",
      "# Show the dataframe\n",
      "df3\n",
      "21/60:\n",
      "#Test\n",
      "df3['standard_value']\n",
      "\n",
      "df3['molar'] = df3['standard_value']*(10**-9)\n",
      "# Show the dataframe\n",
      "df3\n",
      "21/61:\n",
      "#Test\n",
      "df3['standard_value']\n",
      "\n",
      "df3['molar'] = np.df3['standard_value']*(10**-9)\n",
      "# Show the dataframe\n",
      "df3\n",
      "21/62:\n",
      "#We want to know how many of them is high potency agaisnt LAT1 protein\n",
      "#First we should rearrange from IC50 into pIC50 for a better uniformnity distribution\n",
      "#We will convert IC50 to the negative logarithmic scale which is essentially -log10(IC50) or pIC50()\n",
      "# First Take the IC50 values from the standard_value column and converts it from nM to M by multiplying the value by 109\n",
      "# Take the molar value and apply -log10\n",
      "# Delete the standard_value column and create a new pIC50 column\n",
      "\n",
      "#before we do that ,we need to install nescessery package\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "def pIC50(input):\n",
      "    pIC50 = []\n",
      "\n",
      "    for i in input['standard_value_norm']:\n",
      "        molar = i*(10**-9) # Converts nM to M\n",
      "        pIC50.append(-np.log10(molar))\n",
      "\n",
      "    input['pIC50'] = pIC50\n",
      "    x = input.drop('standard_value_norm', 1)\n",
      "        \n",
      "    return x\n",
      "21/63: df3.standard_value.describe()\n",
      "21/64: df4 = pIC50(df3)\n",
      "21/65:\n",
      "#We want to know how many of them is high potency agaisnt LAT1 protein\n",
      "#First we should rearrange from IC50 into pIC50 for a better uniformnity distribution\n",
      "#We will convert IC50 to the negative logarithmic scale which is essentially -log10(IC50) or pIC50()\n",
      "# First Take the IC50 values from the standard_value column and converts it from nM to M by multiplying the value by 109\n",
      "# Take the molar value and apply -log10\n",
      "# Delete the standard_value column and create a new pIC50 column\n",
      "\n",
      "#before we do that ,we need to install nescessery package\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "def pIC50(input):\n",
      "    pIC50 = []\n",
      "\n",
      "    for i in input['standard_value']:\n",
      "        molar = i*(10**-9) # Converts nM to M\n",
      "        pIC50.append(-np.log10(molar))\n",
      "\n",
      "    input['pIC50'] = pIC50\n",
      "    x = input.drop('standard_value', 1)\n",
      "        \n",
      "    return x\n",
      "21/66: df3.standard_value.describe()\n",
      "21/67: df4 = pIC50(df3)\n",
      "21/68:\n",
      "#We want to know how many of them is high potency agaisnt LAT1 protein\n",
      "#First we should rearrange from IC50 into pIC50 for a better uniformnity distribution\n",
      "#We will convert IC50 to the negative logarithmic scale which is essentially -log10(IC50) or pIC50()\n",
      "# First Take the IC50 values from the standard_value column and converts it from nM to M by multiplying the value by 109\n",
      "# Take the molar value and apply -log10\n",
      "# Delete the standard_value column and create a new pIC50 column\n",
      "\n",
      "#before we do that ,we need to install nescessery package\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "def pIC50(input):\n",
      "    pIC50 = []\n",
      "\n",
      "    for i in input['standard_value']:\n",
      "        molar = i/1000000000 # Converts nM to M\n",
      "        pIC50.append(-np.log10(molar))\n",
      "\n",
      "    input['pIC50'] = pIC50\n",
      "    x = input.drop('standard_value', 1)\n",
      "        \n",
      "    return x\n",
      "21/69: df3.standard_value.describe()\n",
      "21/70: df4 = pIC50(df3)\n",
      "21/71:\n",
      "#We want to know how many of them is high potency agaisnt LAT1 protein\n",
      "#First we should rearrange from IC50 into pIC50 for a better uniformnity distribution\n",
      "#We will convert IC50 to the negative logarithmic scale which is essentially -log10(IC50) or pIC50()\n",
      "# First Take the IC50 values from the standard_value column and converts it from nM to M by multiplying the value by 109\n",
      "# Take the molar value and apply -log10\n",
      "# Delete the standard_value column and create a new pIC50 column\n",
      "\n",
      "#before we do that ,we need to install nescessery package\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "def pIC50(input):\n",
      "    pIC50 = []\n",
      "\n",
      "    for ic in input['standard_value']:\n",
      "        molar = ic/1000000000 # Converts nM to M\n",
      "        pIC50.append(-np.log10(molar))\n",
      "\n",
      "    input['pIC50'] = pIC50\n",
      "    x = input.drop('standard_value', 1)\n",
      "        \n",
      "    return x\n",
      "21/72:\n",
      "#We want to know how many of them is high potency agaisnt LAT1 protein\n",
      "#First we should rearrange from IC50 into pIC50 for a better uniformnity distribution\n",
      "#We will convert IC50 to the negative logarithmic scale which is essentially -log10(IC50) or pIC50()\n",
      "# First Take the IC50 values from the standard_value column and converts it from nM to M by multiplying the value by 109\n",
      "# Take the molar value and apply -log10\n",
      "# Delete the standard_value column and create a new pIC50 column\n",
      "\n",
      "#before we do that ,we need to install nescessery package\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "def pIC50(input):\n",
      "    pIC50 = []\n",
      "\n",
      "    for ic in input['standard_value']:\n",
      "        molar = ic / 1000000000 # Converts nM to M\n",
      "        pIC50.append(-np.log10(molar))\n",
      "\n",
      "    input['pIC50'] = pIC50\n",
      "    x = input.drop('standard_value', 1)\n",
      "        \n",
      "    return x\n",
      "21/73: df4 = pIC50(df3)\n",
      "21/74: df3.standard_value.describe()\n",
      "21/75:\n",
      "#We want to know how many of them is high potency agaisnt LAT1 protein\n",
      "#First we should rearrange from IC50 into pIC50 for a better uniformnity distribution\n",
      "#We will convert IC50 to the negative logarithmic scale which is essentially -log10(IC50) or pIC50()\n",
      "# First Take the IC50 values from the standard_value column and converts it from nM to M by multiplying the value by 109\n",
      "# Take the molar value and apply -log10\n",
      "# Delete the standard_value column and create a new pIC50 column\n",
      "\n",
      "#before we do that ,we need to install nescessery package\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "def pIC50(input):\n",
      "    pIC50 = []\n",
      "\n",
      "    for i in input['standard_value']:\n",
      "        molar = i /1000000000 # Converts nM to M\n",
      "        pIC50.append(-np.log10(molar))\n",
      "\n",
      "    input['pIC50'] = pIC50\n",
      "    x = input.drop('standard_value', 1)\n",
      "        \n",
      "    return x\n",
      "21/76: df3.standard_value.describe()\n",
      "21/77:\n",
      "#We want to know how many of them is high potency agaisnt LAT1 protein\n",
      "#First we should rearrange from IC50 into pIC50 for a better uniformnity distribution\n",
      "#We will convert IC50 to the negative logarithmic scale which is essentially -log10(IC50) or pIC50()\n",
      "# First Take the IC50 values from the standard_value column and converts it from nM to M by multiplying the value by 109\n",
      "# Take the molar value and apply -log10\n",
      "# Delete the standard_value column and create a new pIC50 column\n",
      "\n",
      "#before we do that ,we need to install nescessery package\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "def pIC50(input):\n",
      "    pIC50 = []\n",
      "\n",
      "    for i in input['standard_value']:\n",
      "        molar = int(i) /1000000000 # Converts nM to M\n",
      "        pIC50.append(-np.log10(molar))\n",
      "\n",
      "    input['pIC50'] = pIC50\n",
      "    x = input.drop('standard_value', 1)\n",
      "        \n",
      "    return x\n",
      "21/78: df3.standard_value.describe()\n",
      "21/79: df4 = pIC50(df3)\n",
      "21/80:\n",
      "#We want to know how many of them is high potency agaisnt LAT1 protein\n",
      "#First we should rearrange from IC50 into pIC50 for a better uniformnity distribution\n",
      "#We will convert IC50 to the negative logarithmic scale which is essentially -log10(IC50) or pIC50()\n",
      "# First Take the IC50 values from the standard_value column and converts it from nM to M by multiplying the value by 109\n",
      "# Take the molar value and apply -log10\n",
      "# Delete the standard_value column and create a new pIC50 column\n",
      "\n",
      "#before we do that ,we need to install nescessery package\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "def pIC50(input):\n",
      "    pIC50 = []\n",
      "\n",
      "    for i in input['standard_value']:\n",
      "        molar = int(i)/1000000000 # Converts nM to M\n",
      "        pIC50.append(-np.log10(molar))\n",
      "\n",
      "    input['pIC50'] = pIC50\n",
      "    x = input.drop('standard_value', 1)\n",
      "        \n",
      "    return x\n",
      "21/81: df4 = pIC50(df3)\n",
      "21/82: int(i)\n",
      "21/83:\n",
      "#We want to know how many of them is high potency agaisnt LAT1 protein\n",
      "#First we should rearrange from IC50 into pIC50 for a better uniformnity distribution\n",
      "#We will convert IC50 to the negative logarithmic scale which is essentially -log10(IC50) or pIC50()\n",
      "# First Take the IC50 values from the standard_value column and converts it from nM to M by multiplying the value by 109\n",
      "# Take the molar value and apply -log10\n",
      "# Delete the standard_value column and create a new pIC50 column\n",
      "\n",
      "#before we do that ,we need to install nescessery package\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "def pIC50(input):\n",
      "    pIC50 = []\n",
      "\n",
      "    for i in input['standard_value']:\n",
      "        molar = int(i)/1000000000 # Converts nM to M\n",
      "        pIC50.append(-np.log10(molar))\n",
      "\n",
      "    input['pIC50'] = pIC50\n",
      "    x = input.drop('standard_value', 1)\n",
      "        \n",
      "    return x\n",
      "21/84:\n",
      "#We want to know how many of them is high potency agaisnt LAT1 protein\n",
      "#First we should rearrange from IC50 into pIC50 for a better uniformnity distribution\n",
      "#We will convert IC50 to the negative logarithmic scale which is essentially -log10(IC50) or pIC50()\n",
      "# First Take the IC50 values from the standard_value column and converts it from nM to M by multiplying the value by 109\n",
      "# Take the molar value and apply -log10\n",
      "# Delete the standard_value column and create a new pIC50 column\n",
      "\n",
      "#before we do that ,we need to install nescessery package\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "def pIC50(input):\n",
      "    pIC50 = []\n",
      "\n",
      "    for i in input['standard_value']:\n",
      "        molar = int(i/1000000000) # Converts nM to M\n",
      "        pIC50.append(-np.log10(molar))\n",
      "\n",
      "    input['pIC50'] = pIC50\n",
      "    x = input.drop('standard_value', 1)\n",
      "        \n",
      "    return x\n",
      "21/85: df4 = pIC50(df3)\n",
      "21/86:\n",
      "#We want to know how many of them is high potency agaisnt LAT1 protein\n",
      "#First we should rearrange from IC50 into pIC50 for a better uniformnity distribution\n",
      "#We will convert IC50 to the negative logarithmic scale which is essentially -log10(IC50) or pIC50()\n",
      "# First Take the IC50 values from the standard_value column and converts it from nM to M by multiplying the value by 109\n",
      "# Take the molar value and apply -log10\n",
      "# Delete the standard_value column and create a new pIC50 column\n",
      "\n",
      "#before we do that ,we need to install nescessery package\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "def pIC50(input):\n",
      "    pIC50 = []\n",
      "\n",
      "    for i in input['standard_value']:\n",
      "        molar = int(i)/1000000000 # Converts nM to M\n",
      "        pIC50.append(-np.log10(molar))\n",
      "\n",
      "    input['pIC50'] = pIC50\n",
      "    x = input.drop('standard_value', 1)\n",
      "        \n",
      "    return x\n",
      "21/87: df4 = pIC50(df3)\n",
      "21/88:\n",
      "#create lipinski rule of five\n",
      "import numpy as np\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Descriptors, Lipinski\n",
      "21/89:\n",
      "#create lipinski rule of five\n",
      "import numpy as np\n",
      "import rdkit  \n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Descriptors, Lipinski\n",
      "21/90:\n",
      "#create lipinski rule of five\n",
      "import numpy as np\n",
      "import rdkit\n",
      "21/91:\n",
      "#create lipinski rule of five\n",
      "import numpy as np\n",
      "import rdKit\n",
      "21/92:\n",
      "#create lipinski rule of five\n",
      "import numpy as np\n",
      "import rdkit\n",
      "21/93:\n",
      "#create lipinski rule of five\n",
      "import numpy as np\n",
      "import rdkit \n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Descriptors, Lipinski\n",
      "21/94:\n",
      "#create lipinski rule of five\n",
      "import numpy as np\n",
      "import sys\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Descriptors, Lipinski\n",
      "21/95:\n",
      "#create lipinski rule of five\n",
      "import numpy as np\n",
      "import sys\n",
      "import rdkit\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Descriptors, Lipinski\n",
      "21/96:\n",
      "#create lipinski rule of five\n",
      "pip install rdkit-pypi\n",
      "21/97: pip install rdkit-pypi\n",
      "21/98:\n",
      "import numpy as np\n",
      "import rdkit\n",
      "21/99:\n",
      "#We are going to calculate Lipinski rule of five\n",
      "from rdkit import chem\n",
      "from rdkit.Chem import Descriptors, Lipinski\n",
      "21/100:\n",
      "#We are going to calculate Lipinski rule of five\n",
      "from rdkit import chem\n",
      "21/101:\n",
      "#We are going to calculate Lipinski rule of five\n",
      "from rdkit import Chem\n",
      "21/102:\n",
      "#We are going to calculate Lipinski rule of five\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Descriptors, Lipinski\n",
      "21/103: pip install -U notebook-as-pdf\n",
      "21/104: pyppeteer-install\n",
      "21/105: jupyter nbconvert --to html notebook.ipynb\n",
      "21/106: pyppeteer-install\n",
      "21/107:\n",
      "#install pdf file\n",
      "pip install -U notebook-as-pdf\n",
      "pyppeteer-install\n",
      "21/108:\n",
      "#install pdf file\n",
      "pip install -U notebook-as-pdf\n",
      "pip install pyppeteer\n",
      "21/109:\n",
      "#install pdf file\n",
      "pip install -U notebook-as-pdf\n",
      "pip install pyppeteer\n",
      "21/110:\n",
      "pip install -U notebook-as-pdf\n",
      "pip install pyppeteer\n",
      "21/111:\n",
      "pip install -U notebook-as-pdf\n",
      "pip install pyppeteer\n",
      "21/112:\n",
      "pip install -U notebook-as-pdf\n",
      "pip install pyppeteer\n",
      "21/113:\n",
      "import numpy as np\n",
      "import rdkit\n",
      "from time import time\n",
      "\n",
      "from sklearn.metrics import f1_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "import seaborn as sns\n",
      "%matplotlib inline\n",
      "21/114:\n",
      "import numpy as np\n",
      "import rdkit\n",
      "from time import time\n",
      "\n",
      "from sklearn.metrics import f1_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "import matplotlib.pyplot as plt\n",
      "21/115:\n",
      "def lipinski(smiles, verbose=False):\n",
      "\n",
      "    moldata= []\n",
      "    for elem in smiles:\n",
      "        mol=Chem.MolFromSmiles(elem) \n",
      "        moldata.append(mol)\n",
      "       \n",
      "    baseData= np.arange(1,1)\n",
      "    i=0  \n",
      "    for mol in moldata:        \n",
      "       \n",
      "        desc_MolWt = Descriptors.MolWt(mol)\n",
      "        desc_MolLogP = Descriptors.MolLogP(mol)\n",
      "        desc_NumHDonors = Lipinski.NumHDonors(mol)\n",
      "        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)\n",
      "           \n",
      "        row = np.array([desc_MolWt,\n",
      "                        desc_MolLogP,\n",
      "                        desc_NumHDonors,\n",
      "                        desc_NumHAcceptors])   \n",
      "    \n",
      "        if(i==0):\n",
      "            baseData=row\n",
      "        else:\n",
      "            baseData=np.vstack([baseData, row])\n",
      "        i=i+1      \n",
      "    \n",
      "    columnNames=[\"MW\",\"LogP\",\"NumHDonors\",\"NumHAcceptors\"]   \n",
      "    descriptors = pd.DataFrame(data=baseData,columns=columnNames)\n",
      "    \n",
      "    return descriptor\n",
      "21/116: df_lipinski = lipinski(df3.canonical_smiles)\n",
      "21/117:\n",
      "def lipinski(smiles, verbose=False):\n",
      "\n",
      "    moldata= []\n",
      "    for elem in smiles:\n",
      "        mol=Chem.MolFromSmiles(elem) \n",
      "        moldata.append(mol)\n",
      "       \n",
      "    baseData= np.arange(1,1)\n",
      "    i=0  \n",
      "    for mol in moldata:        \n",
      "       \n",
      "        desc_MolWt = Descriptors.MolWt(mol)\n",
      "        desc_MolLogP = Descriptors.MolLogP(mol)\n",
      "        desc_NumHDonors = Lipinski.NumHDonors(mol)\n",
      "        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)\n",
      "           \n",
      "        row = np.array([desc_MolWt,\n",
      "                        desc_MolLogP,\n",
      "                        desc_NumHDonors,\n",
      "                        desc_NumHAcceptors])   \n",
      "    \n",
      "        if(i==0):\n",
      "            baseData=row\n",
      "        else:\n",
      "            baseData=np.vstack([baseData, row])\n",
      "        i=i+1      \n",
      "    \n",
      "    columnNames=[\"MW\",\"LogP\",\"NumHDonors\",\"NumHAcceptors\"]   \n",
      "    descriptors = pd.DataFrame(data=baseData,columns=columnNames)\n",
      "    \n",
      "    return descriptor\n",
      "21/118:\n",
      "def lipinski(smiles, verbose=False):\n",
      "\n",
      "    moldata= []\n",
      "    for elem in smiles:\n",
      "        mol=Chem.MolFromSmiles(elem) \n",
      "        moldata.append(mol)\n",
      "       \n",
      "    baseData= np.arange(1,1)\n",
      "    i=0  \n",
      "    for mol in moldata:        \n",
      "       \n",
      "        desc_MolWt = Descriptors.MolWt(mol)\n",
      "        desc_MolLogP = Descriptors.MolLogP(mol)\n",
      "        desc_NumHDonors = Lipinski.NumHDonors(mol)\n",
      "        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)\n",
      "           \n",
      "        row = np.array([desc_MolWt,\n",
      "                        desc_MolLogP,\n",
      "                        desc_NumHDonors,\n",
      "                        desc_NumHAcceptors])   \n",
      "    \n",
      "        if(i==0):\n",
      "            baseData=row\n",
      "        else:\n",
      "            baseData=np.vstack([baseData, row])\n",
      "        i=i+1      \n",
      "    \n",
      "    columnNames=[\"MW\",\"LogP\",\"NumHDonors\",\"NumHAcceptors\"]   \n",
      "    descriptors = pd.DataFrame(data=baseData,columns=columnNames)\n",
      "    \n",
      "    return descriptors\n",
      "21/119: df_lipinski = lipinski(df3.canonical_smiles)\n",
      "21/120:\n",
      "df_lipinski = lipinski(df3.canonical_smiles)\n",
      "df_lipinski\n",
      "21/121:\n",
      "def lipinski(smiles, verbose=False):\n",
      "\n",
      "    moldata= []\n",
      "    for elem in smiles:\n",
      "        mol=Chem.MolFromSmiles(elem) \n",
      "        moldata.append(mol)\n",
      "       \n",
      "    baseData= np.arange(1,1)\n",
      "    i=0  \n",
      "    for mol in moldata:        \n",
      "       \n",
      "        desc_MolWt = Descriptors.MolWt(mol)\n",
      "        desc_MolLogP = Descriptors.MolLogP(mol)\n",
      "        desc_NumHDonors = Lipinski.NumHDonors(mol)\n",
      "        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)\n",
      "        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)\n",
      "           \n",
      "        row = np.array([desc_MolWt,\n",
      "                        desc_MolLogP,\n",
      "                        desc_NumHDonors,\n",
      "                        desc_NumHAcceptors,\n",
      "                        desc_NumRotatableBonds])   \n",
      "    \n",
      "        if(i==0):\n",
      "            baseData=row\n",
      "        else:\n",
      "            baseData=np.vstack([baseData, row])\n",
      "        i=i+1      \n",
      "    \n",
      "    columnNames=[\"MW\",\"LogP\",\"NumHDonors\",\"NumHAcceptors\", \"NumRotatableBonds\"]   \n",
      "    descriptors = pd.DataFrame(data=baseData,columns=columnNames)\n",
      "    \n",
      "    return descriptors\n",
      "21/122:\n",
      "df_lipinski = lipinski(df3.canonical_smiles)\n",
      "df_lipinski\n",
      "21/123:\n",
      "#combine results\n",
      "df_combined = pd.concat([df3,df_lipinski], axis=1)\n",
      "df_combined\n",
      "21/124:\n",
      "#We want to know the structure and activity\n",
      "selection = ['molecule_chembl_id','canonical_smiles','standard_units', 'standard_value']\n",
      "df3 = df2[selection]\n",
      "df3\n",
      "21/125:\n",
      "#combine results\n",
      "df_combined = pd.concat([df3,df_lipinski], axis=1)\n",
      "df_combined\n",
      "21/126:\n",
      "df_lipinski = lipinski(df3.canonical_smiles)\n",
      "df_lipinski\n",
      "21/127:\n",
      "#combine results\n",
      "df_combined = pd.concat([df3,df_lipinski], axis=1)\n",
      "df_combined\n",
      "21/128:\n",
      "df_lipinski = lipinski(df3.canonical_smiles)\n",
      "df_lipinski\n",
      "df_lipinski,tail(3)\n",
      "21/129:\n",
      "df_lipinski = lipinski(df3.canonical_smiles)\n",
      "df_lipinski\n",
      "df_lipinski.tail(3)\n",
      "21/130:\n",
      "df_lipinski = lipinski(df3.canonical_smiles)\n",
      "df_lipinski\n",
      "df_lipinski.head(3)\n",
      "21/131:\n",
      "df_lipinski = lipinski(df3.canonical_smiles)\n",
      "df_lipinski\n",
      "df_lipinski\n",
      "21/132:\n",
      "#We want to know the structure and activity\n",
      "selection = ['molecule_chembl_id','canonical_smiles','standard_units', 'standard_value']\n",
      "df3 = df2[selection]\n",
      "df3\n",
      "21/133:\n",
      "def lipinski(smiles, verbose=False):\n",
      "\n",
      "    moldata= []\n",
      "    for elem in smiles:\n",
      "        mol=Chem.MolFromSmiles(elem) \n",
      "        moldata.append(mol)\n",
      "       \n",
      "    baseData= np.arange(1,1)\n",
      "    i=0  \n",
      "    for mol in moldata:        \n",
      "       \n",
      "        desc_MolWt = Descriptors.MolWt(mol)\n",
      "        desc_MolLogP = Descriptors.MolLogP(mol)\n",
      "        desc_NumHDonors = Lipinski.NumHDonors(mol)\n",
      "        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)\n",
      "        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)\n",
      "           \n",
      "        row = np.array([desc_MolWt,\n",
      "                        desc_MolLogP,\n",
      "                        desc_NumHDonors,\n",
      "                        desc_NumHAcceptors,\n",
      "                        desc_NumRotatableBonds])   \n",
      "    \n",
      "        if(i==0):\n",
      "            baseData=row\n",
      "        else:\n",
      "            baseData=np.vstack([baseData, row])\n",
      "        i=i+1      \n",
      "    \n",
      "    columnNames=[\"MW\",\"LogP\",\"NumHDonors\",\"NumHAcceptors\", \"NumRotatableBonds\"]   \n",
      "    descriptors = pd.DataFrame(data=baseData,columns=columnNames)\n",
      "    \n",
      "    return descriptors\n",
      "21/134:\n",
      "df4 = df3[df.canonical_smiles.notna()\n",
      "df4\n",
      "21/135:\n",
      "df4 = df3[df.canonical_smiles.notna()\n",
      "df4\n",
      "21/136: df4 = df3[df.canonical_smiles.notna()\n",
      "21/137: df4 = df3[df.canonical_smiles.notna()]\n",
      "21/138:\n",
      "df4 = df3[df.canonical_smiles.notna()]\n",
      "df4\n",
      "21/139:\n",
      "df_lipinski = lipinski(df4.canonical_smiles)\n",
      "df_lipinski\n",
      "df_lipinski\n",
      "21/140:\n",
      "import pandas as pd\n",
      "def lipinski(smiles, verbose=False):\n",
      "\n",
      "    moldata= []\n",
      "    for elem in smiles:\n",
      "        mol=Chem.MolFromSmiles(elem) \n",
      "        moldata.append(mol)\n",
      "       \n",
      "    baseData= np.arange(1,1)\n",
      "    i=0  \n",
      "    for mol in moldata:        \n",
      "       \n",
      "        desc_MolWt = Descriptors.MolWt(mol)\n",
      "        desc_MolLogP = Descriptors.MolLogP(mol)\n",
      "        desc_NumHDonors = Lipinski.NumHDonors(mol)\n",
      "        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)\n",
      "        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)\n",
      "           \n",
      "        row = np.array([desc_MolWt,\n",
      "                        desc_MolLogP,\n",
      "                        desc_NumHDonors,\n",
      "                        desc_NumHAcceptors,\n",
      "                        desc_NumRotatableBonds])   \n",
      "    \n",
      "        if(i==0):\n",
      "            baseData=row\n",
      "        else:\n",
      "            baseData=np.vstack([baseData, row])\n",
      "        i=i+1      \n",
      "    \n",
      "    columnNames=[\"MW\",\"LogP\",\"NumHDonors\",\"NumHAcceptors\", \"NumRotatableBonds\"]   \n",
      "    descriptors = pd.DataFrame(data=baseData,columns=columnNames)\n",
      "    \n",
      "    return descriptors\n",
      "21/141:\n",
      "df_lipinski = lipinski(df4.canonical_smiles)\n",
      "df_lipinski\n",
      "df_lipinski\n",
      "21/142:\n",
      "import pandas as pd\n",
      "def lipinski(smiles, verbose=False):\n",
      "\n",
      "    moldata= []\n",
      "    for elem in smiles:\n",
      "        mol=Chem.MolFromSmiles(elem) \n",
      "        moldata.append(mol)\n",
      "       \n",
      "    baseData= np.arange(1,1)\n",
      "    i=0  \n",
      "    for mol in moldata:        \n",
      "       \n",
      "        desc_MolWt = Descriptors.MolWt(mol)\n",
      "        desc_MolLogP = Descriptors.MolLogP(mol)\n",
      "        desc_NumHDonors = Lipinski.NumHDonors(mol)\n",
      "        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)\n",
      "        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)\n",
      "           \n",
      "        row = np.array([desc_MolWt,\n",
      "                        desc_MolLogP,\n",
      "                        desc_NumHDonors,\n",
      "                        desc_NumHAcceptors,\n",
      "                        desc_NumRotatableBonds])   \n",
      "    \n",
      "        if(i==0):\n",
      "            baseData=row\n",
      "        else:\n",
      "            baseData=np.vstack([baseData, row])\n",
      "        i=i      \n",
      "    \n",
      "    columnNames=[\"MW\",\"LogP\",\"NumHDonors\",\"NumHAcceptors\", \"NumRotatableBonds\"]   \n",
      "    descriptors = pd.DataFrame(data=baseData,columns=columnNames)\n",
      "    \n",
      "    return descriptors\n",
      "21/143:\n",
      "df_lipinski = lipinski(df4.canonical_smiles)\n",
      "df_lipinski\n",
      "df_lipinski\n",
      "21/144:\n",
      "import pandas as pd\n",
      "def lipinski(smiles, verbose=False):\n",
      "\n",
      "    moldata= []\n",
      "    for elem in smiles:\n",
      "        mol=Chem.MolFromSmiles(elem) \n",
      "        moldata.append(mol)\n",
      "       \n",
      "    baseData= np.arange(1,1)\n",
      "    i=0  \n",
      "    for mol in moldata:        \n",
      "       \n",
      "        desc_MolWt = Descriptors.MolWt(mol)\n",
      "        desc_MolLogP = Descriptors.MolLogP(mol)\n",
      "        desc_NumHDonors = Lipinski.NumHDonors(mol)\n",
      "        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)\n",
      "        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)\n",
      "           \n",
      "        row = np.array([desc_MolWt,\n",
      "                        desc_MolLogP,\n",
      "                        desc_NumHDonors,\n",
      "                        desc_NumHAcceptors,\n",
      "                        desc_NumRotatableBonds])   \n",
      "    \n",
      "        if(i==0):\n",
      "            baseData=row\n",
      "        else:\n",
      "            baseData=np.vstack([baseData, row])\n",
      "        i=i+1      \n",
      "    \n",
      "    columnNames=[\"MW\",\"LogP\",\"NumHDonors\",\"NumHAcceptors\", \"NumRotatableBonds\"]   \n",
      "    descriptors = pd.DataFrame(data=baseData,columns=columnNames)\n",
      "    \n",
      "    return descriptors\n",
      "21/145:\n",
      "df_lipinski = lipinski(df4.canonical_smiles)\n",
      "df_lipinski\n",
      "df_lipinski\n",
      "21/146:\n",
      "import pandas as pd\n",
      "def lipinski(smiles, verbose=False):\n",
      "\n",
      "    moldata= []\n",
      "    for elem in smiles:\n",
      "        mol=Chem.MolFromSmiles(elem) \n",
      "        moldata.append(mol)\n",
      "       \n",
      "    baseData= np.arange(0,1)\n",
      "    i=0  \n",
      "    for mol in moldata:        \n",
      "       \n",
      "        desc_MolWt = Descriptors.MolWt(mol)\n",
      "        desc_MolLogP = Descriptors.MolLogP(mol)\n",
      "        desc_NumHDonors = Lipinski.NumHDonors(mol)\n",
      "        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)\n",
      "        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)\n",
      "           \n",
      "        row = np.array([desc_MolWt,\n",
      "                        desc_MolLogP,\n",
      "                        desc_NumHDonors,\n",
      "                        desc_NumHAcceptors,\n",
      "                        desc_NumRotatableBonds])   \n",
      "    \n",
      "        if(i==0):\n",
      "            baseData=row\n",
      "        else:\n",
      "            baseData=np.vstack([baseData, row])\n",
      "        i=i+1      \n",
      "    \n",
      "    columnNames=[\"MW\",\"LogP\",\"NumHDonors\",\"NumHAcceptors\", \"NumRotatableBonds\"]   \n",
      "    descriptors = pd.DataFrame(data=baseData,columns=columnNames)\n",
      "    \n",
      "    return descriptors\n",
      "21/147:\n",
      "df_lipinski = lipinski(df4.canonical_smiles)\n",
      "df_lipinski\n",
      "df_lipinski\n",
      "21/148:\n",
      "import pandas as pd\n",
      "def lipinski(smiles, verbose=False):\n",
      "\n",
      "    moldata= []\n",
      "    for elem in smiles:\n",
      "        mol=Chem.MolFromSmiles(elem) \n",
      "        moldata.append(mol)\n",
      "       \n",
      "    baseData= np.arange(1,1)\n",
      "    i=0  \n",
      "    for mol in moldata:        \n",
      "       \n",
      "        desc_MolWt = Descriptors.MolWt(mol)\n",
      "        desc_MolLogP = Descriptors.MolLogP(mol)\n",
      "        desc_NumHDonors = Lipinski.NumHDonors(mol)\n",
      "        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)\n",
      "        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)\n",
      "           \n",
      "        row = np.array([desc_MolWt,\n",
      "                        desc_MolLogP,\n",
      "                        desc_NumHDonors,\n",
      "                        desc_NumHAcceptors,\n",
      "                        desc_NumRotatableBonds])   \n",
      "    \n",
      "        if(i==0):\n",
      "            baseData=row\n",
      "        else:\n",
      "            baseData=np.vstack([baseData, row])\n",
      "        i=i+1      \n",
      "    \n",
      "    columnNames=[\"MW\",\"LogP\",\"NumHDonors\",\"NumHAcceptors\", \"NumRotatableBonds\"]   \n",
      "    descriptors = pd.DataFrame(data=baseData,columns=columnNames)\n",
      "    \n",
      "    return descriptors\n",
      "21/149:\n",
      "df_lipinski = lipinski(df4.canonical_smiles)\n",
      "df_lipinski\n",
      "df_lipinski\n",
      "21/150: print(df4.\"[canonical_smiles]\".unique()\n",
      "21/151: print(df4.'canonical_smiles]'.unique()\n",
      "21/152: print(df4.canonical_smiles.unique())\n",
      "21/153: print(df4.molecule_chembl_id .unique())\n",
      "21/154: print(df4.canonical_smiles.unique())\n",
      "21/155:\n",
      "import pandas as pd\n",
      "def lipinski(smiles, verbose=False):\n",
      "\n",
      "    moldata= []\n",
      "    for elem in smiles:\n",
      "        mol=Chem.MolFromSmiles(elem) \n",
      "        moldata.append(mol)\n",
      "       \n",
      "    baseData= np.arange(1,1)\n",
      "    i=0  \n",
      "    for mol in moldata:        \n",
      "       \n",
      "        desc_MolWt = Descriptors.MolWt(mol)\n",
      "        desc_MolLogP = Descriptors.MolLogP(mol)\n",
      "        desc_NumHDonors = Lipinski.NumHDonors(mol)\n",
      "        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)\n",
      "        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)\n",
      "           \n",
      "        row = np.array([desc_MolWt,\n",
      "                        desc_MolLogP,\n",
      "                        desc_NumHDonors,\n",
      "                        desc_NumHAcceptors,\n",
      "                        desc_NumRotatableBonds])   \n",
      "    \n",
      "        if(i==0):\n",
      "            baseData=row\n",
      "        else:\n",
      "            baseData=np.vstack([baseData, row])\n",
      "        i=i+1      \n",
      "    \n",
      "    columnNames=[\"MW\",\"LogP\",\"NumHDonors\",\"NumHAcceptors\", \"NumRotatableBonds\"]   \n",
      "    descriptors = pd.DataFrame(data=baseData,columns=columnNames)\n",
      "    \n",
      "    return descriptors\n",
      "21/156:\n",
      "df_lipinski = lipinski(df4.canonical_smiles)\n",
      "df_lipinski\n",
      "df_lipinski\n",
      "21/157:\n",
      "#combine results\n",
      "df_combined = pd.concat([df3,df_lipinski])\n",
      "df_combined\n",
      "21/158:\n",
      "#combine results\n",
      "df_combined = pd.concat([df3,df_lipinski], axis=1)\n",
      "df_combined\n",
      "21/159:\n",
      "#combine results\n",
      "df_combined = pd.concat([df3,df_lipinski], axis=1)\n",
      "df_combined\n",
      "21/160:\n",
      "#remove na\n",
      "final_df_combined = final_df_combined.replace(['None','NONE','none'],np.nan)\n",
      "final_df_combined = final_df_combined.dropna()\n",
      "#print('Unique value in MW column',final_df_combined['MW'].unique())\n",
      "len(final_df_combined)\n",
      "21/161:\n",
      "#remove na\n",
      "final_df_combined = df_combined.replace(['None','NONE','none'],np.nan)\n",
      "final_df_combined = final_df_combined.dropna()\n",
      "#print('Unique value in MW column',final_df_combined['MW'].unique())\n",
      "len(final_df_combined)\n",
      "21/162:\n",
      "#remove na\n",
      "final_df_combined = df_combined.replace(['None','NONE','none'],np.nan)\n",
      "final_df_combined = final_df_combined.dropna()\n",
      "print('Unique value in MW column',final_df_combined['MW'].unique())\n",
      "len(final_df_combined)\n",
      "21/163: print(df4['canonical_smiles'].unique())\n",
      "21/164:\n",
      "#We want to know how many of them is high potency agaisnt LAT1 protein\n",
      "#First we should rearrange from IC50 into pIC50 for a better uniformnity distribution\n",
      "#We will convert IC50 to the negative logarithmic scale which is essentially -log10(IC50) or pIC50()\n",
      "# First Take the IC50 values from the standard_value column and converts it from nM to M by multiplying the value by 109\n",
      "# Take the molar value and apply -log10\n",
      "# Delete the standard_value column and create a new pIC50 column\n",
      "\n",
      "#before we do that ,we need to install nescessery package\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "def pIC50(input):\n",
      "    pIC50 = []\n",
      "\n",
      "    for i in input['standard_value_norm']:\n",
      "        molar = i*(10**-9) # Converts nM to M\n",
      "        pIC50.append(-np.log10(molar))\n",
      "\n",
      "    input['pIC50'] = pIC50\n",
      "    x = input.drop('standard_value_norm', 1)\n",
      "        \n",
      "    return x\n",
      "21/165: final_df_combined.standard_value.describe()\n",
      "21/166: final_df_combined\n",
      "21/167: df% = pIC50(final_df_combined)\n",
      "21/168: df5 = pIC50(final_df_combined)\n",
      "21/169:\n",
      "def norm_value(input):\n",
      "    norm = []\n",
      "\n",
      "    for i in input['standard_value']:\n",
      "        if i > 100000000:\n",
      "          i = 100000000\n",
      "        norm.append(i)\n",
      "\n",
      "    input['standard_value_norm'] = norm\n",
      "    x = input.drop('standard_value', 1)\n",
      "        \n",
      "    return x\n",
      "21/170:\n",
      "def norm_value(input):\n",
      "    norm = []\n",
      "\n",
      "    for i in input['standard_value']:\n",
      "        if i > 100000000:\n",
      "          i = 100000000\n",
      "        norm.append(i)\n",
      "\n",
      "    input['standard_value_norm'] = norm\n",
      "    x = input.drop('standard_value', 1)\n",
      "        \n",
      "    return x\n",
      "21/171:\n",
      "df_norm = norm_value(df_combined)\n",
      "df_norm\n",
      "21/172:\n",
      "def norm_value(input):\n",
      "    norm = []\n",
      "\n",
      "    for i in input['standard_value']:\n",
      "        if int(i) > 100000000:\n",
      "          i = 100000000\n",
      "        norm.append(i)\n",
      "\n",
      "    input['standard_value_norm'] = norm\n",
      "    x = input.drop('standard_value', 1)\n",
      "        \n",
      "    return x\n",
      "21/173:\n",
      "df_norm = norm_value(final_df_combined)\n",
      "df_norm\n",
      "21/174:\n",
      "def norm_value(input):\n",
      "    norm = []\n",
      "\n",
      "    for i in input['standard_value']:\n",
      "        if float(i) > 100000000:\n",
      "          i = 100000000\n",
      "        norm.append(i)\n",
      "\n",
      "    input['standard_value_norm'] = norm\n",
      "    x = input.drop('standard_value', 1)\n",
      "        \n",
      "    return x\n",
      "21/175:\n",
      "df_norm = norm_value(final_df_combined)\n",
      "df_norm\n",
      "21/176:\n",
      "#We want to know how many of them is high potency agaisnt LAT1 protein\n",
      "#First we should rearrange from IC50 into pIC50 for a better uniformnity distribution\n",
      "#We will convert IC50 to the negative logarithmic scale which is essentially -log10(IC50) or pIC50()\n",
      "# First Take the IC50 values from the standard_value column and converts it from nM to M by multiplying the value by 109\n",
      "# Take the molar value and apply -log10\n",
      "# Delete the standard_value column and create a new pIC50 column\n",
      "\n",
      "#before we do that ,we need to install nescessery package\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "def pIC50(input):\n",
      "    pIC50 = []\n",
      "\n",
      "    for i in input['standard_value_norm']:\n",
      "        molar = i*(10**-9) # Converts nM to M\n",
      "        pIC50.append(-np.log10(molar))\n",
      "\n",
      "    input['pIC50'] = pIC50\n",
      "    x = input.drop('standard_value_norm', 1)\n",
      "        \n",
      "    return x\n",
      "21/177: final_df_combined.standard_value.describe()\n",
      "21/178:\n",
      "def norm_value(input):\n",
      "    norm = []\n",
      "\n",
      "    for i in input['standard_value']:\n",
      "        if float(i) > 100000000:\n",
      "          i = 100000000\n",
      "        norm.append(i)\n",
      "\n",
      "    input['standard_value_norm'] = norm\n",
      "    x = input.drop('standard_value', 1)\n",
      "        \n",
      "    return x\n",
      "21/179:\n",
      "df_norm = norm_value(final_df_combined)\n",
      "df_norm\n",
      "21/180: df_norm.standard_value_norm.describe()\n",
      "21/181:\n",
      "df_final = pIC50(df_norm)\n",
      "df_final\n",
      "21/182:\n",
      "#We want to know how many of them is high potency agaisnt LAT1 protein\n",
      "#First we should rearrange from IC50 into pIC50 for a better uniformnity distribution\n",
      "#We will convert IC50 to the negative logarithmic scale which is essentially -log10(IC50) or pIC50()\n",
      "# First Take the IC50 values from the standard_value column and converts it from nM to M by multiplying the value by 109\n",
      "# Take the molar value and apply -log10\n",
      "# Delete the standard_value column and create a new pIC50 column\n",
      "\n",
      "#before we do that ,we need to install nescessery package\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "def pIC50(input):\n",
      "    pIC50 = []\n",
      "\n",
      "    for i in input['standard_value_norm']:\n",
      "        molar = float(i)*(10**-9) # Converts nM to M\n",
      "        pIC50.append(-np.log10(molar))\n",
      "\n",
      "    input['pIC50'] = pIC50\n",
      "    x = input.drop('standard_value_norm', 1)\n",
      "        \n",
      "    return x\n",
      "21/183:\n",
      "df_final = pIC50(df_norm)\n",
      "df_final\n",
      "21/184:\n",
      "# define active, inactive, intermediate compounds\n",
      "\n",
      "STATUS = []\n",
      "\n",
      "for i in fix:\n",
      "    if i <=1000:\n",
      "        STATUS.append(\"active\") #active\n",
      "        \n",
      "    elif i >=10000:\n",
      "        STATUS.append(\"inactive\") #inactive\n",
      "        \n",
      "    else:\n",
      "        STATUS.append(\"intermediate\") #intermediate\n",
      "21/185: -np.log10(10000)\n",
      "21/186: np.log10(10000)\n",
      "21/187: -np.log10(0.000001)\n",
      "21/188: -np.log10(0.00001)\n",
      "21/189:\n",
      "bioactivity_class = []\n",
      "for i in df_final.pIC50:\n",
      "  if float(i) >= 6:\n",
      "    bioactivity_class.append(\"inactive\")\n",
      "  elif float(i) <= 5:\n",
      "    bioactivity_class.append(\"active\")\n",
      "21/190:\n",
      "bioactivity_class = pd.Series(bioactivity_class, name='bioactivity_class')\n",
      "df_class = pd.concat([df_final, bioactivity_class], axis=1)\n",
      "df_class\n",
      "21/191:\n",
      "bioactivity_class = []\n",
      "for i in df_final.pIC50:\n",
      "  if float(i) >= 6:\n",
      "    bioactivity_class.append(\"inactive\")\n",
      "  elif float(i) <= 5:\n",
      "    bioactivity_class.append(\"active\")\n",
      "      else :\n",
      "        bioactivity_class.append(\"intermediate\")\n",
      "21/192:\n",
      "bioactivity_class = []\n",
      "for i in df_final.pIC50:\n",
      "  if float(i) >= 6:\n",
      "    bioactivity_class.append(\"inactive\")\n",
      "  elif float(i) <= 5:\n",
      "    bioactivity_class.append(\"active\")\n",
      "    else :\n",
      "        bioactivity_class.append(\"intermediate\")\n",
      "21/193:\n",
      "bioactivity_class = []\n",
      "for i in df_final.pIC50:\n",
      "  if float(i) >= 6:\n",
      "    bioactivity_class.append(\"inactive\")\n",
      "  elif float(i) <= 5:\n",
      "    bioactivity_class.append(\"active\")\n",
      "  else :\n",
      "        bioactivity_class.append(\"intermediate\")\n",
      "21/194:\n",
      "bioactivity_class = pd.Series(bioactivity_class, name='bioactivity_class')\n",
      "df_class = pd.concat([df_final, bioactivity_class], axis=1)\n",
      "df_class\n",
      "21/195:\n",
      "bioactivity_class = []\n",
      "for i in df_final.pIC50:\n",
      "  if float(i) >= 6:\n",
      "    bioactivity_class.append(\"inactive\")\n",
      "  elif float(i) <= 5:\n",
      "    bioactivity_class.append(\"active\")\n",
      "  else:\n",
      "        bioactivity_class.append(\"intermediate\")\n",
      "21/196:\n",
      "bioactivity_class = pd.Series(bioactivity_class, name='bioactivity_class')\n",
      "df_class = pd.concat([df_final, bioactivity_class], axis=1)\n",
      "df_class\n",
      "21/197:\n",
      "bioactivity_class = []\n",
      "for i in df_final.pIC50:\n",
      "  if float(i) >= 6:\n",
      "    bioactivity_class.append(\"inactive\")\n",
      "  elif float(i) <= 5:\n",
      "    bioactivity_class.append(\"active\")\n",
      "  else:\n",
      "    bioactivity_class.append(\"intermediate\")\n",
      "21/198:\n",
      "bioactivity_class = pd.Series(bioactivity_class, name='bioactivity_class')\n",
      "df_class = pd.concat([df_final, bioactivity_class], axis=1)\n",
      "df_class\n",
      "21/199: bioactivity_class\n",
      "21/200: df_final.pIC50\n",
      "21/201: bioactivity_class\n",
      "21/202: print(bioactivity_class)\n",
      "21/203: print(bioactivity_class.unique())\n",
      "21/204: len(bioactivity_class.unique())\n",
      "21/205: len(bioactivity_class)\n",
      "21/206: len(df_final)\n",
      "21/207: result = pd.concat([df_final, bioactivity_class], axis=1, join=\"inner\")\n",
      "21/208:\n",
      "result = pd.concat([df_final, bioactivity_class], axis=1, join=\"inner\")\n",
      "result\n",
      "21/209: len(result)\n",
      "21/210: bioactivity_class\n",
      "21/211: df_class = pd.DataFrame(bioactivity_class)\n",
      "21/212:\n",
      "df_class = pd.DataFrame(bioactivity_class)\n",
      "df_class\n",
      "21/213: df_final_2 = pdf.concat[df_final, df_class]\n",
      "21/214: df_final_2 = pd.concat[df_final, df_class]\n",
      "21/215: df_final_2 = pd.concat([df_final, df_class])\n",
      "21/216:\n",
      "df_final_2 = pd.concat([df_final, df_class])\n",
      "df_final_2\n",
      "21/217:\n",
      "df_final_2 = pd.concat([df_final, df_class], axis =1)\n",
      "df_final_2\n",
      "21/218: len(df_class)\n",
      "21/219: len(df_final)\n",
      "21/220: print(df_final_2[bioactivity_class].unique())\n",
      "21/221: print(df_final_2['bioactivity_class'].unique())\n",
      "21/222:\n",
      "df_class = pd.DataFrame(bioactivity_class)\n",
      "df_class\n",
      "df.to_csv('df_class.csv')\n",
      "21/223: df_class\n",
      "21/224:\n",
      "df_class = pd.DataFrame(bioactivity_class)\n",
      "df_class\n",
      "df_class.to_csv('df_class.csv')\n",
      "21/225: df_final,to_csv('df_final.csv')\n",
      "21/226: df_final.to_csv('df_final.csv')\n",
      "21/227:\n",
      "df_final_3 = df_final['molecule)chembl_id', 'canonical_smiles', 'MW', 'LogP','NumHDonors',\n",
      "                      'NumHAcceptors', 'NumRotableBonds', 'pIC50']\n",
      "21/228:\n",
      "df_final_3 = pd.DataFrame.df_final['molecule)chembl_id', 'canonical_smiles', 'MW', 'LogP','NumHDonors',\n",
      "                      'NumHAcceptors', 'NumRotableBonds', 'pIC50']\n",
      "21/229:\n",
      "df_final_3 = df_final['molecule)chembl_id', 'canonical_smiles', 'MW', 'LogP','NumHDonors',\n",
      "                      'NumHAcceptors', 'NumRotableBonds', 'pIC50']\n",
      "21/230:\n",
      "selection = ['molecule)chembl_id', 'canonical_smiles', 'MW', 'LogP','NumHDonors',\n",
      "                      'NumHAcceptors', 'NumRotableBonds', 'pIC50']\n",
      "df_final_3 = df_final['molecule)chembl_id', 'canonical_smiles', 'MW', 'LogP','NumHDonors',\n",
      "                      'NumHAcceptors', 'NumRotableBonds', 'pIC50']\n",
      "21/231:\n",
      "selection = ['molecule)chembl_id', 'canonical_smiles', 'MW', 'LogP','NumHDonors',\n",
      "                      'NumHAcceptors', 'NumRotableBonds', 'pIC50']\n",
      "df_final_3 = df_final[selection]\n",
      "21/232:\n",
      "selection = ['molecule_chembl_id', 'canonical_smiles', 'MW', 'LogP','NumHDonors',\n",
      "                      'NumHAcceptors', 'NumRotatableBonds', 'pIC50']\n",
      "df_final_3 = df_final[selection]\n",
      "21/233:\n",
      "selection = ['molecule_chembl_id', 'canonical_smiles', 'MW', 'LogP','NumHDonors',\n",
      "                      'NumHAcceptors', 'NumRotatableBonds', 'pIC50']\n",
      "df_final_3 = df_final[selection]\n",
      "df_final_3\n",
      "21/234:\n",
      "df_final_4 = pd.concat([df_final_3, df_class], axis =1)\n",
      "df_final_2\n",
      "21/235: pd.read_csv('df_final.csv')\n",
      "21/236:\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "21/237:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Histogram for MW\n",
      "hist, bins = np.histogram(MW, 50)\n",
      "width = 1 * (bins[1] - bins[0])\n",
      "center = (bins[:-1] + bins[1:]) / 2\n",
      "plt1.bar(center, hist, align='center', width=width, color='blue',edgecolor='black',\\\n",
      "         error_kw=dict(ecolor='gray', lw=2, capsize=5, capthick=2,linestyle='-', linewidth=0.5))\n",
      "21/238:\n",
      "#Here we got RO5 and pIC50 of 71 cpds agaisnt LAT1 protein\n",
      "\n",
      "Df_analysis = pd.read_csv('df_final.csv')\n",
      "21/239:\n",
      "#Here we got RO5 and pIC50 of 71 cpds agaisnt LAT1 protein\n",
      "\n",
      "df_analysis = pd.read_csv('df_final.csv')\n",
      "21/240:\n",
      "#Here we got RO5 and pIC50 of 71 cpds agaisnt LAT1 protein\n",
      "\n",
      "data = pd.read_csv('df_final.csv')\n",
      "21/241:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Histogram for MW\n",
      "plt.hist(data['MW'])\n",
      "21/242:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Histogram for MW\n",
      "plt.hist(data['MW'], frequency = False)\n",
      "21/243:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Histogram for MW\n",
      "plt.hist(data['MW'], density=True)\n",
      "21/244:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Histogram for MW\n",
      "plt.hist(data['MW'], density=False)\n",
      "21/245:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Histogram for MW\n",
      "plt.hist(data['MW'], density=False)\n",
      "ply.xlabel = Molecular Weight\n",
      "21/246:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Histogram for MW\n",
      "plt.hist(data['MW'], density=False)\n",
      "ply.xlabel(\"Molecular Weight\")\n",
      "21/247:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Histogram for MW\n",
      "plt.hist(data['MW'], density=False)\n",
      "plt.xlabel(\"Molecular Weight\")\n",
      "21/248:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Histogram for MW\n",
      "plt.hist(data['MW'], density=False)\n",
      "plt.xlabel(\"Molecular Weight\")\n",
      "plt.ylabel(\"Frequency\")\n",
      "21/249:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Histogram for MW\n",
      "plt.hist(data['MW'], density=True)\n",
      "plt.xlabel(\"Molecular Weight\")\n",
      "plt.ylabel(\"Frequency\")\n",
      "21/250:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Histogram for MW\n",
      "plt.hist(data['MW'], density=False, bins= 20)\n",
      "plt.xlabel(\"Molecular Weight\")\n",
      "plt.ylabel(\"Frequency\")\n",
      "21/251:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Histogram for MW\n",
      "plt.hist(data['MW'], density=False, bins= 50)\n",
      "plt.xlabel(\"Molecular Weight\")\n",
      "plt.ylabel(\"Frequency\")\n",
      "21/252:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Histogram for MW\n",
      "plt.hist(data['MW'], density=False, bins= 10)\n",
      "plt.xlabel(\"Molecular Weight\")\n",
      "plt.ylabel(\"Frequency\")\n",
      "21/253:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Histogram for MW\n",
      "plt.hist(data['MW'], density=False, bins= 5)\n",
      "plt.xlabel(\"Molecular Weight\")\n",
      "plt.ylabel(\"Frequency\")\n",
      "21/254:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Histogram for MW\n",
      "plt.hist(data['MW'], density=False, bins= 5)\n",
      "plt.xlabel(\"Molecular Weight\")\n",
      "plt.ylabel(\"Frequency\")\n",
      "21/255:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Histogram for MW\n",
      "plt.hist(data['MW'], density=False, bins= 15)\n",
      "plt.xlabel(\"Molecular Weight\")\n",
      "plt.ylabel(\"Frequency\")\n",
      "21/256:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Histogram for MW\n",
      "plt.hist(data['MW'], density=False, bins= 1)\n",
      "plt.xlabel(\"Molecular Weight\")\n",
      "plt.ylabel(\"Frequency\")\n",
      "21/257:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Histogram for MW\n",
      "plt.hist(data['MW'], density=False, bins= 10)\n",
      "plt.xlabel(\"Molecular Weight\")\n",
      "plt.ylabel(\"Frequency\")\n",
      "21/258:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Histogram for MW\n",
      "plt.hist(data['MW'], density=False, bins= 10)\n",
      "plt.xlabel(\"Molecular Weight\")\n",
      "plt.ylabel(\"Frequency\")\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "21/259:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Histogram for MW\n",
      "plt.hist(data['MW'], density=False, bins= 10)\n",
      "plt.xlabel(\"Molecular Weight\")\n",
      "plt.ylabel(\"Frequency\")\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "21/260:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Histogram for MW\n",
      "plt.hist(data['MW'], density=False, bins= 10)\n",
      "plt.xlabel(\"Molecular Weight\")\n",
      "plt.ylabel(\"Frequency\")\n",
      "21/261:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4)) = plt.subplots(2, 2)\n",
      "figure.set_size_inches(15,10)\n",
      "\n",
      "# Histogram for MW\n",
      "plt.hist(data['MW'], density=False, bins= 10)\n",
      "plt.xlabel(\"Molecular Weight\")\n",
      "plt.ylabel(\"Frequency\")\n",
      "21/262:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4)) = plt.subplots(2, 2)\n",
      "figure.set_size_inches(15,10)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 10)\n",
      "plt1.xlabel(\"Molecular Weight\")\n",
      "plt1.ylabel(\"Frequency\")\n",
      "21/263:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4)) = plt.subplots(2, 2)\n",
      "figure.set_size_inches(15,10)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 10)\n",
      "plt1.set_xlabel(\"Molecular Weight\")\n",
      "plt1.ylabel(\"Frequency\")\n",
      "21/264:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4)) = plt.subplots(2, 2)\n",
      "figure.set_size_inches(15,10)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 10)\n",
      "plt1.set_xlabel(\"Molecular Weight\")\n",
      "plt1.set_ylabel(\"Frequency\")\n",
      "21/265:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4)) = plt.subplots(2, 2)\n",
      "figure.set_size_inches(15,10)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 10)\n",
      "plt1.set_xlabel(\"Molecular Weight\")\n",
      "plt1.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt1.hist(data['LogP'], density=False, bins= 10)\n",
      "plt1.set_xlabel(\"LogP\")\n",
      "plt1.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt1.hist(data['NumHDonors'], density=False, bins= 10)\n",
      "plt1.set_xlabel(\"NumHDonors\")\n",
      "plt1.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt1.hist(data['NumHAcceptors'], density=False, bins= 10)\n",
      "plt1.set_xlabel(\"NumHAcceptors\")\n",
      "plt1.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt1.hist(data['NumRotatableBonds'], density=False, bins= 10)\n",
      "plt1.set_xlabel(\"NumRotatableBonds\")\n",
      "plt1.set_ylabel(\"Frequency\")\n",
      "21/266:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4)) = plt.subplots(2, 2)\n",
      "figure.set_size_inches(15,10)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 10)\n",
      "plt1.set_xlabel(\"Molecular Weight\")\n",
      "plt1.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 10)\n",
      "plt2.set_xlabel(\"LogP\")\n",
      "plt2.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 10)\n",
      "plt3.set_xlabel(\"NumHDonors\")\n",
      "plt3.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 10)\n",
      "plt4.set_xlabel(\"NumHAcceptors\")\n",
      "plt4.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 10)\n",
      "plt5.set_xlabel(\"NumRotatableBonds\")\n",
      "plt5.set_ylabel(\"Frequency\")\n",
      "21/267:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,)) = plt.subplots(3, 3)\n",
      "figure.set_size_inches(15,10)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 10)\n",
      "plt1.set_xlabel(\"Molecular Weight\")\n",
      "plt1.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 10)\n",
      "plt2.set_xlabel(\"LogP\")\n",
      "plt2.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 10)\n",
      "plt3.set_xlabel(\"NumHDonors\")\n",
      "plt3.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 10)\n",
      "plt4.set_xlabel(\"NumHAcceptors\")\n",
      "plt4.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 10)\n",
      "plt5.set_xlabel(\"NumRotatableBonds\")\n",
      "plt5.set_ylabel(\"Frequency\")\n",
      "21/268:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,)) = plt.subplots(2, 2)\n",
      "figure.set_size_inches(15,10)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 10)\n",
      "plt1.set_xlabel(\"Molecular Weight\")\n",
      "plt1.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 10)\n",
      "plt2.set_xlabel(\"LogP\")\n",
      "plt2.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 10)\n",
      "plt3.set_xlabel(\"NumHDonors\")\n",
      "plt3.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 10)\n",
      "plt4.set_xlabel(\"NumHAcceptors\")\n",
      "plt4.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 10)\n",
      "plt5.set_xlabel(\"NumRotatableBonds\")\n",
      "plt5.set_ylabel(\"Frequency\")\n",
      "21/269:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,)) = plt.subplots(2, 2)\n",
      "figure.set_size_inches(15,10)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 10)\n",
      "plt1.set_xlabel(\"Molecular Weight\")\n",
      "plt1.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 10)\n",
      "plt2.set_xlabel(\"LogP\")\n",
      "plt2.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 10)\n",
      "plt3.set_xlabel(\"NumHDonors\")\n",
      "plt3.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 10)\n",
      "plt4.set_xlabel(\"NumHAcceptors\")\n",
      "plt4.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 10)\n",
      "plt5.set_xlabel(\"NumRotatableBonds\")\n",
      "plt5.set_ylabel(\"Frequency\")\n",
      "21/270:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,10)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 10)\n",
      "plt1.set_xlabel(\"Molecular Weight\")\n",
      "plt1.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 10)\n",
      "plt2.set_xlabel(\"LogP\")\n",
      "plt2.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 10)\n",
      "plt3.set_xlabel(\"NumHDonors\")\n",
      "plt3.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 10)\n",
      "plt4.set_xlabel(\"NumHAcceptors\")\n",
      "plt4.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 10)\n",
      "plt5.set_xlabel(\"NumRotatableBonds\")\n",
      "plt5.set_ylabel(\"Frequency\")\n",
      "21/271:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,10)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 10)\n",
      "plt1.set_xlabel(\"Molecular Weight\")\n",
      "plt1.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 10)\n",
      "plt2.set_xlabel(\"LogP\")\n",
      "plt2.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 10)\n",
      "plt3.set_xlabel(\"NumHDonors\")\n",
      "plt3.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 10)\n",
      "plt4.set_xlabel(\"NumHAcceptors\")\n",
      "plt4.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 10)\n",
      "plt5.set_xlabel(\"NumRotatableBonds\")\n",
      "plt5.set_ylabel(\"Frequency\")\n",
      "21/272:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,10)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 10)\n",
      "plt1.set_xlabel(\"Molecular Weight\")\n",
      "plt1.set_ylabel(\"Frequency\")\n",
      "plt.axhline(y=0, x=500)\n",
      "\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 10)\n",
      "plt2.set_xlabel(\"LogP\")\n",
      "plt2.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 10)\n",
      "plt3.set_xlabel(\"NumHDonors\")\n",
      "plt3.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 10)\n",
      "plt4.set_xlabel(\"NumHAcceptors\")\n",
      "plt4.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 10)\n",
      "plt5.set_xlabel(\"NumRotatableBonds\")\n",
      "plt5.set_ylabel(\"Frequency\")\n",
      "21/273:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,10)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 10)\n",
      "plt1.set_xlabel(\"Molecular Weight\")\n",
      "plt1.set_ylabel(\"Frequency\")\n",
      "plt.axhline(y=0, xmax=500)\n",
      "\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 10)\n",
      "plt2.set_xlabel(\"LogP\")\n",
      "plt2.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 10)\n",
      "plt3.set_xlabel(\"NumHDonors\")\n",
      "plt3.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 10)\n",
      "plt4.set_xlabel(\"NumHAcceptors\")\n",
      "plt4.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 10)\n",
      "plt5.set_xlabel(\"NumRotatableBonds\")\n",
      "plt5.set_ylabel(\"Frequency\")\n",
      "21/274:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,10)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 10)\n",
      "plt1.set_xlabel(\"Molecular Weight\")\n",
      "plt1.set_ylabel(\"Frequency\")\n",
      "plt.axhline(y=0, xmax=500)\n",
      "\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 10)\n",
      "plt2.set_xlabel(\"LogP\")\n",
      "plt2.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 10)\n",
      "plt3.set_xlabel(\"NumHDonors\")\n",
      "plt3.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 10)\n",
      "plt4.set_xlabel(\"NumHAcceptors\")\n",
      "plt4.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 10)\n",
      "plt5.set_xlabel(\"NumRotatableBonds\")\n",
      "plt5.set_ylabel(\"Frequency\")\n",
      "21/275:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,10)\n",
      "\n",
      "# Histogram for MW\n",
      "hist, bins = np.histogram(MW, 50)\n",
      "width = 1 * (bins[1] - bins[0])\n",
      "center = (bins[:-1] + bins[1:]) / 2\n",
      "plt1.bar(center, hist, align='center', width=width, color='blue',edgecolor='black',\\\n",
      "         error_kw=dict(ecolor='gray', lw=2, capsize=5, capthick=2,linestyle='-', linewidth=0.5))\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel('Frequency', fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "#plt1.set_xlim(200,900)\n",
      "plt1.set_ylim(0, 1200)\n",
      "#plt1.grid(True)\n",
      "plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 10)\n",
      "plt2.set_xlabel(\"LogP\")\n",
      "plt2.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 10)\n",
      "plt3.set_xlabel(\"NumHDonors\")\n",
      "plt3.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 10)\n",
      "plt4.set_xlabel(\"NumHAcceptors\")\n",
      "plt4.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 10)\n",
      "plt5.set_xlabel(\"NumRotatableBonds\")\n",
      "plt5.set_ylabel(\"Frequency\")\n",
      "21/276:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,10)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 10)\n",
      "plt1.set_xlabel(\"Molecular Weight\")\n",
      "plt1.set_ylabel(\"Frequency\")\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "#plt1.set_xlim(200,900)\n",
      "plt1.set_ylim(0, 1200)\n",
      "#plt1.grid(True)\n",
      "plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 10)\n",
      "plt2.set_xlabel(\"LogP\")\n",
      "plt2.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 10)\n",
      "plt3.set_xlabel(\"NumHDonors\")\n",
      "plt3.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 10)\n",
      "plt4.set_xlabel(\"NumHAcceptors\")\n",
      "plt4.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 10)\n",
      "plt5.set_xlabel(\"NumRotatableBonds\")\n",
      "plt5.set_ylabel(\"Frequency\")\n",
      "21/277:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,10)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 10)\n",
      "plt1.set_xlabel(\"Molecular Weight\")\n",
      "plt1.set_ylabel(\"Frequency\")\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt1.set_ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 10)\n",
      "plt2.set_xlabel(\"LogP\")\n",
      "plt2.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 10)\n",
      "plt3.set_xlabel(\"NumHDonors\")\n",
      "plt3.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 10)\n",
      "plt4.set_xlabel(\"NumHAcceptors\")\n",
      "plt4.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 10)\n",
      "plt5.set_xlabel(\"NumRotatableBonds\")\n",
      "plt5.set_ylabel(\"Frequency\")\n",
      "21/278:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,10)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 10)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"Frequency\")\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt1.set_ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 10)\n",
      "plt2.set_xlabel(\"LogP\")\n",
      "plt2.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 10)\n",
      "plt3.set_xlabel(\"NumHDonors\")\n",
      "plt3.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 10)\n",
      "plt4.set_xlabel(\"NumHAcceptors\")\n",
      "plt4.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 10)\n",
      "plt5.set_xlabel(\"NumRotatableBonds\")\n",
      "plt5.set_ylabel(\"Frequency\")\n",
      "21/279:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(20,10)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 10)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"Frequency\")\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt1.set_ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 10)\n",
      "plt2.set_xlabel(\"LogP\")\n",
      "plt2.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 10)\n",
      "plt3.set_xlabel(\"NumHDonors\")\n",
      "plt3.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 10)\n",
      "plt4.set_xlabel(\"NumHAcceptors\")\n",
      "plt4.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 10)\n",
      "plt5.set_xlabel(\"NumRotatableBonds\")\n",
      "plt5.set_ylabel(\"Frequency\")\n",
      "21/280:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,20)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 10)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"Frequency\")\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt1.set_ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 10)\n",
      "plt2.set_xlabel(\"LogP\")\n",
      "plt2.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 10)\n",
      "plt3.set_xlabel(\"NumHDonors\")\n",
      "plt3.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 10)\n",
      "plt4.set_xlabel(\"NumHAcceptors\")\n",
      "plt4.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 10)\n",
      "plt5.set_xlabel(\"NumRotatableBonds\")\n",
      "plt5.set_ylabel(\"Frequency\")\n",
      "21/281:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 10)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"Frequency\")\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt1.set_ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 10)\n",
      "plt2.set_xlabel(\"LogP\")\n",
      "plt2.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 10)\n",
      "plt3.set_xlabel(\"NumHDonors\")\n",
      "plt3.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 10)\n",
      "plt4.set_xlabel(\"NumHAcceptors\")\n",
      "plt4.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 10)\n",
      "plt5.set_xlabel(\"NumRotatableBonds\")\n",
      "plt5.set_ylabel(\"Frequency\")\n",
      "21/282:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 10)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt1.set_ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 10)\n",
      "plt2.set_xlabel(\"LogP\")\n",
      "plt2.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 10)\n",
      "plt3.set_xlabel(\"NumHDonors\")\n",
      "plt3.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 10)\n",
      "plt4.set_xlabel(\"NumHAcceptors\")\n",
      "plt4.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 10)\n",
      "plt5.set_xlabel(\"NumRotatableBonds\")\n",
      "plt5.set_ylabel(\"Frequency\")\n",
      "21/283:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 10, color='blue',edgecolor='black',\\\n",
      "         error_kw=dict(ecolor='gray', lw=2, capsize=5, capthick=2,linestyle='-', linewidth=0.5)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt1.set_ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 10)\n",
      "plt2.set_xlabel(\"LogP\")\n",
      "plt2.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 10)\n",
      "plt3.set_xlabel(\"NumHDonors\")\n",
      "plt3.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 10)\n",
      "plt4.set_xlabel(\"NumHAcceptors\")\n",
      "plt4.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 10)\n",
      "plt5.set_xlabel(\"NumRotatableBonds\")\n",
      "plt5.set_ylabel(\"Frequency\")\n",
      "21/284:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 10, color='blue', edgecolor='black', linewidth=0.5)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt1.set_ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 10)\n",
      "plt2.set_xlabel(\"LogP\")\n",
      "plt2.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 10)\n",
      "plt3.set_xlabel(\"NumHDonors\")\n",
      "plt3.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 10)\n",
      "plt4.set_xlabel(\"NumHAcceptors\")\n",
      "plt4.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 10)\n",
      "plt5.set_xlabel(\"NumRotatableBonds\")\n",
      "plt5.set_ylabel(\"Frequency\")\n",
      "21/285:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 10, color='blue', edgecolor='black', linewidth=0.5)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt1.set_ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 10, color='blue', edgecolor='black', linewidth=0.5)\n",
      "plt2.set_xlabel(\"LogP\")\n",
      "plt2.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 10)\n",
      "plt3.set_xlabel(\"NumHDonors\")\n",
      "plt3.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 10)\n",
      "plt4.set_xlabel(\"NumHAcceptors\")\n",
      "plt4.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 10)\n",
      "plt5.set_xlabel(\"NumRotatableBonds\")\n",
      "plt5.set_ylabel(\"Frequency\")\n",
      "21/286:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 10, color='blue', edgecolor='black', linewidth=0.5)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt1.set_ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 10, color='red', edgecolor='black', linewidth=0.5)\n",
      "plt2.set_xlabel(\"LogP\")\n",
      "plt2.set_ylabel(\"Frequency\")\n",
      "plt2.set_ylim(0, 50)\n",
      "plt1.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "#Histogram for Log P\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 10)\n",
      "plt3.set_xlabel(\"NumHDonors\")\n",
      "plt3.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 10)\n",
      "plt4.set_xlabel(\"NumHAcceptors\")\n",
      "plt4.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 10)\n",
      "plt5.set_xlabel(\"NumRotatableBonds\")\n",
      "plt5.set_ylabel(\"Frequency\")\n",
      "21/287:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 10, color='blue', edgecolor='black', linewidth=0.5)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt1.set_ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 10, color='red', edgecolor='black', linewidth=0.5)\n",
      "plt2.set_xlabel(\"LogP\")\n",
      "plt2.set_ylabel(\"Frequency\")\n",
      "plt2.set_ylim(0, 50)\n",
      "plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "#Histogram for Log P\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 10)\n",
      "plt3.set_xlabel(\"NumHDonors\")\n",
      "plt3.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 10)\n",
      "plt4.set_xlabel(\"NumHAcceptors\")\n",
      "plt4.set_ylabel(\"Frequency\")\n",
      "\n",
      "#Histogram for Log P\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 10)\n",
      "plt5.set_xlabel(\"NumRotatableBonds\")\n",
      "plt5.set_ylabel(\"Frequency\")\n",
      "21/288:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 10, color='blue', edgecolor='black', linewidth=0.5)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt1.set_ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 10, color='red', edgecolor='black', linewidth=0.5)\n",
      "plt2.set_xlabel(\"LogP\", fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt2.set_ylim(0, 50)\n",
      "plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "\n",
      "#Histogram for NumHDonors\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 10, color='yellow', edgecolor='black', linewidth=0.5)\n",
      "plt3.set_xlabel(\"Number of Hydrogen Bond Donors\", fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt3.set_ylim(0, 50)\n",
      "plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 10, color='green', edgecolor='black', linewidth=0.5)\n",
      "plt4.set_xlabel(\"Number of Hydrogen Bond Acceptors\", fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt4.set_ylim(0, 50)\n",
      "plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 10, color='orange', edgecolor='black', linewidth=0.5)\n",
      "plt5.set_xlabel(\"Number of RotatableBonds\", fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt5.set_ylim(0, 50)\n",
      "plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "21/289:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 10, color='blue', edgecolor='black', linewidth=0.5)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt1.set_ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 10, color='red', edgecolor='black', linewidth=0.5)\n",
      "plt2.set_xlabel(\"LogP\", fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt2.set_ylim(0, 50)\n",
      "plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "\n",
      "#Histogram for NumHDonors\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 10, color='yellow', edgecolor='black', linewidth=0.5)\n",
      "plt3.set_xlabel(\"Number of Hydrogen Bond Donors\", fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt3.set_ylim(0, 50)\n",
      "plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 10, color='green', edgecolor='black', linewidth=0.5)\n",
      "plt4.set_xlabel(\"Number of Hydrogen Bond Acceptors\", fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt4.set_ylim(0, 50)\n",
      "plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 10, color='orange', edgecolor='black', linewidth=0.5)\n",
      "plt5.set_xlabel(\"Number of RotatableBonds\", fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt5.set_ylim(0, 50)\n",
      "plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "plt.savefig('Result/LAT1 plots of the descriptors.pdf', dpi=300)\n",
      "21/290:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 10, color='blue', edgecolor='black', linewidth=0.5)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt1.set_ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 10, color='red', edgecolor='black', linewidth=0.5)\n",
      "plt2.set_xlabel(\"LogP\", fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt2.set_ylim(0, 50)\n",
      "plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "\n",
      "#Histogram for NumHDonors\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 10, color='yellow', edgecolor='black', linewidth=0.5)\n",
      "plt3.set_xlabel(\"Number of Hydrogen Bond Donors\", fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt3.set_ylim(0, 50)\n",
      "plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 10, color='green', edgecolor='black', linewidth=0.5)\n",
      "plt4.set_xlabel(\"Number of Hydrogen Bond Acceptors\", fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt4.set_ylim(0, 50)\n",
      "plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 10, color='orange', edgecolor='black', linewidth=0.5)\n",
      "plt5.set_xlabel(\"Number of RotatableBonds\", fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt5.set_ylim(0, 50)\n",
      "plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "plt.savefig('LAT1 plots of the descriptors.pdf', dpi=300)\n",
      "21/291:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 10, color='blue', edgecolor='black', linewidth=0.5)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt1.set_ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 10, color='red', edgecolor='black', linewidth=0.5)\n",
      "plt2.set_xlabel(\"LogP\", fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt2.set_ylim(0, 50)\n",
      "plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "\n",
      "#Histogram for NumHDonors\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 10, color='yellow', edgecolor='black', linewidth=0.5)\n",
      "plt3.set_xlabel(\"Number of Hydrogen Bond Donors\", fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt3.set_ylim(0, 50)\n",
      "plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 10, color='green', edgecolor='black', linewidth=0.5)\n",
      "plt4.set_xlabel(\"Number of Hydrogen Bond Acceptors\", fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt4.set_ylim(0, 50)\n",
      "plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 10, color='orange', edgecolor='black', linewidth=0.5)\n",
      "plt5.set_xlabel(\"Number of RotatableBonds\", fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt5.set_ylim(0, 50)\n",
      "plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#save file\n",
      "plt.savefig('LAT1 plots of the descriptors.pdf', dpi=300)\n",
      "21/292:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 10, color='3EC1D3', edgecolor='black', linewidth=0.5)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt1.set_ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 10, color='red', edgecolor='black', linewidth=0.5)\n",
      "plt2.set_xlabel(\"LogP\", fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt2.set_ylim(0, 50)\n",
      "plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "\n",
      "#Histogram for NumHDonors\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 10, color='yellow', edgecolor='black', linewidth=0.5)\n",
      "plt3.set_xlabel(\"Number of Hydrogen Bond Donors\", fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt3.set_ylim(0, 50)\n",
      "plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 10, color='green', edgecolor='black', linewidth=0.5)\n",
      "plt4.set_xlabel(\"Number of Hydrogen Bond Acceptors\", fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt4.set_ylim(0, 50)\n",
      "plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 10, color='orange', edgecolor='black', linewidth=0.5)\n",
      "plt5.set_xlabel(\"Number of RotatableBonds\", fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt5.set_ylim(0, 50)\n",
      "plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#save file\n",
      "plt.savefig('LAT1 plots of the descriptors.pdf', dpi=300)\n",
      "21/293:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 10, color='#3EC1D3', edgecolor='black', linewidth=0.5)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt1.set_ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 10, color='red', edgecolor='black', linewidth=0.5)\n",
      "plt2.set_xlabel(\"LogP\", fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt2.set_ylim(0, 50)\n",
      "plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "\n",
      "#Histogram for NumHDonors\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 10, color='yellow', edgecolor='black', linewidth=0.5)\n",
      "plt3.set_xlabel(\"Number of Hydrogen Bond Donors\", fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt3.set_ylim(0, 50)\n",
      "plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 10, color='green', edgecolor='black', linewidth=0.5)\n",
      "plt4.set_xlabel(\"Number of Hydrogen Bond Acceptors\", fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt4.set_ylim(0, 50)\n",
      "plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 10, color='orange', edgecolor='black', linewidth=0.5)\n",
      "plt5.set_xlabel(\"Number of RotatableBonds\", fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt5.set_ylim(0, 50)\n",
      "plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#save file\n",
      "plt.savefig('LAT1 plots of the descriptors.pdf', dpi=300)\n",
      "21/294:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 10, color='#3EC1D3', edgecolor='black', linewidth=0.5)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt1.set_ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 10, color='#F6F7D7', edgecolor='black', linewidth=0.5)\n",
      "plt2.set_xlabel(\"LogP\", fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt2.set_ylim(0, 50)\n",
      "plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "\n",
      "#Histogram for NumHDonors\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 10, color='#FF9A00', edgecolor='black', linewidth=0.5)\n",
      "plt3.set_xlabel(\"Number of Hydrogen Bond Donors\", fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt3.set_ylim(0, 50)\n",
      "plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 10, color='#FF165D', edgecolor='black', linewidth=0.5)\n",
      "plt4.set_xlabel(\"Number of Hydrogen Bond Acceptors\", fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt4.set_ylim(0, 50)\n",
      "plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 10, color='#52006A', edgecolor='black', linewidth=0.5)\n",
      "plt5.set_xlabel(\"Number of RotatableBonds\", fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt5.set_ylim(0, 50)\n",
      "plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#save file\n",
      "plt.savefig('LAT1 plots of the descriptors.pdf', dpi=300)\n",
      "21/295:\n",
      "import seaborn as sns\n",
      "\n",
      "# visualize the relationship between the features and the response using scatterplots\n",
      "sns.pairplot(RO5, x_vars=['MW','LogP','nHAcc','nHDon'], y_vars='value', size=7, aspect=0.7)\n",
      "21/296: pip install seaborn\n",
      "21/297: pip install seaborn[all]\n",
      "21/298: import seaborn as sns\n",
      "21/299:\n",
      "import seaborn as sns\n",
      "\n",
      "# visualize the relationship between the features and the response using scatterplots\n",
      "sns.pairplot(RO5, x_vars=['MW','LogP','nHAcc','nHDon'], y_vars='pIC50', size=7, aspect=0.7)\n",
      "21/300:\n",
      "import seaborn as sns\n",
      "\n",
      "# visualize the relationship between the features and the response using scatterplots\n",
      "sns.pairplot(data, x_vars=['MW','LogP','nHAcc','nHDon'], y_vars='pIC50', size=7, aspect=0.7)\n",
      "21/301:\n",
      "import seaborn as sns\n",
      "\n",
      "# visualize the relationship between the features and the response using scatterplots\n",
      "sns.pairplot(data, x_vars=['MW','LogP','NumHDonors','Number of Hydrogen Bond Acceptors','NumRotatableBonds'], y_vars='pIC50', size=7, aspect=0.7)\n",
      "21/302:\n",
      "import seaborn as sns\n",
      "\n",
      "# visualize the relationship between the features and the response using scatterplots\n",
      "sns.pairplot(data, x_vars=['MW','LogP','NumHDonors','NumHAcceptors','NumRotatableBonds'], y_vars='pIC50', size=7, aspect=0.7)\n",
      "21/303:\n",
      "import seaborn as sns\n",
      "\n",
      "# visualize the relationship between the features and the response using scatterplots\n",
      "sns.pairplot(data, x_vars=['MW','LogP','NumHDonors','NumHAcceptors','NumRotatableBonds'], y_vars='pIC50', height=7, aspect=0.7)\n",
      "21/304:\n",
      "#Visuallize pIC50\n",
      "plt.hist(data['pIC50']\n",
      "21/305:\n",
      "#Visuallize pIC50\n",
      "plt.hist(data['pIC50'], density=False, bins= 10)\n",
      "21/306:\n",
      "#Visuallize pIC50\n",
      "plt.hist(data['pIC50'], density=False, bins= 10, color=#FF7600)\n",
      "21/307:\n",
      "#Visuallize pIC50\n",
      "plt.hist(data['pIC50'], density=False, bins= 10, color='#FF7600')\n",
      "21/308:\n",
      "#Visuallize pIC50\n",
      "plt.hist(data['pIC50'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "plt.set_xlabel('pIC50', fontsize=16, fontweight='bold')\n",
      "plt.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.set_ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "21/309:\n",
      "#Visuallize pIC50\n",
      "plt.hist(data['pIC50'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "plt.xlabel('pIC50', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.set_ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "21/310:\n",
      "#Visuallize pIC50\n",
      "plt.hist(data['pIC50'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "plt.xlabel('pIC50', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "21/311:\n",
      "#Visuallize pIC50\n",
      "plt.hist(data['pIC50'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "plt.xlabel('pIC50', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "21/312:\n",
      "#Visuallize pIC50\n",
      "plt.hist(data['pIC50'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "plt.xlabel('pIC50', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt.axvline(5,6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "21/313:\n",
      "#Visuallize pIC50\n",
      "plt.hist(data['pIC50'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "plt.xlabel('pIC50', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "21/314:\n",
      "#Visuallize pIC50\n",
      "plt.hist(data['pIC50'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "plt.xlabel('pIC50', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "21/315:\n",
      "#Visuallize pIC50\n",
      "plt.hist(data['pIC50'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "plt.xlabel('pIC50', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5, lab=inactive)\n",
      "plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "21/316:\n",
      "#Visuallize pIC50\n",
      "plt.hist(data['pIC50'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "plt.xlabel('pIC50', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5, lab='inactive')\n",
      "plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "21/317:\n",
      "#Visuallize pIC50\n",
      "plt.hist(data['pIC50'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "plt.xlabel('pIC50', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "plt.text(x, 5, 'hello', transform=trans\n",
      "21/318:\n",
      "#Visuallize pIC50\n",
      "plt.hist(data['pIC50'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "plt.xlabel('pIC50', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "plt.text(x, 5, 'hello', transform=trans)\n",
      "21/319:\n",
      "#Visuallize pIC50\n",
      "plt.hist(data['pIC50'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "plt.xlabel('pIC50', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "plt.text(5, 'hello', transform=trans)\n",
      "21/320:\n",
      "#Visuallize pIC50\n",
      "plt.hist(data['pIC50'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "plt.xlabel('pIC50', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "plt.text(5, 'hello', transform='trans')\n",
      "21/321:\n",
      "#Visuallize pIC50\n",
      "plt.hist(data['pIC50'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "plt.xlabel('pIC50', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "plt.texts(5, 'hello', transform='trans')\n",
      "21/322:\n",
      "#Visuallize pIC50\n",
      "plt.hist(data['pIC50'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "plt.xlabel('pIC50', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "21/323:\n",
      "#Visuallize pIC50\n",
      "plt.hist(data['pIC50'], density=False, bins= 20, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "plt.xlabel('pIC50', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "21/324:\n",
      "#Visuallize pIC50\n",
      "plt.hist(data['pIC50'], density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "plt.xlabel('pIC50', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "21/325:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt1.set_ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 10, color='#F6F7D7', edgecolor='black', linewidth=0.5)\n",
      "plt2.set_xlabel(\"LogP\", fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt2.set_ylim(0, 50)\n",
      "plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "\n",
      "#Histogram for NumHDonors\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 10, color='#FF9A00', edgecolor='black', linewidth=0.5)\n",
      "plt3.set_xlabel(\"Number of Hydrogen Bond Donors\", fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt3.set_ylim(0, 50)\n",
      "plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 10, color='#FF165D', edgecolor='black', linewidth=0.5)\n",
      "plt4.set_xlabel(\"Number of Hydrogen Bond Acceptors\", fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt4.set_ylim(0, 50)\n",
      "plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 10, color='#52006A', edgecolor='black', linewidth=0.5)\n",
      "plt5.set_xlabel(\"Number of RotatableBonds\", fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt5.set_ylim(0, 50)\n",
      "plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#save file\n",
      "plt.savefig('LAT1 plots of the descriptors.pdf', dpi=300)\n",
      "21/326:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt1.set_ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)\n",
      "plt2.set_xlabel(\"LogP\", fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt2.set_ylim(0, 50)\n",
      "plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "\n",
      "#Histogram for NumHDonors\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 30, color='#FF9A00', edgecolor='black', linewidth=0.5)\n",
      "plt3.set_xlabel(\"Number of Hydrogen Bond Donors\", fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt3.set_ylim(0, 50)\n",
      "plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 30, color='#FF165D', edgecolor='black', linewidth=0.5)\n",
      "plt4.set_xlabel(\"Number of Hydrogen Bond Acceptors\", fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt4.set_ylim(0, 50)\n",
      "plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 30, color='#52006A', edgecolor='black', linewidth=0.5)\n",
      "plt5.set_xlabel(\"Number of RotatableBonds\", fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt5.set_ylim(0, 50)\n",
      "plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#save file\n",
      "plt.savefig('LAT1 plots of the descriptors.pdf', dpi=300)\n",
      "21/327:\n",
      "#Visuallize pIC50\n",
      "plt.hist(data['pIC50'], density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "plt.xlabel('pIC50', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "plt.savefig('LAT1 plots-pIC50 distribution.pdf', dpi=300)\n",
      "21/328:\n",
      "#Visuallize pIC50\n",
      "plt.hist(data['pIC50'], density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "plt.xlabel('pIC50', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "fig.tight_layout()\n",
      "plt.savefig('LAT1 plots-pIC50 distribution.pdf', dpi=300)\n",
      "21/329:\n",
      "#Visuallize pIC50\n",
      "plt.hist(data['pIC50'], density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "plt.xlabel('pIC50', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.savefig('LAT1 plots-pIC50 distribution.pdf', dpi=300)\n",
      "21/330:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt1.set_ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)\n",
      "plt2.set_xlabel(\"LogP\", fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt2.set_ylim(0, 50)\n",
      "plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "\n",
      "#Histogram for NumHDonors\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 30, color='#FF9A00', edgecolor='black', linewidth=0.5)\n",
      "plt3.set_xlabel(\"Number of Hydrogen Bond Donors\", fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt3.set_ylim(0, 50)\n",
      "plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 30, color='#FF165D', edgecolor='black', linewidth=0.5)\n",
      "plt4.set_xlabel(\"Number of Hydrogen Bond Acceptors\", fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt4.set_ylim(0, 50)\n",
      "plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 30, color='#52006A', edgecolor='black', linewidth=0.5)\n",
      "plt5.set_xlabel(\"Number of RotatableBonds\", fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt5.set_ylim(0, 50)\n",
      "plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('LAT1 plots-basic descriptors.pdf', dpi=300)\n",
      "21/331:\n",
      "#Visuallize pIC50\n",
      "plt.hist(data['pIC50'], density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "plt.xlabel('pIC50', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('LAT1 plots-pIC50 distribution.pdf', dpi=300)\n",
      "24/1: ## Objective\n",
      "28/1:\n",
      "#How many of molecules are active against LAT1\n",
      "plt.scatter(data['pIC50'], density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "28/2:\n",
      "#import library\n",
      "import pandas as pd\n",
      "28/3:\n",
      "#Input yourname for this lab notebook\n",
      "labname = input('what is your name?')\n",
      "28/4: print(labname)\n",
      "28/5:\n",
      "#We will use Chembl database for molecular modeling\n",
      "#install package dependencies\n",
      "pip install requests_cache==0.6.4\n",
      "28/6:\n",
      "#We will use Chembl database for molecular modeling\n",
      "#install package dependencies\n",
      "pip install requests_cache==0.6.4\n",
      "28/7:\n",
      "#We will use Chembl database for molecular modeling\n",
      "#install package dependencies\n",
      "pip install requests_cache==0.6.4\n",
      "28/8:\n",
      "#We will use Chembl database for molecular modeling\n",
      "#install package dependencies\n",
      "!pip install requests_cache==0.6.4\n",
      "28/9:\n",
      "#Install Chemabl database\n",
      "pip install chembl_webresource_client\n",
      "28/10:\n",
      "#Install Chemabl database\n",
      "!pip install chembl_webresource_client\n",
      "28/11:\n",
      "#How many of molecules are active against LAT1\n",
      "plt.scatter(data['pIC50'], density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "28/12:\n",
      "import numpy as np\n",
      "import rdkit\n",
      "from time import time\n",
      "\n",
      "from sklearn.metrics import f1_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "import matplotlib.pyplot as plt\n",
      "28/13:\n",
      "#How many of molecules are active against LAT1\n",
      "plt.scatter(data['pIC50'], density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "28/14:\n",
      "#import library\n",
      "import pandas as pd\n",
      "28/15:\n",
      "#Input yourname for this lab notebook\n",
      "labname = input('what is your name?')\n",
      "28/16: print(labname)\n",
      "28/17:\n",
      "#We will use Chembl database for molecular modeling\n",
      "#install package dependencies\n",
      "!pip install requests_cache==0.6.4\n",
      "28/18:\n",
      "#Install Chemabl database\n",
      "!pip install chembl_webresource_client\n",
      "28/19:\n",
      "#Chembl API access...\n",
      "from chembl_webresource_client.new_client import new_client\n",
      "target = new_client.target\n",
      "#Search target protein, we will use LAT1 as a target protein\n",
      "target_query = target.search('LAT1')\n",
      "targets = pd.DataFrame.from_dict(target_query)\n",
      "targets\n",
      "28/20:\n",
      "#As we can see, we will select H.sapiens LAT1 as atarget\n",
      "selected_target = targets.target_chembl_id[1]\n",
      "selected_target\n",
      "28/21:\n",
      "#Here we will access the activity of compounds in this target, first we will try IC50\n",
      "#IC50 is tell us about the potency of the compound that can reduce LAT1 activity for 50%\n",
      "activity = new_client.activity\n",
      "res = activity.filter(target_chembl_id=selected_target).filter(standard_type=\"IC50\")\n",
      "28/22:\n",
      "#show dataframe of IC50\n",
      "df = pd.DataFrame.from_dict(res)\n",
      "df\n",
      "28/23:\n",
      "#if any compound is missing the IC50 value, then it should drop out\n",
      "df2 = df[df.standard_value.notna()]\n",
      "df2\n",
      "28/24:\n",
      "#I want to know what data do we have\n",
      "print(df2.columns)\n",
      "28/25:\n",
      "#we want to see the molecule structure via canonical_smiles\n",
      "df2.canonical_smiles\n",
      "28/26:\n",
      "#We want to know the structure and activity\n",
      "selection = ['molecule_chembl_id','canonical_smiles','standard_units', 'standard_value']\n",
      "df3 = df2[selection]\n",
      "df3\n",
      "28/27:\n",
      "df4 = df3[df.canonical_smiles.notna()]\n",
      "df4\n",
      "28/28:\n",
      "#install rdkit\n",
      "pip install rdkit-pypi\n",
      "28/29:\n",
      "#install rdkit\n",
      "!pip install rdkit-pypi\n",
      "28/30:\n",
      "import numpy as np\n",
      "import rdkit\n",
      "from time import time\n",
      "\n",
      "from sklearn.metrics import f1_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "import matplotlib.pyplot as plt\n",
      "28/31:\n",
      "#We are going to calculate Lipinski rule of five\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Descriptors, Lipinski\n",
      "28/32: print(df4['canonical_smiles'].unique())\n",
      "28/33:\n",
      "import pandas as pd\n",
      "def lipinski(smiles, verbose=False):\n",
      "\n",
      "    moldata= []\n",
      "    for elem in smiles:\n",
      "        mol=Chem.MolFromSmiles(elem) \n",
      "        moldata.append(mol)\n",
      "       \n",
      "    baseData= np.arange(1,1)\n",
      "    i=0  \n",
      "    for mol in moldata:        \n",
      "       \n",
      "        desc_MolWt = Descriptors.MolWt(mol)\n",
      "        desc_MolLogP = Descriptors.MolLogP(mol)\n",
      "        desc_NumHDonors = Lipinski.NumHDonors(mol)\n",
      "        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)\n",
      "        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)\n",
      "           \n",
      "        row = np.array([desc_MolWt,\n",
      "                        desc_MolLogP,\n",
      "                        desc_NumHDonors,\n",
      "                        desc_NumHAcceptors,\n",
      "                        desc_NumRotatableBonds])   \n",
      "    \n",
      "        if(i==0):\n",
      "            baseData=row\n",
      "        else:\n",
      "            baseData=np.vstack([baseData, row])\n",
      "        i=i+1      \n",
      "    \n",
      "    columnNames=[\"MW\",\"LogP\",\"NumHDonors\",\"NumHAcceptors\", \"NumRotatableBonds\"]   \n",
      "    descriptors = pd.DataFrame(data=baseData,columns=columnNames)\n",
      "    \n",
      "    return descriptors\n",
      "28/34:\n",
      "df_lipinski = lipinski(df4.canonical_smiles)\n",
      "df_lipinski\n",
      "df_lipinski\n",
      "28/35:\n",
      "#combine results\n",
      "df_combined = pd.concat([df3,df_lipinski], axis=1)\n",
      "df_combined\n",
      "28/36:\n",
      "#remove na\n",
      "final_df_combined = df_combined.replace(['None','NONE','none'],np.nan)\n",
      "final_df_combined = final_df_combined.dropna()\n",
      "print('Unique value in MW column',final_df_combined['MW'].unique())\n",
      "len(final_df_combined)\n",
      "28/37:\n",
      "#We want to know how many of them is high potency agaisnt LAT1 protein\n",
      "#First we should rearrange from IC50 into pIC50 for a better uniformnity distribution\n",
      "#We will convert IC50 to the negative logarithmic scale which is essentially -log10(IC50) or pIC50()\n",
      "# First Take the IC50 values from the standard_value column and converts it from nM to M by multiplying the value by 109\n",
      "# Take the molar value and apply -log10\n",
      "# Delete the standard_value column and create a new pIC50 column\n",
      "\n",
      "#before we do that ,we need to install nescessery package\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "def pIC50(input):\n",
      "    pIC50 = []\n",
      "\n",
      "    for i in input['standard_value_norm']:\n",
      "        molar = float(i)*(10**-9) # Converts nM to M\n",
      "        pIC50.append(-np.log10(molar))\n",
      "\n",
      "    input['pIC50'] = pIC50\n",
      "    x = input.drop('standard_value_norm', 1)\n",
      "        \n",
      "    return x\n",
      "28/38: final_df_combined.standard_value.describe()\n",
      "28/39:\n",
      "def norm_value(input):\n",
      "    norm = []\n",
      "\n",
      "    for i in input['standard_value']:\n",
      "        if float(i) > 100000000:\n",
      "          i = 100000000\n",
      "        norm.append(i)\n",
      "\n",
      "    input['standard_value_norm'] = norm\n",
      "    x = input.drop('standard_value', 1)\n",
      "        \n",
      "    return x\n",
      "28/40:\n",
      "df_norm = norm_value(final_df_combined)\n",
      "df_norm\n",
      "28/41: df_norm.standard_value_norm.describe()\n",
      "28/42:\n",
      "df_final = pIC50(df_norm)\n",
      "df_final\n",
      "28/43: -np.log10(0.000001)\n",
      "28/44: -np.log10(0.00001)\n",
      "28/45: -np.log10(0.00001)\n",
      "28/46:\n",
      "bioactivity_class = []\n",
      "for i in df_final.pIC50:\n",
      "  if float(i) >= 6:\n",
      "    bioactivity_class.append(\"inactive\")\n",
      "  elif float(i) <= 5:\n",
      "    bioactivity_class.append(\"active\")\n",
      "  else:\n",
      "    bioactivity_class.append(\"intermediate\")\n",
      "28/47: len(bioactivity_class)\n",
      "28/48: bioactivity_class\n",
      "28/49: len(df_final)\n",
      "28/50:\n",
      "df_class = pd.DataFrame(bioactivity_class)\n",
      "df_class\n",
      "df_class.to_csv('df_class.csv')\n",
      "28/51: df_class\n",
      "28/52: df_final.to_csv('df_final.csv')\n",
      "28/53:\n",
      "selection = ['molecule_chembl_id', 'canonical_smiles', 'MW', 'LogP','NumHDonors',\n",
      "                      'NumHAcceptors', 'NumRotatableBonds', 'pIC50']\n",
      "df_final_3 = df_final[selection]\n",
      "df_final_3\n",
      "28/54:\n",
      "df_final_4 = pd.concat([df_final_3, df_class], axis =1)\n",
      "df_final_2\n",
      "28/55:\n",
      "df_final_4 = pd.concat([df_final_3, df_class], axis =1)\n",
      "df_final_4\n",
      "28/56: print(df_final_4['bioactivity_class'].unique())\n",
      "28/57: print(df_final_4['bioactivity_class'].unique())\n",
      "28/58: print(df_final_2['bioactivity_class'].unique())\n",
      "28/59:\n",
      "#Here we got RO5 and pIC50 of 71 cpds agaisnt LAT1 protein\n",
      "\n",
      "data = pd.read_csv('df_final.csv')\n",
      "28/60:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt1.set_ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)\n",
      "plt2.set_xlabel(\"LogP\", fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt2.set_ylim(0, 50)\n",
      "plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "\n",
      "#Histogram for NumHDonors\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 30, color='#FF9A00', edgecolor='black', linewidth=0.5)\n",
      "plt3.set_xlabel(\"Number of Hydrogen Bond Donors\", fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt3.set_ylim(0, 50)\n",
      "plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 30, color='#FF165D', edgecolor='black', linewidth=0.5)\n",
      "plt4.set_xlabel(\"Number of Hydrogen Bond Acceptors\", fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt4.set_ylim(0, 50)\n",
      "plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 30, color='#52006A', edgecolor='black', linewidth=0.5)\n",
      "plt5.set_xlabel(\"Number of RotatableBonds\", fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt5.set_ylim(0, 50)\n",
      "plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('LAT1 plots-basic descriptors.pdf', dpi=300)\n",
      "28/61:\n",
      "#Visuallize pIC50\n",
      "plt.hist(data['pIC50'], density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "plt.xlabel('pIC50', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('LAT1 plots-pIC50 distribution.pdf', dpi=300)\n",
      "28/62:\n",
      "#Install statistic seabon\n",
      "pip install seaborn\n",
      "28/63:\n",
      "import seaborn as sns\n",
      "\n",
      "# visualize the relationship between the features and the response using scatterplots\n",
      "sns.pairplot(data, x_vars=['MW','LogP','NumHDonors','NumHAcceptors','NumRotatableBonds'], y_vars='pIC50', height=7, aspect=0.7)\n",
      "28/64:\n",
      "#How many of molecules are active against LAT1\n",
      "plt.scatter(data['pIC50'], density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "28/65:\n",
      "#How many of molecules are active against LAT1\n",
      "plt.scatter(data['pIC50', 'class'])\n",
      "28/66: print(data)\n",
      "28/67: df.data\n",
      "28/68: df_data\n",
      "28/69:\n",
      "df_final_4 = pd.concat([df_final_3, df_class], axis =1)\n",
      "df_final_4\n",
      "28/70:\n",
      "df_class = pd.DataFrame(bioactivity_class, header = True)\n",
      "df_class\n",
      "df_class.to_csv('df_class.csv')\n",
      "28/71:\n",
      "df_class = pd.DataFrame(bioactivity_class, head = True)\n",
      "df_class\n",
      "df_class.to_csv('df_class.csv')\n",
      "28/72:\n",
      "df_class = pd.DataFrame(bioactivity_class, columns=\"bioactivity_class\")\n",
      "df_class\n",
      "df_class.to_csv('df_class.csv')\n",
      "28/73:\n",
      "df_class = pd.DataFrame(bioactivity_class, name=\"bioactivity_class\")\n",
      "df_class\n",
      "df_class.to_csv('df_class.csv')\n",
      "28/74:\n",
      "df_class = pd.DataFrame(bioactivity_class, columns=['bioactivity_class'])\n",
      "df_class\n",
      "df_class.to_csv('df_class.csv')\n",
      "28/75: df_class\n",
      "28/76: df_final.to_csv('df_final.csv')\n",
      "28/77:\n",
      "selection = ['molecule_chembl_id', 'canonical_smiles', 'MW', 'LogP','NumHDonors',\n",
      "                      'NumHAcceptors', 'NumRotatableBonds', 'pIC50']\n",
      "df_final_3 = df_final[selection]\n",
      "df_final_3\n",
      "28/78:\n",
      "df_final_4 = pd.concat([df_final_3, df_class], axis =1)\n",
      "df_final_4\n",
      "28/79: print(df_final_2['bioactivity_class'].unique())\n",
      "28/80: print(df_final_4['bioactivity_class'].unique())\n",
      "28/81: df_final_2 = read.csv(\"df_final_adjusted\")\n",
      "28/82: df_final_2 = pd.read.csv(\"df_final_adjusted\")\n",
      "28/83: df_final_2 = pd.read_csv(\"df_final_adjusted\")\n",
      "28/84: df_final_2 = pd.read_csv(\"df_final_adjusted.csv\")\n",
      "28/85:\n",
      "df_final_2 = pd.read_csv(\"df_final_adjusted.csv\")\n",
      "df_final_2\n",
      "28/86:\n",
      "selection = ['molecule_chembl_id', 'canonical_smiles', 'MW', 'LogP','NumHDonors',\n",
      "                      'NumHAcceptors', 'NumRotatableBonds', 'pIC50']\n",
      "df_final_3 = df_final[selection]\n",
      "df_final_3\n",
      "28/87:\n",
      "df_final_4 = pd.concat([df_final_3, df_class], axis =1)\n",
      "df_final_4\n",
      "28/88:\n",
      "selection = ['molecule_chembl_id', 'canonical_smiles', 'MW', 'LogP','NumHDonors',\n",
      "                      'NumHAcceptors', 'NumRotatableBonds', 'pIC50']\n",
      "df_final_3 = df_final_adjiusted[selection]\n",
      "df_final_3\n",
      "28/89:\n",
      "selection = ['molecule_chembl_id', 'canonical_smiles', 'MW', 'LogP','NumHDonors',\n",
      "                      'NumHAcceptors', 'NumRotatableBonds', 'pIC50']\n",
      "df_final_3 = df_final_2[selection]\n",
      "df_final_3\n",
      "28/90:\n",
      "df_final_4 = pd.concat([df_final_3, df_class], axis =1)\n",
      "df_final_4\n",
      "28/91: print(df_final_4['bioactivity_class'].unique())\n",
      "28/92:\n",
      "df_final_4 = pd.concat([df_final_3, df_class], axis =1)\n",
      "df_final_4\n",
      "df_final_4.to_csv('df_final_4.csv')\n",
      "28/93: print(df_final_4['bioactivity_class'].unique())\n",
      "28/94:\n",
      "#Here we got RO5 and pIC50 of 71 cpds agaisnt LAT1 protein\n",
      "\n",
      "data = pd.read_csv('df_final_4.csv')\n",
      "28/95:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt1.set_ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)\n",
      "plt2.set_xlabel(\"LogP\", fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt2.set_ylim(0, 50)\n",
      "plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "\n",
      "#Histogram for NumHDonors\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 30, color='#FF9A00', edgecolor='black', linewidth=0.5)\n",
      "plt3.set_xlabel(\"Number of Hydrogen Bond Donors\", fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt3.set_ylim(0, 50)\n",
      "plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 30, color='#FF165D', edgecolor='black', linewidth=0.5)\n",
      "plt4.set_xlabel(\"Number of Hydrogen Bond Acceptors\", fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt4.set_ylim(0, 50)\n",
      "plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 30, color='#52006A', edgecolor='black', linewidth=0.5)\n",
      "plt5.set_xlabel(\"Number of RotatableBonds\", fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt5.set_ylim(0, 50)\n",
      "plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('LAT1 plots-basic descriptors.pdf', dpi=300)\n",
      "28/96:\n",
      "#Visuallize pIC50\n",
      "plt.hist(data['pIC50'], density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "plt.xlabel('pIC50', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('LAT1 plots-pIC50 distribution.pdf', dpi=300)\n",
      "28/97:\n",
      "#Install statistic seabon\n",
      "!pip install seaborn\n",
      "28/98:\n",
      "import seaborn as sns\n",
      "\n",
      "# visualize the relationship between the features and the response using scatterplots\n",
      "sns.pairplot(data, x_vars=['MW','LogP','NumHDonors','NumHAcceptors','NumRotatableBonds'], y_vars='pIC50', height=7, aspect=0.7)\n",
      "28/99:\n",
      "#How many of molecules are active against LAT1\n",
      "plt.scatter(data['pIC50', 'bioactivity_class'])\n",
      "28/100:\n",
      "#How many of molecules are active against LAT1\n",
      "plt.bar(data['pIC50', 'bioactivity_class'])\n",
      "28/101:\n",
      "#How many of molecules are active against LAT1\n",
      "plt.bar('pIC50', 'bioactivity_class')\n",
      "28/102:\n",
      "#How many of molecules are active against LAT1\n",
      "plt.bar(data['pIC50', 'bioactivity_class'])\n",
      "28/103:\n",
      "#How many of molecules are active against LAT1\n",
      "plt.bar(hist['pIC50', 'bioactivity_class'])\n",
      "28/104:\n",
      "#How many of molecules are active against LAT1\n",
      "plt.bar(his['pIC50', 'bioactivity_class'])\n",
      "28/105:\n",
      "#How many of molecules are active against LAT1\n",
      "plt.hist(data['pIC50', 'bioactivity_class'])\n",
      "28/106:\n",
      "#How many of molecules are active against LAT1\n",
      "plt.scatter(data['pIC50', 'bioactivity_class'])\n",
      "28/107:\n",
      "#How many of molecules are active against LAT1\n",
      "plt.hist(data['bioactivity_class'])\n",
      "28/108:\n",
      "#How many of molecules are active against LAT1\n",
      "plt.hist(data['bioactivity_class'] , density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "plt.xlabel('pIC50', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "28/109:\n",
      "#How many of molecules are active against LAT1\n",
      "plt.hist(data['bioactivity_class'] , density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "plt.xlabel('pIC50', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.ylim(0, 100)\n",
      "28/110: count(data['bioactivity_class'])\n",
      "28/111: data.count(bioactivity_class)\n",
      "28/112: data.count(\"bioactivity_class\")\n",
      "28/113: data\n",
      "28/114: data.bioactivity_class.describe()\n",
      "28/115: data.bioactivity_class(count)\n",
      "28/116: data.bioactivity_class.count()\n",
      "28/117: data.bioactivity_class.inactive.count()\n",
      "28/118: data.bioactivity_class.inactive()\n",
      "28/119: data.bioactivity_class.inactive.count()\n",
      "28/120: data.inactive.bioactivity_class.count()\n",
      "28/121: ply.scatter(data['bioactivity_class'])\n",
      "28/122: ply.scatter(data['pIC50', 'MW'])\n",
      "28/123: plt.scatter(data['pIC50', 'MW'])\n",
      "28/124: plt.scatter('pIC50', 'MW')\n",
      "28/125: plt.scatter(x='pIC50', y='MW')\n",
      "28/126: df.plt.scatter(x='pIC50', y='MW')\n",
      "28/127: df.plot.scatter(x='pIC50', y='MW')\n",
      "28/128: df.plt.scatter(x='pIC50', y='MW')\n",
      "28/129: plt.scatter(x='pIC50', y='MW')\n",
      "28/130: sns.pairplot(data, x_vars=['Bioactivity_class'], y_vars='pIC50', height=7, aspect=0.7)\n",
      "28/131: sns.pairplot(data, x_vars=['bioactivity_class'], y_vars='pIC50', height=7, aspect=0.7)\n",
      "28/132:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "28/133:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "\n",
      "x = data['MW']\n",
      "y = data['pIC50']\n",
      "colors = data['bioactivity_class']\n",
      "area = (30 * data['pIC50'])**2  # 0 to 15 point radii\n",
      "\n",
      "plt.scatter(x, y, s=area, c=colors, alpha=0.5)\n",
      "plt.show()\n",
      "28/134:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "\n",
      "x = data['MW']\n",
      "y = data['pIC50']\n",
      "colors = data['bioactivity_class']\n",
      "area = (30 * data['pIC50'])**2  # 0 to 15 point radii\n",
      "\n",
      "plt.scatter(x, y, s=area,  alpha=0.5)\n",
      "plt.show()\n",
      "28/135:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "\n",
      "x = data['MW']\n",
      "y = data['pIC50']\n",
      "colors = data['bioactivity_class']\n",
      "area = (30 * data['pIC50'])  # 0 to 15 point radii\n",
      "\n",
      "plt.scatter(x, y, s=area,  alpha=0.5)\n",
      "plt.show()\n",
      "28/136:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "\n",
      "x = data['MW']\n",
      "y = data['pIC50']\n",
      "colors = data['bioactivity_class']\n",
      "area = (30 * data['pIC50'])  # 0 to 15 point radii\n",
      "\n",
      "plt.scatter(x, y, s=area, c=colors, alpha=0.5)\n",
      "plt.show()\n",
      "28/137:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "\n",
      "x = data['MW']\n",
      "y = data['pIC50']\n",
      "colors = ['active', 'inactive', 'intermediate']\n",
      "area = (30 * data['pIC50'])  # 0 to 15 point radii\n",
      "\n",
      "plt.scatter(x, y, s=area, c=colors, alpha=0.5)\n",
      "plt.show()\n",
      "28/138:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "\n",
      "x = data['MW']\n",
      "y = data['pIC50']\n",
      "colors = ['active', 'inactive', 'intermediate']\n",
      "area = (30 * data['pIC50'])  # 0 to 15 point radii\n",
      "\n",
      "plt.scatter(x, y, s=area, c=df[colors], alpha=0.5)\n",
      "plt.show()\n",
      "28/139:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "\n",
      "x = data['MW']\n",
      "y = data['pIC50']\n",
      "colors = data['active', 'inactive', 'intermediate']\n",
      "area = (30 * data['pIC50'])  # 0 to 15 point radii\n",
      "\n",
      "plt.scatter(x, y, s=area, c=df[colors], alpha=0.5)\n",
      "plt.show()\n",
      "28/140:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "\n",
      "x = data['MW']\n",
      "y = data['pIC50']\n",
      "colors = data['active', 'inactive', 'intermediate']\n",
      "area = (30 * data['pIC50'])  # 0 to 15 point radii\n",
      "\n",
      "plt.scatter(x, y, s=area, c=df[colors], alpha=0.5)\n",
      "plt.show()\n",
      "28/141:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "\n",
      "x = data['MW']\n",
      "y = data['pIC50']\n",
      "colors = data['active', 'inactive', 'intermediate']\n",
      "area = (30 * data['pIC50'])  # 0 to 15 point radii\n",
      "\n",
      "plt.scatter(x, y, s=area, c=colors, alpha=0.5)\n",
      "plt.show()\n",
      "28/142: sns.lmplot('pIC50', 'MW', data=data, hue='bioactivity_class', fit_reg=False)\n",
      "28/143: sns.lmplot('MW', 'pIC50', data=data, hue='bioactivity_class', fit_reg=False)\n",
      "28/144:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "\n",
      "x = data['MW']\n",
      "y = data['pIC50']\n",
      "colors = data['active', 'inactive', 'intermediate']\n",
      "area = (30 * data['pIC50'])  # 0 to 15 point radii\n",
      "\n",
      "plt.scatter(x, y, s=area, alpha=0.5)\n",
      "plt.show()\n",
      "28/145:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "\n",
      "x = data['MW']\n",
      "y = data['pIC50']\n",
      "area = (30 * data['pIC50'])  # 0 to 15 point radii\n",
      "\n",
      "plt.scatter(x, y, s=area, alpha=0.5)\n",
      "plt.show()\n",
      "28/146:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "x = data['MW']\n",
      "y = data['pIC50']\n",
      "area = (30 * data['pIC50'])  # 0 to 15 point radii\n",
      "\n",
      "plt.scatter(x, y, s=area, alpha=0.5)\n",
      "plt.show()\n",
      "28/147: sns.lmplot('MW', 'pIC50', data=data, hue='bioactivity_class', size='pIC50' fit_reg=False)\n",
      "28/148: sns.lmplot('MW', 'pIC50', data=data, hue='bioactivity_class', size='pIC50', fit_reg=False)\n",
      "28/149: sns.lmplot('MW', 'pIC50', data=data, hue='bioactivity_class', area='pIC50', fit_reg=False)\n",
      "28/150: sns.lmplot('MW', 'pIC50', data=data, hue='bioactivity_class', size='pIC50', fit_reg=False)\n",
      "28/151: sns.lmplot('MW', 'pIC50', data=data, hue='bioactivity_class', size='pIC50', )\n",
      "28/152: sns.lmplot('MW', 'pIC50', data=data, hue='bioactivity_class', size='LogP', fit_reg=False)\n",
      "28/153: sns.lmplot('MW', 'pIC50', data=data, hue='bioactivity_class', fit_reg=False)\n",
      "28/154:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "x = data['MW']\n",
      "y = data['pIC50']\n",
      "area = (30 * data['pIC50'])  # 0 to 15 point radii\n",
      "\n",
      "plt.scatter(x, y, s=area, alpha=0.5)\n",
      "plt.xlabel('MW', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt.show()\n",
      "28/155:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "x = data['MW']\n",
      "y = data['pIC50']\n",
      "area = (30 * data['pIC50'])  # 0 to 15 point radii\n",
      "\n",
      "plt.scatter(x, y, s=area, alpha=0.5)\n",
      "plt.xlabel('MW', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.show()\n",
      "28/156:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "x = data['MW']\n",
      "y = data['pIC50']\n",
      "area = (30 * data['pIC50'])  # 0 to 15 point radii\n",
      "\n",
      "plt.scatter(x, y, s=area, alpha=0.5)\n",
      "plt.xlabel('MW', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.show()\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('LAT1 plots-pIC50 distribution.pdf', dpi=300)\n",
      "28/157:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "x = data['MW']\n",
      "y = data['pIC50']\n",
      "area = (30 * data['LogP'])  # 0 to 15 point radii\n",
      "\n",
      "plt.scatter(x, y, s=area, alpha=0.5)\n",
      "plt.xlabel('MW', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.show()\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('LAT1 plots-pIC50 distribution.pdf', dpi=300)\n",
      "28/158:\n",
      "#Visuallize pIC50\n",
      "plt.hist(data['pIC50'], density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "plt.xlabel('pIC50', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('LAT1 plots-pIC50 distribution.pdf', dpi=300)\n",
      "28/159:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "x = data['MW']\n",
      "y = data['pIC50']\n",
      "area = (30 * data['bioactivity_class'])  # 0 to 15 point radii\n",
      "\n",
      "plt.scatter(x, y, s=area, alpha=0.5)\n",
      "plt.xlabel('MW', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.show()\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('MW_and_pIC50.pdf', dpi=300)\n",
      "28/160:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "x = data['MW']\n",
      "y = data['pIC50']\n",
      "area = (30 * data['LogP'])  # 0 to 15 point radii\n",
      "\n",
      "plt.scatter(x, y, s=area, c=data['bioactivity_class'] alpha=0.5)\n",
      "plt.xlabel('MW', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.show()\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('MW_and_pIC50.pdf', dpi=300)\n",
      "28/161:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "x = data['MW']\n",
      "y = data['pIC50']\n",
      "area = (30 * data['LogP'])  # 0 to 15 point radii\n",
      "\n",
      "plt.scatter(x, y, s=area, c=data['bioactivity_class'], alpha=0.5)\n",
      "plt.xlabel('MW', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.show()\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('MW_and_pIC50.pdf', dpi=300)\n",
      "28/162:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "x = data['MW']\n",
      "y = data['pIC50']\n",
      "colors = data.groupby(data['bioactivity_class'])\n",
      "area = (30 * data['LogP'])  # 0 to 15 point radii\n",
      "\n",
      "plt.scatter(x, y, s=area, c=colors, alpha=0.5)\n",
      "plt.xlabel('MW', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.show()\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('MW_and_pIC50.pdf', dpi=300)\n",
      "28/163:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "x = data['MW']\n",
      "y = data['pIC50']\n",
      "#colors = data.groupby(data['bioactivity_class'])\n",
      "area = (30 * data['LogP'])  # 0 to 15 point radii\n",
      "\n",
      "plt.scatter(x, y, s=area, , alpha=0.5)\n",
      "plt.xlabel('MW', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.show()\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('MW_and_pIC50.pdf', dpi=300)\n",
      "28/164:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "x = data['MW']\n",
      "y = data['pIC50']\n",
      "#colors = data.groupby(data['bioactivity_class'])\n",
      "area = (30 * data['LogP'])  # 0 to 15 point radii\n",
      "\n",
      "plt.scatter(x, y, s=area, alpha=0.5)\n",
      "plt.xlabel('MW', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.show()\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('MW_and_pIC50.pdf', dpi=300)\n",
      "28/165:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "x = data['MW']\n",
      "y = data['pIC50']\n",
      "#colors = data.groupby(data['bioactivity_class'])\n",
      "area = (30 * data['LogP'])  # 0 to 15 point radii\n",
      "\n",
      "plt.scatter(x, y, s=area, alpha=0.5)\n",
      "plt.xlabel('MW', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.show()\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('MW_and_pIC50_LogP.pdf', dpi=300)\n",
      "28/166:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "x = data['MW']\n",
      "y = data['pIC50']\n",
      "#colors = data.groupby(data['bioactivity_class'])\n",
      "area = (30 * data['LogP'])  # 0 to 15 point radii\n",
      "\n",
      "plt.scatter(x, y, s=area, alpha=0.5)\n",
      "plt.xlabel('MW', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.legend(area)\n",
      "plt.show()\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('MW_and_pIC50_LogP.pdf', dpi=300)\n",
      "28/167:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "x = data['MW']\n",
      "y = data['pIC50']\n",
      "#colors = data.groupby(data['bioactivity_class'])\n",
      "area = (30 * data['LogP'])  # 0 to 15 point radii\n",
      "\n",
      "plt.scatter(x, y, s=area, alpha=0.5)\n",
      "plt.xlabel('MW', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.legend('area')\n",
      "plt.show()\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('MW_and_pIC50_LogP.pdf', dpi=300)\n",
      "28/168:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "x = data['MW']\n",
      "y = data['pIC50']\n",
      "#colors = data.groupby(data['bioactivity_class'])\n",
      "area = (30 * data['LogP'])  # 0 to 15 point radii\n",
      "\n",
      "plt.scatter(x, y, s=area, alpha=0.5)\n",
      "plt.xlabel('MW', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.legend('LogP')\n",
      "plt.show()\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('MW_and_pIC50_LogP.pdf', dpi=300)\n",
      "28/169:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "x = data['MW']\n",
      "y = data['pIC50']\n",
      "#colors = data.groupby(data['bioactivity_class'])\n",
      "area = (30 * data['LogP'])  # 0 to 15 point radii\n",
      "\n",
      "plt.scatter(x, y, s=area, alpha=0.5)\n",
      "plt.xlabel('MW', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.legend()\n",
      "plt.show()\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('MW_and_pIC50_LogP.pdf', dpi=300)\n",
      "28/170:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "x = data['MW']\n",
      "y = data['pIC50']\n",
      "#colors = data.groupby(data['bioactivity_class'])\n",
      "area = (30 * data['LogP'])  # 0 to 15 point radii\n",
      "\n",
      "plt.scatter(x, y, s=area, alpha=0.5)\n",
      "plt.xlabel('MW', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.legend('LogP', )\n",
      "plt.show()\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('MW_and_pIC50_LogP.pdf', dpi=300)\n",
      "28/171:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "x = data['MW']\n",
      "y = data['pIC50']\n",
      "#colors = data.groupby(data['bioactivity_class'])\n",
      "area = (30 * data['LogP'])  # 0 to 15 point radii\n",
      "\n",
      "plt.scatter(x, y, s=area, alpha=0.5)\n",
      "plt.xlabel('MW', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.legend('LogP',)\n",
      "plt.show()\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('MW_and_pIC50_LogP.pdf', dpi=300)\n",
      "29/1: sns.lmplot('MW', 'pIC50', data=data, hue='bioactivity_class', fit_reg=False)\n",
      "29/2:\n",
      "#import library\n",
      "import pandas as pd\n",
      "29/3:\n",
      "#Input yourname for this lab notebook\n",
      "labname = input('what is your name?')\n",
      "29/4: print(labname)\n",
      "29/5:\n",
      "#We will use Chembl database for molecular modeling\n",
      "#install package dependencies\n",
      "!pip install requests_cache==0.6.4\n",
      "29/6:\n",
      "#Install Chemabl database\n",
      "!pip install chembl_webresource_client\n",
      "29/7:\n",
      "#Chembl API access...\n",
      "from chembl_webresource_client.new_client import new_client\n",
      "target = new_client.target\n",
      "#Search target protein, we will use LAT1 as a target protein\n",
      "target_query = target.search('LAT1')\n",
      "targets = pd.DataFrame.from_dict(target_query)\n",
      "targets\n",
      "29/8:\n",
      "#As we can see, we will select H.sapiens LAT1 as atarget\n",
      "selected_target = targets.target_chembl_id[1]\n",
      "selected_target\n",
      "29/9:\n",
      "#Here we will access the activity of compounds in this target, first we will try IC50\n",
      "#IC50 is tell us about the potency of the compound that can reduce LAT1 activity for 50%\n",
      "activity = new_client.activity\n",
      "res = activity.filter(target_chembl_id=selected_target).filter(standard_type=\"IC50\")\n",
      "29/10:\n",
      "#show dataframe of IC50\n",
      "df = pd.DataFrame.from_dict(res)\n",
      "df\n",
      "29/11:\n",
      "#if any compound is missing the IC50 value, then it should drop out\n",
      "df2 = df[df.standard_value.notna()]\n",
      "df2\n",
      "29/12:\n",
      "#I want to know what data do we have\n",
      "print(df2.columns)\n",
      "29/13:\n",
      "#we want to see the molecule structure via canonical_smiles\n",
      "df2.canonical_smiles\n",
      "29/14:\n",
      "#We want to know the structure and activity\n",
      "selection = ['molecule_chembl_id','canonical_smiles','standard_units', 'standard_value']\n",
      "df3 = df2[selection]\n",
      "df3\n",
      "29/15:\n",
      "df4 = df3[df.canonical_smiles.notna()]\n",
      "df4\n",
      "29/16:\n",
      "#install rdkit\n",
      "!pip install rdkit-pypi\n",
      "29/17:\n",
      "import numpy as np\n",
      "import rdkit\n",
      "from time import time\n",
      "\n",
      "from sklearn.metrics import f1_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "import matplotlib.pyplot as plt\n",
      "29/18:\n",
      "#We are going to calculate Lipinski rule of five\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Descriptors, Lipinski\n",
      "29/19: print(df4['canonical_smiles'].unique())\n",
      "29/20:\n",
      "import pandas as pd\n",
      "def lipinski(smiles, verbose=False):\n",
      "\n",
      "    moldata= []\n",
      "    for elem in smiles:\n",
      "        mol=Chem.MolFromSmiles(elem) \n",
      "        moldata.append(mol)\n",
      "       \n",
      "    baseData= np.arange(1,1)\n",
      "    i=0  \n",
      "    for mol in moldata:        \n",
      "       \n",
      "        desc_MolWt = Descriptors.MolWt(mol)\n",
      "        desc_MolLogP = Descriptors.MolLogP(mol)\n",
      "        desc_NumHDonors = Lipinski.NumHDonors(mol)\n",
      "        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)\n",
      "        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)\n",
      "           \n",
      "        row = np.array([desc_MolWt,\n",
      "                        desc_MolLogP,\n",
      "                        desc_NumHDonors,\n",
      "                        desc_NumHAcceptors,\n",
      "                        desc_NumRotatableBonds])   \n",
      "    \n",
      "        if(i==0):\n",
      "            baseData=row\n",
      "        else:\n",
      "            baseData=np.vstack([baseData, row])\n",
      "        i=i+1      \n",
      "    \n",
      "    columnNames=[\"MW\",\"LogP\",\"NumHDonors\",\"NumHAcceptors\", \"NumRotatableBonds\"]   \n",
      "    descriptors = pd.DataFrame(data=baseData,columns=columnNames)\n",
      "    \n",
      "    return descriptors\n",
      "29/21:\n",
      "df_lipinski = lipinski(df4.canonical_smiles)\n",
      "df_lipinski\n",
      "df_lipinski\n",
      "29/22:\n",
      "#combine results\n",
      "df_combined = pd.concat([df3,df_lipinski], axis=1)\n",
      "df_combined\n",
      "29/23:\n",
      "#remove na\n",
      "final_df_combined = df_combined.replace(['None','NONE','none'],np.nan)\n",
      "final_df_combined = final_df_combined.dropna()\n",
      "print('Unique value in MW column',final_df_combined['MW'].unique())\n",
      "len(final_df_combined)\n",
      "29/24: final_df_combined\n",
      "29/25:\n",
      "#We want to know how many of them is high potency agaisnt LAT1 protein\n",
      "#First we should rearrange from IC50 into pIC50 for a better uniformnity distribution\n",
      "#We will convert IC50 to the negative logarithmic scale which is essentially -log10(IC50) or pIC50()\n",
      "# First Take the IC50 values from the standard_value column and converts it from nM to M by multiplying the value by 109\n",
      "# Take the molar value and apply -log10\n",
      "# Delete the standard_value column and create a new pIC50 column\n",
      "\n",
      "#before we do that ,we need to install nescessery package\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "def pIC50(input):\n",
      "    pIC50 = []\n",
      "\n",
      "    for i in input['standard_value_norm']:\n",
      "        molar = float(i)*(10**-9) # Converts nM to M\n",
      "        pIC50.append(-np.log10(molar))\n",
      "\n",
      "    input['pIC50'] = pIC50\n",
      "    x = input.drop('standard_value_norm', 1)\n",
      "        \n",
      "    return x\n",
      "29/26: final_df_combined.standard_value.describe()\n",
      "29/27:\n",
      "def norm_value(input):\n",
      "    norm = []\n",
      "\n",
      "    for i in input['standard_value']:\n",
      "        if float(i) > 100000000:\n",
      "          i = 100000000\n",
      "        norm.append(i)\n",
      "\n",
      "    input['standard_value_norm'] = norm\n",
      "    x = input.drop('standard_value', 1)\n",
      "        \n",
      "    return x\n",
      "29/28:\n",
      "df_norm = norm_value(final_df_combined)\n",
      "df_norm\n",
      "29/29: df_norm.standard_value_norm.describe()\n",
      "29/30:\n",
      "df_final = pIC50(df_norm)\n",
      "df_final\n",
      "29/31: -np.log10(0.000001)\n",
      "29/32: -np.log10(0.00001)\n",
      "29/33:\n",
      "bioactivity_class = []\n",
      "for i in df_final.pIC50:\n",
      "  if float(i) >= 6:\n",
      "    bioactivity_class.append(\"inactive\")\n",
      "  elif float(i) <= 5:\n",
      "    bioactivity_class.append(\"active\")\n",
      "  else:\n",
      "    bioactivity_class.append(\"intermediate\")\n",
      "29/34: len(bioactivity_class)\n",
      "29/35: bioactivity_class\n",
      "29/36: len(df_final)\n",
      "29/37:\n",
      "df_class = pd.DataFrame(bioactivity_class, columns=['bioactivity_class'])\n",
      "df_class\n",
      "df_class.to_csv('df_class.csv')\n",
      "29/38: df_class\n",
      "29/39: df_final.to_csv('df_final.csv')\n",
      "29/40:\n",
      "df_final_2 = pd.read_csv(\"df_final_adjusted.csv\")\n",
      "df_final_2\n",
      "29/41:\n",
      "selection = ['molecule_chembl_id', 'canonical_smiles', 'MW', 'LogP','NumHDonors',\n",
      "                      'NumHAcceptors', 'NumRotatableBonds', 'pIC50']\n",
      "df_final_3 = df_final_2[selection]\n",
      "df_final_3\n",
      "29/42:\n",
      "df_final_4 = pd.concat([df_final_3, df_class], axis =1)\n",
      "df_final_4\n",
      "df_final_4.to_csv('df_final_4.csv')\n",
      "29/43: print(df_final_4['bioactivity_class'].unique())\n",
      "29/44:\n",
      "#Here we got RO5 and pIC50 of 71 cpds agaisnt LAT1 protein\n",
      "\n",
      "data = pd.read_csv('df_final_4.csv')\n",
      "29/45:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt1.set_ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)\n",
      "plt2.set_xlabel(\"LogP\", fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt2.set_ylim(0, 50)\n",
      "plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "\n",
      "#Histogram for NumHDonors\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 30, color='#FF9A00', edgecolor='black', linewidth=0.5)\n",
      "plt3.set_xlabel(\"Number of Hydrogen Bond Donors\", fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt3.set_ylim(0, 50)\n",
      "plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 30, color='#FF165D', edgecolor='black', linewidth=0.5)\n",
      "plt4.set_xlabel(\"Number of Hydrogen Bond Acceptors\", fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt4.set_ylim(0, 50)\n",
      "plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 30, color='#52006A', edgecolor='black', linewidth=0.5)\n",
      "plt5.set_xlabel(\"Number of RotatableBonds\", fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt5.set_ylim(0, 50)\n",
      "plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('LAT1 plots-basic descriptors.pdf', dpi=300)\n",
      "29/46:\n",
      "#Visuallize pIC50\n",
      "plt.hist(data['pIC50'], density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "plt.xlabel('pIC50', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('LAT1 plots-pIC50 distribution.pdf', dpi=300)\n",
      "29/47:\n",
      "#Install statistic seabon\n",
      "!pip install seaborn\n",
      "29/48:\n",
      "import seaborn as sns\n",
      "\n",
      "# visualize the relationship between the features and the response using scatterplots\n",
      "sns.pairplot(data, x_vars=['MW','LogP','NumHDonors','NumHAcceptors','NumRotatableBonds'], y_vars='pIC50', height=7, aspect=0.7)\n",
      "29/49:\n",
      "#How many of molecules are active against LAT1\n",
      "plt.hist(data['bioactivity_class'] , density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "plt.xlabel('pIC50', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.ylim(0, 100)\n",
      "29/50: sns.pairplot(data, x_vars=['bioactivity_class'], y_vars='pIC50', height=7, aspect=0.7)\n",
      "29/51:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "x = data['MW']\n",
      "y = data['pIC50']\n",
      "#colors = data.groupby(data['bioactivity_class'])\n",
      "area = (30 * data['LogP'])  # 0 to 15 point radii\n",
      "\n",
      "plt.scatter(x, y, s=area, alpha=0.5)\n",
      "plt.xlabel('MW', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.legend('LogP', )\n",
      "plt.show()\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('MW_and_pIC50_LogP.pdf', dpi=300)\n",
      "29/52: sns.lmplot('MW', 'pIC50', data=data, hue='bioactivity_class', fit_reg=False)\n",
      "29/53:\n",
      "bioactivity_class = []\n",
      "for i in df_final.pIC50:\n",
      "  if float(i) >= 6:\n",
      "    bioactivity_class.append(\"inactive\")\n",
      "  elif float(i) <= 4:\n",
      "    bioactivity_class.append(\"active\")\n",
      "  else:\n",
      "    bioactivity_class.append(\"intermediate\")\n",
      "29/54: len(bioactivity_class)\n",
      "29/55: bioactivity_class\n",
      "29/56: len(df_final)\n",
      "29/57:\n",
      "df_class = pd.DataFrame(bioactivity_class, columns=['bioactivity_class'])\n",
      "df_class\n",
      "df_class.to_csv('df_class.csv')\n",
      "29/58: df_class\n",
      "29/59: -np.log10(0.000112)\n",
      "29/60: -np.log10(0.0000182)\n",
      "29/61:\n",
      "bioactivity_class = []\n",
      "for i in df_final.pIC50:\n",
      "  if float(i) >= 6:\n",
      "    bioactivity_class.append(\"inactive\")\n",
      "  elif float(i) <= 5:\n",
      "    bioactivity_class.append(\"active\")\n",
      "  else:\n",
      "    bioactivity_class.append(\"intermediate\")\n",
      "29/62:\n",
      "#active IC50 < 1000\n",
      "-np.log10(0.000001)\n",
      "29/63: -np.log10(0.00001)\n",
      "29/64:\n",
      "bioactivity_class = []\n",
      "for i in df_final.pIC50:\n",
      "  if float(i) >= 6:\n",
      "    bioactivity_class.append(\"active\")\n",
      "  elif float(i) <= 5:\n",
      "    bioactivity_class.append(\"inactive\")\n",
      "  else:\n",
      "    bioactivity_class.append(\"intermediate\")\n",
      "29/65: len(bioactivity_class)\n",
      "29/66: len(bioactivity_class)\n",
      "29/67: bioactivity_class\n",
      "29/68: len(df_final)\n",
      "29/69:\n",
      "df_class = pd.DataFrame(bioactivity_class, columns=['bioactivity_class'])\n",
      "df_class\n",
      "df_class.to_csv('df_class.csv')\n",
      "29/70: df_class\n",
      "29/71:\n",
      "df_final = pIC50(df_norm)\n",
      "df_final\n",
      "df_final.to_csv('df_final.csv')\n",
      "29/72:\n",
      "df_final = pIC50(df_norm)\n",
      "df_final\n",
      "29/73: df_final.to_csv('df_final.csv')\n",
      "29/74: df_class\n",
      "29/75: df_class.count()\n",
      "29/76: df_class.describe()\n",
      "29/77:\n",
      "df_final_2 = pd.read_csv(\"df_final_adjusted.csv\")\n",
      "df_final_2\n",
      "29/78:\n",
      "df_final_2 = pd.read_csv(\"df_final.csv\")\n",
      "df_final_2\n",
      "29/79:\n",
      "selection = ['molecule_chembl_id', 'canonical_smiles', 'MW', 'LogP','NumHDonors',\n",
      "                      'NumHAcceptors', 'NumRotatableBonds', 'pIC50']\n",
      "df_final_3 = df_final_2[selection]\n",
      "df_final_3\n",
      "29/80:\n",
      "df_final_4 = pd.concat([df_final_3, df_class], axis =1)\n",
      "df_final_4\n",
      "29/81: print(df_final_4['bioactivity_class'].unique())\n",
      "29/82: df_final_4.to_csv('df_final_4.csv')\n",
      "29/83:\n",
      "#Here we got RO5 and pIC50 of 71 cpds against LAT1 protein\n",
      "data = pd.read_csv('df_final_4.csv')\n",
      "29/84:\n",
      "#Here we got RO5 and pIC50 of 71 cpds against LAT1 protein\n",
      "data = pd.read_csv('df_final_4.csv')\n",
      "29/85:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt1.set_ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)\n",
      "plt2.set_xlabel(\"LogP\", fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt2.set_ylim(0, 50)\n",
      "plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "\n",
      "#Histogram for NumHDonors\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 30, color='#FF9A00', edgecolor='black', linewidth=0.5)\n",
      "plt3.set_xlabel(\"Number of Hydrogen Bond Donors\", fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt3.set_ylim(0, 50)\n",
      "plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 30, color='#FF165D', edgecolor='black', linewidth=0.5)\n",
      "plt4.set_xlabel(\"Number of Hydrogen Bond Acceptors\", fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt4.set_ylim(0, 50)\n",
      "plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 30, color='#52006A', edgecolor='black', linewidth=0.5)\n",
      "plt5.set_xlabel(\"Number of RotatableBonds\", fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt5.set_ylim(0, 50)\n",
      "plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('LAT1 plots-basic descriptors.pdf', dpi=300)\n",
      "29/86:\n",
      "import seaborn as sns\n",
      "\n",
      "# visualize the relationship between the features and the response using scatterplots\n",
      "sns.pairplot(data, x_vars=['MW','LogP','NumHDonors','NumHAcceptors','NumRotatableBonds'], y_vars='pIC50', height=7, aspect=0.7)\n",
      "29/87:\n",
      "#How many of molecules are active against LAT1\n",
      "plt.hist(data['bioactivity_class'] , density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "plt.xlabel('pIC50', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.ylim(0, 100)\n",
      "29/88: sns.pairplot(data, x_vars=['bioactivity_class'], y_vars='pIC50', height=7, aspect=0.7)\n",
      "29/89:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "x = data['MW']\n",
      "y = data['pIC50']\n",
      "#colors = data.groupby(data['bioactivity_class'])\n",
      "area = (30 * data['LogP'])  # 0 to 15 point radii\n",
      "\n",
      "plt.scatter(x, y, s=area, alpha=0.5)\n",
      "plt.xlabel('MW', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.legend('LogP', )\n",
      "plt.show()\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('MW_and_pIC50_LogP.pdf', dpi=300)\n",
      "29/90: sns.lmplot('MW', 'pIC50', data=data, hue='bioactivity_class', fit_reg=False)\n",
      "31/1:\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "y = [3,2,4,3,4]\n",
      "x = ['Jan','Feb','Mar', 'Apr', 'May']\n",
      "plt.bar(x,y)\n",
      "31/2:\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "y = [3,2,4,3,4]\n",
      "x = ['Jan','Feb','Mar', 'Apr', 'May']\n",
      "plt.bar(x,y)\n",
      "30/1:\n",
      "bioactivity_class = []\n",
      "for i in df_final.pIC50:\n",
      "  if float(i) >= 6:\n",
      "    bioactivity_class.append(\"active\")\n",
      "  elif float(i) <= 4:\n",
      "    bioactivity_class.append(\"inactive\")\n",
      "  else:\n",
      "    bioactivity_class.append(\"intermediate\")\n",
      "30/2:\n",
      "#import library\n",
      "import pandas as pd\n",
      "30/3:\n",
      "#Input yourname for this lab notebook\n",
      "labname = input('what is your name?')\n",
      "30/4: print(labname)\n",
      "30/5:\n",
      "#We will use Chembl database for molecular modeling\n",
      "#install package dependencies\n",
      "!pip install requests_cache==0.6.4\n",
      "30/6:\n",
      "#Install Chemabl database\n",
      "!pip install chembl_webresource_client\n",
      "30/7:\n",
      "#Chembl API access...\n",
      "from chembl_webresource_client.new_client import new_client\n",
      "target = new_client.target\n",
      "#Search target protein, we will use LAT1 as a target protein\n",
      "target_query = target.search('LAT1')\n",
      "targets = pd.DataFrame.from_dict(target_query)\n",
      "targets\n",
      "30/8:\n",
      "#As we can see, we will select H.sapiens LAT1 as atarget\n",
      "selected_target = targets.target_chembl_id[1]\n",
      "selected_target\n",
      "30/9:\n",
      "#Here we will access the activity of compounds in this target, first we will try IC50\n",
      "#IC50 is tell us about the potency of the compound that can reduce LAT1 activity for 50%\n",
      "activity = new_client.activity\n",
      "res = activity.filter(target_chembl_id=selected_target).filter(standard_type=\"IC50\")\n",
      "30/10:\n",
      "#show dataframe of IC50\n",
      "df = pd.DataFrame.from_dict(res)\n",
      "df\n",
      "30/11:\n",
      "#if any compound is missing the IC50 value, then it should drop out\n",
      "df2 = df[df.standard_value.notna()]\n",
      "df2\n",
      "30/12:\n",
      "#I want to know what data do we have\n",
      "print(df2.columns)\n",
      "30/13:\n",
      "#we want to see the molecule structure via canonical_smiles\n",
      "df2.canonical_smiles\n",
      "30/14:\n",
      "#We want to know the structure and activity\n",
      "selection = ['molecule_chembl_id','canonical_smiles','standard_units', 'standard_value']\n",
      "df3 = df2[selection]\n",
      "df3\n",
      "30/15:\n",
      "df4 = df3[df.canonical_smiles.notna()]\n",
      "df4\n",
      "30/16:\n",
      "#install rdkit\n",
      "!pip install rdkit-pypi\n",
      "30/17:\n",
      "import numpy as np\n",
      "import rdkit\n",
      "from time import time\n",
      "\n",
      "from sklearn.metrics import f1_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "import matplotlib.pyplot as plt\n",
      "30/18:\n",
      "#We are going to calculate Lipinski rule of five\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Descriptors, Lipinski\n",
      "30/19: print(df4['canonical_smiles'].unique())\n",
      "30/20:\n",
      "import pandas as pd\n",
      "def lipinski(smiles, verbose=False):\n",
      "\n",
      "    moldata= []\n",
      "    for elem in smiles:\n",
      "        mol=Chem.MolFromSmiles(elem) \n",
      "        moldata.append(mol)\n",
      "       \n",
      "    baseData= np.arange(1,1)\n",
      "    i=0  \n",
      "    for mol in moldata:        \n",
      "       \n",
      "        desc_MolWt = Descriptors.MolWt(mol)\n",
      "        desc_MolLogP = Descriptors.MolLogP(mol)\n",
      "        desc_NumHDonors = Lipinski.NumHDonors(mol)\n",
      "        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)\n",
      "        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)\n",
      "           \n",
      "        row = np.array([desc_MolWt,\n",
      "                        desc_MolLogP,\n",
      "                        desc_NumHDonors,\n",
      "                        desc_NumHAcceptors,\n",
      "                        desc_NumRotatableBonds])   \n",
      "    \n",
      "        if(i==0):\n",
      "            baseData=row\n",
      "        else:\n",
      "            baseData=np.vstack([baseData, row])\n",
      "        i=i+1      \n",
      "    \n",
      "    columnNames=[\"MW\",\"LogP\",\"NumHDonors\",\"NumHAcceptors\", \"NumRotatableBonds\"]   \n",
      "    descriptors = pd.DataFrame(data=baseData,columns=columnNames)\n",
      "    \n",
      "    return descriptors\n",
      "30/21:\n",
      "df_lipinski = lipinski(df4.canonical_smiles)\n",
      "df_lipinski\n",
      "df_lipinski\n",
      "30/22:\n",
      "#combine results\n",
      "df_combined = pd.concat([df3,df_lipinski], axis=1)\n",
      "df_combined\n",
      "30/23:\n",
      "#remove na\n",
      "final_df_combined = df_combined.replace(['None','NONE','none'],np.nan)\n",
      "final_df_combined = final_df_combined.dropna()\n",
      "print('Unique value in MW column',final_df_combined['MW'].unique())\n",
      "len(final_df_combined)\n",
      "30/24: final_df_combined\n",
      "30/25:\n",
      "#We want to know how many of them is high potency agaisnt LAT1 protein\n",
      "#First we should rearrange from IC50 into pIC50 for a better uniformnity distribution\n",
      "#We will convert IC50 to the negative logarithmic scale which is essentially -log10(IC50) or pIC50()\n",
      "# First Take the IC50 values from the standard_value column and converts it from nM to M by multiplying the value by 109\n",
      "# Take the molar value and apply -log10\n",
      "# Delete the standard_value column and create a new pIC50 column\n",
      "\n",
      "#before we do that ,we need to install nescessery package\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "def pIC50(input):\n",
      "    pIC50 = []\n",
      "\n",
      "    for i in input['standard_value_norm']:\n",
      "        molar = float(i)*(10**-9) # Converts nM to M\n",
      "        pIC50.append(-np.log10(molar))\n",
      "\n",
      "    input['pIC50'] = pIC50\n",
      "    x = input.drop('standard_value_norm', 1)\n",
      "        \n",
      "    return x\n",
      "30/26: final_df_combined.standard_value.describe()\n",
      "30/27:\n",
      "def norm_value(input):\n",
      "    norm = []\n",
      "\n",
      "    for i in input['standard_value']:\n",
      "        if float(i) > 100000000:\n",
      "          i = 100000000\n",
      "        norm.append(i)\n",
      "\n",
      "    input['standard_value_norm'] = norm\n",
      "    x = input.drop('standard_value', 1)\n",
      "        \n",
      "    return x\n",
      "30/28:\n",
      "df_norm = norm_value(final_df_combined)\n",
      "df_norm\n",
      "30/29: df_norm.standard_value_norm.describe()\n",
      "30/30:\n",
      "df_final = pIC50(df_norm)\n",
      "df_final\n",
      "30/31: df_final.to_csv('df_final.csv')\n",
      "30/32:\n",
      "#active IC50 < 1 UM\n",
      "-np.log10(0.000001)\n",
      "30/33:\n",
      "#active IC50 < 10 uM\n",
      "\n",
      "-np.log10(0.00001)\n",
      "30/34:\n",
      "bioactivity_class = []\n",
      "for i in df_final.pIC50:\n",
      "  if float(i) >= 6:\n",
      "    bioactivity_class.append(\"active\")\n",
      "  elif float(i) <= 4:\n",
      "    bioactivity_class.append(\"inactive\")\n",
      "  else:\n",
      "    bioactivity_class.append(\"intermediate\")\n",
      "30/35: len(bioactivity_class)\n",
      "30/36: bioactivity_class\n",
      "30/37: len(df_final)\n",
      "30/38:\n",
      "df_class = pd.DataFrame(bioactivity_class, columns=['bioactivity_class'])\n",
      "df_class\n",
      "df_class.to_csv('df_class.csv')\n",
      "30/39: df_class.describe()\n",
      "30/40:\n",
      "df_final_2 = pd.read_csv(\"df_final.csv\")\n",
      "df_final_2\n",
      "30/41:\n",
      "selection = ['molecule_chembl_id', 'canonical_smiles', 'MW', 'LogP','NumHDonors',\n",
      "                      'NumHAcceptors', 'NumRotatableBonds', 'pIC50']\n",
      "df_final_3 = df_final_2[selection]\n",
      "df_final_3\n",
      "30/42:\n",
      "df_final_4 = pd.concat([df_final_3, df_class], axis =1)\n",
      "df_final_4\n",
      "30/43: print(df_final_4['bioactivity_class'].unique())\n",
      "30/44: df_final_4.to_csv('df_final_4.csv')\n",
      "30/45:\n",
      "#Here we got RO5 and pIC50 of 71 cpds against LAT1 protein\n",
      "data = pd.read_csv('df_final_4.csv')\n",
      "30/46:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt1.set_ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)\n",
      "plt2.set_xlabel(\"LogP\", fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt2.set_ylim(0, 50)\n",
      "plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "\n",
      "#Histogram for NumHDonors\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 30, color='#FF9A00', edgecolor='black', linewidth=0.5)\n",
      "plt3.set_xlabel(\"Number of Hydrogen Bond Donors\", fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt3.set_ylim(0, 50)\n",
      "plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 30, color='#FF165D', edgecolor='black', linewidth=0.5)\n",
      "plt4.set_xlabel(\"Number of Hydrogen Bond Acceptors\", fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt4.set_ylim(0, 50)\n",
      "plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 30, color='#52006A', edgecolor='black', linewidth=0.5)\n",
      "plt5.set_xlabel(\"Number of RotatableBonds\", fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt5.set_ylim(0, 50)\n",
      "plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('LAT1 plots-basic descriptors.pdf', dpi=300)\n",
      "30/47:\n",
      "#Visuallize pIC50\n",
      "plt.hist(data['pIC50'], density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "plt.xlabel('pIC50', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('LAT1 plots-pIC50 distribution.pdf', dpi=300)\n",
      "30/48:\n",
      "#Install statistic seabon\n",
      "!pip install seaborn\n",
      "30/49:\n",
      "import seaborn as sns\n",
      "\n",
      "# visualize the relationship between the features and the response using scatterplots\n",
      "sns.pairplot(data, x_vars=['MW','LogP','NumHDonors','NumHAcceptors','NumRotatableBonds'], y_vars='pIC50', height=7, aspect=0.7)\n",
      "30/50:\n",
      "#How many of molecules are active against LAT1\n",
      "plt.hist(data['bioactivity_class'] , density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "plt.xlabel('pIC50', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.ylim(0, 100)\n",
      "30/51: sns.pairplot(data, x_vars=['bioactivity_class'], y_vars='pIC50', height=7, aspect=0.7)\n",
      "30/52:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "x = data['MW']\n",
      "y = data['pIC50']\n",
      "#colors = data.groupby(data['bioactivity_class'])\n",
      "area = (30 * data['LogP'])  # 0 to 15 point radii\n",
      "\n",
      "plt.scatter(x, y, s=area, alpha=0.5)\n",
      "plt.xlabel('MW', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.legend('LogP', )\n",
      "plt.show()\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('MW_and_pIC50_LogP.pdf', dpi=300)\n",
      "30/53: sns.lmplot('MW', 'pIC50', data=data, hue='bioactivity_class', fit_reg=False)\n",
      "30/54: list seabon\n",
      "30/55: list seaborn\n",
      "30/56: list seaborn\n",
      "31/3:\n",
      "x=44-32\n",
      "x\n",
      "30/57:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "x1 = data['MW']\n",
      "x2 = data['LogP']\n",
      "x3 = data['NumHDonors']\n",
      "x4 = data['NumHAcceptors']\n",
      "x5 = data['NumRotatableBonds']\n",
      "y = data['pIC50']\n",
      "#colors = data.groupby(data['bioactivity_class'])\n",
      "area = (30 * data['pIC50'])  # 0 to 15 point radii\n",
      "\n",
      "#MW\n",
      "plt1.scatter(x1, y, s=area, alpha=0.5)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#LogP\n",
      "plt2.scatter(x2, y, s=area, alpha=0.5)\n",
      "plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondDonors\n",
      "plt3.scatter(x3, y, s=area, alpha=0.5)\n",
      "plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt4.scatter(x4, y, s=area, alpha=0.5)\n",
      "plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt5.scatter(x5, y, s=area, alpha=0.5)\n",
      "plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('MW_and_pIC50_LogP.pdf', dpi=300)\n",
      "31/4:\n",
      "price= 110\n",
      "if price >= 100\n",
      "    print('Discount 5%')\n",
      "else\n",
      "    print('discount 2%')\n",
      "31/5:\n",
      "price= 110\n",
      "if price >= 100:\n",
      "    print('Discount 5%')\n",
      "else\n",
      "    print('discount 2%')\n",
      "31/6:\n",
      "price= 110\n",
      "if price >= 100:\n",
      "    print('Discount 5%')\n",
      "else:\n",
      "    print('discount 2%')\n",
      "31/7:\n",
      "price= 110\n",
      "if price >= 100:\n",
      "    print('Discount 5%')\n",
      "else:\n",
      "    print('discount 2%')\n",
      "print('Good bye')\n",
      "31/8: price=90\n",
      "31/9:\n",
      "price= 110\n",
      "def discount(price)\n",
      "if price >= 100:\n",
      "    print('Discount 5%')\n",
      "else:\n",
      "    print('discount 2%')\n",
      "print('Good bye')\n",
      "return print\n",
      "31/10:\n",
      "price= 110\n",
      "def discount(price):\n",
      "if price >= 100:\n",
      "    print('Discount 5%')\n",
      "else:\n",
      "    print('discount 2%')\n",
      "print('Good bye')\n",
      "return print\n",
      "31/11:\n",
      "price= 110\n",
      "def discount(price):\n",
      "    if price >= 100:\n",
      "        print('Discount 5%')\n",
      "    else:\n",
      "        print('discount 2%')\n",
      "        print('Good bye')\n",
      "return print\n",
      "31/12:\n",
      "price= 110\n",
      "def discount(price):\n",
      "    if price >= 100:\n",
      "        print('Discount 5%')\n",
      "    else:\n",
      "        print('discount 2%')\n",
      "        print('Good bye')\n",
      "    return print\n",
      "31/13: price=90\n",
      "31/14: discount(price)\n",
      "31/15: price=110\n",
      "31/16: discount(price)\n",
      "31/17:\n",
      "price= 110\n",
      "def discount(price):\n",
      "    if price >= 100:\n",
      "        print('Discount 5%')\n",
      "    else:\n",
      "        print('discount 2%')\n",
      "        print('Good bye')\n",
      "31/18: discount(price)\n",
      "31/19: price=90\n",
      "31/20: discount(price)\n",
      "31/21:\n",
      "temp=32\n",
      "def fanoperation(temp):\n",
      "    if temp < 30:\n",
      "        print('off')\n",
      "    elif temp >=30 and temp < 40:\n",
      "        print('moderate')\n",
      "    else:\n",
      "        print('high')\n",
      "31/22: fanoperation(temp)\n",
      "31/23: temp=40\n",
      "31/24: fanoperation(temp)\n",
      "31/25: temp=29\n",
      "31/26: fanoperation(temp)\n",
      "31/27:\n",
      "for i in range(3):\n",
      "    print(\"test for\")\n",
      "    print(count)\n",
      "print('done')\n",
      "31/28:\n",
      "for i in range(3):\n",
      "    print(\"test for\")\n",
      "    print(i)\n",
      "print('done')\n",
      "31/29:\n",
      "for i in range(4,6):\n",
      "    print(i)\n",
      "print('done')\n",
      "31/30:\n",
      "count = 0\n",
      "while count < 3:\n",
      "    print('Hello')\n",
      "    count = count + 1\n",
      "31/31:\n",
      "while True:\n",
      "    print('Hello')\n",
      "print('done')\n",
      "31/32:\n",
      "for i in input()\n",
      "    x = i*10\n",
      "print('done')\n",
      "31/33:\n",
      "for i in input():\n",
      "    x = i*10\n",
      "print('done')\n",
      "31/34:\n",
      "for i in input():\n",
      "    x = i*10\n",
      "print('x')\n",
      "31/35:\n",
      "for i in input():\n",
      "    x = i*10\n",
      "print(x)\n",
      "31/36:\n",
      "for i in input():\n",
      "    x = i*10\n",
      "print(i)\n",
      "31/37:\n",
      "for i in input():\n",
      "    x = i*10\n",
      "print(x)\n",
      "31/38:\n",
      "for i in input():\n",
      "    x = i*10\n",
      "print(x)\n",
      "31/39:\n",
      "for i in input():\n",
      "    x = i+10\n",
      "print(x)\n",
      "31/40:\n",
      "for i in input():\n",
      "    x = int(i)+10\n",
      "print(x)\n",
      "31/41:\n",
      "for i in input():\n",
      "    x = int(i) + 10\n",
      "print(x)\n",
      "31/42:\n",
      "for i in input():\n",
      "    x = int(i) + 10\n",
      "print(x)\n",
      "31/43:\n",
      "for i in input():\n",
      "    x = int(i) + 10\n",
      "print(x)\n",
      "31/44:\n",
      "for i in input():\n",
      "    x = int(i) + 10\n",
      "print(x)\n",
      "31/45:\n",
      "x = 10\n",
      "name = tarapong\n",
      "print(name % x)\n",
      "31/46:\n",
      "x = 10\n",
      "name = 'tarapong'\n",
      "print(name % x)\n",
      "31/47:\n",
      "x = 10\n",
      "name = 'tarapong'\n",
      "print(name, x)\n",
      "31/48: print(f\"tar)\n",
      "31/49: print(f\"tar\")\n",
      "31/50: print(f\"tar\" + x)\n",
      "31/51: print(name + x)\n",
      "31/52: print(f' My name is {name} + {x})\n",
      "31/53: print(f' My name is {name} + {x}')\n",
      "31/54: print('a = {}'.format(x))\n",
      "31/55: print('a = {x}')\n",
      "31/56: print('a = ', x)\n",
      "31/57: print('a =', x)\n",
      "31/58:\n",
      "def area(input):\n",
      "    area = input * input\n",
      "    return area\n",
      "31/59: b = area(5):\n",
      "31/60: b = area(5)\n",
      "31/61:\n",
      "b = area(5)\n",
      "print('b box area is', b)\n",
      "31/62: print('b = %.2f' % (b))\n",
      "31/63: print('b=', float(b))\n",
      "31/64: print('b = %.10f' % (b))\n",
      "31/65: print('b=', float(b).0)\n",
      "31/66: print('b=', float(b))\n",
      "31/67:\n",
      "import math\n",
      "print('Pi= %.5f' % (math.pi))\n",
      "31/68:\n",
      "print('Pi= %s and %.2f' % (name, math.pi\n",
      "                        ))\n",
      "31/69: box = area(10)\n",
      "31/70: box\n",
      "31/71:\n",
      "def boxarea(lenght):\n",
      "    x=lenght*lenght\n",
      "    return x\n",
      "31/72: ab = boxarea(50)\n",
      "31/73: ab\n",
      "31/74:\n",
      "if __name__ == '__main__':\n",
      "    b = area(4)\n",
      "31/75:\n",
      "if __name__ == '__main__':\n",
      "    b = area(4)\n",
      "    print('area =', b)\n",
      "30/58:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "x1 = data['MW']\n",
      "x2 = data['LogP']\n",
      "x3 = data['NumHDonors']\n",
      "x4 = data['NumHAcceptors']\n",
      "x5 = data['NumRotatableBonds']\n",
      "y = data['pIC50']\n",
      "#colors = data.groupby(data['bioactivity_class'])\n",
      "area = (30 * data['pIC50'])  # 0 to 15 point radii\n",
      "\n",
      "#MW\n",
      "plt1.scatter(x1, y, s=area, alpha=0.5)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#LogP\n",
      "plt2.scatter(x2, y, s=area, alpha=0.5)\n",
      "plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondDonors\n",
      "plt3.scatter(x3, y, s=area, alpha=0.5)\n",
      "plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt4.scatter(x4, y, s=area, alpha=0.5)\n",
      "plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt5.scatter(x5, y, s=area, alpha=0.5)\n",
      "plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('_and_pIC50.pdf', dpi=300)\n",
      "30/59:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "x1 = data['MW']\n",
      "x2 = data['LogP']\n",
      "x3 = data['NumHDonors']\n",
      "x4 = data['NumHAcceptors']\n",
      "x5 = data['NumRotatableBonds']\n",
      "y = data['pIC50']\n",
      "colors = {'active':'red', 'inactive':'blue', 'intermediate':'yellow'}\n",
      "area = (30 * data['pIC50'])  # 0 to 15 point radii\n",
      "\n",
      "#MW\n",
      "plt1.scatter(x1, y, s=area, c=data['bioacitvity_class'] alpha=0.5)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#LogP\n",
      "plt2.scatter(x2, y, s=area, alpha=0.5)\n",
      "plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondDonors\n",
      "plt3.scatter(x3, y, s=area, alpha=0.5)\n",
      "plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt4.scatter(x4, y, s=area, alpha=0.5)\n",
      "plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt5.scatter(x5, y, s=area, alpha=0.5)\n",
      "plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('_and_pIC50.pdf', dpi=300)\n",
      "30/60:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "x1 = data['MW']\n",
      "x2 = data['LogP']\n",
      "x3 = data['NumHDonors']\n",
      "x4 = data['NumHAcceptors']\n",
      "x5 = data['NumRotatableBonds']\n",
      "y = data['pIC50']\n",
      "colors = {'active':'red', 'inactive':'blue', 'intermediate':'yellow'}\n",
      "area = (30 * data['pIC50'])  # 0 to 15 point radii\n",
      "\n",
      "#MW\n",
      "plt1.scatter(x1, y, s=area, c=data['bioacitvity_class'], alpha=0.5)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#LogP\n",
      "plt2.scatter(x2, y, s=area, alpha=0.5)\n",
      "plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondDonors\n",
      "plt3.scatter(x3, y, s=area, alpha=0.5)\n",
      "plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt4.scatter(x4, y, s=area, alpha=0.5)\n",
      "plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt5.scatter(x5, y, s=area, alpha=0.5)\n",
      "plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('_and_pIC50.pdf', dpi=300)\n",
      "30/61:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "x1 = data['MW']\n",
      "x2 = data['LogP']\n",
      "x3 = data['NumHDonors']\n",
      "x4 = data['NumHAcceptors']\n",
      "x5 = data['NumRotatableBonds']\n",
      "y = data['pIC50']\n",
      "colors = {'active':'red', 'inactive':'blue', 'intermediate':'yellow'}\n",
      "area = (30 * data['pIC50'])  # 0 to 15 point radii\n",
      "\n",
      "#MW\n",
      "plt1.scatter(x1, y, s=area, c=data['bioacitivity_class'], alpha=0.5)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#LogP\n",
      "plt2.scatter(x2, y, s=area, alpha=0.5)\n",
      "plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondDonors\n",
      "plt3.scatter(x3, y, s=area, alpha=0.5)\n",
      "plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt4.scatter(x4, y, s=area, alpha=0.5)\n",
      "plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt5.scatter(x5, y, s=area, alpha=0.5)\n",
      "plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('_and_pIC50.pdf', dpi=300)\n",
      "30/62:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "x1 = data['MW']\n",
      "x2 = data['LogP']\n",
      "x3 = data['NumHDonors']\n",
      "x4 = data['NumHAcceptors']\n",
      "x5 = data['NumRotatableBonds']\n",
      "y = data['pIC50']\n",
      "colors = {'active':'red', 'inactive':'blue', 'intermediate':'yellow'}\n",
      "area = (30 * data['pIC50'])  # 0 to 15 point radii\n",
      "\n",
      "#MW\n",
      "plt1.scatter(x1, y, s=area, c=data['bioactivity_class'], alpha=0.5)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#LogP\n",
      "plt2.scatter(x2, y, s=area, alpha=0.5)\n",
      "plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondDonors\n",
      "plt3.scatter(x3, y, s=area, alpha=0.5)\n",
      "plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt4.scatter(x4, y, s=area, alpha=0.5)\n",
      "plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt5.scatter(x5, y, s=area, alpha=0.5)\n",
      "plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('_and_pIC50.pdf', dpi=300)\n",
      "30/63:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "x1 = data['MW']\n",
      "x2 = data['LogP']\n",
      "x3 = data['NumHDonors']\n",
      "x4 = data['NumHAcceptors']\n",
      "x5 = data['NumRotatableBonds']\n",
      "y = data['pIC50']\n",
      "colors = {'active':'red', 'inactive':'blue', 'intermediate':'yellow'}\n",
      "area = (30 * data['pIC50'])  # 0 to 15 point radii\n",
      "\n",
      "#MW\n",
      "plt1.scatter(x1, y, s=area, c=data['bioactivity_class'].map(colors), alpha=0.5)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#LogP\n",
      "plt2.scatter(x2, y, s=area, alpha=0.5)\n",
      "plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondDonors\n",
      "plt3.scatter(x3, y, s=area, alpha=0.5)\n",
      "plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt4.scatter(x4, y, s=area, alpha=0.5)\n",
      "plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt5.scatter(x5, y, s=area, alpha=0.5)\n",
      "plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('_and_pIC50.pdf', dpi=300)\n",
      "30/64:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "x1 = data['MW']\n",
      "x2 = data['LogP']\n",
      "x3 = data['NumHDonors']\n",
      "x4 = data['NumHAcceptors']\n",
      "x5 = data['NumRotatableBonds']\n",
      "y = data['pIC50']\n",
      "colors = {'active':'red', 'inactive':'blue', 'intermediate':'green'}\n",
      "area = (30 * data['pIC50'])  # 0 to 15 point radii\n",
      "\n",
      "#MW\n",
      "plt1.scatter(x1, y, s=area, c=data['bioactivity_class'].map(colors), alpha=0.5)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#LogP\n",
      "plt2.scatter(x2, y, s=area, alpha=0.5)\n",
      "plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondDonors\n",
      "plt3.scatter(x3, y, s=area, alpha=0.5)\n",
      "plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt4.scatter(x4, y, s=area, alpha=0.5)\n",
      "plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt5.scatter(x5, y, s=area, alpha=0.5)\n",
      "plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('_and_pIC50.pdf', dpi=300)\n",
      "30/65:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "x1 = data['MW']\n",
      "x2 = data['LogP']\n",
      "x3 = data['NumHDonors']\n",
      "x4 = data['NumHAcceptors']\n",
      "x5 = data['NumRotatableBonds']\n",
      "y = data['pIC50']\n",
      "colors = {'active':'red', 'inactive':'blue', 'intermediate':'green'}\n",
      "area = (30 * data['pIC50'])  # 0 to 15 point radii\n",
      "\n",
      "#MW\n",
      "plt1.scatter(x1, y, s=area, c=data['bioactivity_class'].map(colors), alpha=0.5)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#LogP\n",
      "plt2.scatter(x2, y, s=area,c=data['bioactivity_class'].map(colors), alpha=0.5)\n",
      "plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondDonors\n",
      "plt3.scatter(x3, y, s=area, c=data['bioactivity_class'].map(colors), alpha=0.5)\n",
      "plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt4.scatter(x4, y, s=area, c=data['bioactivity_class'].map(colors), alpha=0.5)\n",
      "plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt5.scatter(x5, y, s=area, c=data['bioactivity_class'].map(colors), alpha=0.5)\n",
      "plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('_and_pIC50.pdf', dpi=300)\n",
      "31/76: data=[1,2,3,4,5,6,7]\n",
      "31/77: data(type)\n",
      "31/78: type(data)\n",
      "31/79: data.append(20)\n",
      "31/80: data\n",
      "31/81: ave= sum(data)/len(data)\n",
      "31/82: ave\n",
      "31/83: data=[[1,2,3],[2,3,4]]\n",
      "31/84: data\n",
      "31/85: data[0]\n",
      "31/86: data[0,2]\n",
      "31/87: data[0][2]\n",
      "31/88: tp1 = 2,3,4\n",
      "31/89: type(tp1)\n",
      "31/90: data = [2,1,3,4,5,6,7,8]\n",
      "31/91: data[3:4]\n",
      "31/92: data[0:-1]\n",
      "31/93: data[:3]\n",
      "31/94:\n",
      "for k in data:\n",
      "    b=k*2\n",
      "    print(b)\n",
      "31/95:\n",
      "for k in data:\n",
      "    b=k*2\n",
      "    a.append(b)\n",
      "    print(a)\n",
      "31/96:\n",
      "a=[]\n",
      "for k in data:\n",
      "    b=k*2\n",
      "    a.append(b)\n",
      "    print(a)\n",
      "31/97:\n",
      "a=[]\n",
      "for k in data:\n",
      "    b=k*2\n",
      "a.append(b)\n",
      "  \n",
      "    print(a)\n",
      "31/98:\n",
      "a=[]\n",
      "for k in data:\n",
      "    b=k*2\n",
      "31/99:\n",
      "a=[]\n",
      "for k in data:\n",
      "    b=k*2\n",
      "a.append(b)\n",
      "\n",
      "    print(a)\n",
      "31/100:\n",
      "a=[]\n",
      "for k in data:\n",
      "    b=k*2\n",
      "a.append(b)\n",
      "\n",
      "print(a)\n",
      "31/101:\n",
      "a=[]\n",
      "for k in data:\n",
      "    b=k*2\n",
      "    a.append(b)\n",
      "input['a'] = a\n",
      "print(a)\n",
      "31/102:\n",
      "a=[]\n",
      "for k in data:\n",
      "    b=k*2\n",
      "    a.append(b)\n",
      "input['b'] = a\n",
      "print(a)\n",
      "31/103:\n",
      "a=[]\n",
      "for k in data:\n",
      "    b=k*2\n",
      "    a.append(b)\n",
      "input[b] = a\n",
      "print(a)\n",
      "31/104:\n",
      "a=[]\n",
      "for k in data:\n",
      "    b=k*2\n",
      "    a.append(b)\n",
      "print(a)\n",
      "31/105:\n",
      "a=[]\n",
      "for k in data:\n",
      "    b=k*2\n",
      "    a.append(b)\n",
      "    print(a)\n",
      "31/106: datas = {'Name':'A', 'Age':'29', 'Score':'35'}\n",
      "31/107: type(datas)\n",
      "31/108: datas[0]['Name']\n",
      "31/109: datas = {'Name':'A', 'Age':'29', 'Score':'35'}\n",
      "31/110: datas[0][\"Name\"]\n",
      "31/111: datas = [{'Name':'A', 'Age':'29', 'Score':'35'}, {'Name':'B', 'Age': '20', 'Score':'30'}]\n",
      "31/112: datas[0][Name]\n",
      "31/113: datas[0]['Name']\n",
      "31/114:\n",
      "for k in datas:\n",
      "    print({':1'}{}.format(k['Name'], k['Score']))\n",
      "31/115:\n",
      "for k in datas:\n",
      "    print({':1'} {}.format(k['Name'], k['Score']))\n",
      "31/116:\n",
      "for k in datas:\n",
      "    print('{:1} {}.format(k['Name'], k['Score']))\n",
      "31/117:\n",
      "for k in datas:\n",
      "    print('{:1} {}'.format(k['Name'], k['Score']))\n",
      "31/118:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "31/119: np.mean(data)\n",
      "31/120: np.std(data)\n",
      "31/121: np.median(data)\n",
      "30/66:\n",
      "#How many of inactive\n",
      "np.max(data['bioacitivity_class, inactive'])\n",
      "30/67:\n",
      "#How many of inactive\n",
      "import numpy as np\n",
      "np.max(data['bioacitivity_class, inactive'])\n",
      "30/68:\n",
      "#How many of inactive\n",
      "import numpy as np\n",
      "np.max(data['bioacitivity_class'])\n",
      "30/69:\n",
      "#How many of inactive\n",
      "import numpy as np\n",
      "np.max(data[bioacitivity_class])\n",
      "30/70:\n",
      "#How many of inactive\n",
      "import numpy as np\n",
      "np.max(data['bioactivity_class'])\n",
      "30/71:\n",
      "#How many of inactive?\n",
      "import numpy as np\n",
      "df.data['bioactivity_class'].value_counts\n",
      "30/72:\n",
      "#How many of inactive?\n",
      "import numpy as np\n",
      "df = pd.Dataframe.['bioactivity_class'].value_counts\n",
      "30/73:\n",
      "#How many of inactive?\n",
      "import numpy as np\n",
      "df = pd.Dataframe.data['bioactivity_class'].value_counts\n",
      "30/74:\n",
      "#How many of inactive?\n",
      "import numpy as np\n",
      "df = np.Dataframe.data['bioactivity_class'].value_counts\n",
      "30/75:\n",
      "#How many of inactive?\n",
      "import numpy as np\n",
      "df = pd.DataFrame.data['bioactivity_class'].value_counts\n",
      "30/76:\n",
      "#How many of inactive?\n",
      "import numpy as np\n",
      "data['bioactivity_class']\n",
      "30/77:\n",
      "#How many of inactive?\n",
      "import numpy as np\n",
      "data['bioactivity_class'].value_counts\n",
      "30/78:\n",
      "#How many of inactive?\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "df = DataFrame(data)\n",
      "data['bioactivity_class'].value_counts\n",
      "30/79:\n",
      "#How many of inactive?\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "df = pd.DataFrame(data)\n",
      "data['bioactivity_class'].value_counts\n",
      "30/80:\n",
      "#How many of inactive?\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "df = pd.DataFrame(data['bioactivity_class'])\n",
      "df.data()\n",
      "30/81:\n",
      "#How many of inactive?\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "df = pd.DataFrame(data['bioactivity_class'])\n",
      "30/82:\n",
      "#How many of inactive?\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "df = pd.DataFrame(data['bioactivity_class'])\n",
      "df\n",
      "30/83:\n",
      "#How many of inactive?\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "df = pd.DataFrame(data['bioactivity_class'])\n",
      "df.bioactivity_class.value_count\n",
      "30/84:\n",
      "#How many of inactive?\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "df = pd.DataFrame(data['bioactivity_class'])\n",
      "df.bioactivity_class.value_counts\n",
      "30/85:\n",
      "#How many of inactive?\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "df = pd.DataFrame(data['bioactivity_class'])\n",
      "df.bioactivity_class.value_counts()\n",
      "31/122: x = [1,2,3,4,5,56,6,6767]\n",
      "31/123: np.array(x)\n",
      "31/124: x=[[6,7,4,5,1],[2,8,3,6,4],[1,3,4,5,6],[8,9,10,12,8]]\n",
      "31/125: npx=np.matrix(x)\n",
      "31/126: npx\n",
      "31/127: npx1=np.array(x)\n",
      "31/128: npx1\n",
      "31/129: npx.shape\n",
      "31/130: npx1.shape\n",
      "31/131: npx1[0,3]\n",
      "31/132: npx[0,3]\n",
      "31/133: npx[1,3]\n",
      "31/134:\n",
      "X=npx[:, :-1]\n",
      "X\n",
      "31/135: y=npx[-1:]\n",
      "31/136: y\n",
      "31/137: y=npx[:, -1:]\n",
      "31/138: y\n",
      "31/139: z=npx[:1,:]\n",
      "31/140: z\n",
      "31/141: npx[-1:,:]\n",
      "31/142: npx > 5\n",
      "31/143: np[npx>5]\n",
      "31/144: np[npx > 5]\n",
      "31/145: npx[npx > 5]\n",
      "31/146: npx1[npx1 > 5]\n",
      "31/147: npx.T\n",
      "31/148: npx.reshape(1,-1)\n",
      "31/149: npx.reshape(-1,1)\n",
      "31/150: np.random.rand(4)\n",
      "31/151: np.random.rand(4)*30+20\n",
      "31/152:\n",
      "mu, sd = 100, 10\n",
      "np.random.normal(mu, sd, 1000)\n",
      "31/153:\n",
      "mu, sd = 100, 10\n",
      "s = np.random.normal(mu, sd, 1000)\n",
      "31/154: s[:10]\n",
      "31/155:\n",
      "import matplotlib as plt\n",
      "plt.hist(s)\n",
      "31/156:\n",
      "import matplotlib.pyplot as plt\n",
      "plt.hist(s)\n",
      "31/157:\n",
      "import matplotlib.pyplot as plt\n",
      "plt.hist(s, bin=40)\n",
      "31/158:\n",
      "import matplotlib.pyplot as plt\n",
      "plt.hist(s, bins=40)\n",
      "31/159:\n",
      "import panddas as pd\n",
      "S = pd.Series([1,2,3,xx,3,5])\n",
      "31/160:\n",
      "import pandas as pd\n",
      "S = pd.Series([1,2,3,xx,3,5])\n",
      "31/161:\n",
      "import pandas as pd\n",
      "S = pd.Series([1,2,3,\"xx\",3,5])\n",
      "31/162: S\n",
      "31/163: datas = [13,12,15,16]\n",
      "31/164: ndata = np.array(datas)\n",
      "31/165: ps = pd.Series(ndata)\n",
      "31/166: ps\n",
      "31/167:\n",
      "idx = ['a','b','c','']\n",
      "psd = pd.Series(datas, index=idx)\n",
      "31/168: psd\n",
      "31/169: data = {'Name':'Tarapong', 'Class':'New'}\n",
      "31/170: ps =pd.Series(data)\n",
      "31/171: ps\n",
      "31/172: ps['Name']\n",
      "31/173: data = {'Name':'Tarapong', 'Class':'New', 'Class2':'New2', 'Class3':'New3'}\n",
      "31/174: ps['Name']\n",
      "31/175: ps[0]\n",
      "31/176: ps\n",
      "31/177: ps =pd.Series(data)\n",
      "31/178: ps\n",
      "31/179: ps['Name']\n",
      "31/180: ps[0]\n",
      "31/181: ps\n",
      "31/182: ps[:]\n",
      "31/183: ps[:2]\n",
      "31/184: ps[-1]\n",
      "33/1:\n",
      "#import library\n",
      "import pandas as pd\n",
      "33/2:\n",
      "#Input yourname for this lab notebook\n",
      "labname = input('what is your name?')\n",
      "33/3: print(labname)\n",
      "33/4:\n",
      "#We will use Chembl database for molecular modeling\n",
      "#install package dependencies\n",
      "!pip install requests_cache==0.6.4\n",
      "33/5:\n",
      "#Install Chemabl database\n",
      "!pip install chembl_webresource_client\n",
      "33/6:\n",
      "#Chembl API access...\n",
      "from chembl_webresource_client.new_client import new_client\n",
      "target = new_client.target\n",
      "#Search target protein, we will use LAT1 as a target protein\n",
      "target_query = target.search('LAT1')\n",
      "targets = pd.DataFrame.from_dict(target_query)\n",
      "targets\n",
      "33/7:\n",
      "#As we can see, we will select H.sapiens LAT1 as atarget\n",
      "selected_target = targets.target_chembl_id[1]\n",
      "selected_target\n",
      "33/8:\n",
      "#Here we will access the activity of compounds in this target, first we will try IC50\n",
      "#IC50 is tell us about the potency of the compound that can reduce LAT1 activity for 50%\n",
      "activity = new_client.activity\n",
      "res = activity.filter(target_chembl_id=selected_target).filter(standard_type=\"IC50\")\n",
      "33/9:\n",
      "#show dataframe of IC50\n",
      "df = pd.DataFrame.from_dict(res)\n",
      "df\n",
      "33/10:\n",
      "#if any compound is missing the IC50 value, then it should drop out\n",
      "df2 = df[df.standard_value.notna()]\n",
      "df2\n",
      "33/11:\n",
      "#I want to know what data do we have\n",
      "print(df2.columns)\n",
      "33/12:\n",
      "#we want to see the molecule structure via canonical_smiles\n",
      "df2.canonical_smiles\n",
      "33/13:\n",
      "#We want to know the structure and activity\n",
      "selection = ['molecule_chembl_id','canonical_smiles','standard_units', 'standard_value']\n",
      "df3 = df2[selection]\n",
      "df3\n",
      "33/14:\n",
      "df4 = df3[df.canonical_smiles.notna()]\n",
      "df4\n",
      "33/15:\n",
      "#install rdkit\n",
      "!pip install rdkit-pypi\n",
      "33/16:\n",
      "import numpy as np\n",
      "import rdkit\n",
      "from time import time\n",
      "\n",
      "from sklearn.metrics import f1_score\n",
      "from sklearn.metrics import confusion_matrix\n",
      "import matplotlib.pyplot as plt\n",
      "33/17:\n",
      "#We are going to calculate Lipinski rule of five\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Descriptors, Lipinski\n",
      "33/18: print(df4['canonical_smiles'].unique())\n",
      "33/19:\n",
      "import pandas as pd\n",
      "def lipinski(smiles, verbose=False):\n",
      "\n",
      "    moldata= []\n",
      "    for elem in smiles:\n",
      "        mol=Chem.MolFromSmiles(elem) \n",
      "        moldata.append(mol)\n",
      "       \n",
      "    baseData= np.arange(1,1)\n",
      "    i=0  \n",
      "    for mol in moldata:        \n",
      "       \n",
      "        desc_MolWt = Descriptors.MolWt(mol)\n",
      "        desc_MolLogP = Descriptors.MolLogP(mol)\n",
      "        desc_NumHDonors = Lipinski.NumHDonors(mol)\n",
      "        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)\n",
      "        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)\n",
      "           \n",
      "        row = np.array([desc_MolWt,\n",
      "                        desc_MolLogP,\n",
      "                        desc_NumHDonors,\n",
      "                        desc_NumHAcceptors,\n",
      "                        desc_NumRotatableBonds])   \n",
      "    \n",
      "        if(i==0):\n",
      "            baseData=row\n",
      "        else:\n",
      "            baseData=np.vstack([baseData, row])\n",
      "        i=i+1      \n",
      "    \n",
      "    columnNames=[\"MW\",\"LogP\",\"NumHDonors\",\"NumHAcceptors\", \"NumRotatableBonds\"]   \n",
      "    descriptors = pd.DataFrame(data=baseData,columns=columnNames)\n",
      "    \n",
      "    return descriptors\n",
      "33/20:\n",
      "df_lipinski = lipinski(df4.canonical_smiles)\n",
      "df_lipinski\n",
      "df_lipinski\n",
      "33/21:\n",
      "#combine results\n",
      "df_combined = pd.concat([df3,df_lipinski], axis=1)\n",
      "df_combined\n",
      "33/22:\n",
      "#remove na\n",
      "final_df_combined = df_combined.replace(['None','NONE','none'],np.nan)\n",
      "final_df_combined = final_df_combined.dropna()\n",
      "print('Unique value in MW column',final_df_combined['MW'].unique())\n",
      "len(final_df_combined)\n",
      "33/23: final_df_combined\n",
      "33/24:\n",
      "#We want to know how many of them is high potency agaisnt LAT1 protein\n",
      "#First we should rearrange from IC50 into pIC50 for a better uniformnity distribution\n",
      "#We will convert IC50 to the negative logarithmic scale which is essentially -log10(IC50) or pIC50()\n",
      "# First Take the IC50 values from the standard_value column and converts it from nM to M by multiplying the value by 109\n",
      "# Take the molar value and apply -log10\n",
      "# Delete the standard_value column and create a new pIC50 column\n",
      "\n",
      "#before we do that ,we need to install nescessery package\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "def pIC50(input):\n",
      "    pIC50 = []\n",
      "\n",
      "    for i in input['standard_value_norm']:\n",
      "        molar = float(i)*(10**-9) # Converts nM to M\n",
      "        pIC50.append(-np.log10(molar))\n",
      "\n",
      "    input['pIC50'] = pIC50\n",
      "    x = input.drop('standard_value_norm', 1)\n",
      "        \n",
      "    return x\n",
      "33/25: final_df_combined.standard_value.describe()\n",
      "33/26:\n",
      "def norm_value(input):\n",
      "    norm = []\n",
      "\n",
      "    for i in input['standard_value']:\n",
      "        if float(i) > 100000000:\n",
      "          i = 100000000\n",
      "        norm.append(i)\n",
      "\n",
      "    input['standard_value_norm'] = norm\n",
      "    x = input.drop('standard_value', 1)\n",
      "        \n",
      "    return x\n",
      "33/27:\n",
      "df_norm = norm_value(final_df_combined)\n",
      "df_norm\n",
      "33/28: df_norm.standard_value_norm.describe()\n",
      "33/29:\n",
      "df_final = pIC50(df_norm)\n",
      "df_final\n",
      "33/30: df_final.to_csv('df_final.csv')\n",
      "33/31:\n",
      "#active IC50 < 1 UM\n",
      "-np.log10(0.000001)\n",
      "33/32:\n",
      "#active IC50 < 10 uM\n",
      "\n",
      "-np.log10(0.00001)\n",
      "33/33:\n",
      "bioactivity_class = []\n",
      "for i in df_final.pIC50:\n",
      "  if float(i) >= 6:\n",
      "    bioactivity_class.append(\"active\")\n",
      "  elif float(i) <= 4:\n",
      "    bioactivity_class.append(\"inactive\")\n",
      "  else:\n",
      "    bioactivity_class.append(\"intermediate\")\n",
      "33/34: len(bioactivity_class)\n",
      "33/35: bioactivity_class\n",
      "33/36: len(df_final)\n",
      "33/37:\n",
      "df_class = pd.DataFrame(bioactivity_class, columns=['bioactivity_class'])\n",
      "df_class\n",
      "df_class.to_csv('df_class.csv')\n",
      "33/38: df_class.describe()\n",
      "33/39:\n",
      "df_final_2 = pd.read_csv(\"df_final.csv\")\n",
      "df_final_2\n",
      "33/40:\n",
      "selection = ['molecule_chembl_id', 'canonical_smiles', 'MW', 'LogP','NumHDonors',\n",
      "                      'NumHAcceptors', 'NumRotatableBonds', 'pIC50']\n",
      "df_final_3 = df_final_2[selection]\n",
      "df_final_3\n",
      "33/41:\n",
      "df_final_4 = pd.concat([df_final_3, df_class], axis =1)\n",
      "df_final_4\n",
      "33/42: print(df_final_4['bioactivity_class'].unique())\n",
      "33/43: df_final_4.to_csv('df_final_4.csv')\n",
      "33/44:\n",
      "#Here we got RO5 and pIC50 of 71 cpds against LAT1 protein\n",
      "data = pd.read_csv('df_final_4.csv')\n",
      "33/45:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt1.set_ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)\n",
      "plt2.set_xlabel(\"LogP\", fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt2.set_ylim(0, 50)\n",
      "plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "\n",
      "#Histogram for NumHDonors\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 30, color='#FF9A00', edgecolor='black', linewidth=0.5)\n",
      "plt3.set_xlabel(\"Number of Hydrogen Bond Donors\", fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt3.set_ylim(0, 50)\n",
      "plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 30, color='#FF165D', edgecolor='black', linewidth=0.5)\n",
      "plt4.set_xlabel(\"Number of Hydrogen Bond Acceptors\", fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt4.set_ylim(0, 50)\n",
      "plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 30, color='#52006A', edgecolor='black', linewidth=0.5)\n",
      "plt5.set_xlabel(\"Number of RotatableBonds\", fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt5.set_ylim(0, 50)\n",
      "plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('LAT1 plots-basic descriptors.pdf', dpi=300)\n",
      "33/46:\n",
      "#Visuallize pIC50\n",
      "plt.hist(data['pIC50'], density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "plt.xlabel('pIC50', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('LAT1 plots-pIC50 distribution.pdf', dpi=300)\n",
      "33/47:\n",
      "#Install statistic seabon\n",
      "!pip install seaborn\n",
      "33/48:\n",
      "import seaborn as sns\n",
      "\n",
      "# visualize the relationship between the features and the response using scatterplots\n",
      "sns.pairplot(data, x_vars=['MW','LogP','NumHDonors','NumHAcceptors','NumRotatableBonds'], y_vars='pIC50', height=7, aspect=0.7)\n",
      "33/49:\n",
      "#How many of molecules are active against LAT1\n",
      "plt.hist(data['bioactivity_class'] , density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "plt.xlabel('pIC50', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.ylim(0, 100)\n",
      "33/50:\n",
      "#How many of each categoryinactive?\n",
      "import pandas as pd\n",
      "df = pd.DataFrame(data['bioactivity_class'])\n",
      "df.bioactivity_class.value_counts()\n",
      "33/51: sns.pairplot(data, x_vars=['bioactivity_class'], y_vars='pIC50', height=7, aspect=0.7)\n",
      "33/52:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "x1 = data['MW']\n",
      "x2 = data['LogP']\n",
      "x3 = data['NumHDonors']\n",
      "x4 = data['NumHAcceptors']\n",
      "x5 = data['NumRotatableBonds']\n",
      "y = data['pIC50']\n",
      "colors = {'active':'red', 'inactive':'blue', 'intermediate':'green'}\n",
      "area = (30 * data['pIC50'])  # 0 to 15 point radii\n",
      "\n",
      "#MW\n",
      "plt1.scatter(x1, y, s=area, c=data['bioactivity_class'].map(colors), alpha=0.5)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#LogP\n",
      "plt2.scatter(x2, y, s=area,c=data['bioactivity_class'].map(colors), alpha=0.5)\n",
      "plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondDonors\n",
      "plt3.scatter(x3, y, s=area, c=data['bioactivity_class'].map(colors), alpha=0.5)\n",
      "plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt4.scatter(x4, y, s=area, c=data['bioactivity_class'].map(colors), alpha=0.5)\n",
      "plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt5.scatter(x5, y, s=area, c=data['bioactivity_class'].map(colors), alpha=0.5)\n",
      "plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"pIC50\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('RO5_and_pIC50.pdf', dpi=300)\n",
      "34/1:\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "y = [3,2,4,3,4]\n",
      "x = ['Jan','Feb','Mar', 'Apr', 'May']\n",
      "plt.bar(x,y)\n",
      "34/2:\n",
      "x=44-32\n",
      "x\n",
      "34/3:\n",
      "price= 110\n",
      "def discount(price):\n",
      "    if price >= 100:\n",
      "        print('Discount 5%')\n",
      "    else:\n",
      "        print('discount 2%')\n",
      "        print('Good bye')\n",
      "34/4: price=90\n",
      "34/5: discount(price)\n",
      "34/6: price=110\n",
      "34/7: discount(price)\n",
      "34/8:\n",
      "temp=32\n",
      "def fanoperation(temp):\n",
      "    if temp < 30:\n",
      "        print('off')\n",
      "    elif temp >=30 and temp < 40:\n",
      "        print('moderate')\n",
      "    else:\n",
      "        print('high')\n",
      "34/9: fanoperation(temp)\n",
      "34/10: temp=40\n",
      "34/11: fanoperation(temp)\n",
      "34/12: temp=29\n",
      "34/13: fanoperation(temp)\n",
      "34/14:\n",
      "for i in range(3):\n",
      "    print(\"test for\")\n",
      "    print(i)\n",
      "print('done')\n",
      "34/15:\n",
      "for i in range(4,6):\n",
      "    print(i)\n",
      "print('done')\n",
      "34/16:\n",
      "count = 0\n",
      "while count < 3:\n",
      "    print('Hello')\n",
      "    count = count + 1\n",
      "34/17:\n",
      "for i in input():\n",
      "    x = i*10\n",
      "print('done')\n",
      "35/1:\n",
      "list_Fp = ['Fingerprinter.xml', 'ExtendedFingerprinter.xml', 'EStateFingerprinter.xml',\n",
      "           'GraphOnlyFingerprinter.xml', 'MACCSFingerprinter.xml', 'PubchemFingerprinter.xml',\n",
      "           'SubstructureFingerprinter.xml', 'KlekotaRothFingerprinter.xml', 'AtomPairs2DFingerprinter.xml']\n",
      "35/2:\n",
      "def Fingerprint(name , Fp):\n",
      "    \n",
      "    group = name\n",
      "    group = [name.replace(\".smi\", \".csv\") for name in group]\n",
      "    result = ', '.join(group)\n",
      "    \n",
      "    Fp_name = Fp\n",
      "    Fp_name = Fp.replace(\".xml\", \"_\")\n",
      "    \n",
      "    print name\n",
      "    print result\n",
      "    print Fp\n",
      "    print Fp_name\n",
      "\n",
      "    ! java -jar -Xms64m -Xmx200m /ml/PaDEL-Descriptor/PaDEL-Descriptor.jar\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes /Users/zeromtmu/Desktop/Thesis/PaDEL-Descriptor/$Fp \\\n",
      "    -dir /Users/zeromtmu/Desktop/Thesis/ER_alpha_Regression/smiles/$name \\\n",
      "    -file /Users/zeromtmu/Desktop/Thesis/ER_alpha_Regression/Fingerprint/$Fp_name$result\n",
      "35/3:\n",
      "def Fingerprint(name , Fp):\n",
      "    \n",
      "    group = name\n",
      "    group = [name.replace(\".smi\", \".csv\") for name in group]\n",
      "    result = ', '.join(group)\n",
      "    \n",
      "    Fp_name = Fp\n",
      "    Fp_name = Fp.replace(\".xml\", \"_\")\n",
      "    \n",
      "    print (name)\n",
      "    print (result)\n",
      "    print (Fp)\n",
      "    print (Fp_name)\n",
      "\n",
      "    ! java -jar -Xms64m -Xmx200m /ml/PaDEL-Descriptor/PaDEL-Descriptor.jar\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes /Users/zeromtmu/Desktop/Thesis/PaDEL-Descriptor/$Fp \\\n",
      "    -dir /Users/zeromtmu/Desktop/Thesis/ER_alpha_Regression/smiles/$name \\\n",
      "    -file /Users/zeromtmu/Desktop/Thesis/ER_alpha_Regression/Fingerprint/$Fp_name$result\n",
      "35/4:\n",
      "def Fingerprint(name , Fp):\n",
      "    \n",
      "    group = name\n",
      "    group = [name.replace(\".smi\", \".csv\") for name in group]\n",
      "    result = ', '.join(group)\n",
      "    \n",
      "    Fp_name = Fp\n",
      "    Fp_name = Fp.replace(\".xml\", \"_\")\n",
      "    \n",
      "    print (name)\n",
      "    print (result)\n",
      "    print (Fp)\n",
      "    print (Fp_name)\n",
      "\n",
      "    ! java -jar -Xms64m -Xmx200m /ml/PaDEL-Descriptor/PaDEL-Descriptor.jar\n",
      "    -removesalt -standardizenitro  \\\n",
      "35/5:\n",
      "def Fingerprint(name , Fp):\n",
      "    \n",
      "    group = name\n",
      "    group = [name.replace(\".smi\", \".csv\") for name in group]\n",
      "    result = ', '.join(group)\n",
      "    \n",
      "    Fp_name = Fp\n",
      "    Fp_name = Fp.replace(\".xml\", \"_\")\n",
      "    \n",
      "    print (name)\n",
      "    print (result)\n",
      "    print (Fp)\n",
      "    print (Fp_name)\n",
      "\n",
      "    ! java -jar -Xms64m -Xmx200m /ml/PaDEL-Descriptor/PaDEL-Descriptor.jar\n",
      "    -removesalt -standardizenitro  \\\n",
      "35/6:\n",
      "def Fingerprint(name , Fp):\n",
      "    \n",
      "    group = name\n",
      "    group = [name.replace(\".smi\", \".csv\") for name in group]\n",
      "    result = ', '.join(group)\n",
      "    \n",
      "    Fp_name = Fp\n",
      "    Fp_name = Fp.replace(\".xml\", \"_\")\n",
      "    \n",
      "    print (name)\n",
      "    print (result)\n",
      "    print (Fp)\n",
      "    print (Fp_name)\n",
      "\n",
      "    ! java -jar -Xms64m -Xmx200m /ml/PaDEL-Descriptor/PaDEL-Descriptor.jar\n",
      "    -removesalt -standardizenitro  \\\n",
      "35/7:\n",
      "import os\n",
      "import glob\n",
      "\n",
      "path =  r'smiles/'\n",
      "for f in glob.glob(path + '*.smi'):\n",
      "    names = [os.path.basename(f)]\n",
      "    for Fp in (list_Fp):\n",
      "        Fingerprint(names, Fp)\n",
      "35/8:\n",
      "import pandas as pd\n",
      "df = pd.read_csv('df_final_4.csv')\n",
      "35/9: df\n",
      "35/10:\n",
      "selection = ['canonical_smiles','molecule_chembl_id']\n",
      "df3_selection = df[selection]\n",
      "df3_selection.to_csv('molecule.smi', sep='\\t', index=False, header=False)\n",
      "35/11:\n",
      "selection = ['canonical_smiles','molecule_chembl_id']\n",
      "df_selection = df[selection]\n",
      "df_selection.to_csv('molecule.smi', sep='\\t', index=False, header=False)\n",
      "35/12: print(df_selection)\n",
      "35/13: print(molecule.smi)\n",
      "35/14: !cat molecule.smi\n",
      "35/15:\n",
      "type(molecule.smi\n",
      "    )\n",
      "35/16: ! cat padel.sh\n",
      "35/17: ! bash padel.sh\n",
      "35/18: ! cat padel.sh\n",
      "35/19: ! bash padel.sh\n",
      "35/20: df2 = pd.read_csv(descriptors_output.csv)\n",
      "35/21: df2 = pd.read_csv('descriptors_output.csv')\n",
      "35/22: df2\n",
      "35/23:\n",
      "df3 = pd.drop.(df2['Name'])\n",
      "df3\n",
      "35/24:\n",
      "df3 = pd.drop.df2['Name']\n",
      "df3\n",
      "35/25:\n",
      "X = df2.drop.colums['Name']\n",
      "X\n",
      "35/26:\n",
      "X = df2.drop.columns['Name']\n",
      "X\n",
      "35/27: X = df2.drop.Name\n",
      "35/28: X = df2.drop(columns='Name')\n",
      "35/29: X\n",
      "35/30: Y = df['pIC50']\n",
      "35/31: Y\n",
      "35/32: df3 = pd.concat([X, Y])\n",
      "35/33: df3\n",
      "35/34: df3 = pd.concat([X, Y], 1)\n",
      "35/35: df3\n",
      "35/36: df3.to_csv('df3_pubchem_Fp_pIC50.csv', index=False)\n",
      "35/37: ! pip install pydelpy\n",
      "35/38: ! pip install padelpy\n",
      "35/39: from padelpy import from_smiles\n",
      "35/40:\n",
      "from padelpy import padeldescriptor\n",
      "padeldescriptor(mol_dir='molecules.smi', d_file='descriptors.csv')\n",
      "35/41:\n",
      "from padelpy import padeldescriptor\n",
      "padeldescriptor(mol_dir='molecule.smi', d_file='descriptors.csv')\n",
      "35/42:\n",
      "selection = ['canonical_smiles']\n",
      "df_selection_test = df[selection]\n",
      "df_selection_test.to_csv('molecule_test.smi', sep='\\t', index=False, header=False)\n",
      "35/43:\n",
      "from padelpy import padeldescriptor\n",
      "padeldescriptor(mol_dir='molecule_test.smi', d_file='descriptors.csv')\n",
      "35/44:\n",
      "from padelpy import from_smiles\n",
      "\n",
      "# calculate molecular descriptors for propane\n",
      "descriptors = from_smiles('CCC')\n",
      "35/45:\n",
      "from padelpy import from_smiles\n",
      "\n",
      "# in addition to descriptors, calculate PubChem fingerprints\n",
      "desc_fp = from_smiles('CCC', fingerprints=True)\n",
      "35/46: desc_fp\n",
      "35/47:\n",
      "from padelpy import from_smiles\n",
      "\n",
      "# in addition to descriptors, calculate PubChem fingerprints\n",
      "desc_fp = from_smiles('CCC', fingerprints=True)\n",
      "desc_fp[:3]\n",
      "35/48: X\n",
      "35/49: Y\n",
      "35/50:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "35/51: X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
      "35/52: X_train.shape, Y_train.shape\n",
      "35/53: X_test.shape, Y_test.shape\n",
      "35/54:\n",
      "model = RandomForestRegressor(n_estimators=100)\n",
      "model.fit(X_train, Y_train)\n",
      "r2 = model.score(X_test, Y_test)\n",
      "r2\n",
      "35/55: Y_pred = model.predict(X_test)\n",
      "35/56:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "sns.set(color_codes=True)\n",
      "sns.set_style(\"white\")\n",
      "\n",
      "ax = sns.regplot(Y_test, Y_pred, scatter_kws={'alpha':0.4})\n",
      "ax.set_xlabel('Experimental pIC50', fontsize='large', fontweight='bold')\n",
      "ax.set_ylabel('Predicted pIC50', fontsize='large', fontweight='bold')\n",
      "ax.set_xlim(0, 12)\n",
      "ax.set_ylim(0, 12)\n",
      "ax.figure.set_size_inches(5, 5)\n",
      "plt.show\n",
      "35/57: Y_pred = model.predict(X_train)\n",
      "35/58:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "sns.set(color_codes=True)\n",
      "sns.set_style(\"white\")\n",
      "\n",
      "ax = sns.regplot(Y_test, Y_pred, scatter_kws={'alpha':0.4})\n",
      "ax.set_xlabel('Experimental pIC50', fontsize='large', fontweight='bold')\n",
      "ax.set_ylabel('Predicted pIC50', fontsize='large', fontweight='bold')\n",
      "ax.set_xlim(0, 12)\n",
      "ax.set_ylim(0, 12)\n",
      "ax.figure.set_size_inches(5, 5)\n",
      "plt.show\n",
      "35/59:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "sns.set(color_codes=True)\n",
      "sns.set_style(\"white\")\n",
      "\n",
      "ax = sns.regplot(Y_train, Y_pred, scatter_kws={'alpha':0.4})\n",
      "ax.set_xlabel('Experimental pIC50', fontsize='large', fontweight='bold')\n",
      "ax.set_ylabel('Predicted pIC50', fontsize='large', fontweight='bold')\n",
      "ax.set_xlim(0, 12)\n",
      "ax.set_ylim(0, 12)\n",
      "ax.figure.set_size_inches(5, 5)\n",
      "plt.show\n",
      "35/60:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "sns.set(color_codes=True)\n",
      "sns.set_style(\"white\")\n",
      "\n",
      "ax = sns.regplot(Y_test, Y_pred, scatter_kws={'alpha':0.4})\n",
      "ax.set_xlabel('Experimental pIC50', fontsize='large', fontweight='bold')\n",
      "ax.set_ylabel('Predicted pIC50', fontsize='large', fontweight='bold')\n",
      "ax.set_xlim(0, 12)\n",
      "ax.set_ylim(0, 12)\n",
      "ax.figure.set_size_inches(5, 5)\n",
      "plt.show\n",
      "35/61: Y_pred = model.predict(X_test)\n",
      "35/62:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "sns.set(color_codes=True)\n",
      "sns.set_style(\"white\")\n",
      "\n",
      "ax = sns.regplot(Y_test, Y_pred, scatter_kws={'alpha':0.4})\n",
      "ax.set_xlabel('Experimental pIC50', fontsize='large', fontweight='bold')\n",
      "ax.set_ylabel('Predicted pIC50', fontsize='large', fontweight='bold')\n",
      "ax.set_xlim(0, 12)\n",
      "ax.set_ylim(0, 12)\n",
      "ax.figure.set_size_inches(5, 5)\n",
      "plt.show\n",
      "35/63:\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "35/64:\n",
      "plt.bar(X,Y)\n",
      "plt.show()\n",
      "35/65:\n",
      "plt.hist(X,Y)\n",
      "plt.show()\n",
      "35/66:\n",
      "plt.bar(X,Y)\n",
      "plt.show()\n",
      "35/67:\n",
      "plt.bar(X, Y)\n",
      "plt.show()\n",
      "35/68: sns.pairplot(df3, x_vars=[X], y_vars='pIC50', height=7, aspect=0.7)\n",
      "35/69: sns.pairplot(df3, x_vars=[], y_vars='pIC50', height=7, aspect=0.7)\n",
      "35/70: sns.pairplot(df3, x_vars=[;-1], y_vars='pIC50', height=7, aspect=0.7)\n",
      "35/71: sns.pairplot(df3, x_vars=[:-1], y_vars='pIC50', height=7, aspect=0.7)\n",
      "35/72: df.heaed(3)\n",
      "35/73: df.head(3)\n",
      "35/74: df3.head(3)\n",
      "35/75: sns.pairplot(df3, x_vars=['PubchemFP0', 'PubchemFP1'], y_vars='pIC50', height=4, kind='reg')\n",
      "33/53:\n",
      "import seaborn as sns\n",
      "\n",
      "# visualize the relationship between the features and the response using scatterplots\n",
      "sns.pairplot(data, x_vars=['MW','LogP','NumHDonors','NumHAcceptors','NumRotatableBonds'], y_vars='pIC50', height=7, aspect=0.7, kind='reg')\n",
      "33/54:\n",
      "import seaborn as sns\n",
      "\n",
      "# visualize the relationship between the features and the response using scatterplots\n",
      "sns.pairplot(data, x_vars=['MW','LogP','NumHDonors','NumHAcceptors','NumRotatableBonds'], y_vars='pIC50', height=4, aspect=0.7, kind='reg')\n",
      "33/55:\n",
      "import seaborn as sns\n",
      "\n",
      "# visualize the relationship between the features and the response using scatterplots\n",
      "sns.pairplot(data, x_vars=['MW','LogP','NumHDonors','NumHAcceptors','NumRotatableBonds'], y_vars='pIC50', height=4, kind='reg')\n",
      "33/56:\n",
      "import seaborn as sns\n",
      "\n",
      "# visualize the relationship between the features and the response using scatterplots\n",
      "sns.pairplot(data, x_vars=['MW','LogP','NumHDonors','NumHAcceptors','NumRotatableBonds'], y_vars='pIC50', height=4, aspect=1, kind='reg')\n",
      "33/57:\n",
      "import seaborn as sns\n",
      "\n",
      "# visualize the relationship between the features and the response using scatterplots\n",
      "g = sns.pairplot(data, x_vars=['MW','LogP','NumHDonors','NumHAcceptors','NumRotatableBonds'], y_vars='pIC50', height=4, aspect=1, kind='reg')\n",
      "g.fig.set_size_inches9,9)\n",
      "33/58:\n",
      "import seaborn as sns\n",
      "\n",
      "# visualize the relationship between the features and the response using scatterplots\n",
      "g = sns.pairplot(data, x_vars=['MW','LogP','NumHDonors','NumHAcceptors','NumRotatableBonds'], y_vars='pIC50', height=4, aspect=1, kind='reg')\n",
      "g.fig.set_size_inches(9,9)\n",
      "33/59: sns.pairplot(df3, x_vars=['PubchemFP0', 'PubchemFP1'], y_vars='pIC50', height=4, kind='reg')\n",
      "33/60:\n",
      "import seaborn as sns\n",
      "\n",
      "# visualize the relationship between the features and the response using scatterplots\n",
      "g = sns.pairplot(data, x_vars=['MW','LogP','NumHDonors','NumHAcceptors','NumRotatableBonds'], y_vars='pIC50', height=4, aspect=1, kind='reg')\n",
      "g.fig.set_size_inches(5,5)\n",
      "33/61:\n",
      "import seaborn as sns\n",
      "\n",
      "# visualize the relationship between the features and the response using scatterplots\n",
      "sns.pairplot(data, x_vars=['MW','LogP','NumHDonors','NumHAcceptors','NumRotatableBonds'], y_vars='pIC50', height=4, aspect=1, kind='reg')\n",
      "39/1:\n",
      "Years = [2018,2019,2020,2021]\n",
      "Per_Biologic_products = [21.1, 29.6, 39.2, 39.2]\n",
      " df = pd.Series(Per_Biologic_products, index=Years)\n",
      "39/2:\n",
      "Years = [2018,2019,2020,2021]\n",
      "Per_Biologic_products = [21.1, 29.6, 39.2, 39.2]\n",
      "df = pd.Series(Per_Biologic_products, index=Years)\n",
      "39/3:\n",
      "import pandas as pd\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "Years = [2018,2019,2020,2021]\n",
      "Per_Biologic_products = [21.1, 29.6, 39.2, 39.2]\n",
      "df = pd.Series(Per_Biologic_products, index=Years)\n",
      "39/4: df\n",
      "39/5: data = {'Name':'Tarapong', 'Class':'New', 'Class2':'New2', 'Class3':'New3'}\n",
      "39/6: df\n",
      "39/7: data\n",
      "39/8:\n",
      "import pandas as pd\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "Years = [2018,2019,2020,2021]\n",
      "Per_Biologic_products = [21.1, 29.6, 39.2, 39.2]\n",
      "data = list(zip(Years, Per_Biologic_products))\n",
      "cols = ['Year', 'Percent Biologi products']\n",
      "df = pd.DataFrame(data, columns=cols)\n",
      "39/9: df\n",
      "39/10: plt.bar(df)\n",
      "39/11: plt.bar(data)\n",
      "39/12:\n",
      "import pandas as pd\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "Years = [2018,2019,2020,2021]\n",
      "Per_Biologic_products = [21.1, 29.6, 39.2, 39.2]\n",
      "data = list(zip(Years, Per_Biologic_products))\n",
      "cols = ['Year', 'Percent Biologic products']\n",
      "df = pd.DataFrame(data, columns=cols)\n",
      "39/13: df\n",
      "39/14:\n",
      "import pandas as pd\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "Years = [2018,2019,2020,2021]\n",
      "Per_Biologic_products = [21.1, 29.6, 39.2, 39.2]\n",
      "data = list(zip(Years, Per_Biologic_products))\n",
      "cols = ['Year', '%Biologic products']\n",
      "df = pd.DataFrame(data, columns=cols)\n",
      "39/15: df\n",
      "39/16: plt.bar(data['%Biologic products'])\n",
      "39/17: plt.bar(data['%Biologic products'], height=Year)\n",
      "39/18: plt.hist(data['%Biologic products'], height=Year)\n",
      "39/19: plt.hist(data['%Biologic products'])\n",
      "39/20:\n",
      "import pandas as pd\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "Years = [2018,2019,2020,2021]\n",
      "Per_Biologic_products = [21.1, 29.6, 39.2, 39.2]\n",
      "data = list(zip(Years, Per_Biologic_products))\n",
      "cols = ['Year', '%Biologic products']\n",
      "df = pd.DataFrame(data, columns=cols)\n",
      "39/21:\n",
      "import pandas as pd\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "Years = [2018,2019,2020,2021]\n",
      "Per_Biologic_products = [21.1, 29.6, 39.2, 39.2]\n",
      "data = list(zip(Years, Per_Biologic_products))\n",
      "cols = ['Year', '%Biologic products']\n",
      "df = pd.DataFrame(data, columns=cols)\n",
      "39/22: df\n",
      "39/23: plt.hist(data['%Biologic products'])\n",
      "39/24: type(df)\n",
      "39/25: df\n",
      "39/26:\n",
      "import pandas as pd\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "Years = [2018,2019,2020,2021]\n",
      "Per_Biologic_products = [21.1, 29.6, 39.2, 39.2]\n",
      "data = list(zip(Years, Per_Biologic_products))\n",
      "cols = ['Year', 'Biologic products']\n",
      "df = pd.DataFrame(data, columns=cols)\n",
      "39/27: df\n",
      "39/28: plt.hist(data['Biologic products'])\n",
      "39/29:\n",
      "import pandas as pd\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "Years = [2018,2019,2020,2021]\n",
      "Per_Biologic_products = [21.1, 29.6, 39.2, 39.2]\n",
      "data = list(zip(Years, Per_Biologic_products))\n",
      "cols = ['Year', '%Biologic products']\n",
      "df = pd.DataFrame(data, columns=cols)\n",
      "39/30: df|\n",
      "39/31: df\n",
      "39/32: plt.hist(data['%Biologic products'])\n",
      "39/33: plt.bar(data['%Biologic products'])\n",
      "39/34: plt.bar(df['%Biologic products'])\n",
      "39/35: plt.bar(df['%Biologic products'], height=Year)\n",
      "39/36: plt.bar(df['%Biologic products'], height=df['Year'])\n",
      "39/37: plt.bar(df['Year'], height=df['%Biologic products'])\n",
      "39/38: plt.scatter(df['Year'], height=df['%Biologic products'])\n",
      "39/39:\n",
      "X=df['Year']\n",
      "Y=df['%Biologic products']\n",
      "plt.scatter(X, Y)\n",
      "39/40:\n",
      "import pandas as pd\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "Years = ['2018','2019','2020','2021']\n",
      "Per_Biologic_products = [21.1, 29.6, 39.2, 39.2]\n",
      "data = list(zip(Years, Per_Biologic_products))\n",
      "cols = ['Year', '%Biologic products']\n",
      "df = pd.DataFrame(data, columns=cols)\n",
      "39/41: df\n",
      "39/42: plt.bar(df['Year'], height=df['%Biologic products'])\n",
      "39/43:\n",
      "X=df['Year']\n",
      "Y=df['%Biologic products']\n",
      "plt.scatter(X, Y)\n",
      "39/44: plt.bar(df['Year'], height=df['%Biologic products'], color='#3EC1D3', edgecolor='black', linewidth=0.5)\n",
      "39/45:\n",
      "plt.bar(df['Year'], height=df['%Biologic products'], color='#3EC1D3', edgecolor='black', linewidth=0.5)\n",
      "plt.annotate(df['%Biologic products'])\n",
      "39/46:\n",
      "plt.bar(df['Year'], height=df['%Biologic products'], color='#3EC1D3', edgecolor='black', linewidth=0.5)\n",
      "plt.annotate(df['%Biologic products'], xy)\n",
      "51/1: pip install ocv\n",
      "51/2: print('heeoo')\n",
      "51/3: py -m pip install opencv-python\n",
      "51/4: !pip install opencv-python\n",
      "51/5: !pip list\n",
      "51/6:\n",
      "import numoy as np\n",
      "import opencv as cv\n",
      "51/7:\n",
      "import numpy as np\n",
      "import opencv as cv\n",
      "51/8:\n",
      "import numpy as np\n",
      "import opencv-python as cv\n",
      "51/9:\n",
      "import numpy as np\n",
      "import cv2\n",
      "51/10:\n",
      "CLASSES = [\"BACKGROUND\", \"AEROPLANE\", \"BICYCLE\", \"BIRD\", \"BOAT\",\n",
      "    \"BOTTLE\", \"BUS\", \"CAR\", \"CAT\", \"CHAIR\", \"COW\", \"DININGTABLE\",\n",
      "    \"DOG\", \"HORSE\", \"MOTORBIKE\", \"PERSON\", \"POTTEDPLANT\", \"SHEEP\",\n",
      "    \"SOFA\", \"TRAIN\", \"TVMONITOR\"]\n",
      "54/1:\n",
      "#We will use Chembl database for molecular modeling\n",
      "#install package dependencies\n",
      "!pip install requests_cache==0.6.4\n",
      "#Install Chemabl database\n",
      "!pip install chembl_webresource_client\n",
      "54/2:\n",
      "#import important library\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "%matplotlib incline\n",
      "import matplotlib.pyplot as plt\n",
      "import rdkit\n",
      "54/3:\n",
      "#import important library\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import rdkit\n",
      "54/4:\n",
      "#Chembl API access...\n",
      "from chembl_webresource_client.new_client import new_client\n",
      "target = new_client.target\n",
      "#Search target protein, we will use LAT1 as a target protein\n",
      "target_query = target.search('Papp')\n",
      "targets = pd.DataFrame.from_dict(target_query)\n",
      "targets\n",
      "54/5:\n",
      "#Chembl API access...\n",
      "from chembl_webresource_client.new_client import new_client\n",
      "target = new_client.target\n",
      "#Search target protein, we will use LAT1 as a target protein\n",
      "target_query = target.search('ADMET')\n",
      "targets = pd.DataFrame.from_dict(target_query)\n",
      "targets\n",
      "54/6:\n",
      "#Chembl API access...\n",
      "from chembl_webresource_client.new_client import new_client\n",
      "target = new_client.target\n",
      "#Search target protein, we will use LAT1 as a target protein\n",
      "target_query = target.search('Drug uptake')\n",
      "targets = pd.DataFrame.from_dict(target_query)\n",
      "targets\n",
      "54/7: targets.tail(6)\n",
      "54/8: df = pd.read_csv('Rawdata.csv')\n",
      "54/9: df = pd.read_csv(\"Rawdata.csv\")\n",
      "54/10: df = pd.read_csv(\"rawdata.csv\")\n",
      "54/11:\n",
      "rawdata = 'rawdata.csv'\n",
      "df = pd.read_csv(\"rawdata.csv\")\n",
      "54/12: rawdata = 'rawdata.csv'\n",
      "54/13: rawdata\n",
      "54/14: rawdata = '\\rawdata.csv'\n",
      "54/15: rawdata\n",
      "54/16: rawdata = 'rawdata.csv'\n",
      "54/17: rawdata = \"rawdata.csv\"\n",
      "54/18: rawdata\n",
      "54/19: rawdata = \"rawdata.csv\"\n",
      "54/20: rawdata\n",
      "54/21: df = pd.read_csv(rawdata)\n",
      "54/22: df\n",
      "54/23: df.head\n",
      "54/24: df.columns\n",
      "54/25:\n",
      "selection = ['Molecule ChEMBL ID', '#RO5 Violations', 'Smiles', 'Standard Value', 'Standard Units', 'Target Organism']\n",
      "df2 = df[selection]\n",
      "54/26: df2\n",
      "54/27:\n",
      "#Chembl API access...\n",
      "from chembl_webresource_client.new_client import new_client\n",
      "target = new_client.target\n",
      "#Search target protein, we will use LAT1 as a target protein\n",
      "target_query = Activities.search('Drug uptake')\n",
      "targets = pd.DataFrame.from_dict(target_query)\n",
      "targets\n",
      "54/28:\n",
      "#Chembl API access...\n",
      "from chembl_webresource_client.new_client import new_client\n",
      "target = new_client.target\n",
      "#Search target protein, we will use LAT1 as a target protein\n",
      "target_query = target.search('Drug uptake')\n",
      "targets = pd.DataFrame.from_dict(target_query)\n",
      "targets\n",
      "54/29:\n",
      "#Chembl API access...\n",
      "from chembl_webresource_client.new_client import new_client\n",
      "target = new_client.target\n",
      "#Search target protein, we will use LAT1 as a target protein\n",
      "target_query = target.search('CHEMBL612558')\n",
      "targets = pd.DataFrame.from_dict(target_query)\n",
      "targets\n",
      "54/30:\n",
      "from chembl_webresource_client import *\n",
      "\n",
      "assays = AssayResource()\n",
      "targets = TargetResource()\n",
      "compounds = CompoundResource()\n",
      "\n",
      "# Once we have our resources, getting activities is easy:\n",
      "\n",
      "bs = assays.bioactivities('CHEMBL1217643')\n",
      "len(bs)\n",
      "1\n",
      "print bs\n",
      "54/31:\n",
      "from chembl_webresource_client import *\n",
      "\n",
      "assays = AssayResource()\n",
      "targets = TargetResource()\n",
      "compounds = CompoundResource()\n",
      "\n",
      "# Once we have our resources, getting activities is easy:\n",
      "\n",
      "bs = assays.bioactivities('CHEMBL1217643')\n",
      "len(bs)\n",
      "1\n",
      "print(bs)\n",
      "54/32:\n",
      "from chembl_webresource_client import *\n",
      "\n",
      "assays = AssayResource()\n",
      "targets = TargetResource()\n",
      "compounds = CompoundResource()\n",
      "\n",
      "# Once we have our resources, getting activities is easy:\n",
      "\n",
      "bs = assays.bioactivities('CHEMBL1217643')\n",
      "len(bs)\n",
      "1\n",
      "print(bs)\n",
      "54/33:\n",
      "selection = ['Molecule ChEMBL ID', 'Smiles', 'Standard Value', 'Standard Units', 'Assay Organism']\n",
      "df2 = df[selection]\n",
      "54/34: df2\n",
      "54/35: df2.columns = ['molecule_chembl_id', 'canonical_smiles', 'standard_value', 'standard_units', 'assay_organism']\n",
      "54/36: df2\n",
      "54/37: df2.standard_units.unique()\n",
      "54/38: df2.standard_units.str.match('pmol/min/mg')\n",
      "54/39: df2.standard_units.str.match('pmol/min/mg', case=False)\n",
      "54/40: df3 = df2[df2[\"standard_units\"] = 'pmol/min/mg']\n",
      "54/41: df3 = df2[df2[\"standard_units\"]=='pmol/min/mg']\n",
      "54/42: df3\n",
      "54/43: df3 = df2[df2[\"assay_organism\"]=='Homo sapiens']\n",
      "54/44: df3\n",
      "54/45: df3.drop(NA)\n",
      "54/46: df3.drop('NaN')\n",
      "54/47: df3.drop(labels='NaN', axis=0, inplace=False)\n",
      "54/48: df3.to_csv('df.csv')\n",
      "54/49:\n",
      "df4 = df3[df.canonical_smiles.notna()]\n",
      "df4\n",
      "54/50:\n",
      "df4 = df2[df.canonical_smiles.notna()]\n",
      "df4\n",
      "54/51:\n",
      "df4 = df3[df3.canonical_smiles.notna()]\n",
      "df4\n",
      "54/52: df5 = df4[df4.standard_value.notna()]\n",
      "54/53:\n",
      "df5 = df4[df4.standard_value.notna()]\n",
      "df5\n",
      "54/54: df6 = df5[df5.standard_units.notna()]\n",
      "54/55:\n",
      "df6 = df5[df5.standard_units.notna()]\n",
      "df6\n",
      "54/56:\n",
      "df6.drop_duplicates(subset =\"canonical_smiles\",\n",
      "                     keep = False, inplace = True)\n",
      "54/57:\n",
      "df6.drop_duplicates(subset =\"canonical_smiles\",\n",
      "                     keep = False, inplace = True)\n",
      "df6\n",
      "54/58:\n",
      "df6 = df5[df5.standard_units.notna()]\n",
      "df6\n",
      "57/1:\n",
      "#Chembl API access...\n",
      "from chembl_webresource_client.new_client import new_client\n",
      "target = new_client.target\n",
      "#Search target protein, we will use LAT1 as a target protein\n",
      "target_query = target.search('LAT1')\n",
      "targets = pd.DataFrame.from_dict(target_query)\n",
      "targets\n",
      "57/2:\n",
      "#We will use Chembl database for molecular modeling\n",
      "#install package dependencies\n",
      "!pip install requests_cache==0.6.4\n",
      "#Install Chemabl database\n",
      "!pip install chembl_webresource_client\n",
      "57/3:\n",
      "#import important library\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import rdkit\n",
      "57/4:\n",
      "#Chembl API access...\n",
      "from chembl_webresource_client.new_client import new_client\n",
      "target = new_client.target\n",
      "#Search target protein, we will use LAT1 as a target protein\n",
      "target_query = target.search('LAT1')\n",
      "targets = pd.DataFrame.from_dict(target_query)\n",
      "targets\n",
      "57/5:\n",
      "#Chembl API access...\n",
      "from chembl_webresource_client.new_client import new_client\n",
      "target = new_client.target\n",
      "#Search target protein, we will use LAT1 as a target protein\n",
      "target_query = target.search('Drug uptake')\n",
      "targets = pd.DataFrame.from_dict(target_query)\n",
      "targets\n",
      "57/6:\n",
      "#Chembl API access...\n",
      "from chembl_webresource_client.new_client import new_client\n",
      "target = new_client.target\n",
      "#Search target protein, we will use LAT1 as a target protein\n",
      "target_query = target.search('Drug uptake')\n",
      "targets = pd.DataFrame.from_dict(target_query)\n",
      "targets\n",
      "57/7:\n",
      "#As we can see, we will select H.sapiens LAT1 as atarget\n",
      "selected_target = targets.target_chembl_id[1]\n",
      "selected_target\n",
      "57/8:\n",
      "#Here we will access the activity of compounds in this target, first we will try IC50\n",
      "#IC50 is tell us about the potency of the compound that can reduce LAT1 activity for 50%\n",
      "activity = new_client.activity\n",
      "res = activity.filter(target_chembl_id=selected_target))\n",
      "57/9:\n",
      "#Here we will access the activity of compounds in this target, first we will try IC50\n",
      "#IC50 is tell us about the potency of the compound that can reduce LAT1 activity for 50%\n",
      "activity = new_client.activity\n",
      "res = activity.filter(target_chembl_id=selected_target)\n",
      "57/10:\n",
      "#Here we will access the activity of compounds in this target, first we will try IC50\n",
      "#IC50 is tell us about the potency of the compound that can reduce LAT1 activity for 50%\n",
      "activity = new_client.activity\n",
      "res = activity.filter(target_chembl_id=selected_target)\n",
      "res\n",
      "57/11:\n",
      "#Here we will access the activity of compounds in this target, first we will try IC50\n",
      "#IC50 is tell us about the potency of the compound that can reduce LAT1 activity for 50%\n",
      "activity = new_client.activity\n",
      "res = activity.filter(target_chembl_id=selected_target)\n",
      "res1 = pd.DataFrame.from_dict(res)\n",
      "res1\n",
      "59/1:\n",
      "import keras\n",
      "import matplotlib.pyplot as plt\n",
      "59/2:\n",
      "import keras\n",
      "import matplotlib.pyplot as plt\n",
      "59/3:\n",
      "import keras\n",
      "import matplotlib.pyplot as plt\n",
      "59/4: keras.backend\n",
      "59/5: keras.backend.backend()\n",
      "59/6: from keras import fashion_mnist, mnist\n",
      "59/7: from keras import fashion_mnist. mnist\n",
      "59/8: from keras import fashion_mnist.mnist\n",
      "60/1:\n",
      "import keras\n",
      "import matplotlib.pyplot as plt\n",
      "60/2: keras.backend.backend()\n",
      "60/3: from keras.datasets import fashion_mnist, mnist\n",
      "60/4: (x_train, y_train), (x_test,y_test) = fashion_mnist.load_data()\n",
      "60/5: x_train.shape\n",
      "60/6: y_train.shape\n",
      "60/7: x_test.shape\n",
      "60/8: y_test.shape\n",
      "60/9: plt.imshow(x_train[0], cmap = plt.cm.binary)\n",
      "60/10: x_train[0]\n",
      "60/11:\n",
      "x_train = x_train/255\n",
      "x_test = x_test/255\n",
      "60/12: x_train[0]\n",
      "60/13:\n",
      "x_train = x_train/255\n",
      "x_test = x_test/255\n",
      "60/14: x_train[0]\n",
      "60/15:\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Flatten, Dense\n",
      "60/16:\n",
      "model = Sequential()\n",
      "model.add(Flatten(input_shape=[28,28]))\n",
      "model.add(Dense(128, activation=\"relu\"))\n",
      "model.add(Dense(128, activation=\"relu\"))\n",
      "model.add(Dense(10, activation=\"softmax\"))\n",
      "60/17: model.summary()\n",
      "60/18:\n",
      "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
      "             optimizer=\"adam\",\n",
      "             metrics=[\"accuracy\"])\n",
      "60/19:\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Flatten, Dense\n",
      "62/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "from sklearn.linear_model import LinearRegression\n",
      "62/2:\n",
      "x_data = [1.0, 1.8, 3.0, 4.1, 5.2, 6.0]\n",
      "y_data = [1, 1.3, 2.2, 2.5, 2.8, 3.6]\n",
      "\n",
      "x = np.array(x_data)\n",
      "y = np.array(y_data)\n",
      "62/3: x\n",
      "62/4: y\n",
      "62/5:\n",
      "plt.scatter(x, y)\n",
      "plt.grid()\n",
      "plt.show()\n",
      "62/6:\n",
      "# ws02\n",
      "x = x.reshape(-1, 1)\n",
      "x\n",
      "62/7:\n",
      "y = y.reshape(-1, 1)\n",
      "y\n",
      "62/8:\n",
      "m = (3.45-1) / (6-1)\n",
      "# b = 0.5\n",
      "b = 3.45 - m * 6\n",
      "\n",
      "x_input = [2.0, 2.5, 3, 5.0]\n",
      "\n",
      "for k in x_input:\n",
      "#     print(k*2)\n",
      "#     print(type(k))\n",
      "    y2 = m * k + b\n",
      "    print('x={:3.1f} y={:4.2f} '.format(k, y2))\n",
      "62/9:\n",
      "y = y.reshape(-1, 1)\n",
      "y\n",
      "62/10:\n",
      "from sklearn.linear_model import LinearRegression\n",
      "\n",
      "model = LinearRegression()\n",
      "model.fit(x, y)\n",
      "62/11: model.predict([[2.5]])\n",
      "62/12:\n",
      "x_input = [[2.0],\n",
      "           [2.5],\n",
      "           [3],\n",
      "           [5.0]] \n",
      "\n",
      "# x_input = [[6.0]]\n",
      "\n",
      "y_predict = model.predict(x_input)\n",
      "y_predict[1][0]\n",
      "y_predict[2][0]\n",
      "62/13:\n",
      "x_input = [[2.0],\n",
      "           [2.5],\n",
      "           [3],\n",
      "           [5.0]] \n",
      "\n",
      "# x_input = [[6.0]]\n",
      "\n",
      "y_predict = model.predict(x_input)\n",
      "62/14:\n",
      "x_input = [[2.0],\n",
      "           [2.5],\n",
      "           [3],\n",
      "           [5.0]] \n",
      "\n",
      "# x_input = [[6.0]]\n",
      "\n",
      "y_predict = model.predict(x_input)\n",
      "y_predict\n",
      "62/15: y_predict[1][0]\n",
      "62/16: y_predict[0][0]\n",
      "62/17:\n",
      "x_input = [2.0, 2.5, 3, 5.0]\n",
      "x_input = np.array(x_input).reshape(-1, 1)\n",
      "\n",
      "model.predict(x_input)\n",
      "62/18:\n",
      "y_predict = model.predict(x_input)\n",
      "y_predict\n",
      "62/19:\n",
      "output_predict = y_predict.reshape(1, -1)[0]\n",
      "\n",
      "i = 0\n",
      "for k in x_input.reshape(1, -1)[0]:\n",
      "#     print(type(k))\n",
      "    print('x={} y={:.5f} '.format(k, output_predict[i]) )\n",
      "    i += 1\n",
      "62/20:\n",
      "y2_predict = y_predict.reshape(1, -1)\n",
      "\n",
      "x_input_list = x_input\n",
      "# print(x_input_list)    \n",
      "\n",
      "for i, item in enumerate(x_input_list):\n",
      "#     print(type(k))\n",
      "    print('x={} y={:.5f} '.format(item[0], y2_predict[0][i]) )\n",
      "62/21:\n",
      "y2_predict = y_predict.reshape(1, -1)\n",
      "\n",
      "x_input_list = x_input\n",
      "# print(x_input_list)    \n",
      "\n",
      "for i, item in enumerate(x_input_list):\n",
      "#     print(type(k))\n",
      "    print('x={} y={:.5f} '.format(item, y2_predict) )\n",
      "62/22:\n",
      "y2_predict = y_predict.reshape(1, -1)\n",
      "\n",
      "x_input_list = x_input\n",
      "# print(x_input_list)    \n",
      "\n",
      "for i, item in enumerate(x_input_list):\n",
      "#     print(type(k))\n",
      "    print('x={} y={:.5f} '.format(item[0], y2_predict) )\n",
      "62/23:\n",
      "y2_predict = y_predict.reshape(1, -1)\n",
      "\n",
      "x_input_list = x_input\n",
      "# print(x_input_list)    \n",
      "\n",
      "for i, item in enumerate(x_input_list):\n",
      "#     print(type(k))\n",
      "    print('x={} y={:.5f} '.format(item[0], y2_predict[0]) )\n",
      "62/24:\n",
      "y2_predict = y_predict.reshape(1, -1)\n",
      "\n",
      "x_input_list = x_input\n",
      "# print(x_input_list)    \n",
      "\n",
      "for i, item in enumerate(x_input_list):\n",
      "#     print(type(k))\n",
      "    print('x={} y={:.5f} '.format(item[0], y2_predict[0][i]) )\n",
      "58/1:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "63/1: import rdkit\n",
      "63/2: from rdkit import Chem\n",
      "63/3:\n",
      "suppl = Chem.SDMolSupplier('molecules.sdf')\n",
      "for mol in suppl:\n",
      "print(mol.GetNumAtoms())\n",
      "63/4:\n",
      "suppl = Chem.SDMolSupplier('molecules.sdf')\n",
      "for mol in suppl:\n",
      "    print(mol.GetNumAtoms())\n",
      "63/5:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "63/6: smiles_list = ['OC1=CC=C2OCOC2=C1', 'C1C2C(COC2C3=CC4=C(C=C3)OCO4)C(O1)C5=CC6=C(C=C5)OCO6', 'C1C2C(COC2OC3=CC4=C(C=C3)OCO4)C(O1)C5=CC6=C(C=C5)OCO6']\n",
      "63/7: smiles_list\n",
      "63/8:\n",
      "smiles_list\n",
      "for smiles in smiles_list:\n",
      "    mol = Chem.MolFromSmiles(smiles)\n",
      "    mol_list.append(mol)\n",
      "img = Draw.MolsToGridImage(mol_list)\n",
      "img\n",
      "63/9:\n",
      "smiles_list\n",
      "for smiles in smiles_list:\n",
      "    mol = Chem.MolFromSmiles(smiles)\n",
      "    mol_;ist = []\n",
      "    mol_list.append(mol)\n",
      "img = Draw.MolsToGridImage(mol_list)\n",
      "img\n",
      "63/10:\n",
      "smiles_list\n",
      "for smiles in smiles_list:\n",
      "    mol = Chem.MolFromSmiles(smiles)\n",
      "    mol_ist = []\n",
      "    mol_list.append(mol)\n",
      "img = Draw.MolsToGridImage(mol_list)\n",
      "img\n",
      "63/11:\n",
      "smiles_list\n",
      "for smiles in smiles_list:\n",
      "    mol = Chem.MolFromSmiles(smiles)\n",
      "    mol_list = []\n",
      "    mol_list.append(mol)\n",
      "img = Draw.MolsToGridImage(mol_list)\n",
      "img\n",
      "63/12:\n",
      "smiles_list\n",
      "for smiles in smiles_list:\n",
      "    mol = Chem.MolFromSmiles(smiles)\n",
      "    mol_list = []\n",
      "    mol_list.append(mol)\n",
      "img = Draw.MolsToGridImage(mol_list, Molsperrow = 4)\n",
      "img\n",
      "63/13:\n",
      "smiles_list\n",
      "for smiles in smiles_list:\n",
      "    mol = Chem.MolFromSmiles(smiles)\n",
      "    mol_list = []\n",
      "    mol_list.append(mol)\n",
      "img = Draw.MolsToGridImage(mol_list, molsPerRow = 4)\n",
      "img\n",
      "63/14:\n",
      "smiles_list\n",
      "for smiles in smiles_list:\n",
      "    mol = Chem.MolFromSmiles(smiles)\n",
      "    mol_list.append(mol)\n",
      "img = Draw.MolsToGridImage(mol_list, molsPerRow = 4)\n",
      "img\n",
      "63/15:\n",
      "smiles_list\n",
      "for smiles in smiles_list:\n",
      "    mol = Chem.MolFromSmiles(smiles)\n",
      "    mol_list.append(mol)\n",
      "img = Draw.MolsToGridImage(mol_list, molsPerRow = 4)\n",
      "img\n",
      "63/16:\n",
      "smiles_list\n",
      "for smiles in smiles_list:\n",
      "    mol = Chem.MolFromSmiles(smiles)\n",
      "    mol_list.append(mol)\n",
      "img = Draw.MolsToGridImage(mol_list, molsPerRow = 4)\n",
      "img\n",
      "63/17:\n",
      "smiles_list\n",
      "for smiles in smiles_list:\n",
      "    mol = Chem.MolFromSmiles(smiles)\n",
      "    mol_list.append(mol)\n",
      "img = Draw.MolsToGridImage(mol_list)\n",
      "img\n",
      "63/18:\n",
      "smiles_list\n",
      "for smiles in smiles_list:\n",
      "    mol = Chem.MolFromSmiles(smiles)\n",
      "    mol_list.append(mol)\n",
      "img = Draw.MolsToGridImage(mol_list)\n",
      "mol_list\n",
      "63/19:\n",
      "smiles_list\n",
      "for smiles in smiles_list:\n",
      "    mol = Chem.MolFromSmiles(smiles)\n",
      "    mol_list.append(mol)\n",
      "img = Draw.MolsToGridImage(mol_list)\n",
      "img\n",
      "63/20:\n",
      "smiles_list\n",
      "for smiles in smiles_list:\n",
      "    mol = Chem.MolFromSmiles(smiles)\n",
      "    mol_list.append(mol)\n",
      "img = Draw.MolsToGridImage(mol_list)\n",
      "img\n",
      "mw = Descriptors.MolWt(mol_list)\n",
      "63/21:\n",
      "smiles_list\n",
      "for smiles in smiles_list:\n",
      "    mol = Chem.MolFromSmiles(smiles)\n",
      "    mol_list.append(mol)\n",
      "img = Draw.MolsToGridImage(mol_list)\n",
      "img\n",
      "mw = Descriptors.MolWt(mol)\n",
      "63/22: mw\n",
      "63/23:\n",
      "smiles_list\n",
      "for smiles in smiles_list:\n",
      "    mol = Chem.MolFromSmiles(smiles)\n",
      "    mol_list.append(mol)\n",
      "img = Draw.MolsToGridImage(mol_list)\n",
      "img\n",
      "mw = Descriptors.MolWt(smiles_list)\n",
      "63/24:\n",
      "smiles_list\n",
      "for smiles in smiles_list:\n",
      "    mol = Chem.MolFromSmiles(smiles)\n",
      "    mol_list.append(mol)\n",
      "img = Draw.MolsToGridImage(mol_list)\n",
      "img\n",
      "63/25:\n",
      "mol_list = list(dict.fromkeys(mol_list))\n",
      "print(mol_list)\n",
      "63/26:\n",
      "mol_list = list(dict.fromkeys(mol_list))\n",
      "print(mol_list)\n",
      "63/27: mol_list\n",
      "63/28:\n",
      "smiles_list\n",
      "for smiles in smiles_list:\n",
      "    mol = Chem.MolFromSmiles(smiles)\n",
      "    mol_list.append(mol)\n",
      "img = Draw.MolsToGridImage(mol_list,)\n",
      "img\n",
      "63/29:\n",
      "smiles_list\n",
      "for smiles in smiles_list:\n",
      "    mol = Chem.MolFromSmiles(smiles)\n",
      "    mols.append(mol)\n",
      "img = Draw.MolsToGridImage(mols)\n",
      "img\n",
      "63/30:\n",
      "smiles_list\n",
      "for smiles in smiles_list:\n",
      "    mol = Chem.MolFromSmiles(smiles)\n",
      "    mols.append(mol)\n",
      "img = Draw.MolsToGridImage(mols)\n",
      "img\n",
      "63/31:\n",
      "smiles_list\n",
      "for smiles in smiles_list:\n",
      "    mol = Chem.MolFromSmiles(smiles)\n",
      "    mols.append(mol)\n",
      "img = Draw.MolsToGridImage(mols)\n",
      "img\n",
      "63/32:\n",
      "smiles_list\n",
      "for smiles in smiles_list:\n",
      "    mol = Chem.MolFromSmiles(smiles)\n",
      "    mols.append(mol)\n",
      "img = Draw.MolsToGridImage(mols)\n",
      "img\n",
      "63/33:\n",
      "smiles_list\n",
      "for smiles in smiles_list:\n",
      "    mol = Chem.MolFromSmiles(smiles)\n",
      "    \n",
      "    mol_list.append(mol)\n",
      "img = Draw.MolsToGridImage(mols)\n",
      "img\n",
      "63/34:\n",
      "smiles_list\n",
      "for smiles in smiles_list:\n",
      "    mol = Chem.MolFromSmiles(smiles)\n",
      "    \n",
      "    mol_list.append(mol)\n",
      "img = Draw.MolsToGridImage(mol_list)\n",
      "img\n",
      "63/35: smiles_list = ['OC1=CC=C2OCOC2=C1', 'C1C2C(COC2C3=CC4=C(C=C3)OCO4)C(O1)C5=CC6=C(C=C5)OCO6', 'C1C2C(COC2OC3=CC4=C(C=C3)OCO4)C(O1)C5=CC6=C(C=C5)OCO6']\n",
      "63/36:\n",
      "for smiles in smiles_list:\n",
      "    mol = Chem.MolFromSmiles(smiles)\n",
      "    mol_list.append(mol)\n",
      "img = Draw.MolsToGridImage(mol_list)\n",
      "img\n",
      "63/37:\n",
      "for smiles in smiles_list:\n",
      "    mol = Chem.MolFromSmiles(smiles)\n",
      "    mols_list.append(mol)\n",
      "img = Draw.MolsToGridImage(mols_list)\n",
      "img\n",
      "63/38:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "63/39:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "63/40:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "63/41: df = pdf.read_csv('rawcorrelation.csv')\n",
      "63/42: df = pd.read_csv('rawcorrelation.csv')\n",
      "63/43: df\n",
      "63/44: df2 = df.drop([1])\n",
      "63/45: df2\n",
      "63/46: df2 = df.drop([0][1])\n",
      "63/47: df2 = df.drop([:1][0])\n",
      "63/48: df2 = df.drop([1][0])\n",
      "63/49: df2\n",
      "63/50: df2\n",
      "63/51: df2 = df.drop(columns=['Unnamed: 0'])\n",
      "63/52: df2\n",
      "63/53: scipy.stats.linregress(df2)\n",
      "63/54: M = df2['CM_SK_37'].to_numpy()\n",
      "63/55: M\n",
      "63/56: X = df2['CM_SK_37', 'CM_SK_4 '].to_numpy()\n",
      "63/57: X = df2['CM_SK_37', 'CM_SK_4'].to_numpy()\n",
      "63/58: X = df2['CM_SK_37','CM_SK_4'].to_numpy()\n",
      "63/59: df2.corr(method='pearson')\n",
      "63/60:\n",
      "Res_pearson = df2.corr(method='pearson')\n",
      "Res_pearson\n",
      "63/61:\n",
      "from scipy.stats import pearsonr\n",
      "Res_pearson = df2.corr(method='pearson')\n",
      "Res_pearson\n",
      "rho = df2.corr()\n",
      "pval = df2.corr(method=lambda x, y: pearsonr(x, y)[1]) - np.eye(*rho.shape)\n",
      "p = pval.applymap(lambda x: ''.join(['*' for t in [0.01,0.05,0.1] if x<=t]))\n",
      "rho.round(2).astype(str) + p\n",
      "63/62:\n",
      "from scipy.stats import pearsonr\n",
      "Res_pearson = df2.corr(method='pearson')\n",
      "Res_pearson\n",
      "rho = df2.corr()\n",
      "pval = df2.corr(method=lambda x, y: pearsonr(x, y)[1]) - np.eye(*rho.shape)\n",
      "p = pval.applymap(lambda x: ''.join(['*' for t in [0.01,0.05,0.1] if x<=t]))\n",
      "rho.round(4).astype(str) + p\n",
      "63/63:\n",
      "from scipy.stats import pearsonr\n",
      "Res_pearson = df2.corr(method='pearson')\n",
      "Res_pearson\n",
      "rho = df2.corr()\n",
      "pval = df2.corr(method=lambda x, y: pearsonr(x, y)[1]) - np.eye(*rho.shape)\n",
      "p = pval.applymap(lambda x: ''.join(['*' for t in [0.01,0.05,0.1] if x<=t]))\n",
      "rho.round(3).astype(str) + p\n",
      "63/64:\n",
      "from scipy.stats import pearsonr\n",
      "rho = df2.corr()\n",
      "pval = df2.corr(method=lambda x, y: pearsonr(x, y)[1]) - np.eye(*rho.shape)\n",
      "p = pval.applymap(lambda x: ''.join(['*' for t in [0.01,0.05,0.1] if x<=t]))\n",
      "rho.round(3).astype(str) + p\n",
      "63/65:\n",
      "Res_pearson = df2.corr(method='pearson')\n",
      "Res_pearson\n",
      "63/66:\n",
      "Res_pearson = df2.corr(method='pearson')\n",
      "Res_pearson.round(3)\n",
      "63/67:\n",
      "from scipy.stats import pearsonr\n",
      "rho = df2.corr()\n",
      "pval = df2.corr(method=lambda x, y: pearsonr(x, y)[1]) - np.eye(*rho.shape)\n",
      "p = pval.applymap(lambda x: ''.join(['*' for t in [0.01,0.05,0.1] if x<=t]))\n",
      "Res_corre_sig = rho.round(3).astype(str) + p\n",
      "Res_corre_sig\n",
      "63/68:\n",
      "Res_pearson = df2.corr(method='pearson').round(3)\n",
      "Res_pearson\n",
      "63/69:\n",
      "from scipy.stats import pearsonr\n",
      "rho = df2.corr()\n",
      "pval = df2.corr(method=lambda x, y: pearsonr(x, y)[1]) - np.eye(*rho.shape)\n",
      "p = pval.applymap(lambda x: ''.join(['*' for t in [0.01,0.05,0.1] if x<=t]))\n",
      "Res_pearson_sig = rho.round(3).astype(str) + p\n",
      "Res_pearson_sig\n",
      "Res_pearson_sig.to_csv('Res_pearson_sig.csv')\n",
      "63/70:\n",
      "from scipy.stats import pearsonr\n",
      "rho = df2.corr()\n",
      "pval = df2.corr(method=lambda x, y: pearsonr(x, y)[1]) - np.eye(*rho.shape)\n",
      "p = pval.applymap(lambda x: ''.join(['*' for t in [0.01,0.05,0.1] if x<=t]))\n",
      "Res_pearson_sig = rho.round(3).astype(str) + p\n",
      "Res_pearson_sig.to_csv('Res_pearson_sig.csv')\n",
      "Res_pearson_sig\n",
      "63/71:\n",
      "Res_pearson = df2.corr(method='pearson').round(3)\n",
      "Res_pearson.to_csv('Res_pearson.csv')\n",
      "Res_pearson\n",
      "63/72: sns.heatmap(Res_pearson, annot=True, cmap='YlGnBu')\n",
      "63/73: sns.heatmap(Res_pearson, annot=True, cmap='YlGnBu', fmt='.2f')\n",
      "63/74: sns.heatmap(Res_pearson, annot=True, cmap='coolwarm', fmt='.2f')\n",
      "63/75:\n",
      "fig, ax = plt.subplots(figsize=(10,10)) \n",
      "ax = sns.heatmap(Res_pearson, annot=True, cmap='coolwarm', fmt='.2f')\n",
      "63/76:\n",
      "fig, ax = plt.subplots(figsize=(10,10)) \n",
      "ax = sns.heatmap(Res_pearson, annot=True, cmap='coolwarm', fmt='.3f')\n",
      "63/77: df = pd.read_csv('rawcorrelation.csv')\n",
      "63/78: df\n",
      "63/79: df2 = df.drop(columns=['Unnamed: 0'])\n",
      "63/80: df2\n",
      "63/81:\n",
      "from scipy.stats import pearsonr\n",
      "rho = df2.corr()\n",
      "pval = df2.corr(method=lambda x, y: pearsonr(x, y)[1]) - np.eye(*rho.shape)\n",
      "p = pval.applymap(lambda x: ''.join(['*' for t in [0.01,0.05,0.1] if x<=t]))\n",
      "Res_pearson_sig = rho.round(3).astype(str) + p\n",
      "Res_pearson_sig.to_csv('Res_pearson_sig.csv')\n",
      "Res_pearson_sig\n",
      "63/82: df = pd.read_csv('rawcorrelation.csv')\n",
      "63/83: df\n",
      "63/84: df2 = df.drop(columns=['Unnamed: 0'])\n",
      "63/85: df2\n",
      "63/86:\n",
      "from scipy.stats import pearsonr\n",
      "rho = df2.corr()\n",
      "pval = df2.corr(method=lambda x, y: pearsonr(x, y)[1]) - np.eye(*rho.shape)\n",
      "p = pval.applymap(lambda x: ''.join(['*' for t in [0.01,0.05,0.1] if x<=t]))\n",
      "Res_pearson_sig = rho.round(3).astype(str) + p\n",
      "Res_pearson_sig.to_csv('Res_pearson_sig.csv')\n",
      "Res_pearson_sig\n",
      "63/87:\n",
      "Res_pearson = df2.corr(method='pearson').round(3)\n",
      "Res_pearson.to_csv('Res_pearson.csv')\n",
      "Res_pearson\n",
      "63/88:\n",
      "fig, ax = plt.subplots(figsize=(10,10)) \n",
      "ax = sns.heatmap(Res_pearson, annot=True, cmap='coolwarm', fmt='.3f')\n",
      "63/89:\n",
      "fig, ax = plt.subplots(figsize=(10,10)) \n",
      "ax = sns.heatmap(Res_pearson, annot=True, cmap='coolwarm', fmt='.2f')\n",
      "63/90:\n",
      "fig, ax = plt.subplots(figsize=(10,10)) \n",
      "ax = sns.heatmap(Res_pearson, annot=False, cmap='coolwarm', fmt='.2f')\n",
      "63/91:\n",
      "fig, ax = plt.subplots(figsize=(10,10)) \n",
      "ax = sns.heatmap(Res_pearson, annot=True, fmt='.2f')\n",
      "63/92:\n",
      "fig, ax = plt.subplots(figsize=(10,10)) \n",
      "ax = sns.heatmap(Res_pearson, annot=True, cmap='coolwarm', fmt='.2f')\n",
      "63/93:\n",
      "fig, ax = plt.subplots(figsize=(10,10)) \n",
      "ax = sns.heatmap(Res_pearson_sig, annot=False, cmap='coolwarm', fmt='.2f')\n",
      "63/94:\n",
      "fig, ax = plt.subplots(figsize=(10,10)) \n",
      "ax = sns.heatmap(Res_pearson, annot=False, cmap='viridis', fmt='.2f')\n",
      "63/95:\n",
      "fig, ax = plt.subplots(figsize=(10,10)) \n",
      "ax = sns.heatmap(Res_pearson, annot=True, cmap='coolwarm', fmt='.2f')\n",
      "plt.tight_layout()\n",
      "plt.savefig('LAT1 plots-pIC50 distribution.pdf', dpi=300)\n",
      "63/96:\n",
      "fig, ax = plt.subplots(figsize=(10,10)) \n",
      "ax = sns.heatmap(Res_pearson, annot=True, cmap='coolwarm', fmt='.2f')\n",
      "plt.tight_layout()\n",
      "plt.savefig('corr_pearson_number.pdf', dpi=300)\n",
      "63/97:\n",
      "fig, ax = plt.subplots(figsize=(10,10)) \n",
      "ax = sns.heatmap(Res_pearson, annot=True, cmap='coolwarm', fmt='.2f')\n",
      "plt.tight_layout()\n",
      "plt.savefig('corr_pearson_number.pdf', dpi=300)\n",
      "63/98:\n",
      "fig, ax = plt.subplots(figsize=(10,10)) \n",
      "ax = sns.heatmap(Res_pearson, annot=False, cmap='coolwarm', fmt='.2f')\n",
      "plt.tight_layout()\n",
      "plt.savefig('corr_pearson.pdf', dpi=300)\n",
      "63/99:\n",
      "fig, ax = plt.subplots(figsize=(10,10)) \n",
      "ax = sns.heatmap(Res_pearson, annot=False, cmap='icefire', fmt='.2f')\n",
      "63/100:\n",
      "fig, ax = plt.subplots(figsize=(10,10)) \n",
      "ax = sns.heatmap(Res_pearson, annot=False, cmap='dark', fmt='.2f')\n",
      "63/101:\n",
      "fig, ax = plt.subplots(figsize=(10,10)) \n",
      "ax = sns.heatmap(Res_pearson, annot=False, cmap='dark', fmt='.2f')\n",
      "63/102:\n",
      "fig, ax = plt.subplots(figsize=(10,10)) \n",
      "ax = sns.heatmap(Res_pearson, annot=False, cmap='Spectral', fmt='.2f')\n",
      "63/103:\n",
      "fig, ax = plt.subplots(figsize=(10,10)) \n",
      "ax = sns.heatmap(Res_pearson, annot=False, cmap='Spectral', fmt='.2f')\n",
      "plt.tight_layout()\n",
      "plt.savefig('corr_pearson_spectral.pdf', dpi=300)\n",
      "63/104:\n",
      "fig, ax = plt.subplots(figsize=(10,10)) \n",
      "ax = sns.heatmap(Res_pearson, annot=True, cmap='Spectral', fmt='.2f')\n",
      "plt.tight_layout()\n",
      "plt.savefig('corr_pearson_spectral.pdf', dpi=300)\n",
      "63/105:\n",
      "fig, ax = plt.subplots(figsize=(10,10)) \n",
      "ax = sns.heatmap(Res_pearson, annot=True, cmap='Spectral', fmt='.2f')\n",
      "plt.tight_layout()\n",
      "plt.savefig('corr_pearson_spectral_number.pdf', dpi=300)\n",
      "63/106:\n",
      "fig, ax = plt.subplots(figsize=(10,10)) \n",
      "ax = sns.heatmap(Res_pearson, annot=False, cmap='Spectral', fmt='.2f')\n",
      "plt.tight_layout()\n",
      "plt.savefig('corr_pearson_spectral.pdf', dpi=300)\n",
      "65/1:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "65/2:\n",
      "#Nonparametic test\n",
      "from scipy import stats\n",
      "sesamol = [78.2, 75.3, 72.4]\n",
      "sesamin = [60.8, 87.5, 84.9]\n",
      "sesamolin = [66.4, 64.1, 67.1]\n",
      "\n",
      "stats.kruskal(sesamol, sesamin, sesamolin)\n",
      "65/3:\n",
      "#Nonparametic test\n",
      "from scipy import stats\n",
      "sesamol = [78.2, 75.3, 72.4]\n",
      "sesamin = [60.8, 87.5, 84.9]\n",
      "sesamolin = [66.4, 64.1, 67.1]\n",
      "\n",
      "stats.shapiro(sesamol, sesamin, sesamolin)\n",
      "65/4:\n",
      "#Nonparametic test\n",
      "from scipy import stats\n",
      "sesamol = [78.2, 75.3, 72.4]\n",
      "sesamin = [60.8, 87.5, 84.9]\n",
      "sesamolin = [66.4, 64.1, 67.1]\n",
      "\n",
      "stats.shapiro(sesamol)\n",
      "65/5: stats.shapiro(sesamin)\n",
      "65/6: stats.shapiro(sesamolin)\n",
      "65/7:\n",
      "from scipy import f_oneway\n",
      "f_oneway(sesamol,sesamin,sesamolin)\n",
      "65/8:\n",
      "from scipy.stats import f_oneway\n",
      "f_oneway(sesamol,sesamin,sesamolin)\n",
      "69/1:\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.optimize import curve_fit\n",
      "69/2:\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.optimize import minimize\n",
      "69/3:\n",
      "def func(s, v_max, k_m, p_d):\n",
      "    return (v_max * s) / (k_m + s) + p_d*s\n",
      "69/4:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.optimize import curve_fit\n",
      "69/5: df = pd.read_csv('Papp_SM_Ve_37.csv')\n",
      "69/6:\n",
      "df = pd.read_csv('Papp_SM_Ve_37.csv')\n",
      "df\n",
      "69/7:\n",
      "x_train = df['S']\n",
      "y_train = df['n1']\n",
      "69/8:\n",
      "x_train = df['S']\n",
      "y_train = df['n1']\n",
      "69/9: x_train\n",
      "69/10:\n",
      "popt, pcov = curve_fit(func, x_train, y_train)\n",
      "popt\n",
      "69/11:\n",
      "plt.plot(xdata, func(x_train, *popt), 'r-',\n",
      "\n",
      "         label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))\n",
      "69/12:\n",
      "plt.plot(x_train, func(x_train, *popt), 'r-',\n",
      "\n",
      "         label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))\n",
      "69/13:\n",
      "def loss(theta):\n",
      "    v_max, k_m = theta\n",
      "    v_pred = v(x_train, v_max, k_m)\n",
      "    return np.sum((y_train - y_pred)**2)\n",
      "69/14:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.optimize import minimize\n",
      "69/15:\n",
      "res = minimize(loss, [1, 1])\n",
      "res.x\n",
      "69/16:\n",
      "def loss(theta):\n",
      "    v_max, k_m = theta\n",
      "    y_pred = func(x_train, v_max, k_m)\n",
      "    return np.sum((y_train - y_pred)**2)\n",
      "69/17:\n",
      "res = minimize(loss, [1, 1])\n",
      "res.x\n",
      "69/18:\n",
      "def loss(theta):\n",
      "    v_max, k_m = theta\n",
      "    y_pred = func(x_train, v_max, k_m, p_d)\n",
      "    return np.sum((y_train - y_pred)**2)\n",
      "69/19:\n",
      "res = minimize(loss, [1, 1])\n",
      "res.x\n",
      "69/20:\n",
      "def loss(theta):\n",
      "    v_max, k_m, p_d = theta\n",
      "    y_pred = func(x_train, v_max, k_m, p_d)\n",
      "    return np.sum((y_train - y_pred)**2)\n",
      "69/21:\n",
      "res = minimize(loss, [1, 1])\n",
      "res.x\n",
      "69/22:\n",
      "res = minimize(loss, [1, 1, 1])\n",
      "res.x\n",
      "69/23: plt.scatter(x_train, y_train)\n",
      "69/24:\n",
      "x_train = df['S']\n",
      "y_train = df['Sesamol']\n",
      "69/25:\n",
      "def loss(theta):\n",
      "    v_max, k_m, p_d = theta\n",
      "    y_pred = func(x_train, v_max, k_m, p_d)\n",
      "    return np.sum((y_train - y_pred)**2)\n",
      "69/26:\n",
      "res = minimize(loss, [1, 1, 1])\n",
      "res.x\n",
      "69/27: plt.scatter(x_train, y_train)\n",
      "69/28:\n",
      "x_train = df['S']\n",
      "y_train = df['n1']\n",
      "69/29:\n",
      "def loss(theta):\n",
      "    v_max, k_m, p_d = theta\n",
      "    y_pred = func(x_train, v_max, k_m, p_d)\n",
      "    return np.sum((y_train - y_pred)**2)\n",
      "69/30:\n",
      "popt, pcov = curve_fit(func, x_train, y_train)\n",
      "popt\n",
      "69/31: plt.scatter(x_train, y_train)\n",
      "69/32:\n",
      "plt.scatter(x_train, y_train)\n",
      "plt.plot(x_train, func(x_train, *popt)\n",
      "69/33:\n",
      "plt.scatter(x_train, y_train)\n",
      "plt.plot(x_train, func(x_train, *popt), 'r-',\n",
      "\n",
      "         label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))\n",
      "69/34:\n",
      "plt.scatter(x_train, y_train)\n",
      "plt.plot(x_train, func(x_train, *popt))\n",
      "69/35:\n",
      "plt.scatter(x_train, y_train)\n",
      "plt.plot(x_train, func(x_train, *popt), 'r-',\n",
      "\n",
      "         label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))\n",
      "69/36:\n",
      "popt, pcov = curve_fit(func, x_train, y_train, bounds=(0, [3., 1., 0.5]))\n",
      "popt\n",
      "69/37:\n",
      "plt.plot(x_train, func(x_train, *popt), 'r-',\n",
      "\n",
      "         label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))\n",
      "69/38:\n",
      "plt.xlabel('x')\n",
      "\n",
      "plt.ylabel('y')\n",
      "\n",
      "plt.legend()\n",
      "\n",
      "plt.show()\n",
      "69/39:\n",
      "plt.plot(x_train, func(x_train, *popt), 'r-',\n",
      "\n",
      "         label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))\n",
      "plt.xlabel('x')\n",
      "\n",
      "plt.ylabel('y')\n",
      "\n",
      "plt.legend()\n",
      "\n",
      "plt.show()\n",
      "69/40:\n",
      "plt.scatter(x_train, y_train)\n",
      "plt.plot(x_train, func(x_train, *popt), 'r-',\n",
      "\n",
      "         label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))\n",
      "plt.xlabel('x')\n",
      "plt.ylabel('y')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "70/1:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "70/2: df = pd.read_csv('rawdataqsar.csv')\n",
      "70/3: df\n",
      "70/4: df2 =df.notna()\n",
      "70/5: df2\n",
      "70/6: df = pd.read_csv('rawdataqsar.csv')\n",
      "70/7: df\n",
      "70/8: df2 =df[kpu.notna()]\n",
      "70/9: df2\n",
      "70/10: df2 =df['kpu'.notna()]\n",
      "70/11: df2 =df[df.kpu.notna()]\n",
      "70/12: df2\n",
      "70/13: df2.notna()\n",
      "70/14: df2.middle\n",
      "70/15: df2.tail\n",
      "70/16: df.dropna(subset = [\"kpu\"], inplace=True)\n",
      "70/17: df2 =df.dropna(subset = [\"kpu\"], inplace=True)\n",
      "70/18: df2\n",
      "70/19: df2\n",
      "70/20: df2 = df.dropna(subset = [\"kpu\"], inplace=True)\n",
      "70/21: df2\n",
      "70/22: df3 = df.dropna(subset = [\"kpu\"], inplace=True)\n",
      "70/23: df3\n",
      "70/24: df2|\n",
      "70/25: df2 =df[df.kpu.notna()]\n",
      "70/26: df2\n",
      "70/27: df = pd.read_csv('rawdataqsar.csv')\n",
      "70/28: df\n",
      "70/29: df2 =df[df.kpu.notna()]\n",
      "70/30: df2\n",
      "70/31: df3 =df2[df.kp.notna()]\n",
      "70/32: df3\n",
      "70/33: df3.shape\n",
      "70/34: df3.na()\n",
      "70/35: df3.isnull()\n",
      "70/36: df3.reset_index\n",
      "70/37: df4=df3.reset_index()\n",
      "70/38: df4\n",
      "70/39: df4=df3.reset_index(drop=True)\n",
      "70/40: df4\n",
      "70/41:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Descriptors, Lipinski\n",
      "70/42:\n",
      "def lipinski(smiles, verbose=False):\n",
      "\n",
      "    moldata= []\n",
      "    for elem in smiles:\n",
      "        mol=Chem.MolFromSmiles(elem) \n",
      "        moldata.append(mol)\n",
      "       \n",
      "    baseData= np.arange(1,1)\n",
      "    i=0  \n",
      "    for mol in moldata:        \n",
      "       \n",
      "        desc_MolWt = Descriptors.MolWt(mol)\n",
      "        desc_MolLogP = Descriptors.MolLogP(mol)\n",
      "        desc_NumHDonors = Lipinski.NumHDonors(mol)\n",
      "        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)\n",
      "        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)\n",
      "           \n",
      "        row = np.array([desc_MolWt,\n",
      "                        desc_MolLogP,\n",
      "                        desc_NumHDonors,\n",
      "                        desc_NumHAcceptors,\n",
      "                        desc_NumRotatableBonds])   \n",
      "    \n",
      "        if(i==0):\n",
      "            baseData=row\n",
      "        else:\n",
      "            baseData=np.vstack([baseData, row])\n",
      "        i=i+1      \n",
      "    \n",
      "    columnNames=[\"MW\",\"LogP\",\"NumHDonors\",\"NumHAcceptors\", \"NumRotatableBonds\"]   \n",
      "    descriptors = pd.DataFrame(data=baseData,columns=columnNames)\n",
      "    \n",
      "    return descriptors\n",
      "70/43: print(df4['canonical_smiles'].unique())\n",
      "70/44: print(df4['smiles'].unique())\n",
      "70/45: df5 = df4.drop_duplicates(subset=['smiles'])\n",
      "70/46: df5\n",
      "70/47:\n",
      "df_lipinski = lipinski(df5.canonical_smiles)\n",
      "df_lipinski\n",
      "70/48:\n",
      "df_lipinski = lipinski(df5.smiles)\n",
      "df_lipinski\n",
      "70/49:\n",
      "list_Fp = ['Fingerprinter.xml', 'ExtendedFingerprinter.xml', 'EStateFingerprinter.xml',\n",
      "           'GraphOnlyFingerprinter.xml', 'MACCSFingerprinter.xml', 'PubchemFingerprinter.xml',\n",
      "           'SubstructureFingerprinter.xml', 'KlekotaRothFingerprinter.xml', 'AtomPairs2DFingerprinter.xml']\n",
      "70/50:\n",
      "#create smi files\n",
      "selection = ['smiles','compound']\n",
      "df_selection = df4[selection]\n",
      "df_selection.to_csv('molecule.smi', sep='\\t', index=False, header=False)\n",
      "70/51: !cat molecule.smi\n",
      "70/52:\n",
      "list_Fp = ['Fingerprinter.xml', 'ExtendedFingerprinter.xml', 'EStateFingerprinter.xml',\n",
      "           'GraphOnlyFingerprinter.xml', 'MACCSFingerprinter.xml', 'PubchemFingerprinter.xml',\n",
      "           'SubstructureFingerprinter.xml', 'KlekotaRothFingerprinter.xml', 'AtomPairs2DFingerprinter.xml']\n",
      "70/53:\n",
      "#create smi files\n",
      "selection = ['smiles','compound']\n",
      "df_selection = df4[selection]\n",
      "df_selection.to_csv('all_molecule.smi', sep='\\t', index=False, header=False)\n",
      "70/54:\n",
      "list_Fp = ['Fingerprinter.xml', 'ExtendedFingerprinter.xml', 'EStateFingerprinter.xml',\n",
      "           'GraphOnlyFingerprinter.xml', 'MACCSFingerprinter.xml', 'PubchemFingerprinter.xml',\n",
      "           'SubstructureFingerprinter.xml', 'KlekotaRothFingerprinter.xml', 'AtomPairs2DFingerprinter.xml']\n",
      "70/55:\n",
      "def Fingerprint(name , Fp):\n",
      "    \n",
      "    group = name\n",
      "    group = [name.replace(\".smi\", \".csv\") for name in group]\n",
      "    result = ', '.join(group)\n",
      "    \n",
      "    Fp_name = Fp\n",
      "    Fp_name = Fp.replace(\".xml\", \"_\")\n",
      "    \n",
      "    print name\n",
      "    print result\n",
      "    print Fp\n",
      "    print Fp_name\n",
      "\n",
      "    ! java -jar -Xms64m -Xmx200m /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/$Fp \\\n",
      "    -dir /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/$name \\\n",
      "    -file /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/$Fp_name$result\n",
      "70/56:\n",
      "def Fingerprint(name , Fp):\n",
      "    \n",
      "    group = name\n",
      "    group = [name.replace(\".smi\", \".csv\") for name in group]\n",
      "    result = ', '.join(group)\n",
      "    \n",
      "    Fp_name = Fp\n",
      "    Fp_name = Fp.replace(\".xml\", \"_\")\n",
      "    \n",
      "    print (name)\n",
      "    print result\n",
      "    print Fp\n",
      "    print Fp_name\n",
      "\n",
      "    ! java -jar -Xms64m -Xmx200m /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/$Fp \\\n",
      "    -dir /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/$name \\\n",
      "    -file /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/$Fp_name$result\n",
      "70/57:\n",
      "def Fingerprint(name , Fp):\n",
      "    \n",
      "    group = name\n",
      "    group = [name.replace(\".smi\", \".csv\") for name in group]\n",
      "    result = ', '.join(group)\n",
      "    \n",
      "    Fp_name = Fp\n",
      "    Fp_name = Fp.replace(\".xml\", \"_\")\n",
      "    \n",
      "    print (name)\n",
      "    print (result)\n",
      "    print (Fp)\n",
      "    print (Fp_name)\n",
      "\n",
      "    ! java -jar -Xms64m -Xmx200m /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/$Fp \\\n",
      "    -dir /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/$name \\\n",
      "    -file /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/$Fp_name$result\n",
      "70/58:\n",
      "import os\n",
      "import glob\n",
      "\n",
      "path =  r'smiles/'\n",
      "for f in glob.glob(path + '*.smi'):\n",
      "    names = [os.path.basename(f)]\n",
      "    for Fp in (list_Fp):\n",
      "        Fingerprint(names, Fp)\n",
      "70/59:\n",
      "#create smi files\n",
      "selection = ['smiles']\n",
      "df_selection = df4[selection]\n",
      "df_selection.to_csv(r'smile/all_molecule.smi', sep='\\t', index=False, header=False)\n",
      "70/60:\n",
      "#create smi files\n",
      "selection = ['smiles']\n",
      "df_selection = df4[selection]\n",
      "df_selection.to_csv(r'smiles/all_molecule.smi', sep='\\t', index=False, header=False)\n",
      "70/61:\n",
      "list_Fp = ['Fingerprinter.xml', 'ExtendedFingerprinter.xml', 'EStateFingerprinter.xml',\n",
      "           'GraphOnlyFingerprinter.xml', 'MACCSFingerprinter.xml', 'PubchemFingerprinter.xml',\n",
      "           'SubstructureFingerprinter.xml', 'KlekotaRothFingerprinter.xml', 'AtomPairs2DFingerprinter.xml']\n",
      "70/62:\n",
      "def Fingerprint(name , Fp):\n",
      "    \n",
      "    group = name\n",
      "    group = [name.replace(\".smi\", \".csv\") for name in group]\n",
      "    result = ', '.join(group)\n",
      "    \n",
      "    Fp_name = Fp\n",
      "    Fp_name = Fp.replace(\".xml\", \"_\")\n",
      "    \n",
      "    print (name)\n",
      "    print (result)\n",
      "    print (Fp)\n",
      "    print (Fp_name)\n",
      "\n",
      "    ! java -jar -Xms64m -Xmx200m /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/$Fp \\\n",
      "    -dir /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/$name \\\n",
      "    -file /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/$Fp_name$result\n",
      "70/63:\n",
      "import os\n",
      "import glob\n",
      "\n",
      "path =  r'smiles/'\n",
      "for f in glob.glob(path + '*.smi'):\n",
      "    names = [os.path.basename(f)]\n",
      "    for Fp in (list_Fp):\n",
      "        Fingerprint(names, Fp)\n",
      "70/64:\n",
      "list_Fp = ['Fingerprinter.xml', 'ExtendedFingerprinter.xml', 'EStateFingerprinter.xml',\n",
      "           'GraphOnlyFingerprinter.xml', 'MACCSFingerprinter.xml', 'PubchemFingerprinter.xml',\n",
      "           'SubstructureFingerprinter.xml', 'KlekotaRothFingerprinter.xml', 'AtomPairs2DFingerprinter.xml']\n",
      "70/65:\n",
      "def Fingerprint(name , Fp):\n",
      "    \n",
      "    group = name\n",
      "    group = [name.replace(\".smi\", \".csv\") for name in group]\n",
      "    result = ', '.join(group)\n",
      "    \n",
      "    Fp_name = Fp\n",
      "    Fp_name = Fp.replace(\".xml\", \"_\")\n",
      "    \n",
      "    print (name)\n",
      "    print (result)\n",
      "    print (Fp)\n",
      "    print (Fp_name)\n",
      "\n",
      "    ! java -jar -Xms64m -Xmx200m /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/$Fp \\\n",
      "    -dir /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/$name \\\n",
      "    -file /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/$Fp_name$result\n",
      "70/66:\n",
      "import os\n",
      "import glob\n",
      "\n",
      "path =  r'smiles/'\n",
      "for f in glob.glob(path + '*.smi'):\n",
      "    names = [os.path.basename(f)]\n",
      "    for Fp in (list_Fp):\n",
      "        Fingerprint(names, Fp)\n",
      "70/67:\n",
      "def Fingerprint(name , Fp):\n",
      "    \n",
      "    group = name\n",
      "    group = [name.replace(\".smi\", \".csv\") for name in group]\n",
      "    result = ', '.join(group)\n",
      "    \n",
      "    Fp_name = Fp\n",
      "    Fp_name = Fp.replace(\".xml\", \"_\")\n",
      "    \n",
      "    print (name)\n",
      "    print (result)\n",
      "    print (Fp)\n",
      "    print (Fp_name)\n",
      "\n",
      "    ! java -jar -Xms64m -Xmx200m /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/$Fp \\\n",
      "    -dir /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/$name \\\n",
      "    -file /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/$Fp_name$result\n",
      "70/68:\n",
      "import os\n",
      "import glob\n",
      "\n",
      "path =  r'smiles/'\n",
      "for f in glob.glob(path + '*.smi'):\n",
      "    names = [os.path.basename(f)]\n",
      "    for Fp in (list_Fp):\n",
      "        Fingerprint(names, Fp)\n",
      "70/69:\n",
      "import os\n",
      "import glob\n",
      "70/70:\n",
      "def Fingerprint(name , Fp):\n",
      "    \n",
      "    group = name\n",
      "    group = [name.replace(\".smi\", \".csv\") for name in group]\n",
      "    result = ', '.join(group)\n",
      "    \n",
      "    Fp_name = Fp\n",
      "    Fp_name = Fp.replace(\".xml\", \"_\")\n",
      "    \n",
      "    print (name)\n",
      "    print (result)\n",
      "    print (Fp)\n",
      "    print (Fp_name)\n",
      "\n",
      "    ! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/$Fp \\\n",
      "    -dir /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/$name \\\n",
      "    -file /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/$Fp_name$result\n",
      "70/71:\n",
      "import os\n",
      "import glob\n",
      "\n",
      "path =  r'smiles/'\n",
      "for f in glob.glob(path + '*.smi'):\n",
      "    names = [os.path.basename(f)]\n",
      "    for Fp in (list_Fp):\n",
      "        Fingerprint(names, Fp)\n",
      "70/72:\n",
      "def Fingerprint(name , Fp):\n",
      "    \n",
      "    group = name\n",
      "    group = [name.replace(\".smi\", \".csv\") for name in group]\n",
      "    result = ', '.join(group)\n",
      "    \n",
      "    Fp_name = Fp\n",
      "    Fp_name = Fp.replace(\".xml\", \"_\")\n",
      "    \n",
      "    print (name)\n",
      "    print (result)\n",
      "    print (Fp)\n",
      "    print (Fp_name)\n",
      "\n",
      "    ! java -jar -Xms64m -Xmx200m /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/$Fp \\\n",
      "    -dir r'smiles/$name' \\\n",
      "    -file r'Fingerprint/$Fp_name$result'\n",
      "70/73:\n",
      "import os\n",
      "import glob\n",
      "\n",
      "path =  r'smiles/'\n",
      "for f in glob.glob(path + '*.smi'):\n",
      "    names = [os.path.basename(f)]\n",
      "    for Fp in (list_Fp):\n",
      "        Fingerprint(names, Fp)\n",
      "70/74:\n",
      "def Fingerprint(name , Fp):\n",
      "    \n",
      "    group = name\n",
      "    group = [name.replace(\".smi\", \".csv\") for name in group]\n",
      "    result = ', '.join(group)\n",
      "    \n",
      "    Fp_name = Fp\n",
      "    Fp_name = Fp.replace(\".xml\", \"_\")\n",
      "    \n",
      "    print (name)\n",
      "    print (result)\n",
      "    print (Fp)\n",
      "    print (Fp_name)\n",
      "\n",
      "    ! java -jar -Xms64m -Xmx200m /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints r'PaDEL-Descriptor/$Fp' \\\n",
      "    -dir r'smiles/$name' \\\n",
      "    -file r'Fingerprint/$Fp_name$result'\n",
      "70/75:\n",
      "import os\n",
      "import glob\n",
      "\n",
      "path =  r'smiles/'\n",
      "for f in glob.glob(path + '*.smi'):\n",
      "    names = [os.path.basename(f)]\n",
      "    for Fp in (list_Fp):\n",
      "        Fingerprint(names, Fp)\n",
      "70/76:\n",
      "def Fingerprint(name , Fp):\n",
      "    \n",
      "    group = name\n",
      "    group = [name.replace(\".smi\", \".csv\") for name in group]\n",
      "    result = ', '.join(group)\n",
      "    \n",
      "    Fp_name = Fp\n",
      "    Fp_name = Fp.replace(\".xml\", \"_\")\n",
      "    \n",
      "    print (name)\n",
      "    print (result)\n",
      "    print (Fp)\n",
      "    print (Fp_name)\n",
      "\n",
      "    ! java -jar -Xms64m -Xmx200m /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints PaDEL-Descriptor/$Fp' \\\n",
      "    -dir smiles/$name \\\n",
      "    -file Fingerprint/$Fp_name$result\n",
      "70/77:\n",
      "import os\n",
      "import glob\n",
      "\n",
      "path =  r'smiles/'\n",
      "for f in glob.glob(path + '*.smi'):\n",
      "    names = [os.path.basename(f)]\n",
      "    for Fp in (list_Fp):\n",
      "        Fingerprint(names, Fp)\n",
      "70/78:\n",
      "def Fingerprint(name , Fp):\n",
      "    \n",
      "    group = name\n",
      "    group = [name.replace(\".smi\", \".csv\") for name in group]\n",
      "    result = ', '.join(group)\n",
      "    \n",
      "    Fp_name = Fp\n",
      "    Fp_name = Fp.replace(\".xml\", \"_\")\n",
      "    \n",
      "    print (name)\n",
      "    print (result)\n",
      "    print (Fp)\n",
      "    print (Fp_name)\n",
      "\n",
      "    ! java -jar -Xms64m -Xmx200m /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints PaDEL-Descriptor/$Fp \\\n",
      "    -dir smiles/$name \\\n",
      "    -file Fingerprint/$Fp_name$result\n",
      "70/79:\n",
      "import os\n",
      "import glob\n",
      "\n",
      "path =  r'smiles/'\n",
      "for f in glob.glob(path + '*.smi'):\n",
      "    names = [os.path.basename(f)]\n",
      "    for Fp in (list_Fp):\n",
      "        Fingerprint(names, Fp)\n",
      "70/80:\n",
      "def Fingerprint(name , Fp):\n",
      "    \n",
      "    group = name\n",
      "    group = [name.replace(\".smi\", \".csv\") for name in group]\n",
      "    result = ', '.join(group)\n",
      "    \n",
      "    Fp_name = Fp\n",
      "    Fp_name = Fp.replace(\".xml\", \"_\")\n",
      "    \n",
      "    print (name)\n",
      "    print (result)\n",
      "    print (Fp)\n",
      "    print (Fp_name)\n",
      "\n",
      "    ! java -jar -Xms64m -Xmx200m /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints ./PaDEL-Descriptor/$Fp \\\n",
      "    -dir ./smiles/$name \\\n",
      "    -file ./Fingerprint/$Fp_name$result\n",
      "70/81:\n",
      "import os\n",
      "import glob\n",
      "\n",
      "path =  r'smiles/'\n",
      "for f in glob.glob(path + '*.smi'):\n",
      "    names = [os.path.basename(f)]\n",
      "    for Fp in (list_Fp):\n",
      "        Fingerprint(names, Fp)\n",
      "70/82: java -Xms1G -Xmx1G -Djava.awt.headless=true -jar ./PaDEL-Descriptor/PaDEL-Descriptor.jar -removesalt -standardizenitro -fingerprints -descriptortypes ./PaDEL-Descriptor/PubchemFingerprinter.xml -dir ./ -file descriptors_output.csv\n",
      "70/83: java -Xms1G -Xmx1G -Djava.awt.headless=true -jar PaDEL-Descriptor/PaDEL-Descriptor.jar -removesalt -standardizenitro -fingerprints -descriptortypes ./PaDEL-Descriptor/PubchemFingerprinter.xml -dir ./ -file descriptors_output.csv\n",
      "70/84:\n",
      "def Fingerprint(name , Fp):\n",
      "    \n",
      "    group = name\n",
      "    group = [name.replace(\".smi\", \".csv\") for name in group]\n",
      "    result = ', '.join(group)\n",
      "    \n",
      "    Fp_name = Fp\n",
      "    Fp_name = Fp.replace(\".xml\", \"_\")\n",
      "    \n",
      "    print (name)\n",
      "    print (result)\n",
      "    print (Fp)\n",
      "    print (Fp_name)\n",
      "\n",
      "    ! java -jar -Xms64m -Xmx200m /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/$Fp \\\n",
      "    -dir /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/$name \\\n",
      "    -file /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/$Fp_name$result\n",
      "70/85:\n",
      "import os\n",
      "import glob\n",
      "\n",
      "path =  r'smiles/'\n",
      "for f in glob.glob(path + '*.smi'):\n",
      "    names = [os.path.basename(f)]\n",
      "    for Fp in (list_Fp):\n",
      "        Fingerprint(names, Fp)\n",
      "70/86: Fingerprint('all_molecules.smi', Fingerprinter)\n",
      "70/87: Fingerprint('all_molecules.smi', Fingerprinter.xml)\n",
      "70/88: Fingerprint('all_molecules.smi')\n",
      "70/89: Fingerprint('all_molecules.smi', 'Fingerprinter.xml')\n",
      "70/90:\n",
      "import os\n",
      "import glob\n",
      "\n",
      "path =  r'smiles/'\n",
      "for f in glob.glob(path + '*.smi'):\n",
      "    names = [os.path.basename(f)]\n",
      "    for Fp in (list_Fp):\n",
      "        Fingerprint(names, Fp)\n",
      "70/91:\n",
      "#create smi files\n",
      "selection = ['smiles']\n",
      "df_selection = df4[selection]\n",
      "df_selection.to_csv(r'smiles/molecule.smi', sep='\\t', index=False, header=False)\n",
      "70/92:\n",
      "list_Fp = ['Fingerprinter.xml', 'ExtendedFingerprinter.xml', 'EStateFingerprinter.xml',\n",
      "           'GraphOnlyFingerprinter.xml', 'MACCSFingerprinter.xml', 'PubchemFingerprinter.xml',\n",
      "           'SubstructureFingerprinter.xml', 'KlekotaRothFingerprinter.xml', 'AtomPairs2DFingerprinter.xml']\n",
      "70/93:\n",
      "def Fingerprint(name , Fp):\n",
      "    \n",
      "    group = name\n",
      "    group = [name.replace(\".smi\", \".csv\") for name in group]\n",
      "    result = ', '.join(group)\n",
      "    \n",
      "    Fp_name = Fp\n",
      "    Fp_name = Fp.replace(\".xml\", \"_\")\n",
      "    \n",
      "    print (name)\n",
      "    print (result)\n",
      "    print (Fp)\n",
      "    print (Fp_name)\n",
      "\n",
      "    ! java -jar -Xms64m -Xmx200m /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/$Fp \\\n",
      "    -dir /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/$name \\\n",
      "    -file /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/$Fp_name$result\n",
      "70/94:\n",
      "import os\n",
      "import glob\n",
      "\n",
      "path =  r'smiles/'\n",
      "for f in glob.glob(path + '*.smi'):\n",
      "    names = [os.path.basename(f)]\n",
      "    for Fp in (list_Fp):\n",
      "        Fingerprint(names, Fp)\n",
      "73/1: pwd\n",
      "73/2:\n",
      "list_Fp = ['Fingerprinter.xml', 'ExtendedFingerprinter.xml', 'EStateFingerprinter.xml',\n",
      "           'GraphOnlyFingerprinter.xml', 'MACCSFingerprinter.xml', 'PubchemFingerprinter.xml',\n",
      "           'SubstructureFingerprinter.xml', 'KlekotaRothFingerprinter.xml', 'AtomPairs2DFingerprinter.xml']\n",
      "73/3:\n",
      "def Fingerprint(name , Fp):\n",
      "    \n",
      "    group = name\n",
      "    group = [name.replace(\".smi\", \".csv\") for name in group]\n",
      "    result = ', '.join(group)\n",
      "    \n",
      "    Fp_name = Fp\n",
      "    Fp_name = Fp.replace(\".xml\", \"_\")\n",
      "    \n",
      "    print name\n",
      "    print result\n",
      "    print Fp\n",
      "    print Fp_name\n",
      "\n",
      "    ! java -jar -Xms64m -Xmx200m /Users/zeromtmu/Desktop/Thesis/PaDEL-Descriptor/PaDEL-Descriptor.jar \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes /Users/zeromtmu/Desktop/Thesis/PaDEL-Descriptor/$Fp \\\n",
      "    -dir /Users/zeromtmu/Desktop/Thesis/ER_alpha_Regression/smiles/$name \\\n",
      "    -file /Users/zeromtmu/Desktop/Thesis/ER_alpha_Regression/Fingerprint/$Fp_name$result\n",
      "73/4:\n",
      "list_Fp = ['Fingerprinter.xml', 'ExtendedFingerprinter.xml', 'EStateFingerprinter.xml',\n",
      "           'GraphOnlyFingerprinter.xml', 'MACCSFingerprinter.xml', 'PubchemFingerprinter.xml',\n",
      "           'SubstructureFingerprinter.xml', 'KlekotaRothFingerprinter.xml', 'AtomPairs2DFingerprinter.xml']\n",
      "73/5:\n",
      "def Fingerprint(name , Fp):\n",
      "    \n",
      "    group = name\n",
      "    group = [name.replace(\".smi\", \".csv\") for name in group]\n",
      "    result = ', '.join(group)\n",
      "    \n",
      "    Fp_name = Fp\n",
      "    Fp_name = Fp.replace(\".xml\", \"_\")\n",
      "    \n",
      "    print (name)\n",
      "    print (result)\n",
      "    print (Fp)\n",
      "    print (Fp_name)\n",
      "\n",
      "    ! java -jar -Xms64m -Xmx200m /Users/zeromtmu/Desktop/Thesis/PaDEL-Descriptor/PaDEL-Descriptor.jar \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes /Users/zeromtmu/Desktop/Thesis/PaDEL-Descriptor/$Fp \\\n",
      "    -dir /Users/zeromtmu/Desktop/Thesis/ER_alpha_Regression/smiles/$name \\\n",
      "    -file /Users/zeromtmu/Desktop/Thesis/ER_alpha_Regression/Fingerprint/$Fp_name$result\n",
      "73/6:\n",
      "import os\n",
      "import glob\n",
      "\n",
      "path =  r'smiles/'\n",
      "for f in glob.glob(path + '*.smi'):\n",
      "    names = [os.path.basename(f)]\n",
      "    for Fp in (list_Fp):\n",
      "        Fingerprint(names, Fp)\n",
      "73/7:\n",
      "def Fingerprint(name , Fp):\n",
      "    \n",
      "    group = name\n",
      "    group = [name.replace(\".smi\", \".csv\") for name in group]\n",
      "    result = ', '.join(group)\n",
      "    \n",
      "    Fp_name = Fp\n",
      "    Fp_name = Fp.replace(\".xml\", \"_\")\n",
      "    \n",
      "    print (name)\n",
      "    print (result)\n",
      "    print (Fp)\n",
      "    print (Fp_name)\n",
      "\n",
      "    ! java -jar -Xms64m -Xmx200m /Users/tarapongsrisongkram/Desktop/estrogen-receptor-alpha-qsar-master/PaDEL-Descriptor/PaDEL-Descriptor.jar \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes /Users/tarapongsrisongkram/Desktop/estrogen-receptor-alpha-qsar-master/PaDEL-Descriptor/$Fp \\\n",
      "    -dir /Users/tarapongsrisongkram/Desktop/estrogen-receptor-alpha-qsar-master/smiles/$name \\\n",
      "    -file /Users/tarapongsrisongkram/Desktop/estrogen-receptor-alpha-qsar-master/Fingerprint/$Fp_name$result\n",
      "73/8:\n",
      "import os\n",
      "import glob\n",
      "\n",
      "path =  r'smiles/'\n",
      "for f in glob.glob(path + '*.smi'):\n",
      "    names = [os.path.basename(f)]\n",
      "    for Fp in (list_Fp):\n",
      "        Fingerprint(names, Fp)\n",
      "73/9:\n",
      "def Fingerprint(name , Fp):\n",
      "    \n",
      "    group = name\n",
      "    group = [name.replace(\".smi\", \".csv\") for name in group]\n",
      "    result = ', '.join(group)\n",
      "    \n",
      "    Fp_name = Fp\n",
      "    Fp_name = Fp.replace(\".xml\", \"_\")\n",
      "    \n",
      "    print (name)\n",
      "    print (result)\n",
      "    print (Fp)\n",
      "    print (Fp_name)\n",
      "\n",
      "    ! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/Desktop/estrogen-receptor-alpha-qsar-master/PaDEL-Descriptor/PaDEL-Descriptor.jar' \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/Desktop/estrogen-receptor-alpha-qsar-master/PaDEL-Descriptor/$Fp' \\\n",
      "    -dir '/Users/tarapongsrisongkram/Desktop/estrogen-receptor-alpha-qsar-master/smiles/$name' \\\n",
      "    -file '/Users/tarapongsrisongkram/Desktop/estrogen-receptor-alpha-qsar-master/Fingerprint/$Fp_name$result'\n",
      "73/10:\n",
      "import os\n",
      "import glob\n",
      "\n",
      "path =  r'smiles/'\n",
      "for f in glob.glob(path + '*.smi'):\n",
      "    names = [os.path.basename(f)]\n",
      "    for Fp in (list_Fp):\n",
      "        Fingerprint(names, Fp)\n",
      "70/95:\n",
      "def Fingerprint(name , Fp):\n",
      "    \n",
      "    group = name\n",
      "    group = [name.replace(\".smi\", \".csv\") for name in group]\n",
      "    result = ', '.join(group)\n",
      "    \n",
      "    Fp_name = Fp\n",
      "    Fp_name = Fp.replace(\".xml\", \"_\")\n",
      "    \n",
      "    print (name)\n",
      "    print (result)\n",
      "    print (Fp)\n",
      "    print (Fp_name)\n",
      "\n",
      "    ! java -jar -Xms64m -Xmx200m \"/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar\" \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes \"/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/$Fp\" \\\n",
      "    -dir \"/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/$name\" \\\n",
      "    -file \"/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/$Fp_name$result\"\n",
      "70/96:\n",
      "import os\n",
      "import glob\n",
      "\n",
      "path =  r'smiles/'\n",
      "for f in glob.glob(path + '*.smi'):\n",
      "    names = [os.path.basename(f)]\n",
      "    for Fp in (list_Fp):\n",
      "        Fingerprint(names, Fp)\n",
      "70/97: !cat all_smile.smi\n",
      "70/98: !cat r'smiles/molecule.smi'\n",
      "70/99:\n",
      "#create smi files\n",
      "selection = ['smiles', 'compound']\n",
      "df_selection = df4[selection]\n",
      "df_selection.to_csv(r'smiles/molecule.smi', sep='\\t', index=False, header=False)\n",
      "70/100: !cat r'smiles/molecule.smi'\n",
      "70/101:\n",
      "list_Fp = ['Fingerprinter.xml', 'ExtendedFingerprinter.xml', 'EStateFingerprinter.xml',\n",
      "           'GraphOnlyFingerprinter.xml', 'MACCSFingerprinter.xml', 'PubchemFingerprinter.xml',\n",
      "           'SubstructureFingerprinter.xml', 'KlekotaRothFingerprinter.xml', 'AtomPairs2DFingerprinter.xml']\n",
      "70/102:\n",
      "def Fingerprint(name , Fp):\n",
      "    \n",
      "    group = name\n",
      "    group = [name.replace(\".smi\", \".csv\") for name in group]\n",
      "    result = ', '.join(group)\n",
      "    \n",
      "    Fp_name = Fp\n",
      "    Fp_name = Fp.replace(\".xml\", \"_\")\n",
      "    \n",
      "    print (name)\n",
      "    print (result)\n",
      "    print (Fp)\n",
      "    print (Fp_name)\n",
      "\n",
      "    ! java -jar -Xms64m -Xmx200m \"/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar\" \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes \"/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/$Fp\" \\\n",
      "    -dir \"/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/$name\" \\\n",
      "    -file \"/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/$Fp_name$result\"\n",
      "70/103:\n",
      "import os\n",
      "import glob\n",
      "\n",
      "path =  r'smiles/'\n",
      "for f in glob.glob(path + '*.smi'):\n",
      "    names = [os.path.basename(f)]\n",
      "    for Fp in (list_Fp):\n",
      "        Fingerprint(names, Fp)\n",
      "70/104:\n",
      "def Fingerprint(name , Fp):\n",
      "    \n",
      "    group = name\n",
      "    group = [name.replace(\".smi\", \".csv\") for name in group]\n",
      "    result = ', '.join(group)\n",
      "    \n",
      "    Fp_name = Fp\n",
      "    Fp_name = Fp.replace(\".xml\", \"_\")\n",
      "    \n",
      "    print (name)\n",
      "    print (result)\n",
      "    print (Fp)\n",
      "    print (Fp_name)\n",
      "\n",
      "    ! java -jar -Xms64m -Xmx200m \"/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar\" \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes \"/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/$Fp\" \\\n",
      "    -dir \"/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/$name.xmi\" \\\n",
      "    -file \"/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/$Fp_name$result\"\n",
      "70/105:\n",
      "def Fingerprint(name , Fp):\n",
      "    \n",
      "    group = name\n",
      "    group = [name.replace(\".smi\", \".csv\") for name in group]\n",
      "    result = ', '.join(group)\n",
      "    \n",
      "    Fp_name = Fp\n",
      "    Fp_name = Fp.replace(\".xml\", \"_\")\n",
      "    \n",
      "    print (name)\n",
      "    print (result)\n",
      "    print (Fp)\n",
      "    print (Fp_name)\n",
      "\n",
      "    ! java -jar -Xms64m -Xmx200m \"/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar\" \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes \"/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/$Fp\" \\\n",
      "    -dir \"/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/$name.smi\" \\\n",
      "    -file \"/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/$Fp_name$result\"\n",
      "70/106:\n",
      "import os\n",
      "import glob\n",
      "\n",
      "path =  r'smiles/'\n",
      "for f in glob.glob(path + '*.smi'):\n",
      "    names = [os.path.basename(f)]\n",
      "    for Fp in (list_Fp):\n",
      "        Fingerprint(names, Fp)\n",
      "73/11:\n",
      "def Fingerprint(name , Fp):\n",
      "    \n",
      "    group = name\n",
      "    group = [name.replace(\".smi\", \".csv\") for name in group]\n",
      "    result = ', '.join(group)\n",
      "    \n",
      "    Fp_name = Fp\n",
      "    Fp_name = Fp.replace(\".xml\", \"_\")\n",
      "    \n",
      "    print (name)\n",
      "    print (result)\n",
      "    print (Fp)\n",
      "    print (Fp_name)\n",
      "\n",
      "    ! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/Desktop/estrogen-receptor-alpha-qsar-master/PaDEL-Descriptor/PaDEL-Descriptor.jar' \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/Desktop/estrogen-receptor-alpha-qsar-master/PaDEL-Descriptor/$Fp' \\\n",
      "    -dir '/Users/tarapongsrisongkram/Desktop/estrogen-receptor-alpha-qsar-master/smiles/$name' \\\n",
      "    -file '/Users/tarapongsrisongkram/Desktop/estrogen-receptor-alpha-qsar-master/Fingerprint/$Fp_name$result'\n",
      "73/12:\n",
      "import os\n",
      "import glob\n",
      "\n",
      "path =  r'smiles/'\n",
      "for f in glob.glob(path + '*.smi'):\n",
      "    names = [os.path.basename(f)]\n",
      "    for Fp in (list_Fp):\n",
      "        Fingerprint(names, Fp)\n",
      "70/107:\n",
      "#create smi files\n",
      "selection = ['smiles', 'compound']\n",
      "df_selection = df4[selection]\n",
      "df_selection.to_csv(r'smiles/molecule.smi', sep='\\t', index=False, header=False)\n",
      "70/108: !cat r'smiles/molecule.smi'\n",
      "70/109:\n",
      "list_Fp = ['Fingerprinter.xml', 'ExtendedFingerprinter.xml', 'EStateFingerprinter.xml',\n",
      "           'GraphOnlyFingerprinter.xml', 'MACCSFingerprinter.xml', 'PubchemFingerprinter.xml',\n",
      "           'SubstructureFingerprinter.xml', 'KlekotaRothFingerprinter.xml', 'AtomPairs2DFingerprinter.xml']\n",
      "70/110:\n",
      "def Fingerprint(name , Fp):\n",
      "    \n",
      "    group = name\n",
      "    group = [name.replace(\".smi\", \".csv\") for name in group]\n",
      "    result = ', '.join(group)\n",
      "    \n",
      "    Fp_name = Fp\n",
      "    Fp_name = Fp.replace(\".xml\", \"_\")\n",
      "    \n",
      "    print (name)\n",
      "    print (result)\n",
      "    print (Fp)\n",
      "    print (Fp_name)\n",
      "\n",
      "    ! java -jar -Xms64m -Xmx200m \"/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar\" \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes \"/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/$Fp\" \\\n",
      "    -dir \"/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/$name\" \\\n",
      "    -file \"/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/$Fp_name$result\"\n",
      "70/111:\n",
      "import os\n",
      "import glob\n",
      "\n",
      "path =  r'smiles/'\n",
      "for f in glob.glob(path + '*.smi'):\n",
      "    names = [os.path.basename(f)]\n",
      "    for Fp in (list_Fp):\n",
      "        Fingerprint(names, Fp)\n",
      "70/112:\n",
      "def Fingerprint(name , Fp):\n",
      "    \n",
      "    group = name\n",
      "    group = [name.replace(\".smi\", \".csv\") for name in group]\n",
      "    result = ', '.join(group)\n",
      "    \n",
      "    Fp_name = Fp\n",
      "    Fp_name = Fp.replace(\".xml\", \"_\")\n",
      "    \n",
      "    print (name)\n",
      "    print (result)\n",
      "    print (Fp)\n",
      "    print (Fp_name)\n",
      "\n",
      "    ! java -jar -Xms64m -Xmx200m \"/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar\" \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes \"/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/$Fp\" \\\n",
      "    -dir \"/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/name\" \\\n",
      "    -file \"/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/$Fp_name$result\"\n",
      "70/113:\n",
      "import os\n",
      "import glob\n",
      "\n",
      "path =  r'smiles/'\n",
      "for f in glob.glob(path + '*.smi'):\n",
      "    names = [os.path.basename(f)]\n",
      "    for Fp in (list_Fp):\n",
      "        Fingerprint(names, Fp)\n",
      "70/114:\n",
      "def Fingerprint(name , Fp):\n",
      "    \n",
      "    group = name\n",
      "    group = [name.replace(\".smi\", \".csv\") for name in group]\n",
      "    result = ', '.join(group)\n",
      "    \n",
      "    Fp_name = Fp\n",
      "    Fp_name = Fp.replace(\".xml\", \"_\")\n",
      "    \n",
      "    print (name)\n",
      "    print (result)\n",
      "    print (Fp)\n",
      "    print (Fp_name)\n",
      "\n",
      "    ! java -jar -Xms64m -Xmx200m \"/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar\" \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes \"/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/$Fp\" \\\n",
      "    -dir \"/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/$name\" \\\n",
      "    -file \"/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/$Fp_name$result\"\n",
      "70/115:\n",
      "import os\n",
      "import glob\n",
      "\n",
      "path =  r'smiles/'\n",
      "for f in glob.glob(path + '*.smi'):\n",
      "    names = [os.path.basename(f)]\n",
      "    for Fp in (list_Fp):\n",
      "        Fingerprint(names, Fp)\n",
      "70/116:\n",
      "def Fingerprint(name , Fp):\n",
      "    \n",
      "    group = name\n",
      "    group = [name.replace(\".smi\", \".csv\") for name in group]\n",
      "    result = ', '.join(group)\n",
      "    \n",
      "    Fp_name = Fp\n",
      "    Fp_name = Fp.replace(\".xml\", \"_\")\n",
      "    \n",
      "    print (name)\n",
      "    print (result)\n",
      "    print (Fp)\n",
      "    print (Fp_name)\n",
      "\n",
      "    ! java -Xms1G -Xmx1G -Djava.awt.headless=true -jar  \"/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar\" \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes \"/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/$Fp\" \\\n",
      "    -dir \"/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/$name\" \\\n",
      "    -file \"/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/$Fp_name$result\"\n",
      "70/117:\n",
      "import os\n",
      "import glob\n",
      "\n",
      "path =  r'smiles/'\n",
      "for f in glob.glob(path + '*.smi'):\n",
      "    names = [os.path.basename(f)]\n",
      "    for Fp in (list_Fp):\n",
      "        Fingerprint(names, Fp)\n",
      "70/118:\n",
      "def Fingerprint(name , Fp):\n",
      "    \n",
      "    group = name\n",
      "    group = [name.replace(\".smi\", \".csv\") for name in group]\n",
      "    result = ', '.join(group)\n",
      "    \n",
      "    Fp_name = Fp\n",
      "    Fp_name = Fp.replace(\".xml\", \"_\")\n",
      "    \n",
      "    print (name)\n",
      "    print (result)\n",
      "    print (Fp)\n",
      "    print (Fp_name)\n",
      "\n",
      "    ! java -jar -Xms64m -Xmx200m \"/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar\" \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes \"/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/$Fp\" \\\n",
      "    -dir \"/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/$name\" \\\n",
      "    -file \"/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/$Fp_name$result\"\n",
      "70/119:\n",
      "import os\n",
      "import glob\n",
      "\n",
      "path =  r'smiles/'\n",
      "for f in glob.glob(path + '*.smi'):\n",
      "    names = [os.path.basename(f)]\n",
      "    for Fp in (list_Fp):\n",
      "        Fingerprint(names, Fp)\n",
      "70/120:\n",
      "#create smi files\n",
      "selection = ['smiles', 'compound']\n",
      "df_selection = df4[selection]\n",
      "df_selection.to_csv('molecule.smi', sep='\\t', index=False, header=False)\n",
      "70/121: !cat r'smiles/molecule.smi'\n",
      "70/122: !cat 'molecule.smi'\n",
      "70/123: !cat padel.sh\n",
      "70/124: !cat padel.sh\n",
      "70/125: !bash padel.sh\n",
      "82/1: Analytical Data\n",
      "82/2:\n",
      "| Stretch/Untouched | ProbDistribution | Accuracy |\n",
      "| --- | --- | --- |\n",
      "| Stretched | Gaussian | .843 |\n",
      "82/3:\n",
      "| Stretch/Untouched | ProbDistribution | Accuracy |\n",
      "| --- | --- | --- |\n",
      "| Stretched | Gaussian | .843 |\n",
      "84/1:\n",
      "| Stretch/Untouched | ProbDistribution | Accuracy |\n",
      "| --- | --- | --- |\n",
      "| Stretched | Gaussian | .843 |\n",
      "84/2:\n",
      "|  | A | B |\n",
      "| --- | --- | --- |\n",
      "| Weighing bottle + KHP (before) | 36.1558 | 35.5405 |\n",
      "| Weighing bottle + KHP (after) | 31.1707 | 30.3714 |\n",
      "90/1:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "90/2: df = pd.read_csv('rawdataqsar.csv')\n",
      "90/3: pwd\n",
      "90/4: df = pd.read_csv('rawdataqsar.csv')\n",
      "90/5: df\n",
      "90/6: df2 =df[df.kpu.notna()]\n",
      "90/7: df3 =df2[df.kp.notna()]\n",
      "90/8: df3\n",
      "90/9: df3.shape\n",
      "90/10: df3.isnull()\n",
      "90/11: df4=df3.reset_index(drop=True)\n",
      "90/12: df4\n",
      "90/13:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "90/14: pwd\n",
      "90/15: df = pd.read_csv('rawdataqsar.csv')\n",
      "90/16: df\n",
      "90/17: df2 =df[df.kpu.notna()]\n",
      "90/18: df3 =df2[df.kp.notna()]\n",
      "90/19: df3\n",
      "90/20: df3.shape\n",
      "90/21: df3.isnull()\n",
      "90/22: df4=df3.reset_index(drop=True)\n",
      "90/23: df4\n",
      "90/24:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Descriptors, Lipinski\n",
      "90/25:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Descriptors, Lipinski\n",
      "90/26:\n",
      "def lipinski(smiles, verbose=False):\n",
      "\n",
      "    moldata= []\n",
      "    for elem in smiles:\n",
      "        mol=Chem.MolFromSmiles(elem) \n",
      "        moldata.append(mol)\n",
      "       \n",
      "    baseData= np.arange(1,1)\n",
      "    i=0  \n",
      "    for mol in moldata:        \n",
      "       \n",
      "        desc_MolWt = Descriptors.MolWt(mol)\n",
      "        desc_MolLogP = Descriptors.MolLogP(mol)\n",
      "        desc_NumHDonors = Lipinski.NumHDonors(mol)\n",
      "        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)\n",
      "        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)\n",
      "           \n",
      "        row = np.array([desc_MolWt,\n",
      "                        desc_MolLogP,\n",
      "                        desc_NumHDonors,\n",
      "                        desc_NumHAcceptors,\n",
      "                        desc_NumRotatableBonds])   \n",
      "    \n",
      "        if(i==0):\n",
      "            baseData=row\n",
      "        else:\n",
      "            baseData=np.vstack([baseData, row])\n",
      "        i=i+1      \n",
      "    \n",
      "    columnNames=[\"MW\",\"LogP\",\"NumHDonors\",\"NumHAcceptors\", \"NumRotatableBonds\"]   \n",
      "    descriptors = pd.DataFrame(data=baseData,columns=columnNames)\n",
      "    \n",
      "    return descriptors\n",
      "90/27: print(df4['smiles'].unique())\n",
      "90/28: df5 = df4.drop_duplicates(subset=['smiles'])\n",
      "90/29: df5\n",
      "90/30:\n",
      "df_lipinski = lipinski(df5.smiles)\n",
      "df_lipinski\n",
      "90/31:\n",
      "#create smi files\n",
      "selection = ['smiles', 'compound']\n",
      "df_selection = df4[selection]\n",
      "df_selection.to_csv(r'smiles/molecule.smi', sep='\\t', index=False, header=False)\n",
      "90/32: df_6 = pd.concat([df5,df_lipinski], axis=1)\n",
      "90/33:\n",
      "df6 = pd.concat([df5,df_lipinski], axis=1)\n",
      "df\n",
      "90/34:\n",
      "df6 = pd.concat([df5,df_lipinski], axis=1)\n",
      "df6\n",
      "90/35: df6.tail(6)\n",
      "90/36:\n",
      "data = pd.concat([df5,df_lipinski], axis=1)\n",
      "data\n",
      "90/37: data.tail(6)\n",
      "90/38:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt1.set_ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)\n",
      "plt2.set_xlabel(\"LogP\", fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt2.set_ylim(0, 50)\n",
      "plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "\n",
      "#Histogram for NumHDonors\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 30, color='#FF9A00', edgecolor='black', linewidth=0.5)\n",
      "plt3.set_xlabel(\"Number of Hydrogen Bond Donors\", fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt3.set_ylim(0, 50)\n",
      "plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 30, color='#FF165D', edgecolor='black', linewidth=0.5)\n",
      "plt4.set_xlabel(\"Number of Hydrogen Bond Acceptors\", fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt4.set_ylim(0, 50)\n",
      "plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 30, color='#52006A', edgecolor='black', linewidth=0.5)\n",
      "plt5.set_xlabel(\"Number of RotatableBonds\", fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt5.set_ylim(0, 50)\n",
      "plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Kpuu plots-basic descriptors.pdf', dpi=300)\n",
      "90/39:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt1.set_ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)\n",
      "plt2.set_xlabel(\"LogP\", fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt2.set_ylim(0, 50)\n",
      "plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "\n",
      "#Histogram for NumHDonors\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 30, color='#FF9A00', edgecolor='black', linewidth=0.5)\n",
      "plt3.set_xlabel(\"Number of Hydrogen Bond Donors\", fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt3.set_ylim(0, 50)\n",
      "plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 30, color='#FF165D', edgecolor='black', linewidth=0.5)\n",
      "plt4.set_xlabel(\"Number of Hydrogen Bond Acceptors\", fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt4.set_ylim(0, 50)\n",
      "plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 20, color='#52006A', edgecolor='black', linewidth=0.5)\n",
      "plt5.set_xlabel(\"Number of RotatableBonds\", fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt5.set_ylim(0, 50)\n",
      "plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Kpuu plots-basic descriptors.pdf', dpi=300)\n",
      "90/40:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt1.set_ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)\n",
      "plt2.set_xlabel(\"LogP\", fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt2.set_ylim(0, 50)\n",
      "plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "\n",
      "#Histogram for NumHDonors\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 30, color='#FF9A00', edgecolor='black', linewidth=0.5)\n",
      "plt3.set_xlabel(\"Number of Hydrogen Bond Donors\", fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt3.set_ylim(0, 50)\n",
      "plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 30, color='#FF165D', edgecolor='black', linewidth=0.5)\n",
      "plt4.set_xlabel(\"Number of Hydrogen Bond Acceptors\", fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt4.set_ylim(0, 50)\n",
      "plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 30, color='#52006A', edgecolor='black', linewidth=0.5)\n",
      "plt5.set_xlabel(\"Number of RotatableBonds\", fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt5.set_ylim(0, 50)\n",
      "plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Kpuu plots-basic descriptors.pdf', dpi=300)\n",
      "90/41:\n",
      "#Visuallize kpuu\n",
      "plt.hist(data['kpuu'], density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "plt.xlabel('Kp\\u208u\\u208u', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('LAT1 plots-pIC50 distribution.pdf', dpi=300)\n",
      "90/42:\n",
      "#Visuallize kpuu\n",
      "plt.hist(data['kpuu'], density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "plt.xlabel('Kp$uu$', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('LAT1 plots-pIC50 distribution.pdf', dpi=300)\n",
      "90/43:\n",
      "#Visuallize kpuu\n",
      "plt.hist(data['kpu'], density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "plt.xlabel('Kp$uu$', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('LAT1 plots-pIC50 distribution.pdf', dpi=300)\n",
      "90/44:\n",
      "#Visuallize kpuu\n",
      "plt.hist(data['kpu'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "plt.xlabel('Kp$uu$', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('LAT1 plots-pIC50 distribution.pdf', dpi=300)\n",
      "90/45:\n",
      "#Visuallize kpuu\n",
      "plt.hist(data['kpu'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "plt.xlabel('$Kp{uu}$', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('LAT1 plots-pIC50 distribution.pdf', dpi=300)\n",
      "90/46:\n",
      "#Visuallize kpuu\n",
      "plt.hist(data['kpu'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "plt.xlabel('$Kp_{uu}$', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('LAT1 plots-pIC50 distribution.pdf', dpi=300)\n",
      "90/47:\n",
      "#Visuallize kpuu\n",
      "plt.hist(data['kpu'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "plt.xlabel('$Kp_{uu}$', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=9)\n",
      "plt.ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('LAT1 plots-pIC50 distribution.pdf', dpi=300)\n",
      "90/48:\n",
      "#Visuallize kpuu\n",
      "plt.hist(data['kpu'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "plt.xlabel('$Kp_{uu}$', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Kpuu distribution.pdf', dpi=300)\n",
      "90/49:\n",
      "#Visuallize kpuu\n",
      "plt.hist(data['kpu'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "plt.xlabel('$Kp_{uu}$', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.ylim(0, 50)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Kpuu distribution.pdf', dpi=300)\n",
      "90/50:\n",
      "#Visuallize kpuu\n",
      "logbins = np.logspace(np.log10(bins[0]),np.log10(bins[-1]),len(bins))\n",
      "plt.hist(data['kpu'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "\n",
      "plt.xlabel('$Kp_{uu}$', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.ylim(0, 50)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Kpuu distribution.pdf', dpi=300)\n",
      "90/51:\n",
      "#Visuallize kpuu\n",
      "\n",
      "plt.hist(data['kpu'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "logbins = np.logspace(np.log10(bins[0]),np.log10(bins[-1]),len(bins))\n",
      "plt.xlabel('$Kp_{uu}$', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.ylim(0, 50)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Kpuu distribution.pdf', dpi=300)\n",
      "90/52:\n",
      "#Visuallize kpuu\n",
      "\n",
      "plt.hist(data['kpu'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)\n",
      "plt.xlabel('$Kp_{uu}$', fontsize=16, fontweight='bold')\n",
      "plt.ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.ylim(0, 50)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Kpuu distribution.pdf', dpi=300)\n",
      "90/53:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "x1 = data['MW']\n",
      "x2 = data['LogP']\n",
      "x3 = data['NumHDonors']\n",
      "x4 = data['NumHAcceptors']\n",
      "x5 = data['NumRotatableBonds']\n",
      "y = data['kpu']\n",
      "colors = {'active':'red', 'inactive':'blue', 'intermediate':'green'}\n",
      "area = (30 * data['pIC50'])  # 0 to 15 point radii\n",
      "\n",
      "#MW\n",
      "plt1.scatter(x1, y, s=area, c=data['bioactivity_class'].map(colors), alpha=0.5)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#LogP\n",
      "plt2.scatter(x2, y, s=area,c=data['bioactivity_class'].map(colors), alpha=0.5)\n",
      "plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondDonors\n",
      "plt3.scatter(x3, y, s=area, c=data['bioactivity_class'].map(colors), alpha=0.5)\n",
      "plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt4.scatter(x4, y, s=area, c=data['bioactivity_class'].map(colors), alpha=0.5)\n",
      "plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt5.scatter(x5, y, s=area, c=data['bioactivity_class'].map(colors), alpha=0.5)\n",
      "plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('RO5_and_Kpuu.pdf', dpi=300)\n",
      "90/54:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "x1 = data['MW']\n",
      "x2 = data['LogP']\n",
      "x3 = data['NumHDonors']\n",
      "x4 = data['NumHAcceptors']\n",
      "x5 = data['NumRotatableBonds']\n",
      "y = data['kpu']\n",
      "colors = {'active':'red', 'inactive':'blue', 'intermediate':'green'}\n",
      "area = (30 * data['kpu'])  # 0 to 15 point radii\n",
      "\n",
      "#MW\n",
      "plt1.scatter(x1, y, s=area, c=data['bioactivity_class'].map(colors), alpha=0.5)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#LogP\n",
      "plt2.scatter(x2, y, s=area,c=data['bioactivity_class'].map(colors), alpha=0.5)\n",
      "plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondDonors\n",
      "plt3.scatter(x3, y, s=area, c=data['bioactivity_class'].map(colors), alpha=0.5)\n",
      "plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt4.scatter(x4, y, s=area, c=data['bioactivity_class'].map(colors), alpha=0.5)\n",
      "plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt5.scatter(x5, y, s=area, c=data['bioactivity_class'].map(colors), alpha=0.5)\n",
      "plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('RO5_and_Kpuu.pdf', dpi=300)\n",
      "90/55:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "x1 = data['MW']\n",
      "x2 = data['LogP']\n",
      "x3 = data['NumHDonors']\n",
      "x4 = data['NumHAcceptors']\n",
      "x5 = data['NumRotatableBonds']\n",
      "y = data['kpu']\n",
      "area = (30 * data['kpu'])  # 0 to 15 point radii\n",
      "\n",
      "#MW\n",
      "plt1.scatter(x1, y, s=area, alpha=0.5)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#LogP\n",
      "plt2.scatter(x2, y, s=area, alpha=0.5)\n",
      "plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondDonors\n",
      "plt3.scatter(x3, y, s=area, alpha=0.5)\n",
      "plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt4.scatter(x4, y, s=area, alpha=0.5)\n",
      "plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt5.scatter(x5, y, s=area, alpha=0.5)\n",
      "plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('RO5_and_Kpuu.pdf', dpi=300)\n",
      "90/56:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "x1 = data['MW']\n",
      "x2 = data['LogP']\n",
      "x3 = data['NumHDonors']\n",
      "x4 = data['NumHAcceptors']\n",
      "x5 = data['NumRotatableBonds']\n",
      "y = data['kpu']\n",
      "area = (30 * data['kpu'])  # 0 to 15 point radii\n",
      "\n",
      "#MW\n",
      "plt1.scatter(x1, y, s=area, alpha=0.5)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#LogP\n",
      "plt2.scatter(x2, y, s=area, alpha=0.5)\n",
      "plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondDonors\n",
      "plt3.scatter(x3, y, s=area, alpha=0.5)\n",
      "plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt4.scatter(x4, y, s=area, alpha=0.5)\n",
      "plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt5.scatter(x5, y, s=area, alpha=0.5)\n",
      "plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('RO5_and_Kpuu.pdf', dpi=300)\n",
      "90/57:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "x1 = data['MW']\n",
      "x2 = data['LogP']\n",
      "x3 = data['NumHDonors']\n",
      "x4 = data['NumHAcceptors']\n",
      "x5 = data['NumRotatableBonds']\n",
      "y = data['kpu']\n",
      "area = (30 * data['kpu'])  # 0 to 15 point radii\n",
      "\n",
      "#MW\n",
      "plt1.scatter(x1, y, alpha=0.5)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#LogP\n",
      "plt2.scatter(x2, y, s=area, alpha=0.5)\n",
      "plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondDonors\n",
      "plt3.scatter(x3, y, s=area, alpha=0.5)\n",
      "plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt4.scatter(x4, y, s=area, alpha=0.5)\n",
      "plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt5.scatter(x5, y, s=area, alpha=0.5)\n",
      "plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('RO5_and_Kpuu.pdf', dpi=300)\n",
      "90/58:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "x1 = data['MW']\n",
      "x2 = data['LogP']\n",
      "x3 = data['NumHDonors']\n",
      "x4 = data['NumHAcceptors']\n",
      "x5 = data['NumRotatableBonds']\n",
      "y = data['kpu']\n",
      "area = (30 * data['kpu'])  # 0 to 15 point radii\n",
      "\n",
      "#MW\n",
      "plt1.scatter(x1, y, alpha=0.5)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#LogP\n",
      "plt2.scatter(x2, y, alpha=0.5)\n",
      "plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondDonors\n",
      "plt3.scatter(x3, y, alpha=0.5)\n",
      "plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt4.scatter(x4, y,  alpha=0.5)\n",
      "plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt5.scatter(x5, y, alpha=0.5)\n",
      "plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('RO5_and_Kpuu.pdf', dpi=300)\n",
      "90/59:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "x1 = data['MW']\n",
      "x2 = data['LogP']\n",
      "x3 = data['NumHDonors']\n",
      "x4 = data['NumHAcceptors']\n",
      "x5 = data['NumRotatableBonds']\n",
      "y = data['kpu']\n",
      "area = (30 * data['kpu'])  # 0 to 15 point radii\n",
      "\n",
      "#MW\n",
      "plt1.scatter(x1, y, alpha=0.5)\n",
      "plt1.set_yscale('log')\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#LogP\n",
      "plt2.scatter(x2, y, alpha=0.5)\n",
      "plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondDonors\n",
      "plt3.scatter(x3, y, alpha=0.5)\n",
      "plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt4.scatter(x4, y,  alpha=0.5)\n",
      "plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt5.scatter(x5, y, alpha=0.5)\n",
      "plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('RO5_and_Kpuu.pdf', dpi=300)\n",
      "90/60:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "x1 = data['MW']\n",
      "x2 = data['LogP']\n",
      "x3 = data['NumHDonors']\n",
      "x4 = data['NumHAcceptors']\n",
      "x5 = data['NumRotatableBonds']\n",
      "y = data['kpu']\n",
      "area = (30 * data['kpu'])  # 0 to 15 point radii\n",
      "\n",
      "#MW\n",
      "plt1.scatter(x1, y, alpha=0.5)\n",
      "plt1.set_yscale('log')\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#LogP\n",
      "plt2.scatter(x2, y, alpha=0.5)\n",
      "plt1.set_yscale('log')\n",
      "plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondDonors\n",
      "plt3.scatter(x3, y, alpha=0.5)\n",
      "plt1.set_yscale('log')\n",
      "plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt4.scatter(x4, y,  alpha=0.5)\n",
      "plt1.set_yscale('log')\n",
      "plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt5.scatter(x5, y, alpha=0.5)\n",
      "plt1.set_yscale('log')\n",
      "plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('RO5_and_Kpuu.pdf', dpi=300)\n",
      "90/61:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "x1 = data['MW']\n",
      "x2 = data['LogP']\n",
      "x3 = data['NumHDonors']\n",
      "x4 = data['NumHAcceptors']\n",
      "x5 = data['NumRotatableBonds']\n",
      "y = data['kpu']\n",
      "area = (30 * data['kpu'])  # 0 to 15 point radii\n",
      "\n",
      "#MW\n",
      "plt1.scatter(x1, y, alpha=0.5)\n",
      "plt1.set_yscale('log')\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#LogP\n",
      "plt2.scatter(x2, y, alpha=0.5)\n",
      "plt1.set_yscale('log')\n",
      "plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondDonors\n",
      "plt3.scatter(x3, y, alpha=0.5)\n",
      "plt1.set_yscale('log')\n",
      "plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt4.scatter(x4, y,  alpha=0.5)\n",
      "plt1.set_yscale('log')\n",
      "plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt5.scatter(x5, y, alpha=0.5)\n",
      "plt1.set_yscale('log')\n",
      "plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('RO5_and_Kpuu.pdf', dpi=300)\n",
      "90/62:\n",
      "data = pd.concat([df5,df_lipinski], axis=1)\n",
      "data.to_csv('data_lipinski_kpu.csv')\n",
      "data\n",
      "90/63: print(df5['kpu'].unique())\n",
      "90/64: pwd\n",
      "90/65: df = pd.read_csv('rawdataqsar.csv')\n",
      "90/66: df\n",
      "90/67: df2 =df[df.kpu.notna()]\n",
      "90/68: df3 =df2[df.kp.notna()]\n",
      "90/69: df3\n",
      "90/70: df3.shape\n",
      "90/71: df3.isnull()\n",
      "90/72: df4=df3.reset_index(drop=True)\n",
      "90/73: df4\n",
      "90/74:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Descriptors, Lipinski\n",
      "90/75:\n",
      "def lipinski(smiles, verbose=False):\n",
      "\n",
      "    moldata= []\n",
      "    for elem in smiles:\n",
      "        mol=Chem.MolFromSmiles(elem) \n",
      "        moldata.append(mol)\n",
      "       \n",
      "    baseData= np.arange(1,1)\n",
      "    i=0  \n",
      "    for mol in moldata:        \n",
      "       \n",
      "        desc_MolWt = Descriptors.MolWt(mol)\n",
      "        desc_MolLogP = Descriptors.MolLogP(mol)\n",
      "        desc_NumHDonors = Lipinski.NumHDonors(mol)\n",
      "        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)\n",
      "        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)\n",
      "           \n",
      "        row = np.array([desc_MolWt,\n",
      "                        desc_MolLogP,\n",
      "                        desc_NumHDonors,\n",
      "                        desc_NumHAcceptors,\n",
      "                        desc_NumRotatableBonds])   \n",
      "    \n",
      "        if(i==0):\n",
      "            baseData=row\n",
      "        else:\n",
      "            baseData=np.vstack([baseData, row])\n",
      "        i=i+1      \n",
      "    \n",
      "    columnNames=[\"MW\",\"LogP\",\"NumHDonors\",\"NumHAcceptors\", \"NumRotatableBonds\"]   \n",
      "    descriptors = pd.DataFrame(data=baseData,columns=columnNames)\n",
      "    \n",
      "    return descriptors\n",
      "90/76: print(df4['smiles'].unique())\n",
      "90/77: df5 = df4.drop_duplicates(subset=['smiles'])\n",
      "90/78: df5\n",
      "90/79: print(df5['kpu'].unique())\n",
      "90/80:\n",
      "df_lipinski = lipinski(df5.smiles)\n",
      "df_lipinski\n",
      "90/81:\n",
      "#create smi files\n",
      "selection = ['smiles', 'compound']\n",
      "df_selection = df4[selection]\n",
      "df_selection.to_csv(r'smiles/molecule.smi', sep='\\t', index=False, header=False)\n",
      "90/82:\n",
      "#create smi files\n",
      "selection = ['smiles', 'compound']\n",
      "df_selection = df4[selection]\n",
      "df_selection.to_csv('molecule.smi', sep='\\t', index=False, header=False)\n",
      "90/83:\n",
      "data = pd.concat([df5,df_lipinski], axis=1)\n",
      "data.to_csv('data_lipinski_kpu.csv')\n",
      "data\n",
      "90/84:\n",
      "df_lipinski = lipinski(df5.smiles)\n",
      "df_lipinski\n",
      "90/85: df_lipinski.shape\n",
      "90/86: df4\n",
      "90/87: df5.shape\n",
      "90/88: df6 = df5.reset_index(drop=True))\n",
      "90/89:\n",
      "df6 = df5.reset_index(drop=True)\n",
      "df6\n",
      "90/90:\n",
      "df6_lipinski = df_lipinski.reset_index(drop=True)\n",
      "df6\n",
      "90/91:\n",
      "df6_lipinski = df_lipinski.reset_index(drop=True)\n",
      "df6_lipinski\n",
      "90/92:\n",
      "data = pd.concat([df6,df6_lipinski], axis=1)\n",
      "data.to_csv('data_lipinski_kpu.csv')\n",
      "data\n",
      "89/1: pwd\n",
      "89/2:\n",
      "list_Fp = ['Fingerprinter.xml', 'ExtendedFingerprinter.xml', 'EStateFingerprinter.xml',\n",
      "           'GraphOnlyFingerprinter.xml', 'MACCSFingerprinter.xml', 'PubchemFingerprinter.xml',\n",
      "           'SubstructureFingerprinter.xml', 'KlekotaRothFingerprinter.xml', 'AtomPairs2DFingerprinter.xml']\n",
      "95/1:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "95/2: pwd\n",
      "95/3: df = pd.read_csv('rawdataqsar.csv')\n",
      "95/4: df\n",
      "95/5: df2 =df[df.kpu.notna()]\n",
      "95/6: df3 =df2[df.kp.notna()]\n",
      "95/7: df3\n",
      "95/8: df3.shape\n",
      "95/9: df3.isnull()\n",
      "95/10: df4=df3.reset_index(drop=True)\n",
      "95/11: df4\n",
      "95/12:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Descriptors, Lipinski\n",
      "95/13:\n",
      "def lipinski(smiles, verbose=False):\n",
      "\n",
      "    moldata= []\n",
      "    for elem in smiles:\n",
      "        mol=Chem.MolFromSmiles(elem) \n",
      "        moldata.append(mol)\n",
      "       \n",
      "    baseData= np.arange(1,1)\n",
      "    i=0  \n",
      "    for mol in moldata:        \n",
      "       \n",
      "        desc_MolWt = Descriptors.MolWt(mol)\n",
      "        desc_MolLogP = Descriptors.MolLogP(mol)\n",
      "        desc_NumHDonors = Lipinski.NumHDonors(mol)\n",
      "        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)\n",
      "        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)\n",
      "           \n",
      "        row = np.array([desc_MolWt,\n",
      "                        desc_MolLogP,\n",
      "                        desc_NumHDonors,\n",
      "                        desc_NumHAcceptors,\n",
      "                        desc_NumRotatableBonds])   \n",
      "    \n",
      "        if(i==0):\n",
      "            baseData=row\n",
      "        else:\n",
      "            baseData=np.vstack([baseData, row])\n",
      "        i=i+1      \n",
      "    \n",
      "    columnNames=[\"MW\",\"LogP\",\"NumHDonors\",\"NumHAcceptors\", \"NumRotatableBonds\"]   \n",
      "    descriptors = pd.DataFrame(data=baseData,columns=columnNames)\n",
      "    \n",
      "    return descriptors\n",
      "95/14:\n",
      "def lipinski(smiles, verbose=False):\n",
      "\n",
      "    moldata= []\n",
      "    for elem in smiles:\n",
      "        mol=Chem.MolFromSmiles(elem) \n",
      "        moldata.append(mol)\n",
      "       \n",
      "    baseData= np.arange(1,1)\n",
      "    i=0  \n",
      "    for mol in moldata:        \n",
      "       \n",
      "        desc_MolWt = Descriptors.MolWt(mol)\n",
      "        desc_TPSA = Descriptors.TPSA(mol)\n",
      "        desc_MolLogP = Descriptors.MolLogP(mol)\n",
      "        desc_NumHDonors = Lipinski.NumHDonors(mol)\n",
      "        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)\n",
      "        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)\n",
      "           \n",
      "        row = np.array([desc_MolWt,\n",
      "                        desc_TPSA,\n",
      "                        desc_MolLogP,\n",
      "                        desc_NumHDonors,\n",
      "                        desc_NumHAcceptors,\n",
      "                        desc_NumRotatableBonds])   \n",
      "    \n",
      "        if(i==0):\n",
      "            baseData=row\n",
      "        else:\n",
      "            baseData=np.vstack([baseData, row])\n",
      "        i=i+1      \n",
      "    \n",
      "    columnNames=[\"MW\",\"TPSA\", \"LogP\",\"NumHDonors\",\"NumHAcceptors\", \"NumRotatableBonds\"]   \n",
      "    descriptors = pd.DataFrame(data=baseData,columns=columnNames)\n",
      "    \n",
      "    return descriptors\n",
      "95/15: print(df4['smiles'].unique())\n",
      "95/16:\n",
      "dfall_lipinski = lipinski(df4.smiles)\n",
      "dfall_lipinski\n",
      "95/17: print(df4['kpu'].unique())\n",
      "95/18:\n",
      "df4_all = df4.reset_index(drop=True)\n",
      "df4_all\n",
      "95/19:\n",
      "df4_all = df4.reset_index(drop=True)\n",
      "df4_all_lipinski = dfall_lipinski.reset_index(drop=True)\n",
      "data2 = pd.concat([df4_all,df4_all_lipinski], axis=1)\n",
      "data2.to_csv('data2_lipinski_kpu.csv')\n",
      "data2\n",
      "93/1:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split\n",
      "93/2: pwd\n",
      "93/3: df = pd.read_csv('data2_all_lipinski.csv')\n",
      "93/4: df = pd.read_csv('data2_all_lipinski_pku.csv')\n",
      "93/5: df = pd.read_csv('data2_all_lipinski_kpu.csv')\n",
      "93/6: df = pd.read_csv('data2_lipinski_kpu.csv')\n",
      "93/7:\n",
      "df = pd.read_csv('data2_lipinski_kpu.csv')\n",
      "df\n",
      "93/8:\n",
      "df = pd.read_csv('data2_lipinski_kpu.csv')\n",
      "df1 =df.drop(['Unnamed'], axis =1)\n",
      "93/9:\n",
      "df = pd.read_csv('data2_lipinski_kpu.csv')\n",
      "df1 =df.drop(['Unnamed'], axis =1)\n",
      "93/10: df = pd.read_csv('data2_lipinski_kpu.csv')\n",
      "93/11:\n",
      "df = pd.read_csv('data2_lipinski_kpu.csv')\n",
      "df\n",
      "93/12: df1 =df.drop(['Unnamed:'], axis =1)\n",
      "93/13: df1 =df.drop(['Unnamed: 0'], axis =1)\n",
      "93/14:\n",
      "df1 =df.drop(['Unnamed: 0'], axis =1)\n",
      "df1\n",
      "93/15:\n",
      "X_train, X_test, y_train, y_test = train_test_split(\n",
      "    X, y, test_size=0.2, random_state=42)\n",
      "93/16: X = df1.drop(['smiles', 'kp', 'Cells'])\n",
      "93/17: X = df1.drop(c['smiles', 'kp', 'Cells'])\n",
      "93/18: X = df1.drop(['smiles', 'kp', 'Cells'])\n",
      "93/19: X = df1.drop(['smiles', 'kp', 'Cells'], axis =1)\n",
      "93/20: X\n",
      "93/21: X = df1.drop(['compound','smiles', 'kp', 'Cells'], axis =1)\n",
      "93/22: X\n",
      "100/1: X = df1.drop(['compound','smiles', 'kp','kpu' 'Cells'], axis =1)\n",
      "100/2:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split\n",
      "100/3: pwd\n",
      "100/4:\n",
      "df = pd.read_csv('data2_lipinski_kpu.csv')\n",
      "df\n",
      "100/5:\n",
      "df1 =df.drop(['Unnamed: 0'], axis =1)\n",
      "df1\n",
      "100/6: X = df1.drop(['compound','smiles', 'kp','kpu' 'Cells'], axis =1)\n",
      "100/7: X = df1.drop(['compound','smiles', 'kp','kpu', 'Cells'], axis =1)\n",
      "100/8: X\n",
      "100/9: Y = df1['kp']\n",
      "100/10:\n",
      "Y = df1['kp']\n",
      "Y\n",
      "100/11: df2 = pd.concat([X, Y], 1)\n",
      "100/12: df2\n",
      "100/13: X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
      "100/14: X_train.shape, Y_train.shape\n",
      "100/15: X_test.shape, Y_test.shape\n",
      "100/16:\n",
      "model = RandomForestRegressor(n_estimators=100)\n",
      "model.fit(X_train, Y_train)\n",
      "r2 = model.score(X_test, Y_test)\n",
      "r2\n",
      "100/17:\n",
      "rom sklearn.ensemble import RandomForestRegressor\n",
      "model = RandomForestRegressor(n_estimators=100)\n",
      "model.fit(X_train, Y_train)\n",
      "r2 = model.score(X_test, Y_test)\n",
      "r2\n",
      "100/18:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "model = RandomForestRegressor(n_estimators=100)\n",
      "model.fit(X_train, Y_train)\n",
      "r2 = model.score(X_test, Y_test)\n",
      "r2\n",
      "100/19: Y_pred = model.predict(X_test)\n",
      "100/20:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "sns.set(color_codes=True)\n",
      "sns.set_style(\"white\")\n",
      "\n",
      "ax = sns.regplot(Y_test, Y_pred, scatter_kws={'alpha':0.4})\n",
      "ax.set_xlabel('Experimental pIC50', fontsize='large', fontweight='bold')\n",
      "ax.set_ylabel('Predicted pIC50', fontsize='large', fontweight='bold')\n",
      "ax.set_xlim(0, 12)\n",
      "ax.set_ylim(0, 12)\n",
      "ax.figure.set_size_inches(5, 5)\n",
      "plt.show\n",
      "100/21:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "sns.set(color_codes=True)\n",
      "sns.set_style(\"white\")\n",
      "\n",
      "ax = sns.regplot(Y_test, Y_pred, scatter_kws={'alpha':0.4})\n",
      "ax.set_xlabel('Experimental pIC50', fontsize='large', fontweight='bold')\n",
      "ax.set_ylabel('Predicted pIC50', fontsize='large', fontweight='bold')\n",
      "ax.figure.set_size_inches(5, 5)\n",
      "plt.show\n",
      "100/22:\n",
      "Y = df1['kpu']\n",
      "Y\n",
      "100/23: df2 = pd.concat([X, Y], 1)\n",
      "100/24: df2\n",
      "100/25: X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
      "100/26: X_train.shape, Y_train.shape\n",
      "100/27: X_test.shape, Y_test.shape\n",
      "100/28:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "model = RandomForestRegressor(n_estimators=100)\n",
      "model.fit(X_train, Y_train)\n",
      "r2 = model.score(X_test, Y_test)\n",
      "r2\n",
      "100/29: Y_pred = model.predict(X_test)\n",
      "100/30:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "sns.set(color_codes=True)\n",
      "sns.set_style(\"white\")\n",
      "\n",
      "ax = sns.regplot(Y_test, Y_pred, scatter_kws={'alpha':0.4})\n",
      "ax.set_xlabel('Experimental pIC50', fontsize='large', fontweight='bold')\n",
      "ax.set_ylabel('Predicted pIC50', fontsize='large', fontweight='bold')\n",
      "ax.figure.set_size_inches(5, 5)\n",
      "plt.show\n",
      "100/31:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "sns.set(color_codes=True)\n",
      "sns.set_style(\"white\")\n",
      "\n",
      "ax = sns.regplot(Y_test, Y_pred, scatter_kws={'alpha':0.4})\n",
      "ax.set_xlabel('Experimental kpuu', fontsize='large', fontweight='bold')\n",
      "ax.set_ylabel('Predicted pkuu', fontsize='large', fontweight='bold')\n",
      "\n",
      "ax.figure.set_size_inches(5, 5)\n",
      "plt.show\n",
      "100/32:\n",
      "from rdkit import Chem\n",
      "from mordred import Calculator, all_descriptors\n",
      "\n",
      "# create descriptor calculator with all descriptors\n",
      "calc = Calculator(all_descriptors())\n",
      "\n",
      "# calculate and print descriptors\n",
      "for desc, value in calc(Chem.MolFromSmiles('c1ccccc1O')):\n",
      "   print('{}\\t{}'.format(desc, value))\n",
      "107/1:\n",
      "#create smi files \n",
      "#re-think\n",
      "selection = ['smiles', 'compound']\n",
      "df_selection = data2[selection]\n",
      "df_selection.to_csv('molecules.smi', sep='\\t', index=False, header=False)\n",
      "df_selection\n",
      "107/2:\n",
      "df4_all = df4.reset_index(drop=True)\n",
      "df4_all_lipinski = dfall_lipinski.reset_index(drop=True)\n",
      "data2 = pd.concat([df4_all,df4_all_lipinski], axis=1)\n",
      "data2.to_csv('data2_lipinski_kpu.csv')\n",
      "data2\n",
      "107/3:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "107/4: pwd\n",
      "107/5: df = pd.read_csv('rawdataqsar.csv')\n",
      "107/6: df\n",
      "107/7: df2 =df[df.kpu.notna()]\n",
      "107/8: df3 =df2[df.kp.notna()]\n",
      "107/9: df3\n",
      "107/10: df3.shape\n",
      "107/11: df3.isnull()\n",
      "107/12: df4=df3.reset_index(drop=True)\n",
      "107/13: df4\n",
      "107/14:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Descriptors, Lipinski\n",
      "107/15:\n",
      "def lipinski(smiles, verbose=False):\n",
      "\n",
      "    moldata= []\n",
      "    for elem in smiles:\n",
      "        mol=Chem.MolFromSmiles(elem) \n",
      "        moldata.append(mol)\n",
      "       \n",
      "    baseData= np.arange(1,1)\n",
      "    i=0  \n",
      "    for mol in moldata:        \n",
      "       \n",
      "        desc_MolWt = Descriptors.MolWt(mol)\n",
      "        desc_TPSA = Descriptors.TPSA(mol)\n",
      "        desc_MolLogP = Descriptors.MolLogP(mol)\n",
      "        desc_NumHDonors = Lipinski.NumHDonors(mol)\n",
      "        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)\n",
      "        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)\n",
      "           \n",
      "        row = np.array([desc_MolWt,\n",
      "                        desc_TPSA,\n",
      "                        desc_MolLogP,\n",
      "                        desc_NumHDonors,\n",
      "                        desc_NumHAcceptors,\n",
      "                        desc_NumRotatableBonds])   \n",
      "    \n",
      "        if(i==0):\n",
      "            baseData=row\n",
      "        else:\n",
      "            baseData=np.vstack([baseData, row])\n",
      "        i=i+1      \n",
      "    \n",
      "    columnNames=[\"MW\",\"TPSA\", \"LogP\",\"NumHDonors\",\"NumHAcceptors\", \"NumRotatableBonds\"]   \n",
      "    descriptors = pd.DataFrame(data=baseData,columns=columnNames)\n",
      "    \n",
      "    return descriptors\n",
      "107/16: print(df4['smiles'].unique())\n",
      "107/17:\n",
      "dfall_lipinski = lipinski(df4.smiles)\n",
      "dfall_lipinski\n",
      "107/18: print(df4['kpu'].unique())\n",
      "107/19:\n",
      "df4_all = df4.reset_index(drop=True)\n",
      "df4_all_lipinski = dfall_lipinski.reset_index(drop=True)\n",
      "data2 = pd.concat([df4_all,df4_all_lipinski], axis=1)\n",
      "data2.to_csv('data2_lipinski_kpu.csv')\n",
      "data2\n",
      "107/20:\n",
      "#create smi files \n",
      "#re-think\n",
      "selection = ['smiles', 'compound']\n",
      "df_selection = data2[selection]\n",
      "df_selection.to_csv('molecules.smi', sep='\\t', index=False, header=False)\n",
      "df_selection\n",
      "107/21:\n",
      "#create smi files \n",
      "#re-think\n",
      "selection = ['smiles', 'compound']\n",
      "df_selection = data2[selection]\n",
      "df_selection.to_csv('molecules.smi', sep='\\t', index=False, header=False)\n",
      "! cat moleculkes.smi\n",
      "107/22:\n",
      "#create smi files \n",
      "#re-think\n",
      "selection = ['smiles', 'compound']\n",
      "df_selection = data2[selection]\n",
      "df_selection.to_csv('molecules.smi', sep='\\t', index=False, header=False)\n",
      "! cat molecules.smi\n",
      "105/1:\n",
      ">>> from rdkit import Chem\n",
      ">>> from mordred import Calculator, descriptors\n",
      "\n",
      "# create descriptor calculator with all descriptors\n",
      ">>> calc = Calculator(descriptors, ignore_3D=True)\n",
      "\n",
      ">>> len(calc.descriptors)\n",
      "1613\n",
      "\n",
      ">>> len(Calculator(descriptors, ignore_3D=True, version=\"1.0.0\"))\n",
      "1612\n",
      "\n",
      "# calculate single molecule\n",
      ">>> mol = Chem.MolFromSmiles('c1ccccc1')\n",
      ">>> calc(mol)[:3]\n",
      "[4.242640687119286, 3.9999999999999996, 0]\n",
      "\n",
      "# calculate multiple molecule\n",
      ">>> mols = [Chem.MolFromSmiles(smi) for smi in ['c1ccccc1Cl', 'c1ccccc1O', 'c1ccccc1N']]\n",
      "\n",
      "# as pandas\n",
      ">>> df = calc.pandas(mols)\n",
      ">>> df['SLogP']\n",
      "0    2.3400\n",
      "1    1.3922\n",
      "2    1.2688\n",
      "Name: SLogP, dtype: float64\n",
      "105/2:\n",
      "from rdkit import Chem\n",
      "from mordred import Calculator, descriptors\n",
      "\n",
      "# create descriptor calculator with all descriptors\n",
      ">>> calc = Calculator(descriptors, ignore_3D=True)\n",
      "\n",
      ">>> len(calc.descriptors)\n",
      "1613\n",
      "\n",
      ">>> len(Calculator(descriptors, ignore_3D=True, version=\"1.0.0\"))\n",
      "1612\n",
      "\n",
      "# calculate single molecule\n",
      ">>> mol = Chem.MolFromSmiles('c1ccccc1')\n",
      ">>> calc(mol)[:3]\n",
      "[4.242640687119286, 3.9999999999999996, 0]\n",
      "\n",
      "# calculate multiple molecule\n",
      ">>> mols = [Chem.MolFromSmiles(smi) for smi in ['c1ccccc1Cl', 'c1ccccc1O', 'c1ccccc1N']]\n",
      "\n",
      "# as pandas\n",
      ">>> df = calc.pandas(mols)\n",
      ">>> df['SLogP']\n",
      "0    2.3400\n",
      "1    1.3922\n",
      "2    1.2688\n",
      "Name: SLogP, dtype: float64\n",
      "105/3:\n",
      "from rdkit import Chem\n",
      "from mordred import Calculator, descriptors\n",
      "\n",
      "# create descriptor calculator with all descriptors\n",
      "calc = Calculator(descriptors, ignore_3D=True)\n",
      "105/4: len(calc)\n",
      "100/33:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split\n",
      "100/34: pwd\n",
      "100/35:\n",
      "df = pd.read_csv('data2_lipinski_kpu.csv')\n",
      "df\n",
      "100/36:\n",
      "df1 =df.drop(['Unnamed: 0'], axis =1)\n",
      "df1\n",
      "100/37: X = df1.drop(['compound','smiles', 'kp','kpu', 'Cells'], axis =1)\n",
      "100/38: X\n",
      "100/39:\n",
      "Y = df1['kpu']\n",
      "Y\n",
      "100/40: df2 = pd.concat([X, Y], 1)\n",
      "100/41: df2\n",
      "100/42: X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
      "100/43: df2\n",
      "100/44: X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
      "100/45: X_train.shape, Y_train.shape\n",
      "100/46: X_test.shape, Y_test.shape\n",
      "100/47:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "model = RandomForestRegressor(n_estimators=100)\n",
      "model.fit(X_train, Y_train)\n",
      "r2 = model.score(X_test, Y_test)\n",
      "r2\n",
      "100/48: Y_pred = model.predict(X_test)\n",
      "100/49:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "sns.set(color_codes=True)\n",
      "sns.set_style(\"white\")\n",
      "ax = sns.regplot(Y_test, Y_pred, scatter_kws={'alpha':0.4})\n",
      "ax.set_xlabel('Experimental kpuu', fontsize='large', fontweight='bold')\n",
      "ax.set_ylabel('Predicted pkuu', fontsize='large', fontweight='bold')\n",
      "ax.figure.set_size_inches(5, 5)\n",
      "plt.show\n",
      "100/50:\n",
      "from rdkit import Chem\n",
      "from mordred import Calculator, all_descriptors\n",
      "\n",
      "# create descriptor calculator with all descriptors\n",
      "calc = Calculator(all_descriptors())\n",
      "100/51:\n",
      "from rdkit import Chem\n",
      "from mordred import Calculator, all_descriptors\n",
      "\n",
      "# create descriptor calculator with all descriptors\n",
      "calc = Calculator(descriptors, ignore_3D=True)\n",
      "100/52:\n",
      "from rdkit import Chem\n",
      "from mordred import Calculator\n",
      "\n",
      "# create descriptor calculator with all descriptors\n",
      "calc = Calculator(descriptors, ignore_3D=True)\n",
      "100/53:\n",
      "from rdkit import Chem\n",
      "from mordred import Calculator, descriptors\n",
      "\n",
      "# create descriptor calculator with all descriptors\n",
      "calc = Calculator(descriptors, ignore_3D=True)\n",
      "100/54: len(calc)\n",
      "100/55: df3 = pd.read_csv('molecules.csv')\n",
      "100/56: df3 = pd.read_csv('molecules3.csv')\n",
      "100/57: python -m mordred -i molecules.smi -o molecules.csv\n",
      "100/58: !python -m mordred -i molecules.smi -o molecules.csv\n",
      "100/59: !python -m mordred -t smi /molecules.smi -o molecules.csv\n",
      "100/60: pwd\n",
      "100/61: !python -m mordred -t smi \\molecules.smi -o molecules.csv\n",
      "100/62: df3 = pd.read_csv('molecules.csv')\n",
      "100/63: df3\n",
      "100/64: df3.shape\n",
      "100/65: df4.dropna()\n",
      "100/66: df4 = df3.dropna()\n",
      "100/67: df4\n",
      "100/68: df4 = df3.dropna(axis='columns', how='all')\n",
      "100/69: df4\n",
      "100/70: df4 = df3.dropna(axis='columns', how='any')\n",
      "100/71: df4\n",
      "100/72: df4.column()\n",
      "100/73: df4.column.unique()\n",
      "100/74: df4.unique()\n",
      "100/75: print(df4.column)\n",
      "100/76: print(df4.columns)\n",
      "100/77: print(df4.columns.unqiue)\n",
      "100/78: print(df4.columns.unique)\n",
      "100/79: df4.columns.unique\n",
      "100/80:\n",
      "df4.columns.unique\n",
      "df4.to_csv('Fingerprint.csv')\n",
      "100/81:\n",
      "Y = df1['kpu']\n",
      "Y\n",
      "100/82: df5 = pd.concat([df4, Y], axis =1)\n",
      "100/83: df5\n",
      "100/84: X = df4\n",
      "100/85: df5 = pd.concat([X, Y], axis =1)\n",
      "100/86: df5\n",
      "100/87: df6 = df5.drop('name')\n",
      "100/88: df6 = df5.drop(['name'], axis=1)\n",
      "100/89: df6\n",
      "100/90: df5 = pd.concat([df4, Y], axis =1)\n",
      "100/91: df5\n",
      "100/92: df6 = df5.drop(['name'], axis=1)\n",
      "100/93: df6\n",
      "100/94: X = df6.drop(['kpu', axis =1])\n",
      "100/95: X = df6.drop(['kpu'], , axis=1)\n",
      "100/96: X = df6.drop(['kpu'],axis=1)\n",
      "100/97: X = df6.drop(['kpu'], axis=1)\n",
      "100/98: X,shape\n",
      "100/99: X.shape\n",
      "100/100: Y.shape\n",
      "100/101:\n",
      "from sklearn.model_selection import KFold\n",
      "kf = KFold(n_splits=10)\n",
      "kf\n",
      "100/102:\n",
      "from sklearn.model_selection import KFold\n",
      "kf = KFold(n_splits=10, shuffle=True)\n",
      "kf\n",
      "100/103:\n",
      "from sklearn.model_selection import cross_val_score\n",
      "model = GaussianNB()\n",
      "cvs = cross_val_score(model, X, Y, CV=10)\n",
      "100/104:\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "model = GaussianNB()\n",
      "cvs = cross_val_score(model, X, Y, CV=10)\n",
      "100/105:\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "model = GaussianNB()\n",
      "cvs = cross_val_score(model, X, Y, cv=10)\n",
      "100/106: csv\n",
      "100/107: cvs\n",
      "100/108: y = df6(['kpu'], axis = 1)\n",
      "100/109: y = df6['kpu']\n",
      "100/110:\n",
      "y = df6['kpu']\n",
      "y\n",
      "100/111:\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "model = GaussianNB()\n",
      "cvs = cross_val_score(model, X, y, cv=10)\n",
      "100/112:\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "model = GaussianNB()\n",
      "100/113: X_train, X_test, y_train, y_test = train_set_split(X, y, test_size = 0.2)\n",
      "100/114:\n",
      "from sklearn.model_selection import train_test_split\n",
      "X_train, X_test, y_train, y_test = train_set_split(X, y, test_size = 0.2)\n",
      "100/115:\n",
      "from sklearn.model_selection import train_test_split\n",
      "X_train, X_test, y_train, y_test = train_set_split(X, y, test_size = 0.2)\n",
      "100/116:\n",
      "from sklearn.model_selection import train_test_split\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
      "100/117:\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "model = GaussianNB()\n",
      "model.fit(X_train,y_train)\n",
      "model.score(X_test,y_train)\n",
      "100/118:\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "model = GaussianNB()\n",
      "model.fit(X_train,y_train)\n",
      "model.score(X_train,y_train)\n",
      "100/119: X.type()\n",
      "100/120: type(X)\n",
      "100/121: X.dtypes\n",
      "100/122: X.dtypes\n",
      "100/123: Y.dtypes\n",
      "100/124: y.dtypes\n",
      "100/125: X.dtypes.unique()\n",
      "100/126: X = X.astype('int')\n",
      "100/127:\n",
      "from sklearn.model_selection import train_test_split\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
      "100/128:\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "model = GaussianNB()\n",
      "model.fit(X_train,y_train)\n",
      "model.score(X_train,y_train)\n",
      "100/129:\n",
      "from sklearn.naive_bayes import RandomForestRegressor\n",
      "model = RandomForestRegressor()\n",
      "model.fit(X_train,y_train)\n",
      "model.score(X_train,y_train)\n",
      "100/130:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "model = RandomForestRegressor()\n",
      "model.fit(X_train,y_train)\n",
      "model.score(X_train,y_train)\n",
      "100/131:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "model = RandomForestRegressor()\n",
      "model.fit(X_train,y_train)\n",
      "model.score(X_train,y_train)\n",
      "100/132:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "model = RandomForestRegressor()\n",
      "model.fit(X_train,y_train)\n",
      "r2 = model.score(X_test, Y_test)\n",
      "r2\n",
      "100/133:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "model = RandomForestRegressor()\n",
      "model.fit(X_train,y_train)\n",
      "r2 = model.score(X_test, Y_test)\n",
      "r2\n",
      "100/134: code issues!\n",
      "100/135:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "model = RandomForestRegressor()\n",
      "model.fit(X_train,y_train)\n",
      "r2 = model.score(X_test, Y_test)\n",
      "r2\n",
      "100/136:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "model = RandomForestRegressor()\n",
      "model.fit(X_train,y_train)\n",
      "r2 = model.score(X_train,y_train)\n",
      "r2\n",
      "100/137:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "model = RandomForestRegressor()\n",
      "model.fit(X_train,y_train)\n",
      "r2 = model.score(X_train,y_train)\n",
      "r2\n",
      "100/138:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "model = RandomForestRegressor()\n",
      "model.fit(X_train,y_train)\n",
      "r2 = model.score(X_train,y_train)\n",
      "r2\n",
      "100/139:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "model = RandomForestRegressor()\n",
      "model.fit(X_train,y_train)\n",
      "r2 = model.score(X_train,y_train)\n",
      "r2\n",
      "100/140:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "model = RandomForestRegressor()\n",
      "model.fit(X_train,y_train)\n",
      "r2 = model.score(X_train,y_train)\n",
      "r2\n",
      "100/141:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "model = RandomForestRegressor()\n",
      "model.fit(X_train,y_train)\n",
      "r2 = model.score(X_train,y_train)\n",
      "r2\n",
      "100/142: Y_pred = model.predict(X_test)\n",
      "100/143: y_pred = model.predict(X_test)\n",
      "100/144:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "sns.set(color_codes=True)\n",
      "sns.set_style(\"white\")\n",
      "\n",
      "ax = sns.regplot(y_test, y_pred, scatter_kws={'alpha':0.4})\n",
      "ax.set_xlabel('Experimental Kpu', fontsize='large', fontweight='bold')\n",
      "ax.set_ylabel('Predicted Kpu', fontsize='large', fontweight='bold')\n",
      "ax.set_xlim(0, 12)\n",
      "ax.set_ylim(0, 12)\n",
      "ax.figure.set_size_inches(5, 5)\n",
      "plt.show\n",
      "100/145:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "sns.set(color_codes=True)\n",
      "sns.set_style(\"white\")\n",
      "\n",
      "ax = sns.regplot(y_test, y_pred, scatter_kws={'alpha':0.4})\n",
      "ax.set_xlabel('Experimental Kpu', fontsize='large', fontweight='bold')\n",
      "ax.set_ylabel('Predicted Kpu', fontsize='large', fontweight='bold')\n",
      "ax.set_xlim(0, 12)\n",
      "ax.set_xscale('log')\n",
      "ax.set_yscale('log')\n",
      "ax.set_ylim(0, 12)\n",
      "ax.figure.set_size_inches(5, 5)\n",
      "plt.show\n",
      "100/146:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "sns.set(color_codes=True)\n",
      "sns.set_style(\"white\")\n",
      "\n",
      "ax = sns.regplot(y_test, y_pred, scatter_kws={'alpha':0.4})\n",
      "ax.set_xlabel('Experimental Kpu', fontsize='large', fontweight='bold')\n",
      "ax.set_ylabel('Predicted Kpu', fontsize='large', fontweight='bold')\n",
      "\n",
      "ax.set_xscale('log')\n",
      "ax.set_yscale('log')\n",
      "\n",
      "ax.figure.set_size_inches(5, 5)\n",
      "plt.show\n",
      "100/147:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import cross_val_score\n",
      "model2 = RandomForestRegressor()\n",
      "model2.fit()\n",
      "cvs = cross_val_score(model2, X, y, cv=10)\n",
      "print('cross val scores {}'.format(cvs))\n",
      "print('mean {:4f}'.format(cvs.mean()), 'sd {:4}'.format(cvs.sd()))\n",
      "100/148:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import cross_val_score\n",
      "model2 = RandomForestRegressor()\n",
      "cvs = cross_val_score(model2, X, y, cv=10)\n",
      "print('cross val scores {}'.format(cvs))\n",
      "print('mean {:4f}'.format(cvs.mean()), 'sd {:4}'.format(cvs.sd()))\n",
      "100/149:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import cross_val_score\n",
      "model2 = RandomForestRegressor()\n",
      "cvs = cross_val_score(model2, X, y, cv=10)\n",
      "print('cross val scores {}'.format(cvs))\n",
      "print('mean {:4f}'.format(cvs.mean()))\n",
      "100/150:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import cross_val_score\n",
      "model = RandomForestRegressor()\n",
      "cvs = cross_val_score(model, X, y, cv=10)\n",
      "print('cross val scores {}'.format(cvs))\n",
      "print('mean {:4f}'.format(cvs.mean()))\n",
      "109/1:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "sns.set(color_codes=True)\n",
      "sns.set_style(\"white\")\n",
      "\n",
      "ax = sns.regplot(y_test, y_pred, scatter_kws={'alpha':0.4})\n",
      "ax.set_xlabel('Experimental Kpu', fontsize='large', fontweight='bold')\n",
      "ax.set_ylabel('Predicted Kpu', fontsize='large', fontweight='bold')\n",
      "\n",
      "\n",
      "ax.figure.set_size_inches(5, 5)\n",
      "plt.show\n",
      "109/2:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split\n",
      "109/3: pwd\n",
      "110/1: pwd\n",
      "110/2: df = pd.read_csv('Fingerprint.csv')\n",
      "110/3:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split\n",
      "110/4: df = pd.read_csv('Fingerprint.csv')\n",
      "110/5: df\n",
      "110/6: df1 = df.drop('Unnamed: 0')\n",
      "110/7: df1 = df.drop('Unnamed: 0', axis =1)\n",
      "110/8: df1\n",
      "110/9: df2 = pd.read_csv('data2_lipinski_kpu.csv')\n",
      "110/10: df2\n",
      "110/11:\n",
      "df3 = df2.drop(['smiles','MW','TPSA','LogP'])\n",
      "df3\n",
      "110/12:\n",
      "df3 = df2.drop(['smiles','MW','TPSA','LogP'], axis=1)\n",
      "df3\n",
      "110/13:\n",
      "df3 = df2.drop(['Unnamed: 0','smiles','MW','TPSA','LogP'], axis=1)\n",
      "df3\n",
      "110/14:\n",
      "df3 = df2.drop(['Unnamed: 0','compound','smiles','MW','TPSA','LogP'], axis=1)\n",
      "df3\n",
      "110/15: df4 =pd.concat([df1,df3], axis=1)\n",
      "110/16: df4\n",
      "110/17: df4\n",
      "110/18:\n",
      "from sklearn import preprocessing\n",
      "min_max_scaler = preprocessing.MinMaxScaler()\n",
      "np_scaled = min_max_scaler.fit_transform()\n",
      "110/19: X = df4.drop(['name', 'kp', 'kpu','Cells'], axis=1)\n",
      "110/20:\n",
      "X = df4.drop(['name', 'kp', 'kpu','Cells'], axis=1)\n",
      "y = df4('kp')\n",
      "110/21:\n",
      "X = df4.drop(['name', 'kp', 'kpu','Cells'], axis=1)\n",
      "y = df4['kp']\n",
      "110/22:\n",
      "from sklearn import preprocessing\n",
      "min_max_scaler = preprocessing.MinMaxScaler()\n",
      "np_scaled = min_max_scaler.fit_transform(X)\n",
      "110/23: np_scaled\n",
      "110/24:\n",
      "from sklearn import preprocessing\n",
      "min_max_scaler = preprocessing.MinMaxScaler()\n",
      "np_scaled = min_max_scaler.fit_transform(X)\n",
      "Fp_normalized = pd.DataFrame(np_scaled)\n",
      "Fp_normalized\n",
      "110/25:\n",
      "from sklearn import preprocessing\n",
      "min_max_scaler = preprocessing.MinMaxScaler()\n",
      "np_scaled = min_max_scaler.fit_transform(X)\n",
      "Fp_normalized = pd.DataFrame(np_scaled, columns=X.columns)\n",
      "110/26:\n",
      "from sklearn import preprocessing\n",
      "min_max_scaler = preprocessing.MinMaxScaler()\n",
      "np_scaled = min_max_scaler.fit_transform(X)\n",
      "Fp_normalized = pd.DataFrame(np_scaled, columns=X.columns)\n",
      "Fp_normalized\n",
      "110/27:\n",
      "from sklearn import preprocessing\n",
      "min_max_scaler = preprocessing.MinMaxScaler()\n",
      "np_scaled = min_max_scaler.fit_transform(X)\n",
      "Fp_normalized = pd.DataFrame(np_scaled, columns=X.columns)\n",
      "Fp_normalized.to_csv('Fp_normalized.csv')\n",
      "Fp_normalized\n",
      "110/28:\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.cross_validation import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn import cross_validation\n",
      "from collections import defaultdict\n",
      "110/29:\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn import cross_validation\n",
      "from collections import defaultdict\n",
      "110/30:\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.model_selection import cross_validaton\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from collections import defaultdict\n",
      "110/31:\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.model_selection import cross_validate\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from collections import defaultdict\n",
      "110/32:\n",
      "X_internal, X_external, y_internal, y_external = train_test_split(X,\n",
      "                                                        y, test_size=0.2,\n",
      "                                                        random_state=seed)\n",
      "110/33:\n",
      "X_internal, X_external, y_internal, y_external = train_test_split(X,\n",
      "                                                        y, test_size=0.2,\n",
      "                                                        random_state=5)\n",
      "110/34:\n",
      "X_train, X_test, y_train, y_test = train_test_split(X,\n",
      "                                                        y, test_size=0.2,\n",
      "                                                        random_state=5)\n",
      "110/35:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "model = RandomForestRegressor()\n",
      "model.fit(X_train,y_train)\n",
      "r2 = model.score(X_train,y_train)\n",
      "r2\n",
      "110/36: y_pred = model.predict(X_train)\n",
      "110/37: y_pred = model.predict(X_train)\n",
      "110/38:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "ax = plt.scatter(y_test, y_pred, scatter_kws={'alpha':0.4})\n",
      "ax.set_xlabel('Experimental Kpu', fontsize='large', fontweight='bold')\n",
      "ax.set_ylabel('Predicted Kpu', fontsize='large', fontweight='bold')\n",
      "#log\n",
      "ax.set_xscale('log')\n",
      "ax.set_yscale('log')\n",
      "\n",
      "ax.figure.set_size_inches(5, 5)\n",
      "plt.show\n",
      "110/39:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "ax = plt.scatter(y_train, y_pred, scatter_kws={'alpha':0.4})\n",
      "ax.set_xlabel('Experimental Kpu', fontsize='large', fontweight='bold')\n",
      "ax.set_ylabel('Predicted Kpu', fontsize='large', fontweight='bold')\n",
      "#log\n",
      "ax.set_xscale('log')\n",
      "ax.set_yscale('log')\n",
      "\n",
      "ax.figure.set_size_inches(5, 5)\n",
      "plt.show\n",
      "110/40:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "ax = plt.scatter(y_train, y_pred, alpha=0.4)\n",
      "ax.set_xlabel('Experimental Kpu', fontsize='large', fontweight='bold')\n",
      "ax.set_ylabel('Predicted Kpu', fontsize='large', fontweight='bold')\n",
      "#log\n",
      "ax.set_xscale('log')\n",
      "ax.set_yscale('log')\n",
      "\n",
      "ax.figure.set_size_inches(5, 5)\n",
      "plt.show\n",
      "110/41:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "ax = plt.scatter(y_train, y_pred, alpha=0.4)\n",
      "ax.xlabel('Experimental Kpu', fontsize='large', fontweight='bold')\n",
      "ax.ylabel('Predicted Kpu', fontsize='large', fontweight='bold')\n",
      "ax.xscale('log')\n",
      "ax.yscale('log')\n",
      "\n",
      "ax.figure.set_size_inches(5, 5)\n",
      "plt.show\n",
      "110/42:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "ax.scatter(y_train, y_pred, alpha=0.4)\n",
      "ax.xlabel('Experimental Kpu', fontsize='large', fontweight='bold')\n",
      "ax.ylabel('Predicted Kpu', fontsize='large', fontweight='bold')\n",
      "ax.xscale('log')\n",
      "ax.yscale('log')\n",
      "\n",
      "ax.figure.set_size_inches(5, 5)\n",
      "plt.show\n",
      "110/43:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.scatter(y_train, y_pred, alpha=0.4)\n",
      "ax.xlabel('Experimental Kpu', fontsize='large', fontweight='bold')\n",
      "ax.ylabel('Predicted Kpu', fontsize='large', fontweight='bold')\n",
      "ax.xscale('log')\n",
      "ax.yscale('log')\n",
      "\n",
      "ax.figure.set_size_inches(5, 5)\n",
      "plt.show\n",
      "110/44:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.scatter(y_train, y_pred, alpha=0.4)\n",
      "plt.xlabel('Experimental Kpu', fontsize='large', fontweight='bold')\n",
      "plt.ylabel('Predicted Kpu', fontsize='large', fontweight='bold')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "\n",
      "ax.figure.set_size_inches(5, 5)\n",
      "plt.show\n",
      "110/45:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.scatter(y_train, y_pred, alpha=0.5)\n",
      "plt.xlabel('Experimental Kpu', fontsize='large', fontweight='bold')\n",
      "plt.ylabel('Predicted Kpu', fontsize='large', fontweight='bold')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "\n",
      "ax.figure.set_size_inches(5, 5)\n",
      "plt.show\n",
      "110/46:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.scatter(y_train, y_pred, alpha=1)\n",
      "plt.xlabel('Experimental Kpu', fontsize='large', fontweight='bold')\n",
      "plt.ylabel('Predicted Kpu', fontsize='large', fontweight='bold')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "\n",
      "ax.figure.set_size_inches(5, 5)\n",
      "plt.show\n",
      "110/47:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.scatter(y_train, y_pred, alpha=0.4)\n",
      "plt.xlabel('Experimental Kpu', fontsize='large', fontweight='bold')\n",
      "plt.ylabel('Predicted Kpu', fontsize='large', fontweight='bold')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "\n",
      "ax.figure.set_size_inches(5, 5)\n",
      "plt.show\n",
      "110/48:\n",
      "y1_pred = model.predict(X_train)\n",
      "y2_pred = model.predict(X_test)\n",
      "110/49:\n",
      "import matplotlib.pyplot as plt\n",
      "color \n",
      "plt.scatter(y_train, y1_pred, alpha=0.4, c='coral')\n",
      "plt.scatter(y_test, y2_pred, alpha=0.4, c='lightblue')\n",
      "plt.xlabel('Experimental Kpu', fontsize='large', fontweight='bold')\n",
      "plt.ylabel('Predicted Kpu', fontsize='large', fontweight='bold')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "\n",
      "ax.figure.set_size_inches(5, 5)\n",
      "plt.show\n",
      "110/50:\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y_train, y1_pred, alpha=0.4, c='coral')\n",
      "plt.scatter(y_test, y2_pred, alpha=0.4, c='lightblue')\n",
      "plt.xlabel('Experimental Kpu', fontsize='large', fontweight='bold')\n",
      "plt.ylabel('Predicted Kpu', fontsize='large', fontweight='bold')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "\n",
      "ax.figure.set_size_inches(5, 5)\n",
      "plt.show\n",
      "110/51:\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y_train, y1_pred, alpha=0.7, c='coral')\n",
      "plt.scatter(y_test, y2_pred, alpha=0.7, c='lightblue')\n",
      "plt.xlabel('Experimental Kpu', fontsize='large', fontweight='bold')\n",
      "plt.ylabel('Predicted Kpu', fontsize='large', fontweight='bold')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "\n",
      "ax.figure.set_size_inches(5, 5)\n",
      "plt.show\n",
      "110/52:\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y_train, y1_pred, alpha=0.7, c='coral')\n",
      "plt.scatter(y_test, y2_pred, alpha=0.7, c='lightblue')\n",
      "plt.xlabel('Experimental Kpu', fontsize='large', fontweight='bold')\n",
      "plt.ylabel('Predicted Kpu', fontsize='large', fontweight='bold')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.legend(loc'upper right')\n",
      "ax.figure.set_size_inches(5, 5)\n",
      "plt.show\n",
      "110/53:\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y_train, y1_pred, alpha=0.7, c='coral')\n",
      "plt.scatter(y_test, y2_pred, alpha=0.7, c='lightblue')\n",
      "plt.xlabel('Experimental Kpu', fontsize='large', fontweight='bold')\n",
      "plt.ylabel('Predicted Kpu', fontsize='large', fontweight='bold')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.legend(loc='upper right')\n",
      "ax.figure.set_size_inches(5, 5)\n",
      "plt.show\n",
      "110/54:\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y_train, y1_pred, alpha=0.7, c='coral')\n",
      "plt.scatter(y_test, y2_pred, alpha=0.7, c='lightblue')\n",
      "plt.xlabel('Experimental Kpu', fontsize='large', fontweight='bold')\n",
      "plt.ylabel('Predicted Kpu', fontsize='large', fontweight='bold')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.legend()\n",
      "ax.figure.set_size_inches(5, 5)\n",
      "plt.show\n",
      "110/55:\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y_train, y1_pred, alpha=0.7, c='coral', label='Inline label')\n",
      "plt.scatter(y_test, y2_pred, alpha=0.7, c='lightblue', label='Inline label')\n",
      "plt.xlabel('Experimental Kpu', fontsize='large', fontweight='bold')\n",
      "plt.ylabel('Predicted Kpu', fontsize='large', fontweight='bold')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.legend()\n",
      "ax.figure.set_size_inches(5, 5)\n",
      "plt.show\n",
      "110/56:\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y_train, y1_pred, alpha=0.7, c='coral', label='Traing set')\n",
      "plt.scatter(y_test, y2_pred, alpha=0.7, c='lightblue', label='Test set')\n",
      "plt.xlabel('Experimental Kpu', fontsize='large', fontweight='bold')\n",
      "plt.ylabel('Predicted Kpu', fontsize='large', fontweight='bold')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.legend()\n",
      "ax.figure.set_size_inches(5, 5)\n",
      "plt.show\n",
      "110/57:\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y_train, y1_pred, alpha=0.7, c='coral', label='Traing set')\n",
      "plt.scatter(y_test, y2_pred, alpha=0.7, c='lightblue', label='Test set')\n",
      "plt.xlabel('Experimental Kpu', fontsize='large', fontweight='bold')\n",
      "plt.ylabel('Predicted Kpu', fontsize='large', fontweight='bold')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.legend()\n",
      "ax.figure.set_size_inches(5, 5)\n",
      "plt.show\n",
      "110/58:\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y_train, y1_pred, alpha=0.7, c='coral', label='Traing set')\n",
      "plt.scatter(y_test, y2_pred, alpha=0.7, c='lightblue', label='Test set')\n",
      "plt.xlabel('Experimental Kpu', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted Kpu', fontsize='16', fontweight='bold')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.legend()\n",
      "ax.figure.set_size_inches(5, 5)\n",
      "plt.show\n",
      "110/59:\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y_train, y1_pred, alpha=0.7, c='coral', label='Traing set')\n",
      "plt.scatter(y_test, y2_pred, alpha=0.7, c='lightblue', label='Test set')\n",
      "plt.xlabel('Experimental Kp', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted Kp', fontsize='16', fontweight='bold')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.legend()\n",
      "ax.figure.set_size_inches(5, 5)\n",
      "plt.show\n",
      "110/60:\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y_train, y1_pred, alpha=0.7, c='coral', label='Traing set')\n",
      "plt.scatter(y_test, y2_pred, alpha=0.7, c='lightblue', label='Test set')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.legend()\n",
      "ax.figure.set_size_inches(5, 5)\n",
      "plt.show\n",
      "110/61:\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y_train, y1_pred, alpha=0.7, c='coral', label='Traing set')\n",
      "plt.scatter(y_test, y2_pred, alpha=0.7, c='blue', label='Test set')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.legend()\n",
      "ax.figure.set_size_inches(5, 5)\n",
      "plt.show\n",
      "110/62:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y_train, y1_pred, alpha=0.7, c='coral', label='Traing set')\n",
      "plt.scatter(y_test, y2_pred, alpha=0.7, c='blue', label='Test set')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.legend()\n",
      "ax.figure.set_size_inches(5, 5)\n",
      "plt.show\n",
      "110/63:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import cross_val_score\n",
      "model = RandomForestRegressor()\n",
      "cvs = cross_val_score(model, X, y, cv=10)\n",
      "110/64: cvs\n",
      "110/65:\n",
      "from sklearn.model_selection import cross_val_score\n",
      "cvs = cross_val_score(model, X, y, cv=5)\n",
      "110/66:\n",
      "from sklearn.model_selection import cross_val_score\n",
      "scores= cross_val_score(model, X, y, cv=5)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "109/4:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
      "\n",
      "kf =KFold(n_splits=5, shuffle=True, random_state=42)\n",
      "110/67:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
      "kf =KFold(n_splits=10, shuffle=True, random_state=5)\n",
      "110/68:\n",
      "cnt = 1\n",
      "# split()  method generate indices to split data into training and test set.\n",
      "for train_index, test_index in kf.split(X, y):\n",
      "    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n",
      "    cnt += 1\n",
      "110/69:\n",
      "def rmse(score):\n",
      "    rmse = np.sqrt(-score)\n",
      "    print(f'rmse= {\"{:.2f}\".format(rmse)}')\n",
      "110/70:\n",
      "score = cross_val_score(model, X, y, cv= kf, scoring=\"neg_mean_squared_error\")\n",
      "print(f'Scores for each fold: {score}')\n",
      "rmse(score.mean())\n",
      "110/71:\n",
      "score = cross_val_score(model, X, y, cv= kf)\n",
      "print(f'Scores for each fold: {score}')\n",
      "rmse(score.mean())\n",
      "110/72:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
      "kf =KFold(n_splits=10, shuffle=True, random_state=40)\n",
      "110/73:\n",
      "cnt = 1\n",
      "# split()  method generate indices to split data into training and test set.\n",
      "for train_index, test_index in kf.split(X, y):\n",
      "    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n",
      "    cnt += 1\n",
      "110/74:\n",
      "def rmse(score):\n",
      "    rmse = np.sqrt(-score)\n",
      "    print(f'rmse= {\"{:.2f}\".format(rmse)}')\n",
      "110/75:\n",
      "score = cross_val_score(model, X, y, cv= kf)\n",
      "print(f'Scores for each fold: {score}')\n",
      "rmse(score.mean())\n",
      "110/76:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
      "kf =KFold(n_splits=10, shuffle=True, random_state=5)\n",
      "110/77:\n",
      "cnt = 1\n",
      "# split()  method generate indices to split data into training and test set.\n",
      "for train_index, test_index in kf.split(X, y):\n",
      "    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n",
      "    cnt += 1\n",
      "110/78:\n",
      "def rmse(score):\n",
      "    rmse = np.sqrt(-score)\n",
      "    print(f'rmse= {\"{:.2f}\".format(rmse)}')\n",
      "110/79:\n",
      "score = cross_val_score(model, X, y, cv= kf)\n",
      "print(f'Scores for each fold: {score}')\n",
      "rmse(score.mean())\n",
      "110/80:\n",
      "from sklearn import LogisticRegression\n",
      "clf = LogisticRegression(random_state=0).fit(X, y)\n",
      "110/81:\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "clf = LogisticRegression(random_state=0).fit(X, y)\n",
      "110/82:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y_train, y1_pred, alpha=0.7, c='coral', label='Traing set')\n",
      "plt.scatter(y_test, y2_pred, alpha=0.7, c='blue', label='Test set')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.legend()\n",
      "ax.figure.set_size_inches(5, 5)\n",
      "plt.show\n",
      "110/83:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y_train, y1_pred, alpha=0.7, c='coral', label='Traing set')\n",
      "plt.scatter(y_test, y2_pred, alpha=0.7, c='blue', label='Test set')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.legend()\n",
      "ax.figure.set_size_inches(5, 5)\n",
      "plt.show\n",
      "110/84:\n",
      "from sklearn.svm import SVR\n",
      "svr_rbf = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=.1)\n",
      "svr_lin = SVR(kernel='linear', C=100, gamma='auto')\n",
      "svr_poly = SVR(kernel='poly', C=100, gamma='auto', degree=3, epsilon=.1,\n",
      "               coef0=1)\n",
      "110/85:\n",
      "# Look at the results\n",
      "lw = 2\n",
      "\n",
      "svrs = [svr_rbf, svr_lin, svr_poly]\n",
      "kernel_label = ['RBF', 'Linear', 'Polynomial']\n",
      "model_color = ['m', 'c', 'g']\n",
      "110/86:\n",
      "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 10), sharey=True)\n",
      "for ix, svr in enumerate(svrs):\n",
      "    axes[ix].plot(X, svr.fit(X_train, y_train).predict(X_test), color=model_color[ix], lw=lw,\n",
      "                  label='{} model'.format(kernel_label[ix]))\n",
      "    axes[ix].scatter(X_train[svr.support_], y[svr.support_], facecolor=\"none\",\n",
      "                     edgecolor=model_color[ix], s=50,\n",
      "                     label='{} support vectors'.format(kernel_label[ix]))\n",
      "    axes[ix].scatter(X[np.setdiff1d(np.arange(len(X)), svr.support_)],\n",
      "                     y[np.setdiff1d(np.arange(len(X)), svr.support_)],\n",
      "                     facecolor=\"none\", edgecolor=\"k\", s=50,\n",
      "                     label='other training data')\n",
      "    axes[ix].legend(loc='upper center', bbox_to_anchor=(0.5, 1.1),\n",
      "                    ncol=1, fancybox=True, shadow=True)\n",
      "\n",
      "fig.text(0.5, 0.04, 'data', ha='center', va='center')\n",
      "fig.text(0.06, 0.5, 'target', ha='center', va='center', rotation='vertical')\n",
      "fig.suptitle(\"Support Vector Regression\", fontsize=14)\n",
      "plt.show()\n",
      "110/87:\n",
      "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 10), sharey=True)\n",
      "for ix, svr in enumerate(svrs):\n",
      "    axes[ix].plot(X_train, svr.fit(X_train, y_train).predict(X_test), color=model_color[ix], lw=lw,\n",
      "                  label='{} model'.format(kernel_label[ix]))\n",
      "    axes[ix].scatter(X_train[svr.support_], y)train[svr.support_], facecolor=\"none\",\n",
      "                     edgecolor=model_color[ix], s=50,\n",
      "                     label='{} support vectors'.format(kernel_label[ix]))\n",
      "    axes[ix].scatter(X_train[np.setdiff1d(np.arange(len(X)), svr.support_)],\n",
      "                     y_train[np.setdiff1d(np.arange(len(X)), svr.support_)],\n",
      "                     facecolor=\"none\", edgecolor=\"k\", s=50,\n",
      "                     label='other training data')\n",
      "    axes[ix].legend(loc='upper center', bbox_to_anchor=(0.5, 1.1),\n",
      "                    ncol=1, fancybox=True, shadow=True)\n",
      "\n",
      "fig.text(0.5, 0.04, 'data', ha='center', va='center')\n",
      "fig.text(0.06, 0.5, 'target', ha='center', va='center', rotation='vertical')\n",
      "fig.suptitle(\"Support Vector Regression\", fontsize=14)\n",
      "plt.show()\n",
      "110/89:\n",
      "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 10), sharey=True)\n",
      "for ix, svr in enumerate(svrs):\n",
      "    axes[ix].plot(X_train, svr.fit(X_train, y_train).predict(X_test), color=model_color[ix], lw=lw,\n",
      "                  label='{} model'.format(kernel_label[ix]))\n",
      "    axes[ix].scatter(X_train[svr.support_], y)train[svr.support_], facecolor=\"none\",\n",
      "                     edgecolor=model_color[ix], s=50,\n",
      "                     label='{} support vectors'.format(kernel_label[ix]))\n",
      "    axes[ix].scatter(X_train[np.setdiff1d(np.arange(len(X)), svr.support_)],\n",
      "                     y_train[np.setdiff1d(np.arange(len(X)), svr.support_)],\n",
      "                     facecolor=\"none\", edgecolor=\"k\", s=50,\n",
      "                     label='other training data')\n",
      "    axes[ix].legend(loc='upper center', bbox_to_anchor=(0.5, 1.1), ncol=1, fancybox=True, shadow=True)\n",
      "\n",
      "fig.text(0.5, 0.04, 'data', ha='center', va='center')\n",
      "fig.text(0.06, 0.5, 'target', ha='center', va='center', rotation='vertical')\n",
      "fig.suptitle(\"Support Vector Regression\", fontsize=14)\n",
      "plt.show()\n",
      "110/90:\n",
      "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 10), sharey=True)\n",
      "for ix, svr in enumerate(svrs):\n",
      "    axes[ix].plot(X_train, svr.fit(X_train, y_train).predict(X_test), color=model_color[ix], lw=lw,\n",
      "                  label='{} model'.format(kernel_label[ix]))\n",
      "    axes[ix].scatter(X_train[svr.support_], y_train[svr.support_], facecolor=\"none\",\n",
      "                     edgecolor=model_color[ix], s=50,\n",
      "                     label='{} support vectors'.format(kernel_label[ix]))\n",
      "    axes[ix].scatter(X_train[np.setdiff1d(np.arange(len(X)), svr.support_)],\n",
      "                     y_train[np.setdiff1d(np.arange(len(X)), svr.support_)],\n",
      "                     facecolor=\"none\", edgecolor=\"k\", s=50,\n",
      "                     label='other training data')\n",
      "    axes[ix].legend(loc='upper center', bbox_to_anchor=(0.5, 1.1), ncol=1, fancybox=True, shadow=True)\n",
      "\n",
      "fig.text(0.5, 0.04, 'data', ha='center', va='center')\n",
      "fig.text(0.06, 0.5, 'target', ha='center', va='center', rotation='vertical')\n",
      "fig.suptitle(\"Support Vector Regression\", fontsize=14)\n",
      "plt.show()\n",
      "110/91:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
      "kf =KFold(n_splits=5, shuffle=True, random_state=5)\n",
      "110/92:\n",
      "cnt = 1\n",
      "# split()  method generate indices to split data into training and test set.\n",
      "for train_index, test_index in kf.split(X, y):\n",
      "    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n",
      "    cnt += 1\n",
      "110/93:\n",
      "score = cross_val_score(model, X, y, cv= kf)\n",
      "print(f'Scores for each fold: {score}')\n",
      "rmse(score.mean())\n",
      "110/94:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
      "kf =KFold(n_splits=5, shuffle=True)\n",
      "110/95:\n",
      "cnt = 1\n",
      "# split()  method generate indices to split data into training and test set.\n",
      "for train_index, test_index in kf.split(X, y):\n",
      "    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n",
      "    cnt += 1\n",
      "110/96:\n",
      "def rmse(score):\n",
      "    rmse = np.sqrt(-score)\n",
      "    print(f'rmse= {\"{:.2f}\".format(rmse)}')\n",
      "110/97:\n",
      "score = cross_val_score(model, X, y, cv= kf)\n",
      "print(f'Scores for each fold: {score}')\n",
      "rmse(score.mean())\n",
      "110/98:\n",
      "from sklearn.svm import SVC\n",
      "model2 = SVC(kernel='polynomial')\n",
      "core = cross_val_score(model2, X, y, cv= kf)\n",
      "110/99:\n",
      "from sklearn.svm import SVC\n",
      "model2 = SVC(kernel='linear')\n",
      "core = cross_val_score(model2, X, y, cv= kf)\n",
      "110/100:\n",
      "from sklearn.svm import SVR\n",
      "model2 = SVR(kernel='linear')\n",
      "score = cross_val_score(model2, X, y, cv= kf)\n",
      "score\n",
      "128/1: pwd\n",
      "128/2:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split\n",
      "128/3: df = pd.read_csv('Fingerprint.csv')\n",
      "128/4: df\n",
      "128/5: df1 = df.drop('Unnamed: 0', axis =1)\n",
      "128/6: df1\n",
      "128/7: df2 = pd.read_csv('data2_lipinski_kpu.csv')\n",
      "128/8: df2\n",
      "128/9:\n",
      "df3 = df2.drop(['Unnamed: 0','compound','smiles','MW','TPSA','LogP'], axis=1)\n",
      "df3\n",
      "128/10: df4 =pd.concat([df1,df3], axis=1)\n",
      "128/11: df4\n",
      "128/12:\n",
      "X = df4.drop(['name', 'kp', 'kpu','Cells'], axis=1)\n",
      "y = df4['kp']\n",
      "128/13:\n",
      "from sklearn import preprocessing\n",
      "min_max_scaler = preprocessing.MinMaxScaler()\n",
      "np_scaled = min_max_scaler.fit_transform(X)\n",
      "Fp_normalized = pd.DataFrame(np_scaled, columns=X.columns)\n",
      "Fp_normalized.to_csv('Fp_normalized.csv')\n",
      "Fp_normalized\n",
      "128/14:\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.model_selection import cross_validate\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from collections import defaultdict\n",
      "128/15:\n",
      "X_train, X_test, y_train, y_test = train_test_split(X,\n",
      "                                                        y, test_size=0.2,\n",
      "                                                        random_state=5)\n",
      "128/16:\n",
      "#rf = RandomForestRegressor(n_estimators=400, \n",
      "                               max_features='sqrt', \n",
      "                               min_samples_leaf = 1,\n",
      "                               random_state = 13,\n",
      "                               n_jobs=-1)\n",
      "    \n",
      "    rf.fit(X_internal,Y_internal)\n",
      "    prediction = rf.predict(X_internal)\n",
      "128/17:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "model = RandomForestRegressor()\n",
      "model.fit(X_train,y_train)\n",
      "r2 = model.score(X_train,y_train)\n",
      "r2\n",
      "128/18:\n",
      "y1_pred = model.predict(X_train)\n",
      "y2_pred = model.predict(X_test)\n",
      "128/19:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y_train, y1_pred, alpha=0.7, c='coral', label='Traing set')\n",
      "plt.scatter(y_test, y2_pred, alpha=0.7, c='blue', label='Test set')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.legend()\n",
      "ax.figure.set_size_inches(5, 5)\n",
      "plt.show\n",
      "128/20:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y_train, y1_pred, alpha=0.7, c='coral', label='Traing set')\n",
      "plt.scatter(y_test, y2_pred, alpha=0.7, c='blue', label='Test set')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.legend()\n",
      "plt.figure.set_size_inches(5, 5)\n",
      "plt.show\n",
      "128/21:\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import ShuffleSplit\n",
      "scores= cross_val_score(model, X, y, cv=5)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "128/22:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
      "kf =KFold(n_splits=5, shuffle=True)\n",
      "128/23:\n",
      "cnt = 1\n",
      "# split()  method generate indices to split data into training and test set.\n",
      "for train_index, test_index in kf.split(X, y):\n",
      "    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n",
      "    cnt += 1\n",
      "128/24:\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import ShuffleSplit\n",
      "scores= cross_val_score(model, X, y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "128/25:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y_train, y1_pred, alpha=0.7, c='coral', label='Traing set')\n",
      "plt.scatter(y_test, y2_pred, alpha=0.7, c='blue', label='Test set')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.legend()\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)\n",
      "128/26: scores\n",
      "128/27:\n",
      "from sklearn.svm import SVR\n",
      "model2 = SVR(kernel='polynomial')\n",
      "score = cross_val_score(model2, X, y, cv= kf)\n",
      "score\n",
      "128/28:\n",
      "from sklearn.svm import SVR\n",
      "model2 = SVR(kernel='linear')\n",
      "score = cross_val_score(model2, X, y, cv= kf)\n",
      "score\n",
      "129/1:\n",
      "df4 =pd.concat([df1,df3], axis=1)\n",
      "df4.to_csv('Fn_normalized_kpu.csv')\n",
      "129/2: pwd\n",
      "129/3:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split\n",
      "129/4: df = pd.read_csv('Fingerprint.csv')\n",
      "129/5: df\n",
      "129/6: df1 = df.drop('Unnamed: 0', axis =1)\n",
      "129/7: df1\n",
      "129/8: df2 = pd.read_csv('data2_lipinski_kpu.csv')\n",
      "129/9: df2\n",
      "129/10: df2\n",
      "129/11:\n",
      "df3 = df2.drop(['Unnamed: 0','compound','smiles','MW','TPSA','LogP'], axis=1)\n",
      "df3\n",
      "129/12:\n",
      "df4 =pd.concat([df1,df3], axis=1)\n",
      "df4.to_csv('Fn_normalized_kpu.csv')\n",
      "129/13: df4\n",
      "129/14:\n",
      "X = df4.drop(['name', 'kp', 'kpu','Cells'], axis=1)\n",
      "y = df4['kp']\n",
      "129/15:\n",
      "from sklearn import preprocessing\n",
      "min_max_scaler = preprocessing.MinMaxScaler()\n",
      "np_scaled = min_max_scaler.fit_transform(X)\n",
      "Fp_normalized = pd.DataFrame(np_scaled, columns=X.columns)\n",
      "Fp_normalized.to_csv('Fp_normalized.csv')\n",
      "Fp_normalized\n",
      "129/16:\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.model_selection import cross_validate\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from collections import defaultdict\n",
      "129/17:\n",
      "X_train, X_test, y_train, y_test = train_test_split(X,\n",
      "                                                        y, test_size=0.2,\n",
      "                                                        random_state=5)\n",
      "129/18:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "model = RandomForestRegressor()\n",
      "model.fit(X_train,y_train)\n",
      "r2 = model.score(X_train,y_train)\n",
      "r2\n",
      "129/19:\n",
      "y1_pred = model.predict(X_train)\n",
      "y2_pred = model.predict(X_test)\n",
      "129/20:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y_train, y1_pred, alpha=0.7, c='coral', label='Traing set')\n",
      "plt.scatter(y_test, y2_pred, alpha=0.7, c='blue', label='Test set')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.legend()\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)\n",
      "129/21:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
      "kf =KFold(n_splits=5, shuffle=True)\n",
      "129/22:\n",
      "cnt = 1\n",
      "# split()  method generate indices to split data into training and test set.\n",
      "for train_index, test_index in kf.split(X, y):\n",
      "    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n",
      "    cnt += 1\n",
      "129/23:\n",
      "def rmse(score):\n",
      "    rmse = np.sqrt(-score)\n",
      "    print(f'rmse= {\"{:.2f}\".format(rmse)}')\n",
      "129/24:\n",
      "from sklearn.model_selection import cross_val_score\n",
      "scores= cross_val_score(model, X, y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "130/1: from sklearn.model_selection import SVR\n",
      "129/25:\n",
      "from sklearn import datasets\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "from sklearn import linear_model\n",
      "import matplotlib.pyplot as plt\n",
      "129/26:\n",
      "lr = linear_model.LinearRegression()\n",
      "\n",
      "# cross_val_predict returns an array of the same size as `y` where each entry\n",
      "# is a prediction obtained by cross validation:\n",
      "predicted = cross_val_predict(lr, X, y, cv=10)\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.scatter(y, predicted, edgecolors=(0, 0, 0))\n",
      "ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n",
      "ax.set_xlabel('Measured')\n",
      "ax.set_ylabel('Predicted')\n",
      "plt.show()\n",
      "129/27:\n",
      "lr = linear_model.LinearRegression()\n",
      "\n",
      "# cross_val_predict returns an array of the same size as `y` where each entry\n",
      "# is a prediction obtained by cross validation:\n",
      "predicted = cross_val_predict(lr, X, y, cv=10)\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.scatter(y, predicted, edgecolors=(0, 0, 0))\n",
      "ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n",
      "ax.set_xlabel('Measured')\n",
      "ax.set_ylabel('Predicted')\n",
      "ax.xscale('log')\n",
      "ax.yscale('log')\n",
      "plt.show()\n",
      "129/28:\n",
      "lr = linear_model.LinearRegression()\n",
      "\n",
      "# cross_val_predict returns an array of the same size as `y` where each entry\n",
      "# is a prediction obtained by cross validation:\n",
      "predicted = cross_val_predict(lr, X, y, cv=10)\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.scatter(y, predicted, edgecolors=(0, 0, 0))\n",
      "129/29: y\n",
      "129/30:\n",
      "lr = linear_model.LinearRegression()\n",
      "\n",
      "# cross_val_predict returns an array of the same size as `y` where each entry\n",
      "# is a prediction obtained by cross validation:\n",
      "predicted = cross_val_predict(lr, X, y, cv=10)\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.scatter(y, predicted, edgecolors=(0, 0,\n",
      "129/31:\n",
      "lr = linear_model.LinearRegression()\n",
      "\n",
      "# cross_val_predict returns an array of the same size as `y` where each entry\n",
      "# is a prediction obtained by cross validation:\n",
      "predicted = cross_val_predict(lr, X, y, cv=10)\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.scatter(y, predicte)\n",
      "129/32:\n",
      "lr = linear_model.LinearRegression()\n",
      "\n",
      "# cross_val_predict returns an array of the same size as `y` where each entry\n",
      "# is a prediction obtained by cross validation:\n",
      "predicted = cross_val_predict(lr, X, y, cv=10)\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.scatter(y, predicted)\n",
      "129/33:\n",
      "lr = linear_model.LinearRegression()\n",
      "\n",
      "# cross_val_predict returns an array of the same size as `y` where each entry\n",
      "# is a prediction obtained by cross validation:\n",
      "predicted = cross_val_predict(lr, X, y, cv=10)\n",
      "\n",
      "plt.subplots()\n",
      "plt.scatter(y, predicted)\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.show()\n",
      "129/34:\n",
      "lr = linear_model.LinearRegression()\n",
      "\n",
      "# cross_val_predict returns an array of the same size as `y` where each entry\n",
      "# is a prediction obtained by cross validation:\n",
      "predicted = cross_val_predict(lr, X, y, cv=10)\n",
      "\n",
      "plt.scatter(y, predicted)\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.show()\n",
      "129/35:\n",
      "lr = linear_model.LinearRegression()\n",
      "\n",
      "# cross_val_predict returns an array of the same size as `y` where each entry\n",
      "# is a prediction obtained by cross validation:\n",
      "predicted = cross_val_predict(lr, X, y, cv=10)\n",
      "\n",
      "plt.scatter(y, predicted)\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.show()\n",
      "129/36:\n",
      "lr = linear_model.LinearRegression()\n",
      "\n",
      "# cross_val_predict returns an array of the same size as `y` where each entry\n",
      "# is a prediction obtained by cross validation:\n",
      "predicted = cross_val_predict(lr, X, y, cv=10)\n",
      "\n",
      "plt.scatter(y, predicted)\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.show()\n",
      "129/37:\n",
      "lr = linear_model.LinearRegression()\n",
      "\n",
      "# cross_val_predict returns an array of the same size as `y` where each entry\n",
      "# is a prediction obtained by cross validation:\n",
      "predicted = cross_val_predict(lr, X, y, cv=10)\n",
      "\n",
      "plt.scatter(y, predicted)\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "[lt]\n",
      "129/38:\n",
      "lr = linear_model.LinearRegression()\n",
      "\n",
      "# cross_val_predict returns an array of the same size as `y` where each entry\n",
      "# is a prediction obtained by cross validation:\n",
      "predicted = cross_val_predict(lr, X, y, cv=10)\n",
      "\n",
      "plt.scatter(y, predicted)\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt\n",
      "129/39:\n",
      "lr = linear_model.LinearRegression()\n",
      "\n",
      "# cross_val_predict returns an array of the same size as `y` where each entry\n",
      "# is a prediction obtained by cross validation:\n",
      "predicted = cross_val_predict(lr, X, y, cv=10)\n",
      "\n",
      "plt.scatter(y, predicted)\n",
      "\n",
      "plt\n",
      "129/40:\n",
      "lr = linear_model.LinearRegression()\n",
      "\n",
      "# cross_val_predict returns an array of the same size as `y` where each entry\n",
      "# is a prediction obtained by cross validation:\n",
      "predicted = cross_val_predict(lr, X, y, cv=10)\n",
      "\n",
      "plt.scatter(y, predicted)\n",
      "\n",
      "plt.shape()\n",
      "129/41:\n",
      "lr = linear_model.LinearRegression()\n",
      "\n",
      "# cross_val_predict returns an array of the same size as `y` where each entry\n",
      "# is a prediction obtained by cross validation:\n",
      "predicted = cross_val_predict(lr, X, y, cv=10)\n",
      "129/42:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(X, y, alpha=0.7, c='coral', label='Traing set')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.legend()\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)\n",
      "129/43:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(predicted, y, alpha=0.7, c='coral', label='Traing set')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.legend()\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)\n",
      "129/44:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y_train, y1_pred, alpha=0.7, c='coral', label='Traing set')\n",
      "plt.scatter(y_test, y2_pred, alpha=0.7, c='blue', label='Test set')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.legend()\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)\n",
      "129/45:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(X, X, alpha=0.7, c='coral', label='Traing set')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.legend()\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)\n",
      "129/46: X\n",
      "129/47:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, y, alpha=0.7, c='coral', label='Traing set')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.legend()\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)\n",
      "129/48:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, predicted, alpha=0.7, c='coral', label='Traing set')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.legend()\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)\n",
      "129/49:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y_train, y1_pred, alpha=0.7, c='coral', label='Traing set')\n",
      "plt.scatter(y_test, y2_pred, alpha=0.7, c='blue', label='Test set')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.legend()\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)\n",
      "129/50:\n",
      "lr = linear_model.LinearRegression()\n",
      "\n",
      "# cross_val_predict returns an array of the same size as `y` where each entry\n",
      "# is a prediction obtained by cross validation:\n",
      "predicted = cross_val_predict(lr, X, y, cv=10)\n",
      "predicted\n",
      "129/51:\n",
      "lr = linear_model.LinearRegression()\n",
      "\n",
      "# cross_val_predict returns an array of the same size as `y` where each entry\n",
      "# is a prediction obtained by cross validation:\n",
      "predicted = cross_val_predict(lr, X, y, cv=10)\n",
      "predicted.shape\n",
      "129/52:\n",
      "lr = linear_model.LinearRegression()\n",
      "\n",
      "# cross_val_predict returns an array of the same size as `y` where each entry\n",
      "# is a prediction obtained by cross validation:\n",
      "predicted = cross_val_predict(lr, X, y, cv=10)\n",
      "predicted.shape\n",
      "y.shape\n",
      "129/53: y.shape\n",
      "129/54:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, predicted, alpha=0.7, c='coral', label='Traing set')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.legend()\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)\n",
      "129/55:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, predicted, alpha=0.7, c='coral', label='Traing set')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.legend()\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)\n",
      "129/56:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, predicted, alpha=0.7, c='coral', label='Traing set')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.ylim(0, 1000000)\n",
      "plt.legend()\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)\n",
      "129/57:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, predicted, alpha=0.7, c='coral', label='Traing set')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.ylim(0, 10000)\n",
      "plt.legend()\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)\n",
      "129/58:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, predicted, alpha=0.7, c='coral', label='Traing set')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.ylim(0, 1)\n",
      "plt.legend()\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)\n",
      "129/59:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, predicted, alpha=0.7, c='coral', label='Traing set')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.ylim(0, 10)\n",
      "plt.legend()\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)\n",
      "129/60:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, predicted, alpha=0.7, c='coral', label='Traing set')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.ylim(0, 100)\n",
      "plt.legend()\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)\n",
      "129/61:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, predicted, alpha=0.7, c='coral', label='Traing set')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.ylim(-1, 100)\n",
      "plt.legend()\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)\n",
      "129/62:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, predicted, alpha=0.7, c='coral', label='Traing set')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.ylim(-10, 100)\n",
      "plt.legend()\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)\n",
      "129/63:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, predicted, alpha=0.7, c='coral', label='Traing set')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.ylim(-10, 10)\n",
      "plt.legend()\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)\n",
      "129/64:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, predicted, alpha=0.7, c='coral', label='Traing set')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.ylim(-10, 1)\n",
      "plt.legend()\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)\n",
      "129/65:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, predicted, alpha=0.7, c='coral', label='Traing set')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.ylim(-10, 1)\n",
      "plt.legend()\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)\n",
      "129/66:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, predicted, alpha=0.7, c='coral', label='Traing set')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n",
      "plt.legend()\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)\n",
      "129/67:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, predicted, alpha=0.7, c='coral', label='Traing set')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n",
      "plt.legend()\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)\n",
      "129/68:\n",
      "lr = linear_model.LinearRegression()\n",
      "\n",
      "# cross_val_predict returns an array of the same size as `y` where each entry\n",
      "# is a prediction obtained by cross validation:\n",
      "predicted = cross_val_predict(lr, X, y, cv=1kF)\n",
      "predicted.shape\n",
      "129/69:\n",
      "lr = linear_model.LinearRegression()\n",
      "\n",
      "# cross_val_predict returns an array of the same size as `y` where each entry\n",
      "# is a prediction obtained by cross validation:\n",
      "predicted = cross_val_predict(lr, X, y, cv=kf)\n",
      "predicted.shape\n",
      "129/70:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, predicted, alpha=0.7, c='coral', label='Traing set')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n",
      "plt.legend()\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)\n",
      "129/71:\n",
      "lr = linear_model.LinearRegression()\n",
      "\n",
      "# cross_val_predict returns an array of the same size as `y` where each entry\n",
      "# is a prediction obtained by cross validation:\n",
      "predicted = cross_val_predict(lr, X, y, cv=k10\n",
      "predicted.shape\n",
      "129/72:\n",
      "lr = linear_model.LinearRegression()\n",
      "\n",
      "# cross_val_predict returns an array of the same size as `y` where each entry\n",
      "# is a prediction obtained by cross validation:\n",
      "predicted = cross_val_predict(lr, X, y, cv=10)\n",
      "predicted.shape\n",
      "129/73:\n",
      "lr = linear_model.LinearRegression()\n",
      "\n",
      "# cross_val_predict returns an array of the same size as `y` where each entry\n",
      "# is a prediction obtained by cross validation:\n",
      "cv = cross_val_score(ls, X, y, cv=kf)\n",
      "cv\n",
      "predicted = cross_val_predict(lr, X, y, cv=10)\n",
      "predicted.shape\n",
      "129/74:\n",
      "lr = linear_model.LinearRegression()\n",
      "\n",
      "# cross_val_predict returns an array of the same size as `y` where each entry\n",
      "# is a prediction obtained by cross validation:\n",
      "cv = cross_val_score(lr, X, y, cv=kf)\n",
      "cv\n",
      "129/75:\n",
      "lr = linear_model.LinearRegression()\n",
      "\n",
      "# cross_val_predict returns an array of the same size as `y` where each entry\n",
      "# is a prediction obtained by cross validation:\n",
      "predicted = cross_val_predict(lr, X, y, cv=10)\n",
      "predicted.shape\n",
      "129/76: predicted\n",
      "129/77: predicted = abs(predicted)\n",
      "129/78:\n",
      "lr = linear_model.LinearRegression()\n",
      "\n",
      "# cross_val_predict returns an array of the same size as `y` where each entry\n",
      "# is a prediction obtained by cross validation:\n",
      "reg = LinearRegression()\n",
      "scores= cross_val_score(reg, X, y, cv=kf)\n",
      "scores\n",
      "129/79:\n",
      "lr = linear_model.LinearRegression()\n",
      "\n",
      "# cross_val_predict returns an array of the same size as `y` where each entry\n",
      "# is a prediction obtained by cross validation:\n",
      "scores= cross_val_score(reg, X, y, cv=kf)\n",
      "scores\n",
      "129/80:\n",
      "lr = linear_model.LinearRegression()\n",
      "\n",
      "# cross_val_predict returns an array of the same size as `y` where each entry\n",
      "# is a prediction obtained by cross validation:\n",
      "scores= cross_val_score(lr, X, y, cv=kf)\n",
      "scores\n",
      "129/81:\n",
      "lr = linear_model.LinearRegression()\n",
      "\n",
      "# cross_val_predict returns an array of the same size as `y` where each entry\n",
      "# is a prediction obtained by cross validation:\n",
      "scores = cross_val_score(lr, X, y, cv=kf)\n",
      "scores\n",
      "129/82:\n",
      "lr = linear_model.LinearRegression()\n",
      "\n",
      "# cross_val_predict returns an array of the same size as `y` where each entry\n",
      "# is a prediction obtained by cross validation:\n",
      "lr = linear_model.LinearRegression()\n",
      "lr.fit(X_train,y_train)\n",
      "r3 = lr.score(X_train,y_train)\n",
      "r3\n",
      "129/83: predicted = lr.predict(X_test)\n",
      "129/84:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, predicted, alpha=0.7, c='coral', label='Traing set')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n",
      "plt.legend()\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)\n",
      "129/85:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y_test, predicted, alpha=0.7, c='coral', label='Traing set')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n",
      "plt.legend()\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)\n",
      "129/86: predicted = lr.predict(X)\n",
      "129/87:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, predicted, alpha=0.7, c='coral', label='Traing set')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n",
      "plt.legend()\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)\n",
      "129/88:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "model = RandomForestRegressor(n_estimators=200)\n",
      "model.fit(X_train,y_train)\n",
      "r2 = model.score(X_train,y_train)\n",
      "r2\n",
      "129/89:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y_train, y1_pred, alpha=0.7, c='coral', label='Traing set')\n",
      "plt.scatter(y_test, y2_pred, alpha=0.7, c='blue', label='Test set')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.legend()\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)\n",
      "129/90: predicted = lr.predict(X_train)\n",
      "129/91:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, predicted, alpha=0.7, c='coral', label='Traing set')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n",
      "plt.legend()\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)\n",
      "129/92:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y_train, predicted, alpha=0.7, c='coral', label='Traing set')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n",
      "plt.legend()\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)\n",
      "129/93:\n",
      "lr = linear_model.LinearRegression()\n",
      "\n",
      "# cross_val_predict returns an array of the same size as `y` where each entry\n",
      "# is a prediction obtained by cross validation:\n",
      "predicted = cross_val_predict(lr, X_train, y_train, cv=10)\n",
      "129/94:\n",
      "lr = linear_model.LinearRegression()\n",
      "\n",
      "# cross_val_predict returns an array of the same size as `y` where each entry\n",
      "# is a prediction obtained by cross validation:\n",
      "predicted = cross_val_predict(lr, X_train, y_train, cv=10)\n",
      "predicted\n",
      "129/95:\n",
      "lr = linear_model.LinearRegression()\n",
      "\n",
      "# cross_val_predict returns an array of the same size as `y` where each entry\n",
      "# is a prediction obtained by cross validation:\n",
      "predicted = cross_validation.cross_val_predict(lr, X_train, y_train, cv=10)\n",
      "predicted\n",
      "129/96:\n",
      "lr = linear_model.LinearRegression()\n",
      "\n",
      "# cross_val_predict returns an array of the same size as `y` where each entry\n",
      "# is a prediction obtained by cross validation:\n",
      "predicted = cross_val_predict(lr, X_train, y_train, cv=10)\n",
      "predicted\n",
      "129/97:\n",
      "lr = linear_model.LinearRegression()\n",
      "\n",
      "# cross_val_predict returns an array of the same size as `y` where each entry\n",
      "# is a prediction obtained by cross validation:\n",
      "predicted = cross_val_predict(lr, X_train, y_train, cv=10)\n",
      "predicted\n",
      "fig, ax = plt.subplots()\n",
      "ax.scatter(y_train, predicted, edgecolors=(0, 0, 0))\n",
      "129/98:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
      "kf =KFold(n_splits=5, shuffle=True)\n",
      "129/99:\n",
      "cnt = 1\n",
      "# split()  method generate indices to split data into training and test set.\n",
      "for train_index, test_index in kf.split(X, y):\n",
      "    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n",
      "    cnt += 1\n",
      "130/2:\n",
      "import numopy as np\n",
      "import pandas as pd\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn import SVM\n",
      "130/3:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn import SVM\n",
      "130/4:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn import svm\n",
      "130/5: pwd\n",
      "130/6: Fn = pd.read_csv('Fn_normalized_kpu.csv')\n",
      "130/7: Fn\n",
      "130/8: Fn = Fn.drop(['Unnamed: 0'])\n",
      "130/9: Fn = Fn.drop('Unnamed: 0', axis=1)\n",
      "130/10:\n",
      "Fn = Fn.drop('Unnamed: 0', axis=1)\n",
      "Fn\n",
      "130/11:\n",
      "Fn = Fn.drop('Unnamed: 0', axis=1)\n",
      "Fn\n",
      "130/12: Fn1 = Fn.drop('Unnamed: 0', axis=1)\n",
      "130/13: Fn1 = Fn.drop('Unnamed:0', axis=1)\n",
      "130/14: Fn = pd.read_csv('Fn_normalized_kpu.csv')\n",
      "130/15: Fn1 = Fn.drop('Unnamed: 0', axis=1)\n",
      "130/16: Fn1\n",
      "130/17: Fn2 = pd.read_csv('Fp_nomalized.csv')\n",
      "130/18: Fn2 = pd.read_csv('Fp_normalized.csv')\n",
      "130/19: Fn2\n",
      "129/100:\n",
      "df4 =pd.concat([df1,df3], axis=1)\n",
      "df4.to_csv('Fn_kpu.csv')\n",
      "129/101: df4\n",
      "130/20: Fn = pd.read_csv('Fn_kpu.csv')\n",
      "130/21: Fn1 = Fn.drop('Unnamed: 0', axis=1)\n",
      "130/22: Fn1\n",
      "130/23: Fn1 = Fn(['kp', 'kpu', 'Cells'])\n",
      "130/24: Fn1 = Fn['kp', 'kpu', 'Cells']\n",
      "130/25: Fn1 = pd.Fn['kp', 'kpu', 'Cells']\n",
      "130/26: Fn1 = Fn['kp', 'kpu', 'Cells']\n",
      "130/27: Fn1 = Fn(['kp', 'kpu', 'Cells'])\n",
      "130/28: Fn1 = Fn['kp', 'kpu', 'Cells'])\n",
      "130/29: Fn1 = Fn['kp', 'kpu', 'Cells']\n",
      "130/30: Fn1 = Fn[Fn.'kp', 'kpu', 'Cells']\n",
      "130/31: Fn1 = Fn[Fn[]'kp', 'kpu', 'Cells']\n",
      "130/32: Fn1\n",
      "130/33:\n",
      "Fn = pd.read_csv('Fn_kpu.csv')\n",
      "Fn\n",
      "130/34: Fn1 = Fn['kp', 'kpu', 'Cells']\n",
      "130/35: Fn1 = Fn[\"kp\", \"kpu\", \"Cells\"]\n",
      "130/36: Fn1\n",
      "130/37: df1 = Fn['kp']\n",
      "130/38: df1 = Fn['kp', 'kpu']\n",
      "130/39: df1 = Fn(['kp', 'kpu'], axis=1)\n",
      "130/40: df1 = Fn['kp', 'kpu']\n",
      "130/41: Fn1 = Fn[['kp', 'kpu', 'Cells']]\n",
      "130/42: Fn3 = pd.concatZ['Fn2', 'Fn1'], axis=1)\n",
      "130/43: Fn3 = pd.concat(['Fn2', 'Fn1'], axis=1)\n",
      "130/44: Fn3 = pd.concat(['Fn2', 'Fn1'], axis=1)\n",
      "129/102: data = pd.concat(['X', 'y'], axis =1)\n",
      "129/103: data = pd.concat([X, y], axis =1)\n",
      "129/104:\n",
      "data = pd.concat([X, y], axis =1)\n",
      "data.to_csv('datasetforbiuildmodel.csv')\n",
      "130/45:\n",
      "data = pd.read_csv('datasetforbiuildmodel.csv')\n",
      "data\n",
      "129/105:\n",
      "X = df4.drop(['name', 'kp', 'kpu','Cells'], axis=1)\n",
      "y = df4['kp']\n",
      "y_all = df4[['kp', 'kpu', 'Cells']]\n",
      "129/106:\n",
      "data = pd.concat([X, y_all], axis =1)\n",
      "data.to_csv('datasetforbiuildmodel.csv')\n",
      "130/46:\n",
      "data = pd.read_csv('datasetforbiuildmodel.csv')\n",
      "data\n",
      "130/47: Fn2 = pd.read_csv('Fingerprint')\n",
      "130/48: Fn2 = pd.read_csv('Fingerprint.csv')\n",
      "130/49: Fn2\n",
      "129/107:\n",
      "X = df4.drop(['name', 'kp', 'kpu','Cells'], axis=1)\n",
      "y = df4['kp']\n",
      "y_all = df4[['kp', 'kpu', 'Cells']]\n",
      "name = df4['name']\n",
      "129/108:\n",
      "data = pd.concat([name, X, y_all], axis =1, set_index())\n",
      "data.to_csv('datasetforbiuildmodel.csv')\n",
      "129/109:\n",
      "data = pd.concat([name, X, y_all], axis =1, set_index(name))\n",
      "data.to_csv('datasetforbiuildmodel.csv')\n",
      "129/110:\n",
      "data = pd.concat([name, X, y_all], axis =1)\n",
      "data.to_csv('datasetforbiuildmodel.csv')\n",
      "129/111:\n",
      "data = pd.concat([name, X, y_all], axis =1)\n",
      "data.set_index(name)\n",
      "data.to_csv('datasetforbiuildmodel.csv')\n",
      "130/50:\n",
      "data = pd.read_csv('datasetforbiuildmodel.csv')\n",
      "data\n",
      "129/112:\n",
      "data = pd.concat([name, X, y_all], axis =1)\n",
      "data2 = data.set_index(name)\n",
      "data2.to_csv('datasetforbiuildmodel.csv')\n",
      "130/51:\n",
      "data = pd.read_csv('datasetforbiuildmodel.csv')\n",
      "data\n",
      "129/113:\n",
      "data = pd.concat([name, X, y_all], axis =1)\n",
      "data2 = data.set_index(name)\n",
      "data2.drop('name.1')\n",
      "data2.to_csv('datasetforbiuildmodel.csv')\n",
      "129/114:\n",
      "data = pd.concat([name, X, y_all], axis =1)\n",
      "data2 = data.set_index(name)\n",
      "data2.drop(['name.1'], axis=1)\n",
      "data2.to_csv('datasetforbiuildmodel.csv')\n",
      "129/115:\n",
      "data = pd.concat([name, X, y_all], axis =1)\n",
      "data2 = data.set_index(name)\n",
      "data3 = data2.drop(['name.1 '], axis=1)\n",
      "data2.to_csv('datasetforbiuildmodel.csv')\n",
      "129/116:\n",
      "data = pd.concat([name, X, y_all], axis =1)\n",
      "data2 = data.set_index(name)\n",
      "data3 = data2.drop(['name.1 '], axis=1)\n",
      "data3.to_csv('datasetforbiuildmodel.csv')\n",
      "129/117:\n",
      "data = pd.concat([name, X, y_all], axis =1)\n",
      "data2 = data.set_index(name)\n",
      "data3 = data2.drop(['name.1'], axis=1)\n",
      "data3.to_csv('datasetforbiuildmodel.csv')\n",
      "129/118:\n",
      "data = pd.concat([name, X, y_all], axis =1)\n",
      "data2 = data.set_index(name)\n",
      "data3 = data2.drop([name.1], axis=1)\n",
      "data3.to_csv('datasetforbiuildmodel.csv')\n",
      "129/119:\n",
      "data = pd.concat([name, X, y_all], axis =1)\n",
      "data2 = data.set_index(name)\n",
      "data3 = data2.drop(['name.1'], axis=1)\n",
      "data3.to_csv('datasetforbiuildmodel.csv')\n",
      "129/120:\n",
      "data = pd.concat([name, X, y_all], axis =1)\n",
      "data2 = data.set_index(name, drop=True)\n",
      "data3.to_csv('datasetforbiuildmodel.csv')\n",
      "129/121:\n",
      "data = pd.concat([name, X, y_all], axis =1)\n",
      "data2 = data.set_index(name, drop=True)\n",
      "data2.to_csv('datasetforbiuildmodel.csv')\n",
      "130/52:\n",
      "data = pd.read_csv('datasetforbiuildmodel.csv')\n",
      "data\n",
      "130/53: data2 = data.drop(['name.1'])\n",
      "130/54: data2 = data.drop('name.1')\n",
      "130/55: data2 = data.drop('name.1', axis=1)\n",
      "130/56:\n",
      "data2 = data.drop('name.1', axis=1)\n",
      "data2\n",
      "130/57: data3 = data2.set_index('name')\n",
      "130/58: data3\n",
      "130/59: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)\n",
      "130/60: X = data3.drop(['kp', 'kpu', 'Cells'], axis=1)\n",
      "130/61: y = data3['kpu']\n",
      "130/62: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=5)\n",
      "130/63: X_train.shape, X_test.shape\n",
      "130/64: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=5)\n",
      "130/65: X_train.shape, X_test.shape\n",
      "130/66: y_train.shape, y_test.shape\n",
      "130/67:\n",
      "clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
      "clf.score(X_test, y_test)\n",
      "129/122: model.score\n",
      "129/123:\n",
      "r2_test = model.score(X_test,y_test)\n",
      "r2_test\n",
      "129/124:\n",
      "from sklearn.metrics import mean_squared_error\n",
      "rms = sqrt(mean_squared_error(y_train, y1_predict))\n",
      "129/125:\n",
      "from sklearn.metrics import mean_squared_error\n",
      "rms1 = mean_squared_error(y_train, y1_predict)\n",
      "129/126:\n",
      "from sklearn.metrics import mean_squared_error\n",
      "rms1 = mean_squared_error(y_train, y1_pred)\n",
      "129/127:\n",
      "from sklearn.metrics import mean_squared_error\n",
      "rms1 = mean_squared_error(y_train, y1_pred)\n",
      "rms2 = mean_squared_error(y_test, y2_pred)\n",
      "129/128:\n",
      "from sklearn.metrics import mean_squared_error\n",
      "rms1 = mean_squared_error(y_train, y1_pred)\n",
      "rms2 = mean_squared_error(y_test, y2_pred)\n",
      "print(rms1, rms2)\n",
      "129/129:\n",
      "from sklearn.metrics import mean_squared_error\n",
      "rms1 = mean_squared_error(y_train, y1_pred, squared=False)\n",
      "rms2 = mean_squared_error(y_test, y2_pred, squared=False)\n",
      "print(rms1, rms2)\n",
      "130/68: from sklearn.linear_model import LinearRegression\n",
      "129/130: X = Fp_normalized.drop(['name', 'kp', 'kpu','Cells'], axis=1)\n",
      "129/131: X = Fp_normalized\n",
      "129/132:\n",
      "X = Fp_normalized\n",
      "y = df4['kp']\n",
      "129/133: X.shape\n",
      "129/134: X.shape y.shape\n",
      "129/135: X.shape, y.shape\n",
      "129/136: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)\n",
      "129/137:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "model = RandomForestRegressor(n_estimators=200)\n",
      "model.fit(X_train,y_train)\n",
      "r2 = model.score(X_train,y_train)\n",
      "r2\n",
      "129/138:\n",
      "y1_pred = model.predict(X_train)\n",
      "y2_pred = model.predict(X_test)\n",
      "129/139:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y_train, y1_pred, alpha=0.7, c='coral', label='Traing set')\n",
      "plt.scatter(y_test, y2_pred, alpha=0.7, c='blue', label='Test set')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.legend()\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)\n",
      "129/140:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
      "kf =KFold(n_splits=5, shuffle=False)\n",
      "129/141:\n",
      "cnt = 1\n",
      "# split()  method generate indices to split data into training and test set.\n",
      "for train_index, test_index in kf.split(X, y):\n",
      "    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n",
      "    cnt += 1\n",
      "129/142:\n",
      "cnt = 1\n",
      "# split()  method generate indices to split data into training and test set.\n",
      "for train_index, test_index in kf.split(X, y):\n",
      "    print(f'Fold:{cnt}, Train set: {(train_index)}, Test set:{(test_index)}')\n",
      "    cnt += 1\n",
      "129/143:\n",
      "cnt = 1\n",
      "# split()  method generate indices to split data into training and test set.\n",
      "for train_index, test_index in kf.split(X, y):\n",
      "    print(f'Fold:{cnt}, Train set: {(train_index)}, Test set:{(test_index)}')\n",
      "    cnt += 1\n",
      "for train_index, test_index in kf.split(X, y):\n",
      "    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n",
      "    cnt += 1\n",
      "129/144:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
      "kf =KFold(n_splits=10, shuffle=False)\n",
      "129/145:\n",
      "cnt = 1\n",
      "# split()  method generate indices to split data into training and test set.\n",
      "for train_index, test_index in kf.split(X, y):\n",
      "    print(f'Fold:{cnt}, Train set: {(train_index)}, Test set:{(test_index)}')\n",
      "    cnt += 1\n",
      "for train_index, test_index in kf.split(X, y):\n",
      "    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n",
      "    cnt += 1\n",
      "129/146:\n",
      "from sklearn.model_selection import cross_val_score\n",
      "scores= cross_val_score(model, X, y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "130/69:\n",
      "data = pd.read_csv('data2_lipinski_kpu.csv')\n",
      "data\n",
      "130/70:\n",
      "data2 = data.drop('Unnamed: 0', axis=1)\n",
      "data2\n",
      "129/147: scores\n",
      "130/71: Fp = pd.read_csv('Fp_normalized.csv')\n",
      "130/72:\n",
      "Fp = pd.read_csv('Fp_normalized.csv')\n",
      "Fp\n",
      "130/73: X = Fp.drop['Unnamed: 0']\n",
      "130/74: X = Fp.drop()'Unnamed: 0', axis=1)\n",
      "130/75: X = Fp.drop('Unnamed: 0', axis=1)\n",
      "130/76:\n",
      "X = Fp.drop('Unnamed: 0', axis=1)\n",
      "X.shape\n",
      "130/77: y = data3['kpu']\n",
      "130/78:\n",
      "y = data3['kpu']\n",
      "y.shape\n",
      "130/79:\n",
      "y = data3['kpu']\n",
      "y.shape\n",
      "130/80:\n",
      "from sklearn.linear_model import LinearRegression\n",
      "lr = linear_model.LinearRegression()\n",
      "130/81:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.model_selection import cross_val_score\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "130/82:\n",
      "\n",
      "# 10-fold cross-validation with K=5 for KNN (the n_neighbors parameter)\n",
      "knn = KNeighborsClassifier(n_neighbors=5)\n",
      "scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n",
      "print(scores)\n",
      "130/83:\n",
      "from sklearn.linear_model import LinearRegression\n",
      "lr = LinearRegression()\n",
      "scores = cross_val_score(lr, X, y, cv=10, scoring='accuracy')\n",
      "print(scores)\n",
      "130/84:\n",
      "from sklearn.linear_model import LinearRegression\n",
      "lr = LinearRegression()\n",
      "scores = cross_val_score(lr, X, y, cv=10)\n",
      "print(scores)\n",
      "130/85:\n",
      "lr = LinearRegression()\n",
      "scores = cross_val_score(lr, X, y, cv=10)\n",
      "print(scores)\n",
      "130/86: from sklearn.model_selection import cross_val_score\n",
      "130/87:\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
      "kf =KFold(n_splits=10, shuffle=False)\n",
      "130/88:\n",
      "cnt = 1\n",
      "# split()  method generate indices to split data into training and test set.\n",
      "for train_index, test_index in kf.split(X, y):\n",
      "    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n",
      "    cnt += 1\n",
      "130/89:\n",
      "from sklearn.linear_model import LinearRegression\n",
      "lr = LinearRegression()\n",
      "scores = cross_val_score(lr, X, y, cv=kF)\n",
      "print(scores)\n",
      "130/90:\n",
      "from sklearn.linear_model import LinearRegression\n",
      "lr = LinearRegression()\n",
      "scores = cross_val_score(lr, X, y, cv=kf)\n",
      "print(scores)\n",
      "130/91:\n",
      "lr = LinearRegression()\n",
      "scores = cross_val_score(lr, X, y, cv=kf)\n",
      "print(scores)\n",
      "130/92:\n",
      "from sklearn.linear_model import LinearRegression\n",
      "lr = LinearRegression()\n",
      "scores = cross_val_score(lr, X, y, cv=kf)\n",
      "print(scores)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "130/93:\n",
      "from sklearn.decomposition import PCA\n",
      "pca = PCA(n_components=10)\n",
      "130/94: scores = cross_val_score(pca, X, y, cv=kf)\n",
      "130/95: scores\n",
      "130/96:\n",
      "from sklearn.decomposition import PCA\n",
      "pca = PCA(n_components=2)\n",
      "130/97: scores = cross_val_score(pca, X, y, cv=kf)\n",
      "130/98: scores\n",
      "130/99:\n",
      "scores = cross_val_score(pca, X, y, cv=kf)\n",
      "print(scores)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "130/100:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "pls = PLSRegression(n_components=1)\n",
      "130/101:\n",
      "scores = cross_val_score(pls, X, y, cv=kf)\n",
      "print(scores)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "130/102:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "pls = PLSRegression(n_components=2)\n",
      "130/103:\n",
      "scores = cross_val_score(pls, X, y, cv=kf)\n",
      "print(scores)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "130/104:\n",
      "print(f\"PCR r-squared {pcr.score(X_test, y_test):.3f}\")\n",
      "print(f\"PLS r-squared {pls.score(X_test, y_test):.3f}\")\n",
      "130/105:\n",
      "print(f\"PCA r-squared {pca.score(X_test, y_test):.3f}\")\n",
      "print(f\"PLS r-squared {pls.score(X_test, y_test):.3f}\")\n",
      "130/106:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "pls2 = PLSRegression(n_components=2)\n",
      "pls2.fit(X, y)\n",
      "PLSRegression()\n",
      "130/107:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "pls2 = PLSRegression(n_components=2)\n",
      "pls2.fit(X, y)\n",
      "r2 = pls2.score(X, y)\n",
      "130/108:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "pls2 = PLSRegression(n_components=2)\n",
      "pls2.fit(X, y)\n",
      "r2 = pls2.score(X, y)\n",
      "r2\n",
      "130/109:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "pls2 = PLSRegression(n_components=2)\n",
      "pls2.fit(X_train, y_train)\n",
      "r2 = pls2.score(X_train, y_train)\n",
      "r2\n",
      "130/110:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "pls2 = PLSRegression(n_components=2)\n",
      "scores = cross_val_score(pls2, X, y, cv=kf)\n",
      "130/111:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "pls2 = PLSRegression(n_components=2)\n",
      "scores = cross_val_score(pls2, X, y, cv=kf)\n",
      "scores\n",
      "129/148:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y_train, y1_pred, alpha=0.7, c='coral', label='Traing set')\n",
      "plt.scatter(y_test, y2_pred, alpha=0.7, c='blue', label='Test set')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.legend()\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)\n",
      "129/149:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y_train, y1_pred, alpha=0.7, c='coral', label='Traing set')\n",
      "plt.scatter(y_test, y2_pred, alpha=0.7, c='blue', label='Test set')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.legend()\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)\n",
      "130/112:\n",
      "from scipy import stats\n",
      "import numpy as np\n",
      "z = np.abs(stats.zscore(Y))\n",
      "print(z)\n",
      "130/113:\n",
      "from scipy import stats\n",
      "import numpy as np\n",
      "z = np.abs(stats.zscore(y))\n",
      "print(z)\n",
      "130/114:\n",
      "threshold = 3\n",
      "print(np.where(z > 3))\n",
      "130/115:\n",
      "data2 = data.drop('Unnamed: 0', axis=1)\n",
      "selection = data2['kp','kpu']\n",
      "data2\n",
      "130/116:\n",
      "data2 = data.drop('Unnamed: 0', axis=1)\n",
      "selection = data['kp','kpu']\n",
      "data2\n",
      "130/117:\n",
      "data2 = data.drop('Unnamed: 0', axis=1)\n",
      "selection = data[['kp','kpu']]\n",
      "data2\n",
      "130/118: Xy = pd.concat([X,y], axis =1)\n",
      "130/119: Xy = pd.concat([X, y], axis =1)\n",
      "130/120: Xy = pd.concat([X, selection], axis =1)\n",
      "130/121:\n",
      "from scipy import stats\n",
      "import numpy as np\n",
      "z = np.abs(stats.zscore(Xy))\n",
      "print(z)\n",
      "130/122:\n",
      "threshold = 3\n",
      "print(np.where(z > 3))\n",
      "130/123:\n",
      "from scipy import stats\n",
      "import numpy as np\n",
      "z = np.abs(stats.zscore(y))\n",
      "print(z)\n",
      "130/124:\n",
      "threshold = 3\n",
      "print(np.where(z > 3))\n",
      "130/125: Xy1 = Xy.drop(['72','84','85','86','87'])\n",
      "130/126: Xy1 = Xy.drop(['72','84','85','86','87'],axis=0)\n",
      "130/127: Xy1 = Xy.drop(['72','84','85','86','87'], axis=0)\n",
      "130/128: Xy1 = Xy.drop(['72','84','85','86','87'], index=labels)\n",
      "130/129: Xy1 = Xy.drop([72,84,85,86,87])\n",
      "130/130: Xy1\n",
      "130/131: X = Xy1.drop(['kp', 'kpu'], axis=1)\n",
      "130/132: X\n",
      "130/133: y = Xy1[['kp','kpu']]\n",
      "130/134:\n",
      "from sklearn.linear_model import LinearRegression\n",
      "lr = LinearRegression()\n",
      "scores = cross_val_score(lr, X, y, cv=kf)\n",
      "print(scores)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "130/135:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "pls2 = PLSRegression(n_components=2)\n",
      "scores = cross_val_score(pls2, X, y, cv=kf)\n",
      "scores\n",
      "130/136: print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "130/137:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "model = RandomForestRegressor(n_estimators=200)\n",
      "scores = cross_val_score(model, X, y, cv=kf)\n",
      "scores\n",
      "130/138: print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "130/139: y = Xy1[['kp']]\n",
      "130/140: y = Xy1[['kp']]\n",
      "130/141:\n",
      "from sklearn.linear_model import LinearRegression\n",
      "lr = LinearRegression()\n",
      "scores = cross_val_score(lr, X, y, cv=kf)\n",
      "print(scores)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "130/142:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "pls2 = PLSRegression(n_components=2)\n",
      "scores = cross_val_score(pls2, X, y, cv=kf)\n",
      "scores\n",
      "130/143: print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "130/144:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "model = RandomForestRegressor(n_estimators=200)\n",
      "scores = cross_val_score(model, X, y, cv=kf)\n",
      "scores\n",
      "130/145: print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "130/146: y = Xy1[['kpu']]\n",
      "130/147:\n",
      "from sklearn.linear_model import LinearRegression\n",
      "lr = LinearRegression()\n",
      "scores = cross_val_score(lr, X, y, cv=kf)\n",
      "print(scores)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "130/148:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "pls2 = PLSRegression(n_components=2)\n",
      "scores = cross_val_score(pls2, X, y, cv=kf)\n",
      "scores\n",
      "130/149: print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "130/150:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "model = RandomForestRegressor(n_estimators=200)\n",
      "scores = cross_val_score(model, X, y, cv=kf)\n",
      "scores\n",
      "130/151: print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "130/152:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "pls2 = PLSRegression(n_components=7)\n",
      "scores = cross_val_score(pls2, X, y, cv=kf)\n",
      "scores\n",
      "130/153:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "pls2 = PLSRegression(n_components=10)\n",
      "scores = cross_val_score(pls2, X, y, cv=kf)\n",
      "scores\n",
      "130/154:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "pls2 = PLSRegression(n_components=10)\n",
      "scores = cross_val_score(pls2, X, y, cv=kf)\n",
      "scores\n",
      "130/155:\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "y_pred = cross_val_predict(pls2, X, y, cv=10)\n",
      "130/156:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.legend()\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('PLS_model_CV_cut.pdf', dpi=300)\n",
      "130/157:\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "y_pred = cross_val_predict(pls2, X, y, cv=10)\n",
      "Y_pred\n",
      "130/158:\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "y_pred = cross_val_predict(pls2, X, y, cv=10)\n",
      "y_pred\n",
      "130/159:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.legend()\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('PLS_model_CV_cut.pdf', dpi=300)\n",
      "130/160:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.legend()\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('PLS_model_CV_cut.pdf', dpi=300)\n",
      "130/161:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('PLS_model_CV_cut.pdf', dpi=300)\n",
      "130/162:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.ylimit(0.000001. 10)\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('PLS_model_CV_cut.pdf', dpi=300)\n",
      "130/163:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.ylimit(0.000001, 10)\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('PLS_model_CV_cut.pdf', dpi=300)\n",
      "130/164:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.ylim(0.000001, 10)\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('PLS_model_CV_cut.pdf', dpi=300)\n",
      "130/165:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.ylim(0.0001, 10)\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('PLS_model_CV_cut.pdf', dpi=300)\n",
      "130/166:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.ylim(0.01, 10)\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('PLS_model_CV_cut.pdf', dpi=300)\n",
      "130/167:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.ylim(0.001, 10)\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('PLS_model_CV_cut.pdf', dpi=300)\n",
      "130/168:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.ylim(0.01, 10)\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('PLS_model_CV_cut.pdf', dpi=300)\n",
      "130/169:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.ylim(0.02, 10)\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('PLS_model_CV_cut.pdf', dpi=300)\n",
      "130/170:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.ylim(0.0015, 10)\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('PLS_model_CV_cut.pdf', dpi=300)\n",
      "130/171:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.ylim(0.0011, 10)\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('PLS_model_CV_cut.pdf', dpi=300)\n",
      "130/172:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.ylim(0.0011, 10)\n",
      "plt.xlim(0.0011, 10)\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('PLS_model_CV_cut.pdf', dpi=300)\n",
      "130/173:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.ylim(0.0011, 100)\n",
      "plt.xlim(0.0011, 100)\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('PLS_model_CV_cut.pdf', dpi=300)\n",
      "130/174:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.ylim(0.0011, 50)\n",
      "plt.xlim(0.0011, 50)\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('PLS_model_CV_cut.pdf', dpi=300)\n",
      "134/1:\n",
      "# Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>\n",
      "#          Maria Telenczuk    <https://github.com/maikia>\n",
      "# License: BSD 3 clause\n",
      "\n",
      "print(__doc__)\n",
      "\n",
      "from sklearn import set_config\n",
      "set_config(display='diagram')\n",
      "134/2:\n",
      "import numpy as np\n",
      "\n",
      "from sklearn.datasets import fetch_openml\n",
      "from sklearn.utils import shuffle\n",
      "\n",
      "\n",
      "def load_ames_housing():\n",
      "    df = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "    X = df.data\n",
      "    y = df.target\n",
      "\n",
      "    features = ['YrSold', 'HeatingQC', 'Street', 'YearRemodAdd', 'Heating',\n",
      "                'MasVnrType', 'BsmtUnfSF', 'Foundation', 'MasVnrArea',\n",
      "                'MSSubClass', 'ExterQual', 'Condition2', 'GarageCars',\n",
      "                'GarageType', 'OverallQual', 'TotalBsmtSF', 'BsmtFinSF1',\n",
      "                'HouseStyle', 'MiscFeature', 'MoSold']\n",
      "\n",
      "    X = X[features]\n",
      "    X, y = shuffle(X, y, random_state=0)\n",
      "\n",
      "    X = X[:600]\n",
      "    y = y[:600]\n",
      "    return X, np.log(y)\n",
      "\n",
      "\n",
      "X, y = load_ames_housing()\n",
      "134/3:\n",
      "from sklearn.compose import make_column_selector\n",
      "\n",
      "cat_selector = make_column_selector(dtype_include=object)\n",
      "num_selector = make_column_selector(dtype_include=np.number)\n",
      "cat_selector(X)\n",
      "134/4: num_selector(X)\n",
      "134/5:\n",
      "from sklearn.compose import make_column_transformer\n",
      "from sklearn.impute import SimpleImputer\n",
      "from sklearn.pipeline import make_pipeline\n",
      "from sklearn.preprocessing import OrdinalEncoder\n",
      "\n",
      "cat_tree_processor = OrdinalEncoder(\n",
      "    handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
      "num_tree_processor = SimpleImputer(strategy=\"mean\", add_indicator=True)\n",
      "\n",
      "tree_preprocessor = make_column_transformer(\n",
      "    (num_tree_processor, num_selector), (cat_tree_processor, cat_selector))\n",
      "tree_preprocessor\n",
      "134/6:\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "cat_linear_processor = OneHotEncoder(handle_unknown=\"ignore\")\n",
      "num_linear_processor = make_pipeline(\n",
      "    StandardScaler(), SimpleImputer(strategy=\"mean\", add_indicator=True))\n",
      "\n",
      "linear_preprocessor = make_column_transformer(\n",
      "    (num_linear_processor, num_selector), (cat_linear_processor, cat_selector))\n",
      "linear_preprocessor\n",
      "130/175:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')\n",
      "plot_regression_results(\n",
      "        ax, y, y_pred,\n",
      "        name,\n",
      "        (r'$R^2={:.2f} \\pm {:.2f}$' + '\\n' + r'$MAE={:.2f} \\pm {:.2f}$')\n",
      "        .format(np.mean(score['test_r2']),\n",
      "                np.std(score['test_r2']),\n",
      "                -np.mean(score['test_neg_mean_absolute_error']),\n",
      "                np.std(score['test_neg_mean_absolute_error'])),\n",
      "        elapsed_time)\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.ylim(0.0011, 50)\n",
      "plt.xlim(0.0011, 50)\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('PLS_model_CV_cut.pdf', dpi=300)\n",
      "130/176:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.ylim(0.0011, 50)\n",
      "plt.xlim(0.0011, 50)\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('PLS_model_CV_cut.pdf', dpi=300)\n",
      "130/177:\n",
      "import time\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.model_selection import cross_validate, cross_val_predict\n",
      "\n",
      "\n",
      "def plot_regression_results(ax, y_true, y_pred, title, scores, elapsed_time):\n",
      "    \"\"\"Scatter plot of the predicted vs true targets.\"\"\"\n",
      "    ax.plot([y_true.min(), y_true.max()],\n",
      "            [y_true.min(), y_true.max()],\n",
      "            '--r', linewidth=2)\n",
      "    ax.scatter(y_true, y_pred, alpha=0.2)\n",
      "\n",
      "    ax.spines['top'].set_visible(False)\n",
      "    ax.spines['right'].set_visible(False)\n",
      "    ax.get_xaxis().tick_bottom()\n",
      "    ax.get_yaxis().tick_left()\n",
      "    ax.spines['left'].set_position(('outward', 10))\n",
      "    ax.spines['bottom'].set_position(('outward', 10))\n",
      "    ax.set_xlim([y_true.min(), y_true.max()])\n",
      "    ax.set_ylim([y_true.min(), y_true.max()])\n",
      "    ax.set_xlabel('Measured')\n",
      "    ax.set_ylabel('Predicted')\n",
      "    extra = plt.Rectangle((0, 0), 0, 0, fc=\"w\", fill=False,\n",
      "                          edgecolor='none', linewidth=0)\n",
      "    ax.legend([extra], [scores], loc='upper left')\n",
      "    title = title + '\\n Evaluation in {:.2f} seconds'.format(elapsed_time)\n",
      "    ax.set_title(title)\n",
      "\n",
      "\n",
      "fig, axs = plt.subplots(2, 2, figsize=(9, 7))\n",
      "axs = np.ravel(axs)\n",
      "\n",
      "for ax, (name, est) in zip(axs, estimators + [('Stacking Regressor',\n",
      "                                               stacking_regressor)]):\n",
      "    start_time = time.time()\n",
      "    score = cross_validate(est, X, y,\n",
      "                           scoring=['r2', 'neg_mean_absolute_error'],\n",
      "                           n_jobs=-1, verbose=0)\n",
      "    elapsed_time = time.time() - start_time\n",
      "\n",
      "    y_pred = cross_val_predict(est, X, y, n_jobs=-1, verbose=0)\n",
      "\n",
      "    plot_regression_results(\n",
      "        ax, y, y_pred,\n",
      "        name,\n",
      "        (r'$R^2={:.2f} \\pm {:.2f}$' + '\\n' + r'$MAE={:.2f} \\pm {:.2f}$')\n",
      "        .format(np.mean(score['test_r2']),\n",
      "                np.std(score['test_r2']),\n",
      "                -np.mean(score['test_neg_mean_absolute_error']),\n",
      "                np.std(score['test_neg_mean_absolute_error'])),\n",
      "        elapsed_time)\n",
      "\n",
      "plt.suptitle('Single predictors versus stacked predictors')\n",
      "plt.tight_layout()\n",
      "plt.subplots_adjust(top=0.9)\n",
      "plt.show()\n",
      "130/178:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')\n",
      "\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.ylim(0.0011, 50)\n",
      "plt.xlim(0.0011, 50)\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('PLS_model_CV_cut.pdf', dpi=300)\n",
      "130/179:\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "y_pred = cross_val_predict(pls2, X, y, cv=10)\n",
      "130/180: np.mean(score['test_r2']\n",
      "130/181: np.mean(score['test_r2'])\n",
      "130/182:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.ylim(0.0011, 50)\n",
      "plt.xlim(0.0011, 50)\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.annotate('R2: ' + str(r2_score(y, y_pred)))\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('PLS_model_CV_cut.pdf', dpi=300)\n",
      "130/183:\n",
      "from sklearn.metrics import r2_score\n",
      "r2_score(y, y_pred)\n",
      "130/184:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.ylim(0.0011, 50)\n",
      "plt.xlim(0.0011, 50)\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.annotate('R2: ' + str(r2_score(y, y_pred)))\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('PLS_model_CV_cut.pdf', dpi=300)\n",
      "130/185:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.ylim(0.0011, 50)\n",
      "plt.xlim(0.0011, 50)\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.annotate('R2: ' + str(r2_score(y, y_pred)), y, y_pred)\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('PLS_model_CV_cut.pdf', dpi=300)\n",
      "130/186:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.ylim(0.0011, 50)\n",
      "plt.xlim(0.0011, 50)\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.annotate('R2: ' + str(r2_score))\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('PLS_model_CV_cut.pdf', dpi=300)\n",
      "130/187:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.ylim(0.0011, 50)\n",
      "plt.xlim(0.0011, 50)\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.annotate('R2: ', str(r2_score))\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('PLS_model_CV_cut.pdf', dpi=300)\n",
      "130/188:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.ylim(0.0011, 50)\n",
      "plt.xlim(0.0011, 50)\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.annotate(r2_score)\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('PLS_model_CV_cut.pdf', dpi=300)\n",
      "130/189:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.ylim(0.0011, 50)\n",
      "plt.xlim(0.0011, 50)\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.annotate(r2_scoree(y, y_pred))\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('PLS_model_CV_cut.pdf', dpi=300)\n",
      "130/190:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.ylim(0.0011, 50)\n",
      "plt.xlim(0.0011, 50)\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt.annotate(r2_score(y, y_pred))\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('PLS_model_CV_cut.pdf', dpi=300)\n",
      "130/191:\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "y_pred = cross_val_predict(model, X, y, cv=kf)\n",
      "130/192:\n",
      "#no CV\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')\n",
      "plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')\n",
      "plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')\n",
      "plt.title('')\n",
      "plt.xscale('log')\n",
      "plt.yscale('log')\n",
      "plt.ylim(0.0011, 50)\n",
      "plt.xlim(0.0011, 50)\n",
      "plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('PLS_model_CV_cut.pdf', dpi=300)\n",
      "130/193: r2_score(y, y_pred)\n",
      "129/150: r2_score(y_train, y1_pred)\n",
      "129/151: r2_score(y_test, y2_pred)\n",
      "129/152:\n",
      "from sklearn.metrics import mean_squared_error\n",
      "rms1 = mean_squared_error(y_train, y1_pred, squared=False)\n",
      "rms2 = mean_squared_error(y_test, y2_pred, squared=False)\n",
      "print(rms1, rms2)\n",
      "130/194:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "model = RandomForestRegressor(n_estimators=200)\n",
      "scores = cross_val_score(model, X, y, cv=kf)\n",
      "scores\n",
      "124/1:\n",
      "df = pd.read_csv('Test_compounds.csv')\n",
      "df\n",
      "124/2:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split\n",
      "124/3: pwd\n",
      "124/4:\n",
      "df = pd.read_csv('Test_compounds.csv')\n",
      "df\n",
      "124/5: df.to_csv('test_molecule.smi', sep='\\t', index=False, header=False)\n",
      "124/6: df.to_csv('test_molecules.smi', sep='\\t', index=False, header=False)\n",
      "124/7:\n",
      "#calculate molecular descriptor\n",
      "!python -m mordred -t smi \\test_molecules.smi -o test_molecules.csv\n",
      "124/8: df_test = pd.read_csv('test_molecules.csv')\n",
      "124/9: df_test\n",
      "135/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "135/2:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "135/3: df.read_csv('test_molecules.csv')\n",
      "135/4: df = pd.read_csv('test_molecules.csv')\n",
      "135/5: df.shape\n",
      "135/6: df2 = pd.read_csv('molecules.csv')\n",
      "135/7: df2.sha\n",
      "135/8: df2.shape\n",
      "135/9: df3 = pd.concat(df, df2)\n",
      "135/10: df3 = pd.concat[df, df2]\n",
      "135/11: df3 = pd.concat[[df, df2]]\n",
      "135/12: df3 = pd.concat([df, df2])\n",
      "135/13: df3\n",
      "135/14: df4 =df3.reset_index()\n",
      "135/15: df4\n",
      "135/16: df4 =df3.reset_index(drop=Ture)\n",
      "135/17: df4 =df3.reset_index(drop=True)\n",
      "135/18: df4\n",
      "135/19:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "136/1:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "136/2: pwd\n",
      "136/3: df = pd.read_csv('rawdataqsar2.csv')\n",
      "136/4: df = pd.read_csv('rawdataqsar_sm.csv')\n",
      "136/5: df\n",
      "136/6: df2 =df[df.logpk.notna()]\n",
      "126/1: df2 =df[df.kpu.notna()]\n",
      "136/7: df\n",
      "136/8: df2 =df[df.logpk.notna()]\n",
      "136/9: df2 =df[df.logkp.notna()]\n",
      "136/10: df2\n",
      "136/11: df2 =df[df.kp.notna()]\n",
      "136/12: df2\n",
      "136/13: df3 = df.drop[logkp]\n",
      "136/14: df3 = df.drop['logkp']\n",
      "136/15: df3 = df.drop(['logkp', 'kpu'])\n",
      "136/16:\n",
      "df3 = df.drop(['logkp', 'kpu'], axis=1\n",
      "             )\n",
      "136/17: df3\n",
      "136/18:\n",
      "df3 = df2.drop(['logkp', 'kpu'], axis=1\n",
      "             )\n",
      "136/19: df3\n",
      "136/20:\n",
      "def LogKp(input):\n",
      "    LogKp = []\n",
      "\n",
      "    for i in input['kp']:\n",
      "        LogKp.append(np.log10(i))\n",
      "\n",
      "    input['LogKp'] = LogKp\n",
      "        \n",
      "    return x\n",
      "136/21: df4 = LogKp(df3)\n",
      "136/22:\n",
      "def LogKp(input):\n",
      "    LogKp = []\n",
      "\n",
      "    for i in input['kp']:\n",
      "        LogKp.append(np.log10(i))\n",
      "\n",
      "    input['LogKp'] = LogKp\n",
      "        \n",
      "    return x\n",
      "136/23: df4 = LogKp(df3)\n",
      "136/24:\n",
      "def LogKp(input):\n",
      "    LogKp = []\n",
      "\n",
      "    for i in input['kp']:\n",
      "        LogKp.append(np.log10(i))\n",
      "\n",
      "    input['LogKp'] = LogKp\n",
      "        \n",
      "    return x\n",
      "136/25: df4 = LogKp(df3)\n",
      "136/26:\n",
      "def LogKp(input):\n",
      "    LogKp = []\n",
      "\n",
      "    for i in input['kp']:\n",
      "        LogKp.append(np.log10(i))\n",
      "\n",
      "    input['LogKp'] = LogKp\n",
      "        \n",
      "    return LogKp\n",
      "136/27:\n",
      "def LogKp(input):\n",
      "    LogKp = []\n",
      "\n",
      "    for i in input['kp']:\n",
      "        LogKp.append(np.log10(i))\n",
      "\n",
      "    input['LogKp'] = LogKp\n",
      "        \n",
      "    return LogKp\n",
      "136/28: df4 = LogKp(df3)\n",
      "136/29: df4\n",
      "136/30:\n",
      "def LogKp(input):\n",
      "    LogKp = []\n",
      "\n",
      "    for i in input['kp']:\n",
      "        LogKp.append(np.log10(i))\n",
      "\n",
      "    input['LogKp'] = LogKp\n",
      "    x = input.drop('standard_value_norm', 1)   \n",
      "    return LogKp\n",
      "136/31:\n",
      "def LogKp(input):\n",
      "    LogKp = []\n",
      "\n",
      "    for i in input['kp']:\n",
      "        LogKp.append(np.log10(i))\n",
      "\n",
      "    input['LogKp'] = LogKp\n",
      "    x = input.drop('standard_value_norm', 1)   \n",
      "    return x\n",
      "136/32: df4 = LogKp(df3)\n",
      "136/33:\n",
      "def LogKp(input):\n",
      "    LogKp = []\n",
      "\n",
      "    for i in input['kp']:\n",
      "        LogKp.append(np.log10(i))\n",
      "\n",
      "    input['LogKp'] = LogKp\n",
      "    x = input.drop('LogKp', 1)   \n",
      "    return x\n",
      "136/34: df4 = LogKp(df3)\n",
      "136/35: df4\n",
      "136/36:\n",
      "def LogKp(input):\n",
      "    LogKp = []\n",
      "\n",
      "    for i in input['kp']:\n",
      "        LogKp.append(np.log10(i))\n",
      "\n",
      "    input['LogKp'] = LogKp\n",
      "    x = input.append('LogKp', 1)   \n",
      "    return x\n",
      "136/37: df4 = LogKp(df3)\n",
      "136/38: df4\n",
      "136/39:\n",
      "def LogKp(input):\n",
      "    LogKp = []\n",
      "\n",
      "    for i in input['kp']:\n",
      "        LogKp.append(np.log10(i))\n",
      "\n",
      "    input['LogKp'] = LogKp\n",
      "    x = input.drop('LogKp', 1)   \n",
      "    return x\n",
      "136/40: df4 = LogKp(df3)\n",
      "136/41:\n",
      "def LogKp(input):\n",
      "    LogKp = []\n",
      "\n",
      "    for i in input['kp']:\n",
      "        LogKp.append(np.log10(i))\n",
      "\n",
      "    input['LogKp'] = LogKp\n",
      "    x = input.drop('kp', 1)   \n",
      "    return x\n",
      "136/42: df4 = LogKp(df3)\n",
      "136/43: df4\n",
      "136/44: selection = df4['smile', 'compound']\n",
      "136/45: selection = df4[['smile', 'compound']]\n",
      "136/46: selection = df4(['smile', 'compound'])\n",
      "136/47: selection = df4[['smile', 'compound']]\n",
      "136/48: selection = ['smile', 'compound']\n",
      "136/49: df5 = df4[selection]\n",
      "136/50: selection = ['smiles', 'compound']\n",
      "136/51: df5 = df4[selection]\n",
      "136/52: df5\n",
      "136/53:\n",
      "df5 = df4[selection]\n",
      "df_selection.to_csv('allmolecules.smi', sep='\\t', index=False, header=False)\n",
      "136/54:\n",
      "df5 = df4[selection]\n",
      "df5.to_csv('allmolecules.smi', sep='\\t', index=False, header=False)\n",
      "136/55:\n",
      "#calculate molecular descriptor\n",
      "!python -m mordred -t smi \\allmolecules.smi -o allmolecules.csv\n",
      "136/56:\n",
      "#calculate molecular fingerprint\n",
      "from rdkit import Chem\n",
      "from mordred import Calculator, descriptors\n",
      "calc = Calculator(descriptors, ignore_3D=True)\n",
      "136/57: len(calc)\n",
      "136/58:\n",
      "\n",
      "mols = [Chem.MolFromSmiles(smi) \n",
      "    for smi in input['smiles']\n",
      "df = calc.pandas(mols)\n",
      "136/59:\n",
      "def lipinski(smiles, verbose=False):\n",
      "\n",
      "    moldata= []\n",
      "    for elem in smiles:\n",
      "        mol=Chem.MolFromSmiles(elem) \n",
      "        moldata.append(mol)\n",
      "       \n",
      "    baseData= np.arange(1,1)\n",
      "    i=0  \n",
      "    for mol in moldata:        \n",
      "       \n",
      "        desc_MolWt = Descriptors.MolWt(mol)\n",
      "        desc_TPSA = Descriptors.TPSA(mol)\n",
      "        desc_MolLogP = Descriptors.MolLogP(mol)\n",
      "        desc_NumHDonors = Lipinski.NumHDonors(mol)\n",
      "        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)\n",
      "        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)\n",
      "           \n",
      "        row = np.array([desc_MolWt,\n",
      "                        desc_TPSA,\n",
      "                        desc_MolLogP,\n",
      "                        desc_NumHDonors,\n",
      "                        desc_NumHAcceptors,\n",
      "                        desc_NumRotatableBonds])   \n",
      "    \n",
      "        if(i==0):\n",
      "            baseData=row\n",
      "        else:\n",
      "            baseData=np.vstack([baseData, row])\n",
      "        i=i+1      \n",
      "    \n",
      "    columnNames=[\"MW\",\"TPSA\", \"LogP\",\"NumHDonors\",\"NumHAcceptors\", \"NumRotatableBonds\"]   \n",
      "    descriptors = pd.DataFrame(data=baseData,columns=columnNames)\n",
      "    \n",
      "    return descriptors\n",
      "136/60:\n",
      "def lipinski(smiles, verbose=False):\n",
      "\n",
      "    moldata= []\n",
      "    for elem in smiles:\n",
      "        mol=Chem.MolFromSmiles(elem) \n",
      "        moldata.append(mol)\n",
      "       \n",
      "    baseData= np.arange(1,1)\n",
      "    i=0  \n",
      "    for mol in moldata:        \n",
      "       \n",
      "        desc_MolWt = Descriptors.MolWt(mol)\n",
      "        desc_MolLogP = Descriptors.MolLogP(mol)\n",
      "        desc_NumHDonors = Lipinski.NumHDonors(mol)\n",
      "        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)\n",
      "        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)\n",
      "           \n",
      "        row = np.array([desc_MolWt,\n",
      "                        desc_TPSA,\n",
      "                        desc_MolLogP,\n",
      "                        desc_NumHDonors,\n",
      "                        desc_NumHAcceptors,\n",
      "                        desc_NumRotatableBonds])   \n",
      "    \n",
      "        if(i==0):\n",
      "            baseData=row\n",
      "        else:\n",
      "            baseData=np.vstack([baseData, row])\n",
      "        i=i+1      \n",
      "    \n",
      "    columnNames=[\"MW\", \"LogP\",\"NumHDonors\",\"NumHAcceptors\", \"NumRotatableBonds\"]   \n",
      "    descriptors = pd.DataFrame(data=baseData,columns=columnNames)\n",
      "    \n",
      "    return descriptors\n",
      "136/61:\n",
      "def lipinski(smiles, verbose=False):\n",
      "\n",
      "    moldata= []\n",
      "    for elem in smiles:\n",
      "        mol=Chem.MolFromSmiles(elem) \n",
      "        moldata.append(mol)\n",
      "       \n",
      "    baseData= np.arange(1,1)\n",
      "    i=0  \n",
      "    for mol in moldata:        \n",
      "       \n",
      "        desc_MolWt = Descriptors.MolWt(mol)\n",
      "        desc_TPSA = Descriptors.TPSA(mol)\n",
      "        desc_MolLogP = Descriptors.MolLogP(mol)\n",
      "        desc_NumHDonors = Lipinski.NumHDonors(mol)\n",
      "        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)\n",
      "        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)\n",
      "           \n",
      "        row = np.array([desc_MolWt,\n",
      "                        desc_TPSA,\n",
      "                        desc_MolLogP,\n",
      "                        desc_NumHDonors,\n",
      "                        desc_NumHAcceptors,\n",
      "                        desc_NumRotatableBonds])   \n",
      "    \n",
      "        if(i==0):\n",
      "            baseData=row\n",
      "        else:\n",
      "            baseData=np.vstack([baseData, row])\n",
      "        i=i+1      \n",
      "    \n",
      "    columnNames=[\"MW\",\"TPSA\", \"LogP\",\"NumHDonors\",\"NumHAcceptors\", \"NumRotatableBonds\"]   \n",
      "    descriptors = pd.DataFrame(data=baseData,columns=columnNames)\n",
      "    \n",
      "    return descriptors\n",
      "136/62: df5 = pd.read_csv('allmolecules.smi')\n",
      "136/63:\n",
      "df5 = pd.read_csv('allmolecules.smi')\n",
      "df5\n",
      "136/64:\n",
      "df5 = pd.read_csv('allmolecules.csv')\n",
      "df5\n",
      "136/65:\n",
      "from rdkit import Chem, DataStructs\n",
      "from rdkit.Chem import PandasTools, AllChem\n",
      "PandasTools.AddMoleculeColumnToFrame(data,'SMILES','Molecule')\n",
      "data[[\"SMILES\",\"Molecule\"]].head(1)\n",
      "136/66:\n",
      "from rdkit import Chem, DataStructs\n",
      "from rdkit.Chem import PandasTools, AllChem\n",
      "PandasTools.AddMoleculeColumnToFrame(df4,'smiles','Molecule')\n",
      "data[[\"smiles\",\"Molecule\"]].head(1)\n",
      "136/67:\n",
      "from rdkit import Chem, DataStructs\n",
      "from rdkit.Chem import PandasTools, AllChem\n",
      "PandasTools.AddMoleculeColumnToFrame(df4,'smiles','Molecule')\n",
      "df4[[\"smiles\",\"Molecule\"]].head(1)\n",
      "136/68:\n",
      "from rdkit import Chem, DataStructs\n",
      "from rdkit.Chem import PandasTools, AllChem\n",
      "PandasTools.AddMoleculeColumnToFrame(df4,'smiles','Molecule')\n",
      "df4[[\"smiles\",\"Molecule\"]]\n",
      "136/69: data.Molecule.isna().sum()\n",
      "136/70: df4.Molecule.isna().sum()\n",
      "136/71:\n",
      "def mol2fp(mol):\n",
      "    fp = AllChem.GetHashedMorganFingerprint(mol, 2, nBits=4096)\n",
      "    ar = np.zeros((1,), dtype=np.int8)\n",
      "    DataStructs.ConvertToNumpyArray(fp, ar)\n",
      "    return ar\n",
      "     \n",
      "fp =mol2fp(Chem.MolFromSmiles(df4.loc[1,\"SMILES\"]))\n",
      "plt.matshow(fp.reshape((64,-1)) &amp;gt; 0)\n",
      "136/72:\n",
      "def mol2fp(mol):\n",
      "    fp = AllChem.GetHashedMorganFingerprint(mol, 2, nBits=4096)\n",
      "    ar = np.zeros((1,), dtype=np.int8)\n",
      "    DataStructs.ConvertToNumpyArray(fp, ar)\n",
      "    return ar\n",
      "     \n",
      "fp =mol2fp(Chem.MolFromSmiles(df4.loc[1,\"smiles\"]))\n",
      "plt.matshow(fp.reshape((64,-1)) &amp;gt; 0)\n",
      "136/73:\n",
      "def mol2fp(mol):\n",
      "    fp = AllChem.GetHashedMorganFingerprint(mol, 2, nBits=4096)\n",
      "    ar = np.zeros((1,), dtype=np.int8)\n",
      "    DataStructs.ConvertToNumpyArray(fp, ar)\n",
      "    return ar\n",
      "     \n",
      "fp =mol2fp(Chem.MolFromSmiles(df4.loc[1,\"smiles\"]))\n",
      "plt.matshow(fp.reshape((64,-1)))\n",
      "136/74: data[\"FPs\"] = df4.Molecule.apply(mol2fp)\n",
      "136/75: df4[\"FPs\"] = df4.Molecule.apply(mol2fp)\n",
      "136/76: df6 = pd.concat([df5], [df4['LogKp']])\n",
      "136/77: df6 = pd.concat([df5, df4.LogKp], axis=1)\n",
      "136/78: df6\n",
      "136/79: df7 = df6.name.drop_duplicates()\n",
      "136/80: df7\n",
      "136/81:\n",
      "selection = [LogKp, Cells]\n",
      "df6 = pd.concat([df5, df4[selection]], axis=1)\n",
      "136/82: selection = [LogKp, Cells]\n",
      "136/83:\n",
      "selection = ['LogKp', 'Cells']\n",
      "df6 = pd.concat([df5, df4[selection]], axis=1)\n",
      "136/84: df6\n",
      "136/85: df7 = df6.drop_duplicates(subset=['name'])\n",
      "136/86: df7\n",
      "136/87: df7 = df6.drop_duplicates()\n",
      "136/88: df7\n",
      "136/89: df6\n",
      "136/90: df = pd.read_csv('rawdataqsar_sm.csv')\n",
      "136/91: df\n",
      "136/92: df2 =df[df.kp.notna()]\n",
      "136/93: df2\n",
      "136/94: df3 = df2.drop(['logkp', 'kpu'], axis=1)\n",
      "136/95: df3\n",
      "136/96:\n",
      "def LogKp(input):\n",
      "    LogKp = []\n",
      "\n",
      "    for i in input['kp']:\n",
      "        LogKp.append(np.log10(i))\n",
      "\n",
      "    input['LogKp'] = LogKp\n",
      "    x = input.drop('kp', 1)   \n",
      "    return x\n",
      "136/97: df4 = LogKp(df3)\n",
      "136/98: df4\n",
      "136/99: df5 = df4.drop_duplicates(subset=['smiles'])\n",
      "136/100:\n",
      "df5 = df4.drop_duplicates(subset=['smiles'])\n",
      "df5\n",
      "136/101: df6 = df5.reset_index(drop=True)\n",
      "136/102:\n",
      "df6 = df5.reset_index(drop=True)\n",
      "df6\n",
      "136/103:\n",
      "# Calculate Molecular Descriptor\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Descriptors, Lipinski\n",
      "\n",
      "def lipinski(smiles, verbose=False):\n",
      "\n",
      "    moldata= []\n",
      "    for elem in smiles:\n",
      "        mol=Chem.MolFromSmiles(elem) \n",
      "        moldata.append(mol)\n",
      "       \n",
      "    baseData= np.arange(1,1)\n",
      "    i=0  \n",
      "    for mol in moldata:        \n",
      "       \n",
      "        desc_MolWt = Descriptors.MolWt(mol)\n",
      "        desc_TPSA = Descriptors.TPSA(mol)\n",
      "        desc_MolLogP = Descriptors.MolLogP(mol)\n",
      "        desc_NumHDonors = Lipinski.NumHDonors(mol)\n",
      "        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)\n",
      "        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)\n",
      "           \n",
      "        row = np.array([desc_MolWt,\n",
      "                        desc_TPSA,\n",
      "                        desc_MolLogP,\n",
      "                        desc_NumHDonors,\n",
      "                        desc_NumHAcceptors,\n",
      "                        desc_NumRotatableBonds])   \n",
      "    \n",
      "        if(i==0):\n",
      "            baseData=row\n",
      "        else:\n",
      "            baseData=np.vstack([baseData, row])\n",
      "        i=i+1      \n",
      "    \n",
      "    columnNames=[\"MW\",\"TPSA\", \"LogP\",\"NumHDonors\",\"NumHAcceptors\", \"NumRotatableBonds\"]   \n",
      "    descriptors = pd.DataFrame(data=baseData,columns=columnNames)\n",
      "    \n",
      "    return descriptors\n",
      "136/104: df6_lipinski = lipinski(df6)\n",
      "136/105: df6_lipinski = lipinski(df6.smiles)\n",
      "136/106:\n",
      "df6_lipinski = lipinski(df6.smiles)\n",
      "df6_lipinski\n",
      "136/107:\n",
      "# Calculate Molecular Descriptor\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Descriptors, Lipinski\n",
      "\n",
      "def lipinski(smiles, verbose=False):\n",
      "\n",
      "    moldata= []\n",
      "    for elem in smiles:\n",
      "        mol=Chem.MolFromSmiles(elem) \n",
      "        moldata.append(mol)\n",
      "       \n",
      "    baseData= np.arange(1,1)\n",
      "    i=0  \n",
      "    for mol in moldata:        \n",
      "       \n",
      "        desc_MolWt = Descriptors.MolWt(mol)\n",
      "        desc_TPSA = Descriptors.TPSA(mol)\n",
      "        desc_MolLogP = Descriptors.MolLogP(mol)\n",
      "        desc_NumHDonors = Lipinski.NumHDonors(mol)\n",
      "        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)\n",
      "        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)\n",
      "        desc_FpDensityMorgan1 = Descriptors.FpDensityMorgan1(mol)\n",
      "        desc_FpDensityMorgan2 = Descriptors.FpDensityMorgan2(mol)\n",
      "        desc_FpDensityMorgan3 = Descriptors.FpDensityMorgan3(mol)\n",
      "        desc_NumRadicalElectrons = Descriptors.NumRadicalElectrons(mol)\n",
      "        desc_NumValenceElectrons = Descriptors.NumValenceElectrons(mol)\n",
      "           \n",
      "        row = np.array([desc_MolWt,\n",
      "                        desc_TPSA,\n",
      "                        desc_MolLogP,\n",
      "                        desc_NumHDonors,\n",
      "                        desc_NumHAcceptors,\n",
      "                        desc_NumRotatableBonds,\n",
      "                        desc_FpDensityMorgan1,\n",
      "                        desc_FpDensityMorgan2,\n",
      "                        desc_FpDensityMorgan3,\n",
      "                        desc_NumRadicalElectrons,\n",
      "                        desc_NumValenceElectrons])   \n",
      "    \n",
      "        if(i==0):\n",
      "            baseData=row\n",
      "        else:\n",
      "            baseData=np.vstack([baseData, row])\n",
      "        i=i+1      \n",
      "    \n",
      "    columnNames=[\"MW\",\"TPSA\", \"LogP\",\"NumHDonors\",\"NumHAcceptors\", \"NumRotatableBonds\", 'FpDensityMorgan1', 'FpDensityMorgan2', 'FpDensityMorgan3', 'NumRadicalElectrons', 'NumValenceElectrons']   \n",
      "    descriptors = pd.DataFrame(data=baseData,columns=columnNames)\n",
      "    \n",
      "    return descriptors\n",
      "136/108:\n",
      "df6_lipinski = lipinski(df6.smiles)\n",
      "df6_lipinski\n",
      "136/109: df7 = pd.concat([df6, df6_lipinski], axis=1)\n",
      "136/110:\n",
      "df7 = pd.concat([df6, df6_lipinski], axis=1)\n",
      "df7\n",
      "136/111:\n",
      "data = pd.concat([df6, df6_lipinski], axis=1)\n",
      "data\n",
      "136/112:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt1.set_ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)\n",
      "plt2.set_xlabel(\"LogP\", fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt2.set_ylim(0, 50)\n",
      "plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "\n",
      "#Histogram for NumHDonors\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 30, color='#FF9A00', edgecolor='black', linewidth=0.5)\n",
      "plt3.set_xlabel(\"Number of Hydrogen Bond Donors\", fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt3.set_ylim(0, 50)\n",
      "plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for NumHAcceptors\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 30, color='#FF165D', edgecolor='black', linewidth=0.5)\n",
      "plt4.set_xlabel(\"Number of Hydrogen Bond Acceptors\", fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt4.set_ylim(0, 50)\n",
      "plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for NumRotatableBonds\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 30, color='#52006A', edgecolor='black', linewidth=0.5)\n",
      "plt5.set_xlabel(\"Number of RotatableBonds\", fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt5.set_ylim(0, 50)\n",
      "plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for TPSA\n",
      "plt6.hist(data['TPSA'], density=False, bins= 30, color='#52006A', edgecolor='black', linewidth=0.5)\n",
      "plt6.set_xlabel(\"Topolocial Plar Surface Area\", fontsize=16, fontweight='bold')\n",
      "plt6.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt6.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt6.set_ylim(0, 50)\n",
      "plt6.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Lipinski_basic descriptors.pdf', dpi=300)\n",
      "136/113:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt1.set_ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)\n",
      "plt2.set_xlabel(\"LogP\", fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt2.set_ylim(0, 50)\n",
      "plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "\n",
      "#Histogram for NumHDonors\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 30, color='#FF9A00', edgecolor='black', linewidth=0.5)\n",
      "plt3.set_xlabel(\"Number of Hydrogen Bond Donors\", fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt3.set_ylim(0, 50)\n",
      "plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for NumHAcceptors\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 30, color='#FF165D', edgecolor='black', linewidth=0.5)\n",
      "plt4.set_xlabel(\"Number of Hydrogen Bond Acceptors\", fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt4.set_ylim(0, 50)\n",
      "plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for NumRotatableBonds\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 30, color='#52006A', edgecolor='black', linewidth=0.5)\n",
      "plt5.set_xlabel(\"Number of RotatableBonds\", fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt5.set_ylim(0, 50)\n",
      "plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for TPSA\n",
      "plt6.hist(data['TPSA'], density=False, bins= 30, color='#52006A', edgecolor='black', linewidth=0.5)\n",
      "plt6.set_xlabel(\"Topolocial Plar Surface Area\", fontsize=16, fontweight='bold')\n",
      "plt6.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt6.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt6.set_ylim(0, 50)\n",
      "plt6.axvline(90, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Lipinski_basic descriptors.pdf', dpi=300)\n",
      "136/114:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt1.set_ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)\n",
      "plt2.set_xlabel(\"LogP\", fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt2.set_ylim(0, 50)\n",
      "plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "\n",
      "#Histogram for NumHDonors\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 30, color='#FF9A00', edgecolor='black', linewidth=0.5)\n",
      "plt3.set_xlabel(\"Number of Hydrogen Bond Donors\", fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt3.set_ylim(0, 50)\n",
      "plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for NumHAcceptors\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 30, color='#FF165D', edgecolor='black', linewidth=0.5)\n",
      "plt4.set_xlabel(\"Number of Hydrogen Bond Acceptors\", fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt4.set_ylim(0, 50)\n",
      "plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for NumRotatableBonds\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 30, color='#52006A', edgecolor='black', linewidth=0.5)\n",
      "plt5.set_xlabel(\"Number of RotatableBonds\", fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt5.set_ylim(0, 50)\n",
      "plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for TPSA\n",
      "plt6.hist(data['TPSA'], density=False, bins= 30, color='#DF711B', edgecolor='black', linewidth=0.5)\n",
      "plt6.set_xlabel(\"Topolocial Plar Surface Area\", fontsize=16, fontweight='bold')\n",
      "plt6.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt6.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt6.set_ylim(0, 50)\n",
      "plt6.axvline(90, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Lipinski_basic descriptors.pdf', dpi=300)\n",
      "136/115:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt1.set_ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)\n",
      "plt2.set_xlabel(\"LogP\", fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt2.set_ylim(0, 50)\n",
      "plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "\n",
      "#Histogram for NumHDonors\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 30, color='#FF9A00', edgecolor='black', linewidth=0.5)\n",
      "plt3.set_xlabel(\"Number of Hydrogen Bond Donors\", fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt3.set_ylim(0, 50)\n",
      "plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for NumHAcceptors\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 30, color='#FF165D', edgecolor='black', linewidth=0.5)\n",
      "plt4.set_xlabel(\"Number of Hydrogen Bond Acceptors\", fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt4.set_ylim(0, 50)\n",
      "plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for NumRotatableBonds\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 30, color='#52006A', edgecolor='black', linewidth=0.5)\n",
      "plt5.set_xlabel(\"Number of RotatableBonds\", fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt5.set_ylim(0, 50)\n",
      "plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for TPSA\n",
      "plt6.hist(data['TPSA'], density=False, bins= 30, color='#01937C', edgecolor='black', linewidth=0.5)\n",
      "plt6.set_xlabel(\"Topolocial Plar Surface Area\", fontsize=16, fontweight='bold')\n",
      "plt6.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt6.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt6.set_ylim(0, 50)\n",
      "plt6.axvline(90, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Lipinski_basic descriptors.pdf', dpi=300)\n",
      "136/116:\n",
      "selection = ['smiles', 'compound']\n",
      "smiles = data[selection]\n",
      "smiles.to_csv('new_smiles.smi', sep='\\t', index=False, header=False)\n",
      "136/117:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "x1 = data['MW']\n",
      "x2 = data['LogP']\n",
      "x3 = data['NumHDonors']\n",
      "x4 = data['NumHAcceptors']\n",
      "x5 = data['NumRotatableBonds']\n",
      "x6 = data['TPSA']\n",
      "y = data['LogKp']\n",
      "colors = {'HEK293':'red', 'SK-MEL-2':'blue', 'HL60':'green', 'SH-5Y':'lightblue', 'PBMC':'yellow'}\n",
      "\n",
      "#MW\n",
      "plt1.scatter(x1, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#LogP\n",
      "plt2.scatter(x2, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondDonors\n",
      "plt3.scatter(x3, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt4.scatter(x4, y,  alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt5.scatter(x5, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#TPSA\n",
      "plt5.scatter(x6, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt5.set_xlabel('Topological Polar Surface Area', fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('RO5_and_Kpuu.pdf', dpi=300)\n",
      "136/118:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "x1 = data['MW']\n",
      "x2 = data['LogP']\n",
      "x3 = data['NumHDonors']\n",
      "x4 = data['NumHAcceptors']\n",
      "x5 = data['NumRotatableBonds']\n",
      "x6 = data['TPSA']\n",
      "y = data['LogKp']\n",
      "colors = {'HEK293':'red', 'SK-MEL-2':'blue', 'HL60':'green', 'SH-5Y':'lightblue', 'PBMC':'yellow'}\n",
      "\n",
      "#MW\n",
      "plt1.scatter(x1, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#LogP\n",
      "plt2.scatter(x2, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondDonors\n",
      "plt3.scatter(x3, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt4.scatter(x4, y,  alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt5.scatter(x5, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#TPSA\n",
      "plt6.scatter(x6, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt6.set_xlabel('Topological Polar Surface Area', fontsize=16, fontweight='bold')\n",
      "plt6.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt6.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('RO5_and_Kpuu.pdf', dpi=300)\n",
      "136/119:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "x1 = data['MW']\n",
      "x2 = data['LogP']\n",
      "x3 = data['NumHDonors']\n",
      "x4 = data['NumHAcceptors']\n",
      "x5 = data['NumRotatableBonds']\n",
      "x6 = data['TPSA']\n",
      "y = data['LogKp']\n",
      "colors = {'HEK293':'red', 'SK-MEL-2':'blue', 'HL60':'green', 'SH-5Y':'lightblue', 'PBMC':'yellow'}\n",
      "\n",
      "#MW\n",
      "plt1.scatter(x1, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#LogP\n",
      "plt2.scatter(x2, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondDonors\n",
      "plt3.scatter(x3, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt4.scatter(x4, y,  alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt5.scatter(x5, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#TPSA\n",
      "plt6.scatter(x6, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt6.set_xlabel('Topological Polar Surface Area', fontsize=16, fontweight='bold')\n",
      "plt6.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt6.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('RO5_and_LogPk.pdf', dpi=300)\n",
      "136/120:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "x1 = data['MW']\n",
      "x2 = data['LogP']\n",
      "x3 = data['NumHDonors']\n",
      "x4 = data['NumHAcceptors']\n",
      "x5 = data['NumRotatableBonds']\n",
      "x6 = data['TPSA']\n",
      "y = data['LogKp']\n",
      "colors = {'HEK293':'red', 'SK-MEL-2':'blue', 'HL60':'green', 'SH-5Y':'lightblue', 'PBMC':'yellow'}\n",
      "\n",
      "#MW\n",
      "plt1.scatter(x1, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#LogP\n",
      "plt2.scatter(x2, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondDonors\n",
      "plt3.scatter(x3, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt4.scatter(x4, y,  alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt5.scatter(x5, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#TPSA\n",
      "plt6.scatter(x6, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt6.set_xlabel('Topological Polar Surface Area', fontsize=16, fontweight='bold')\n",
      "plt6.set_ylabel(\"$Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt6.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('RO5_and_LogKp.pdf', dpi=300)\n",
      "136/121:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "x1 = data['MW']\n",
      "x2 = data['LogP']\n",
      "x3 = data['NumHDonors']\n",
      "x4 = data['NumHAcceptors']\n",
      "x5 = data['NumRotatableBonds']\n",
      "x6 = data['TPSA']\n",
      "y = data['LogKp']\n",
      "colors = {'HEK293':'red', 'SK-MEL-2':'blue', 'HL60':'green', 'SH-5Y':'lightblue', 'PBMC':'yellow'}\n",
      "\n",
      "#MW\n",
      "plt1.scatter(x1, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"log $Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#LogP\n",
      "plt2.scatter(x2, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"log $Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondDonors\n",
      "plt3.scatter(x3, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"log $Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt4.scatter(x4, y,  alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"log $Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt5.scatter(x5, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"log $Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#TPSA\n",
      "plt6.scatter(x6, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt6.set_xlabel('Topological Polar Surface Area', fontsize=16, fontweight='bold')\n",
      "plt6.set_ylabel(\"log $Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt6.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('RO5_and_LogKp.pdf', dpi=300)\n",
      "136/122:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "x1 = data['MW']\n",
      "x2 = data['LogP']\n",
      "x3 = data['NumHDonors']\n",
      "x4 = data['NumHAcceptors']\n",
      "x5 = data['NumRotatableBonds']\n",
      "x6 = data['TPSA']\n",
      "y = data['LogKp']\n",
      "colors = {'HEK293':'red', 'SK-MEL-2':'blue', 'HL60':'green', 'SH-5Y':'lightblue', 'PBMC':'yellow'}\n",
      "\n",
      "#MW\n",
      "plt1.scatter(x1, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"log $Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#LogP\n",
      "plt2.scatter(x2, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"log $Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondDonors\n",
      "plt3.scatter(x3, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"log $Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt4.scatter(x4, y,  alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"log $Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt5.scatter(x5, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"log $Kp_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#TPSA\n",
      "plt6.scatter(x6, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt6.set_xlabel('Topological Polar Surface Area', fontsize=16, fontweight='bold')\n",
      "plt6.set_ylabel(\"log Kp$_{uu}$\", fontsize=16, fontweight='bold')\n",
      "plt6.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('RO5_and_LogKp.pdf', dpi=300)\n",
      "136/123:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "x1 = data['MW']\n",
      "x2 = data['LogP']\n",
      "x3 = data['NumHDonors']\n",
      "x4 = data['NumHAcceptors']\n",
      "x5 = data['NumRotatableBonds']\n",
      "x6 = data['TPSA']\n",
      "y = data['LogKp']\n",
      "colors = {'HEK293':'red', 'SK-MEL-2':'blue', 'HL60':'green', 'SH-5Y':'lightblue', 'PBMC':'yellow'}\n",
      "\n",
      "#MW\n",
      "plt1.scatter(x1, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"log Kp\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#LogP\n",
      "plt2.scatter(x2, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"log Kp\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondDonors\n",
      "plt3.scatter(x3, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"log Kp\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt4.scatter(x4, y,  alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"log Kp\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt5.scatter(x5, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"log Kp\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#TPSA\n",
      "plt6.scatter(x6, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt6.set_xlabel('Topological Polar Surface Area', fontsize=16, fontweight='bold')\n",
      "plt6.set_ylabel(\"log Kp\", fontsize=16, fontweight='bold')\n",
      "plt6.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('RO5_and_LogKp.pdf', dpi=300)\n",
      "136/124:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "x1 = data['MW']\n",
      "x2 = data['LogP']\n",
      "x3 = data['NumHDonors']\n",
      "x4 = data['NumHAcceptors']\n",
      "x5 = data['NumRotatableBonds']\n",
      "x6 = data['TPSA']\n",
      "y = data['LogKp']\n",
      "colors = {'HEK293':'red', 'SK-MEL-2':'blue', 'HL60':'green', 'SH-5Y':'lightblue', 'PBMC':'yellow'}\n",
      "\n",
      "#MW\n",
      "plt1.scatter(x1, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"log Kp\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#LogP\n",
      "plt2.scatter(x2, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"log Kp\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondDonors\n",
      "plt3.scatter(x3, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"log Kp\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt4.scatter(x4, y,  alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"log Kp\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt5.scatter(x5, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"log Kp\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#TPSA\n",
      "plt6.scatter(x6, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt6.set_xlabel('Topological Polar Surface Area', fontsize=16, fontweight='bold')\n",
      "plt6.set_ylabel(\"log Kp\", fontsize=16, fontweight='bold')\n",
      "plt6.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#save file\n",
      "plt.legend(colors)\n",
      "plt.tight_layout()\n",
      "plt.savefig('RO5_and_LogKp.pdf', dpi=300)\n",
      "124/10:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split\n",
      "124/11: pwd\n",
      "124/12: df = pd.read_csv('new_molecules.smi')\n",
      "124/13: df = pd.read_csv('new_smiles.smi.smi')\n",
      "124/14: df = pd.read_csv('new_smiles.smi')\n",
      "124/15:\n",
      "df = pd.read_csv('new_smiles.smi')\n",
      "df\n",
      "124/16:\n",
      "list_Fp = ['Fingerprinter.xml', 'ExtendedFingerprinter.xml', 'EStateFingerprinter.xml',\n",
      "           'GraphOnlyFingerprinter.xml', 'MACCSFingerprinter.xml', 'PubchemFingerprinter.xml',\n",
      "           'SubstructureFingerprinter.xml', 'KlekotaRothFingerprinter.xml', 'AtomPairs2DFingerprinter.xml']\n",
      "124/17:\n",
      "def Fingerprint(name , Fp):\n",
      "    \n",
      "    group = name\n",
      "    group = [name.replace(\".smi\", \".csv\") for name in group]\n",
      "    result = ', '.join(group)\n",
      "    \n",
      "    Fp_name = Fp\n",
      "    Fp_name = Fp.replace(\".xml\", \"_\")\n",
      "    \n",
      "    print (name)\n",
      "    print (result)\n",
      "    print (Fp)\n",
      "    print (Fp_name)\n",
      "\n",
      "    ! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/$Fp' \\\n",
      "    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/$name' \\\n",
      "    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/$Fp_name$result'\n",
      "124/18:\n",
      "path =  '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/'\n",
      "for f in glob.glob(path + '*.smi'):\n",
      "    names = [os.path.basename(f)]\n",
      "    for Fp in (list_Fp):\n",
      "        Fingerprint(names, Fp)\n",
      "124/19:\n",
      "import os\n",
      "import glob\n",
      "path =  '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/'\n",
      "for f in glob.glob(path + '*.smi'):\n",
      "    names = [os.path.basename(f)]\n",
      "    for Fp in (list_Fp):\n",
      "        Fingerprint(names, Fp)\n",
      "124/20:\n",
      "def Fingerprint(name , Fp):\n",
      "    \n",
      "    group = name\n",
      "    group = [name.replace(\".smi\", \".csv\") for name in group]\n",
      "    result = ', '.join(group)\n",
      "    \n",
      "    Fp_name = Fp\n",
      "    Fp_name = Fp.replace(\".xml\", \"_\")\n",
      "    \n",
      "    print (name)\n",
      "    print (result)\n",
      "    print (Fp)\n",
      "    print (Fp_name)\n",
      "\n",
      "    ! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/$Fp' \\\n",
      "    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/$name' \\\n",
      "    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/$Fp_name$result'\n",
      "124/21:\n",
      "import os\n",
      "import glob\n",
      "path =  '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/'\n",
      "for f in glob.glob(path + '*.smi'):\n",
      "    names = [os.path.basename(f)]\n",
      "    for Fp in (list_Fp):\n",
      "        Fingerprint(names, Fp)\n",
      "136/125:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "x1 = data['MW']\n",
      "x2 = data['LogP']\n",
      "x3 = data['NumHDonors']\n",
      "x4 = data['NumHAcceptors']\n",
      "x5 = data['NumRotatableBonds']\n",
      "x6 = data['TPSA']\n",
      "y = data['LogKp']\n",
      "colors = {'HEK293':'red', 'SK-MEL-2':'blue', 'HL60':'green', 'SH-5Y':'lightblue', 'PBMC':'yellow'}\n",
      "\n",
      "#MW\n",
      "plt1.scatter(x1, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"log Kp\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#LogP\n",
      "plt2.scatter(x2, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"log Kp\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondDonors\n",
      "plt3.scatter(x3, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"log Kp\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt4.scatter(x4, y,  alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"log Kp\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt5.scatter(x5, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"log Kp\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#TPSA\n",
      "plt6.scatter(x6, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt6.set_xlabel('Topological Polar Surface Area', fontsize=16, fontweight='bold')\n",
      "plt6.set_ylabel(\"log Kp\", fontsize=16, fontweight='bold')\n",
      "plt6.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#save file\n",
      "plt.legend()\n",
      "plt.tight_layout()\n",
      "plt.savefig('RO5_and_LogKp.pdf', dpi=300)\n",
      "136/126:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "x1 = data['MW']\n",
      "x2 = data['LogP']\n",
      "x3 = data['NumHDonors']\n",
      "x4 = data['NumHAcceptors']\n",
      "x5 = data['NumRotatableBonds']\n",
      "x6 = data['TPSA']\n",
      "y = data['LogKp']\n",
      "colors = {'HEK293':'red', 'SK-MEL-2':'blue', 'HL60':'green', 'SH-5Y':'lightblue', 'PBMC':'yellow'}\n",
      "\n",
      "#MW\n",
      "plt1.scatter(x1, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"log Kp\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#LogP\n",
      "plt2.scatter(x2, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"log Kp\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondDonors\n",
      "plt3.scatter(x3, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"log Kp\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt4.scatter(x4, y,  alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"log Kp\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt5.scatter(x5, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"log Kp\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#TPSA\n",
      "plt6.scatter(x6, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt6.set_xlabel('Topological Polar Surface Area', fontsize=16, fontweight='bold')\n",
      "plt6.set_ylabel(\"log Kp\", fontsize=16, fontweight='bold')\n",
      "plt6.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#save file\n",
      "plt1.legend()\n",
      "plt.tight_layout()\n",
      "plt.savefig('RO5_and_LogKp.pdf', dpi=300)\n",
      "136/127:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "x1 = data['MW']\n",
      "x2 = data['LogP']\n",
      "x3 = data['NumHDonors']\n",
      "x4 = data['NumHAcceptors']\n",
      "x5 = data['NumRotatableBonds']\n",
      "x6 = data['TPSA']\n",
      "y = data['LogKp']\n",
      "colors = {'HEK293':'red', 'SK-MEL-2':'blue', 'HL60':'green', 'SH-5Y':'lightblue', 'PBMC':'yellow'}\n",
      "\n",
      "#MW\n",
      "plt1.scatter(x1, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"log Kp\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#LogP\n",
      "plt2.scatter(x2, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"log Kp\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondDonors\n",
      "plt3.scatter(x3, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"log Kp\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt4.scatter(x4, y,  alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"log Kp\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt5.scatter(x5, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"log Kp\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#TPSA\n",
      "plt6.scatter(x6, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt6.set_xlabel('Topological Polar Surface Area', fontsize=16, fontweight='bold')\n",
      "plt6.set_ylabel(\"log Kp\", fontsize=16, fontweight='bold')\n",
      "plt6.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('RO5_and_LogKp.pdf', dpi=300)\n",
      "124/22:\n",
      "import os\n",
      "import glob\n",
      "path =  '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/'\n",
      "for f in glob.glob(path + '*.smi'):\n",
      "    names = [os.path.basename(f)]\n",
      "    for Fp in (list_Fp):\n",
      "        Fingerprint(names, Fp)\n",
      "124/23:\n",
      "list_Fp = ['Descriptor.xml'. 'Fingerprinter.xml', 'ExtendedFingerprinter.xml', 'EStateFingerprinter.xml',\n",
      "           'GraphOnlyFingerprinter.xml', 'MACCSFingerprinter.xml', 'PubchemFingerprinter.xml',\n",
      "           'SubstructureFingerprinter.xml', 'KlekotaRothFingerprinter.xml', 'AtomPairs2DFingerprinter.xml']\n",
      "124/24:\n",
      "list_Fp = ['Descriptor.xml', 'Fingerprinter.xml', 'ExtendedFingerprinter.xml', 'EStateFingerprinter.xml',\n",
      "           'GraphOnlyFingerprinter.xml', 'MACCSFingerprinter.xml', 'PubchemFingerprinter.xml',\n",
      "           'SubstructureFingerprinter.xml', 'KlekotaRothFingerprinter.xml', 'AtomPairs2DFingerprinter.xml']\n",
      "124/25:\n",
      "def Fingerprint(name , Fp):\n",
      "    \n",
      "    group = name\n",
      "    group = [name.replace(\".smi\", \".csv\") for name in group]\n",
      "    result = ', '.join(group)\n",
      "    \n",
      "    Fp_name = Fp\n",
      "    Fp_name = Fp.replace(\".xml\", \"_\")\n",
      "    \n",
      "    print (name)\n",
      "    print (result)\n",
      "    print (Fp)\n",
      "    print (Fp_name)\n",
      "\n",
      "    ! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/$Fp' \\\n",
      "    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/$name' \\\n",
      "    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/$Fp_name$result'\n",
      "124/26:\n",
      "import os\n",
      "import glob\n",
      "path =  '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/'\n",
      "for f in glob.glob(path + '*.smi'):\n",
      "    names = [os.path.basename(f)]\n",
      "    for Fp in (list_Fp):\n",
      "        Fingerprint(names, Fp)\n",
      "124/27:\n",
      "! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/Fingerprinter.xml' \\\n",
      "    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/$name' \\\n",
      "    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/$Fp_name$result'\n",
      "124/28:\n",
      "! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/Fingerprinter.xml' \\\n",
      "    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \\\n",
      "    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/Fingerprinter.csv'\n",
      "124/29:\n",
      "! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/ExtendedFingerprinter.xml' \\\n",
      "    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \\\n",
      "    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/ExtendedFingerprinter.csv'\n",
      "124/30:\n",
      "! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/EStateFingerprinter.xml' \\\n",
      "    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \\\n",
      "    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/EStateFingerprinter.csv'\n",
      "124/31:\n",
      "! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/GraphOnlyFingerprinter.xml' \\\n",
      "    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \\\n",
      "    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/GraphOnlyFingerprinter.csv'\n",
      "124/32:\n",
      "! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/MACCSFingerprinter.xml' \\\n",
      "    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \\\n",
      "    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/MACCSFingerprinter.csv'\n",
      "124/33:\n",
      "! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PubchemFingerprinter.xml' \\\n",
      "    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \\\n",
      "    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PubchemFingerprinter.csv'\n",
      "124/34:\n",
      "! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/SubstructureFingerprinter.xml' \\\n",
      "    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \\\n",
      "    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/SubstructureFingerprinter.csv'\n",
      "124/35:\n",
      "! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/KlekotaRothFingerprinter.xml' \\\n",
      "    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \\\n",
      "    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/KlekotaRothFingerprinter.csv'\n",
      "124/36:\n",
      "! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/AtomPairs2DFingerprinter.xml' \\\n",
      "    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \\\n",
      "    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/AtomPairs2DFingerprinter.csv'\n",
      "135/20:\n",
      "path = r'Fingerprint/'\n",
      "\n",
      "Fp1  = pd.read_csv(path + 'Train_ER_alpha-Fingerprinter.csv'               , header = 0)\n",
      "Fp2  = pd.read_csv(path + 'Train_ER_alpha-ExtendedFingerprinter.csv'       , header = 0)\n",
      "Fp3  = pd.read_csv(path + 'Train_ER_alpha-EStateFingerprinter.csv'         , header = 0)\n",
      "Fp4  = pd.read_csv(path + 'Train_ER_alpha-GraphOnlyFingerprinter.csv'      , header = 0)\n",
      "Fp5  = pd.read_csv(path + 'Train_ER_alpha-MACCSFingerprinter.csv'          , header = 0)\n",
      "Fp6  = pd.read_csv(path + 'Train_ER_alpha-PubchemFingerprinter.csv'        , header = 0)\n",
      "Fp7  = pd.read_csv(path + 'Train_ER_alpha-SubstructureFingerprinter.csv'   , header = 0)\n",
      "Fp8  = pd.read_csv(path + 'Train_ER_alpha-SubstructureFingerprintCount.csv', header = 0)\n",
      "Fp9  = pd.read_csv(path + 'Train_ER_alpha-KlekotaRothFingerprinter.csv'    , header = 0)\n",
      "Fp10 = pd.read_csv(path + 'Train_ER_alpha-KlekotaRothFingerprintCount.csv' , header = 0)\n",
      "Fp11 = pd.read_csv(path + 'Train_ER_alpha-AtomPairs2DFingerprinter.csv'    , header = 0)\n",
      "Fp12 = pd.read_csv(path + 'Train_ER_alpha-AtomPairs2DFingerprintCount.csv' , header = 0)\n",
      "135/21:\n",
      "path = r'Fingerprint/'\n",
      "\n",
      "Fp1  = pd.read_csv(path + 'Fingerprinter.csv'               , header = 0)\n",
      "Fp2  = pd.read_csv(path + 'ExtendedFingerprinter.csv'       , header = 0)\n",
      "Fp3  = pd.read_csv(path + 'EStateFingerprinter.csv'         , header = 0)\n",
      "Fp4  = pd.read_csv(path + 'GraphOnlyFingerprinter.csv'      , header = 0)\n",
      "Fp5  = pd.read_csv(path + 'MACCSFingerprinter.csv'          , header = 0)\n",
      "Fp6  = pd.read_csv(path + 'PubchemFingerprinter.csv'        , header = 0)\n",
      "Fp7  = pd.read_csv(path + 'SubstructureFingerprinter.csv'   , header = 0)\n",
      "Fp8  = pd.read_csv(path + 'SubstructureFingerprintCount.csv', header = 0)\n",
      "Fp9  = pd.read_csv(path + 'KlekotaRothFingerprinter.csv'    , header = 0)\n",
      "Fp10 = pd.read_csv(path + 'KlekotaRothFingerprintCount.csv' , header = 0)\n",
      "Fp11 = pd.read_csv(path + 'AtomPairs2DFingerprinter.csv'    , header = 0)\n",
      "Fp12 = pd.read_csv(path + 'AtomPairs2DFingerprintCount.csv' , header = 0)\n",
      "124/37:\n",
      "list_Fp = ['Descriptor.xml', 'Fingerprinter.xml', 'ExtendedFingerprinter.xml', 'EStateFingerprinter.xml',\n",
      "           'GraphOnlyFingerprinter.xml', 'MACCSFingerprinter.xml', 'PubchemFingerprinter.xml',\n",
      "           'SubstructureFingerprinter.xml', 'KlekotaRothFingerprinter.xml', 'AtomPairs2DFingerprinter.xml', 'AtomPairs2DFingerprintCount.xml', 'KlekotaRothFingerprintCount.xml', 'SubstructureFingerprintCount.xml' ]\n",
      "124/38:\n",
      "! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/AtomPairs2DFingerprintCount.xml' \\\n",
      "    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \\\n",
      "    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/AtomPairs2DFingerprintCount.csv'\n",
      "124/39:\n",
      "! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/KlekotaRothFingerprintCount.xml' \\\n",
      "    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \\\n",
      "    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/KlekotaRothFingerprintCount.csv'\n",
      "124/40:\n",
      "! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/SubstructureFingerprintCount.xml' \\\n",
      "    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \\\n",
      "    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/SubstructureFingerprintCount.csv'\n",
      "135/22:\n",
      "path = r'Fingerprint/'\n",
      "\n",
      "Fp1  = pd.read_csv(path + 'Fingerprinter.csv'               , header = 0)\n",
      "Fp2  = pd.read_csv(path + 'ExtendedFingerprinter.csv'       , header = 0)\n",
      "Fp3  = pd.read_csv(path + 'EStateFingerprinter.csv'         , header = 0)\n",
      "Fp4  = pd.read_csv(path + 'GraphOnlyFingerprinter.csv'      , header = 0)\n",
      "Fp5  = pd.read_csv(path + 'MACCSFingerprinter.csv'          , header = 0)\n",
      "Fp6  = pd.read_csv(path + 'PubchemFingerprinter.csv'        , header = 0)\n",
      "Fp7  = pd.read_csv(path + 'SubstructureFingerprinter.csv'   , header = 0)\n",
      "Fp8  = pd.read_csv(path + 'SubstructureFingerprintCount.csv', header = 0)\n",
      "Fp9  = pd.read_csv(path + 'KlekotaRothFingerprinter.csv'    , header = 0)\n",
      "Fp10 = pd.read_csv(path + 'KlekotaRothFingerprintCount.csv' , header = 0)\n",
      "Fp11 = pd.read_csv(path + 'AtomPairs2DFingerprinter.csv'    , header = 0)\n",
      "Fp12 = pd.read_csv(path + 'AtomPairs2DFingerprintCount.csv' , header = 0)\n",
      "135/23: Fp12\n",
      "124/41:\n",
      "! java -jar  '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/Fingerprinter.xml' \\\n",
      "    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \\\n",
      "    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/Fingerprinter.csv'\n",
      "124/42:\n",
      "! java -jar  '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/Fingerprinter.xml' \\\n",
      "    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \\\n",
      "    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/Fingerprinter.csv'\n",
      "124/43:\n",
      "! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/ExtendedFingerprinter.xml' \\\n",
      "    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \\\n",
      "    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/ExtendedFingerprinter.csv'\n",
      "136/128:\n",
      "selection = ['smiles']\n",
      "smiles = data[selection]\n",
      "smiles.to_csv('new_smiles.smi', sep='\\t', index=False, header=False)\n",
      "136/129:\n",
      "selection = ['smiles']\n",
      "smiles = data[selection]\n",
      "smiles.to_csv(r'/smiles/new_smiles.smi', sep='\\t', index=False, header=False)\n",
      "136/130:\n",
      "selection = ['smiles']\n",
      "smiles = data[selection]\n",
      "smiles.to_csv(r'smiles/new_smiles.smi', sep='\\t', index=False, header=False)\n",
      "124/44:\n",
      "! java -jar  '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/Fingerprinter.xml' \\\n",
      "    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \\\n",
      "    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/Fingerprinter.csv'\n",
      "124/45:\n",
      "! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/ExtendedFingerprinter.xml' \\\n",
      "    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \\\n",
      "    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/ExtendedFingerprinter.csv'\n",
      "124/46:\n",
      "! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/EStateFingerprinter.xml' \\\n",
      "    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \\\n",
      "    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/EStateFingerprinter.csv'\n",
      "124/47:\n",
      "! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/GraphOnlyFingerprinter.xml' \\\n",
      "    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \\\n",
      "    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/GraphOnlyFingerprinter.csv'\n",
      "124/48:\n",
      "! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/MACCSFingerprinter.xml' \\\n",
      "    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \\\n",
      "    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/MACCSFingerprinter.csv'\n",
      "124/49:\n",
      "! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PubchemFingerprinter.xml' \\\n",
      "    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \\\n",
      "    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/PubchemFingerprinter.csv'\n",
      "124/50:\n",
      "! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/SubstructureFingerprinter.xml' \\\n",
      "    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \\\n",
      "    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/SubstructureFingerprinter.csv'\n",
      "124/51:\n",
      "! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/KlekotaRothFingerprinter.xml' \\\n",
      "    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \\\n",
      "    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/KlekotaRothFingerprinter.csv'\n",
      "124/52:\n",
      "! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/AtomPairs2DFingerprinter.xml' \\\n",
      "    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \\\n",
      "    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/AtomPairs2DFingerprinter.csv'\n",
      "124/53:\n",
      "! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/AtomPairs2DFingerprintCount.xml' \\\n",
      "    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \\\n",
      "    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/AtomPairs2DFingerprintCount.csv'\n",
      "124/54:\n",
      "! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/KlekotaRothFingerprintCount.xml' \\\n",
      "    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \\\n",
      "    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/KlekotaRothFingerprintCount.csv'\n",
      "124/55:\n",
      "! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/SubstructureFingerprintCount.xml' \\\n",
      "    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \\\n",
      "    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/SubstructureFingerprintCount.csv'\n",
      "135/24:\n",
      "path = r'Fingerprint/'\n",
      "\n",
      "Fp1  = pd.read_csv(path + 'Fingerprinter.csv'               , header = 0)\n",
      "Fp2  = pd.read_csv(path + 'ExtendedFingerprinter.csv'       , header = 0)\n",
      "Fp3  = pd.read_csv(path + 'EStateFingerprinter.csv'         , header = 0)\n",
      "Fp4  = pd.read_csv(path + 'GraphOnlyFingerprinter.csv'      , header = 0)\n",
      "Fp5  = pd.read_csv(path + 'MACCSFingerprinter.csv'          , header = 0)\n",
      "Fp6  = pd.read_csv(path + 'PubchemFingerprinter.csv'        , header = 0)\n",
      "Fp7  = pd.read_csv(path + 'SubstructureFingerprinter.csv'   , header = 0)\n",
      "Fp8  = pd.read_csv(path + 'SubstructureFingerprintCount.csv', header = 0)\n",
      "Fp9  = pd.read_csv(path + 'KlekotaRothFingerprinter.csv'    , header = 0)\n",
      "Fp10 = pd.read_csv(path + 'KlekotaRothFingerprintCount.csv' , header = 0)\n",
      "Fp11 = pd.read_csv(path + 'AtomPairs2DFingerprinter.csv'    , header = 0)\n",
      "Fp12 = pd.read_csv(path + 'AtomPairs2DFingerprintCount.csv' , header = 0)\n",
      "135/25:\n",
      "path = r'Fingerprint/'\n",
      "\n",
      "Fp1  = pd.read_csv(path + 'Fingerprinter.csv'               , header = 0)\n",
      "Fp2  = pd.read_csv(path + 'ExtendedFingerprinter.csv'       , header = 0)\n",
      "Fp3  = pd.read_csv(path + 'EStateFingerprinter.csv'         , header = 0)\n",
      "Fp4  = pd.read_csv(path + 'GraphOnlyFingerprinter.csv'      , header = 0)\n",
      "Fp5  = pd.read_csv(path + 'MACCSFingerprinter.csv'          , header = 0)\n",
      "Fp6  = pd.read_csv(path + 'PubchemFingerprinter.csv'        , header = 0)\n",
      "Fp7  = pd.read_csv(path + 'SubstructureFingerprinter.csv'   , header = 0)\n",
      "Fp8  = pd.read_csv(path + 'SubstructureFingerprintCount.csv', header = 0)\n",
      "Fp9  = pd.read_csv(path + 'KlekotaRothFingerprinter.csv'    , header = 0)\n",
      "Fp10 = pd.read_csv(path + 'KlekotaRothFingerprintCount.csv' , header = 0)\n",
      "Fp11 = pd.read_csv(path + 'AtomPairs2DFingerprinter.csv'    , header = 0)\n",
      "Fp12 = pd.read_csv(path + 'AtomPairs2DFingerprintCount.csv' , header = 0)\n",
      "135/26: Fp12\n",
      "135/27: pwd\n",
      "136/131:\n",
      "data = pd.concat([df6, df6_lipinski], axis=1)\n",
      "data.to_csv('df6_lipinski.csv')\n",
      "data\n",
      "136/132:\n",
      "data = pd.concat([df6, df6_lipinski], axis=1)\n",
      "data.to_csv('df6_lipinski.csv')\n",
      "data\n",
      "135/28: data = pd.read_csv('df6_lipinski.csv')\n",
      "135/29:\n",
      "data = pd.read_csv('df6_lipinski.csv')\n",
      "data\n",
      "135/30:\n",
      "df = pd.read_csv('df6_lipinski.csv')\n",
      "df\n",
      "135/31: name = df['name']\n",
      "135/32: name = df[name]\n",
      "135/33: name = df[compound]\n",
      "135/34: name = df.compound\n",
      "135/35: name\n",
      "135/36: name = df['compound']\n",
      "135/37: name\n",
      "135/38: name = pd.df['compound']\n",
      "135/39: name = df[compound]\n",
      "135/40: name.shape\n",
      "135/41: name = df['compound']\n",
      "135/42: name.shape\n",
      "135/43: Fp1 = Fp1.replace(columns = {'Name':'compound'})\n",
      "135/44: Fp1 = Fp1.replace({'Name':'compound'})\n",
      "135/45: Fp1\n",
      "135/46: Fp1 = Fp1.replace({'Name':'name'})\n",
      "135/47: Fp1\n",
      "135/48: Fp1 = Fp1.replace(['Name':'name'])\n",
      "135/49: Fp1 = Fp1.replace(['Name','name'])\n",
      "135/50: Fp1\n",
      "135/51: Fp1.Name = name\n",
      "135/52: Fp1\n",
      "135/53:\n",
      "Fp1.Name = name\n",
      "Fp2.Name = name\n",
      "Fp3.Name = name\n",
      "Fp4.Name = name\n",
      "Fp5.Name = name\n",
      "Fp6.Name = name\n",
      "Fp7.Name = name\n",
      "Fp8.Name = name\n",
      "Fp9.Name = name\n",
      "Fp10.Name = name\n",
      "Fp11.Name = name\n",
      "Fp12.Name = name\n",
      "135/54:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    Name = Fp.Name\n",
      "    Fp_ix = Fp.ix[:,1:]\n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp_ix)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled)\n",
      "    Fp_normalized\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp_ix.columns)\n",
      "    Fp_normalized['Name'] = Name\n",
      "    \n",
      "    return Fp_normalized\n",
      "135/55:\n",
      "Fp1_n  = normalized (Fp1 )\n",
      "Fp2_n  = normalized (Fp2 )\n",
      "Fp3_n  = normalized (Fp3 )\n",
      "Fp4_n  = normalized (Fp4 )\n",
      "Fp5_n  = normalized (Fp5 )\n",
      "Fp6_n  = normalized (Fp6 )\n",
      "Fp7_n  = normalized (Fp7 )\n",
      "Fp8_n  = normalized (Fp8 )\n",
      "Fp9_n  = normalized (Fp9 )\n",
      "Fp10_n = normalized (Fp10)\n",
      "Fp11_n = normalized (Fp11)\n",
      "Fp12_n = normalized (Fp12)\n",
      "135/56: Fp1\n",
      "135/57:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    Name = Fp.Name\n",
      "    Fp_ix = Fp.x[:,1:]\n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp_ix)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled)\n",
      "    Fp_normalized\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp_ix.columns)\n",
      "    Fp_normalized['Name'] = Name\n",
      "    \n",
      "    return Fp_normalized\n",
      "135/58:\n",
      "Fp1_n  = normalized (Fp1 )\n",
      "Fp2_n  = normalized (Fp2 )\n",
      "Fp3_n  = normalized (Fp3 )\n",
      "Fp4_n  = normalized (Fp4 )\n",
      "Fp5_n  = normalized (Fp5 )\n",
      "Fp6_n  = normalized (Fp6 )\n",
      "Fp7_n  = normalized (Fp7 )\n",
      "Fp8_n  = normalized (Fp8 )\n",
      "Fp9_n  = normalized (Fp9 )\n",
      "Fp10_n = normalized (Fp10)\n",
      "Fp11_n = normalized (Fp11)\n",
      "Fp12_n = normalized (Fp12)\n",
      "135/59:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    Name = Fp.Name\n",
      "    Fp_ix = Fp[:,1:]\n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp_ix)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled)\n",
      "    Fp_normalized\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp_ix.columns)\n",
      "    Fp_normalized['Name'] = Name\n",
      "    \n",
      "    return Fp_normalized\n",
      "135/60:\n",
      "Fp1_n  = normalized (Fp1 )\n",
      "Fp2_n  = normalized (Fp2 )\n",
      "Fp3_n  = normalized (Fp3 )\n",
      "Fp4_n  = normalized (Fp4 )\n",
      "Fp5_n  = normalized (Fp5 )\n",
      "Fp6_n  = normalized (Fp6 )\n",
      "Fp7_n  = normalized (Fp7 )\n",
      "Fp8_n  = normalized (Fp8 )\n",
      "Fp9_n  = normalized (Fp9 )\n",
      "Fp10_n = normalized (Fp10)\n",
      "Fp11_n = normalized (Fp11)\n",
      "Fp12_n = normalized (Fp12)\n",
      "135/61: Fp1.drop(Name)\n",
      "135/62: Fp1.drop('Name')\n",
      "135/63: Fp1.drop('Name', axis=1)\n",
      "135/64:\n",
      "Fp1.Name = name\n",
      "Fp2.Name = name\n",
      "Fp3.Name = name\n",
      "Fp4.Name = name\n",
      "Fp5.Name = name\n",
      "Fp6.Name = name\n",
      "Fp7.Name = name\n",
      "Fp8.Name = name\n",
      "Fp9.Name = name\n",
      "Fp10.Name = name\n",
      "Fp11.Name = name\n",
      "Fp12.Name = name\n",
      "135/65: Fp1\n",
      "135/66:\n",
      "Fp1.drop('Name', axis=1)\n",
      "Fp2.drop('Name', axis=1)\n",
      "Fp3.drop('Name', axis=1)\n",
      "Fp4.drop('Name', axis=1)\n",
      "Fp5.drop('Name', axis=1)\n",
      "Fp6.drop('Name', axis=1)\n",
      "Fp7.drop('Name', axis=1)\n",
      "Fp8.drop('Name', axis=1)\n",
      "Fp9.drop('Name', axis=1)\n",
      "Fp10.drop('Name', axis=1)\n",
      "Fp11.drop('Name', axis=1)\n",
      "Fp12.drop('Name', axis=1)\n",
      "135/67:\n",
      "Fp1.drop('Name', axis=1)\n",
      "Fp2.drop('Name', axis=1)\n",
      "Fp3.drop('Name', axis=1)\n",
      "Fp4.drop('Name', axis=1)\n",
      "Fp5.drop('Name', axis=1)\n",
      "Fp6.drop('Name', axis=1)\n",
      "Fp7.drop('Name', axis=1)\n",
      "Fp8.drop('Name', axis=1)\n",
      "Fp9.drop('Name', axis=1)\n",
      "Fp10.drop('Name', axis=1)\n",
      "Fp11.drop('Name', axis=1)\n",
      "Fp12.drop('Name', axis=1)\n",
      "135/68:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled)\n",
      "    Fp_normalized\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)\n",
      "    Fp_normalized['Name'] = Name\n",
      "    \n",
      "    return Fp_normalized\n",
      "135/69:\n",
      "Fp1_n  = normalized (Fp1 )\n",
      "Fp2_n  = normalized (Fp2 )\n",
      "Fp3_n  = normalized (Fp3 )\n",
      "Fp4_n  = normalized (Fp4 )\n",
      "Fp5_n  = normalized (Fp5 )\n",
      "Fp6_n  = normalized (Fp6 )\n",
      "Fp7_n  = normalized (Fp7 )\n",
      "Fp8_n  = normalized (Fp8 )\n",
      "Fp9_n  = normalized (Fp9 )\n",
      "Fp10_n = normalized (Fp10)\n",
      "Fp11_n = normalized (Fp11)\n",
      "Fp12_n = normalized (Fp12)\n",
      "135/70: Fp1\n",
      "135/71:\n",
      "Fp1.drop('Name', axis=1)\n",
      "Fp2.drop('Name', axis=1)\n",
      "Fp3.drop('Name', axis=1)\n",
      "Fp4.drop('Name', axis=1)\n",
      "Fp5.drop('Name', axis=1)\n",
      "Fp6.drop('Name', axis=1)\n",
      "Fp7.drop('Name', axis=1)\n",
      "Fp8.drop('Name', axis=1)\n",
      "Fp9.drop('Name', axis=1)\n",
      "Fp10.drop('Name', axis=1)\n",
      "Fp11.drop('Name', axis=1)\n",
      "Fp12.drop('Name', axis=1)\n",
      "135/72: Fp1\n",
      "135/73:\n",
      "Fpp1 = Fp1.drop('Name', axis=1)\n",
      "Fp2.drop('Name', axis=1)\n",
      "Fp3.drop('Name', axis=1)\n",
      "Fp4.drop('Name', axis=1)\n",
      "Fp5.drop('Name', axis=1)\n",
      "Fp6.drop('Name', axis=1)\n",
      "Fp7.drop('Name', axis=1)\n",
      "Fp8.drop('Name', axis=1)\n",
      "Fp9.drop('Name', axis=1)\n",
      "Fp10.drop('Name', axis=1)\n",
      "Fp11.drop('Name', axis=1)\n",
      "Fp12.drop('Name', axis=1)\n",
      "135/74: Fpp1\n",
      "135/75: Fp1_n  = normalized (Fpp1 )\n",
      "135/76: Fp1_n  = normalized (Fpp1)\n",
      "135/77:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled)\n",
      "    Fp_normalized\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)\n",
      "    \n",
      "    return Fp_normalized\n",
      "135/78: Fp1_n  = normalized (Fpp1)\n",
      "135/79: Fp1_n\n",
      "135/80:\n",
      "Fpp1 = Fp1.drop('Name', axis=1)\n",
      "Fpp2 = Fp2.drop('Name', axis=1)\n",
      "Fpp3 = Fp3.drop('Name', axis=1)\n",
      "Fpp4 = Fp4.drop('Name', axis=1)\n",
      "Fpp5 = Fp5.drop('Name', axis=1)\n",
      "Fpp6 = Fp6.drop('Name', axis=1)\n",
      "Fpp7 = Fp7.drop('Name', axis=1)\n",
      "Fpp8 = Fp8.drop('Name', axis=1)\n",
      "Fpp9 = Fp9.drop('Name', axis=1)\n",
      "Fpp10 = Fp10.drop('Name', axis=1)\n",
      "Fpp11 = Fp11.drop('Name', axis=1)\n",
      "Fpp12 = Fp12.drop('Name', axis=1)\n",
      "135/81:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled)\n",
      "    Fp_normalized\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)\n",
      "    \n",
      "    return Fp_normalized\n",
      "135/82:\n",
      "Fp1_n  = normalized (Fpp1 )\n",
      "Fp2_n  = normalized (Fpp2 )\n",
      "Fp3_n  = normalized (Fpp3 )\n",
      "Fp4_n  = normalized (Fpp4 )\n",
      "Fp5_n  = normalized (Fpp5 )\n",
      "Fp6_n  = normalized (Fpp6 )\n",
      "Fp7_n  = normalized (Fpp7 )\n",
      "Fp8_n  = normalized (Fpp8 )\n",
      "Fp9_n  = normalized (Fpp9 )\n",
      "Fp10_n = normalized (Fpp10)\n",
      "Fp11_n = normalized (Fpp11)\n",
      "Fp12_n = normalized (Fpp12)\n",
      "135/83: Fp1_n\n",
      "140/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "140/2:\n",
      "path = r'Fingerprint/'\n",
      "\n",
      "Fp1  = pd.read_csv(path + 'Fingerprinter.csv'               , header = 0)\n",
      "Fp2  = pd.read_csv(path + 'ExtendedFingerprinter.csv'       , header = 0)\n",
      "Fp3  = pd.read_csv(path + 'EStateFingerprinter.csv'         , header = 0)\n",
      "Fp4  = pd.read_csv(path + 'GraphOnlyFingerprinter.csv'      , header = 0)\n",
      "Fp5  = pd.read_csv(path + 'MACCSFingerprinter.csv'          , header = 0)\n",
      "Fp6  = pd.read_csv(path + 'PubchemFingerprinter.csv'        , header = 0)\n",
      "Fp7  = pd.read_csv(path + 'SubstructureFingerprinter.csv'   , header = 0)\n",
      "Fp8  = pd.read_csv(path + 'SubstructureFingerprintCount.csv', header = 0)\n",
      "Fp9  = pd.read_csv(path + 'KlekotaRothFingerprinter.csv'    , header = 0)\n",
      "Fp10 = pd.read_csv(path + 'KlekotaRothFingerprintCount.csv' , header = 0)\n",
      "Fp11 = pd.read_csv(path + 'AtomPairs2DFingerprinter.csv'    , header = 0)\n",
      "Fp12 = pd.read_csv(path + 'AtomPairs2DFingerprintCount.csv' , header = 0)\n",
      "140/3: Fp12\n",
      "140/4:\n",
      "df = pd.read_csv('df6_lipinski.csv')\n",
      "df\n",
      "140/5: name = df['compound']\n",
      "140/6: name.shape\n",
      "140/7:\n",
      "Fp1.Name = name\n",
      "Fp2.Name = name\n",
      "Fp3.Name = name\n",
      "Fp4.Name = name\n",
      "Fp5.Name = name\n",
      "Fp6.Name = name\n",
      "Fp7.Name = name\n",
      "Fp8.Name = name\n",
      "Fp9.Name = name\n",
      "Fp10.Name = name\n",
      "Fp11.Name = name\n",
      "Fp12.Name = name\n",
      "140/8:\n",
      "Fpp1 = Fp1.drop('Name', axis=1)\n",
      "Fpp2 = Fp2.drop('Name', axis=1)\n",
      "Fpp3 = Fp3.drop('Name', axis=1)\n",
      "Fpp4 = Fp4.drop('Name', axis=1)\n",
      "Fpp5 = Fp5.drop('Name', axis=1)\n",
      "Fpp6 = Fp6.drop('Name', axis=1)\n",
      "Fpp7 = Fp7.drop('Name', axis=1)\n",
      "Fpp8 = Fp8.drop('Name', axis=1)\n",
      "Fpp9 = Fp9.drop('Name', axis=1)\n",
      "Fpp10 = Fp10.drop('Name', axis=1)\n",
      "Fpp11 = Fp11.drop('Name', axis=1)\n",
      "Fpp12 = Fp12.drop('Name', axis=1)\n",
      "140/9:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled)\n",
      "    Fp_normalized\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)\n",
      "    \n",
      "    return Fp_normalized\n",
      "140/10:\n",
      "Fp1_n  = normalized (Fpp1 )\n",
      "Fp2_n  = normalized (Fpp2 )\n",
      "Fp3_n  = normalized (Fpp3 )\n",
      "Fp4_n  = normalized (Fpp4 )\n",
      "Fp5_n  = normalized (Fpp5 )\n",
      "Fp6_n  = normalized (Fpp6 )\n",
      "Fp7_n  = normalized (Fpp7 )\n",
      "Fp8_n  = normalized (Fpp8 )\n",
      "Fp9_n  = normalized (Fpp9 )\n",
      "Fp10_n = normalized (Fpp10)\n",
      "Fp11_n = normalized (Fpp11)\n",
      "Fp12_n = normalized (Fpp12)\n",
      "140/11: Fp1_n\n",
      "140/12:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled)\n",
      "    Fp_normalized\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)\n",
      "    \n",
      "    return Fp_normalized\n",
      "140/13:\n",
      "raw1  = df.merge(Fp1_n , on='chemblId', how='outer')\n",
      "raw2  = df.merge(Fp2_n , on='chemblId', how='outer')\n",
      "raw3  = df.merge(Fp3_n , on='chemblId', how='outer')\n",
      "raw4  = df.merge(Fp4_n , on='chemblId', how='outer')\n",
      "raw5  = df.merge(Fp5_n , on='chemblId', how='outer')\n",
      "raw6  = df.merge(Fp6_n , on='chemblId', how='outer')\n",
      "raw7  = df.merge(Fp7_n , on='chemblId', how='outer')\n",
      "raw8  = df.merge(Fp8_n , on='chemblId', how='outer')\n",
      "raw9  = df.merge(Fp9_n , on='chemblId', how='outer')\n",
      "raw10 = df.merge(Fp10_n, on='chemblId', how='outer')\n",
      "raw11 = df.merge(Fp11_n, on='chemblId', how='outer')\n",
      "raw12 = df.merge(Fp12_n, on='chemblId', how='outer')\n",
      "140/14: raw1  = df.merge(Fp1_n , on='Name', how='outer')\n",
      "140/15: raw1  = df.merge(Fp1_n , on='Name', how='outer')\n",
      "140/16:\n",
      "Fp1_n .to_csv('Train_Fp_normalized/Fingerprinter.csv'               , sep=',' ,index=False)\n",
      "Fp2_n .to_csv('Train_Fp_normalized/ExtendedFingerprinter.csv'       , sep=',' ,index=False)\n",
      "Fp3_n .to_csv('Train_Fp_normalized/EStateFingerprinter.csv'         , sep=',' ,index=False)\n",
      "Fp4_n .to_csv('Train_Fp_normalized/GraphOnlyFingerprinter.csv'      , sep=',' ,index=False)\n",
      "Fp5_n .to_csv('Train_Fp_normalized/MACCSFingerprinter.csv'          , sep=',' ,index=False)\n",
      "Fp6_n .to_csv('Train_Fp_normalized/PubchemFingerprinter.csv'        , sep=',' ,index=False)\n",
      "Fp7_n .to_csv('Train_Fp_normalized/SubstructureFingerprinter.csv'   , sep=',' ,index=False)\n",
      "Fp8_n .to_csv('Train_Fp_normalized/SubstructureFingerprintCount.csv', sep=',' ,index=False)\n",
      "Fp9_n .to_csv('Train_Fp_normalized/KlekotaRothFingerprinter.csv'    , sep=',' ,index=False)\n",
      "Fp10_n.to_csv('Train_Fp_normalized/KlekotaRothFingerprintCount.csv' , sep=',' ,index=False)\n",
      "Fp11_n.to_csv('Train_Fp_normalized/AtomPairs2DFingerprinter.csv'    , sep=',' ,index=False)\n",
      "Fp12_n.to_csv('Train_Fp_normalized/AtomPairs2DFingerprintCount.csv' , sep=',' ,index=False)\n",
      "140/17:\n",
      "Fp1_n .to_csv('Fp_normalized/Fingerprinter.csv'               , sep=',' ,index=False)\n",
      "Fp2_n .to_csv('Fp_normalized/ExtendedFingerprinter.csv'       , sep=',' ,index=False)\n",
      "Fp3_n .to_csv('Fp_normalized/EStateFingerprinter.csv'         , sep=',' ,index=False)\n",
      "Fp4_n .to_csv('Fp_normalized/GraphOnlyFingerprinter.csv'      , sep=',' ,index=False)\n",
      "Fp5_n .to_csv('Fp_normalized/MACCSFingerprinter.csv'          , sep=',' ,index=False)\n",
      "Fp6_n .to_csv('Fp_normalized/PubchemFingerprinter.csv'        , sep=',' ,index=False)\n",
      "Fp7_n .to_csv('Fp_normalized/SubstructureFingerprinter.csv'   , sep=',' ,index=False)\n",
      "Fp8_n .to_csv('Fp_normalized/SubstructureFingerprintCount.csv', sep=',' ,index=False)\n",
      "Fp9_n .to_csv('Fp_normalized/KlekotaRothFingerprinter.csv'    , sep=',' ,index=False)\n",
      "Fp10_n.to_csv('Fp_normalized/KlekotaRothFingerprintCount.csv' , sep=',' ,index=False)\n",
      "Fp11_n.to_csv('Fp_normalized/AtomPairs2DFingerprinter.csv'    , sep=',' ,index=False)\n",
      "Fp12_n.to_csv('Fp_normalized/AtomPairs2DFingerprintCount.csv' , sep=',' ,index=False)\n",
      "140/18: raw1  = df.merge(Fp1_n , on='Name', how='outer')\n",
      "140/19:\n",
      "Fp1_n.loc[:, 'Name'] = Fp1.Name\n",
      "Fp1_n\n",
      "140/20:\n",
      "Fp1_n.loc['Name', :] = Fp1.Name\n",
      "Fp1_n\n",
      "140/21:\n",
      "Fp1_n.loc['Name', :] = Fp1.Name\n",
      "Fp1_n\n",
      "140/22:\n",
      "Fp1_n.loc['Name', -1:] = Fp1.Name\n",
      "Fp1_n\n",
      "140/23:\n",
      "Fp1_n.loc['Name', :-1] = Fp1.Name\n",
      "Fp1_n\n",
      "140/24:\n",
      "Fp1_n.loc['Name', :-1] = Fp1.Name\n",
      "Fp1_n\n",
      "140/25:\n",
      "Fp1_n.loc['Name', 1:] = Fp1.Name\n",
      "Fp1_n\n",
      "140/26:\n",
      "Fp1_n.loc['Name', 1:] = Fp1.Name\n",
      "Fp1_n\n",
      "140/27:\n",
      "Fp1_n.loc[1:, 'Name'] = Fp1.Name\n",
      "Fp1_n\n",
      "140/28:\n",
      "Fp1_n.loc[1:, 'Name'] = Fp1.Name\n",
      "Fp1_n\n",
      "140/29:\n",
      "Fp1_n.loc[:, 'Name'] = Fp1.Name\n",
      "Fp1_n\n",
      "140/30:\n",
      "Fp1_n.loc[:, 'Name'] = Fp1.Name\n",
      "Fp1_n.reset_index('Name', drop=True)\n",
      "140/31:\n",
      "Fp1_n.loc[:, 'Name'] = Fp1.Name\n",
      "Fp1_n.reset_index(Name, drop=True)\n",
      "140/32:\n",
      "Fp1_n.loc[:, 'Name'] = Fp1.Name\n",
      "Fp1_n.reset_index('Name', drop=True)\n",
      "140/33:\n",
      "Fp1_n.loc[:, 'Name'] = Fp1.Name\n",
      "Fp1_n.set_index('Name', drop=True)\n",
      "140/34:\n",
      "Fp1_n.loc[:, 'Name'] = Fp1.Name\n",
      "Fp1_n\n",
      "140/35: Fp1_n\n",
      "140/36: Fp1.shape\n",
      "140/37: Fp1_n.shape\n",
      "140/38: Fp1_n\n",
      "140/39:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled)\n",
      "    Fp_normalized\n",
      "\n",
      "    \n",
      "    return Fp_normalized\n",
      "140/40:\n",
      "Fp1_n  = normalized (Fpp1 )\n",
      "Fp2_n  = normalized (Fpp2 )\n",
      "Fp3_n  = normalized (Fpp3 )\n",
      "Fp4_n  = normalized (Fpp4 )\n",
      "Fp5_n  = normalized (Fpp5 )\n",
      "Fp6_n  = normalized (Fpp6 )\n",
      "Fp7_n  = normalized (Fpp7 )\n",
      "Fp8_n  = normalized (Fpp8 )\n",
      "Fp9_n  = normalized (Fpp9 )\n",
      "Fp10_n = normalized (Fpp10)\n",
      "Fp11_n = normalized (Fpp11)\n",
      "Fp12_n = normalized (Fpp12)\n",
      "140/41: Fp1_n\n",
      "140/42:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)\n",
      "    Fp_normalized\n",
      "    \n",
      "    return Fp_normalized\n",
      "140/43:\n",
      "Fp1_n  = normalized (Fpp1 )\n",
      "Fp2_n  = normalized (Fpp2 )\n",
      "Fp3_n  = normalized (Fpp3 )\n",
      "Fp4_n  = normalized (Fpp4 )\n",
      "Fp5_n  = normalized (Fpp5 )\n",
      "Fp6_n  = normalized (Fpp6 )\n",
      "Fp7_n  = normalized (Fpp7 )\n",
      "Fp8_n  = normalized (Fpp8 )\n",
      "Fp9_n  = normalized (Fpp9 )\n",
      "Fp10_n = normalized (Fpp10)\n",
      "Fp11_n = normalized (Fpp11)\n",
      "Fp12_n = normalized (Fpp12)\n",
      "140/44: Fp1_n\n",
      "140/45: Fp1_n.loc['Name'] = Fp1.Name\n",
      "140/46:\n",
      "Fp1_n.loc['Name'] = Fp1.Name\n",
      "Fp1_n\n",
      "140/47:\n",
      "Fp1_n.loc['Name': 1] = Fp1.Name\n",
      "Fp1_n\n",
      "140/48:\n",
      "Fp1_n.[\"Name\"] = Fp1.Name\n",
      "Fp1_n\n",
      "140/49:\n",
      "Fp1_n[\"Name\"] = Fp1.Name\n",
      "Fp1_n\n",
      "141/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "141/2:\n",
      "path = r'Fingerprint/'\n",
      "\n",
      "Fp1  = pd.read_csv(path + 'Fingerprinter.csv'               , header = 0)\n",
      "Fp2  = pd.read_csv(path + 'ExtendedFingerprinter.csv'       , header = 0)\n",
      "Fp3  = pd.read_csv(path + 'EStateFingerprinter.csv'         , header = 0)\n",
      "Fp4  = pd.read_csv(path + 'GraphOnlyFingerprinter.csv'      , header = 0)\n",
      "Fp5  = pd.read_csv(path + 'MACCSFingerprinter.csv'          , header = 0)\n",
      "Fp6  = pd.read_csv(path + 'PubchemFingerprinter.csv'        , header = 0)\n",
      "Fp7  = pd.read_csv(path + 'SubstructureFingerprinter.csv'   , header = 0)\n",
      "Fp8  = pd.read_csv(path + 'SubstructureFingerprintCount.csv', header = 0)\n",
      "Fp9  = pd.read_csv(path + 'KlekotaRothFingerprinter.csv'    , header = 0)\n",
      "Fp10 = pd.read_csv(path + 'KlekotaRothFingerprintCount.csv' , header = 0)\n",
      "Fp11 = pd.read_csv(path + 'AtomPairs2DFingerprinter.csv'    , header = 0)\n",
      "Fp12 = pd.read_csv(path + 'AtomPairs2DFingerprintCount.csv' , header = 0)\n",
      "141/3: Fp1.shape\n",
      "141/4:\n",
      "df = pd.read_csv('df6_lipinski.csv')\n",
      "df\n",
      "141/5: name = df['compound']\n",
      "141/6: name.shape\n",
      "141/7:\n",
      "Fp1.Name = name\n",
      "Fp2.Name = name\n",
      "Fp3.Name = name\n",
      "Fp4.Name = name\n",
      "Fp5.Name = name\n",
      "Fp6.Name = name\n",
      "Fp7.Name = name\n",
      "Fp8.Name = name\n",
      "Fp9.Name = name\n",
      "Fp10.Name = name\n",
      "Fp11.Name = name\n",
      "Fp12.Name = name\n",
      "141/8:\n",
      "Fpp1 = Fp1.drop('Name', axis=1)\n",
      "Fpp2 = Fp2.drop('Name', axis=1)\n",
      "Fpp3 = Fp3.drop('Name', axis=1)\n",
      "Fpp4 = Fp4.drop('Name', axis=1)\n",
      "Fpp5 = Fp5.drop('Name', axis=1)\n",
      "Fpp6 = Fp6.drop('Name', axis=1)\n",
      "Fpp7 = Fp7.drop('Name', axis=1)\n",
      "Fpp8 = Fp8.drop('Name', axis=1)\n",
      "Fpp9 = Fp9.drop('Name', axis=1)\n",
      "Fpp10 = Fp10.drop('Name', axis=1)\n",
      "Fpp11 = Fp11.drop('Name', axis=1)\n",
      "Fpp12 = Fp12.drop('Name', axis=1)\n",
      "141/9:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)\n",
      "    Fp_normalized\n",
      "    \n",
      "    return Fp_normalized\n",
      "141/10:\n",
      "Fp1_n  = normalized (Fpp1 )\n",
      "Fp2_n  = normalized (Fpp2 )\n",
      "Fp3_n  = normalized (Fpp3 )\n",
      "Fp4_n  = normalized (Fpp4 )\n",
      "Fp5_n  = normalized (Fpp5 )\n",
      "Fp6_n  = normalized (Fpp6 )\n",
      "Fp7_n  = normalized (Fpp7 )\n",
      "Fp8_n  = normalized (Fpp8 )\n",
      "Fp9_n  = normalized (Fpp9 )\n",
      "Fp10_n = normalized (Fpp10)\n",
      "Fp11_n = normalized (Fpp11)\n",
      "Fp12_n = normalized (Fpp12)\n",
      "141/11: Fp1_n\n",
      "141/12:\n",
      "Fp1_n[\"Name\"] = Fp1.Name\n",
      "Fp1_n\n",
      "141/13:\n",
      "Fp1_n[\"Name\"] = Fp1.Name\n",
      "Fp1_n.set_index('Name', drop=True)\n",
      "141/14:\n",
      "Fp1_n[\"Name\"] = Fp1.Name\n",
      "Fp2_n[\"Name\"] = Fp2.Name\n",
      "Fp3_n[\"Name\"] = Fp3.Name\n",
      "Fp4_n[\"Name\"] = Fp4.Name\n",
      "Fp5_n[\"Name\"] = Fp5.Name\n",
      "Fp6_n[\"Name\"] = Fp6.Name\n",
      "Fp7_n[\"Name\"] = Fp7.Name\n",
      "Fp8_n[\"Name\"] = Fp8.Name\n",
      "Fp9_n[\"Name\"] = Fp9.Name\n",
      "Fp10_n[\"Name\"] = Fp10.Name\n",
      "Fp11_n[\"Name\"] = Fp11.Name\n",
      "141/15:\n",
      "Fp1_n[\"Name\"] = Fp1.Name\n",
      "Fp2_n[\"Name\"] = Fp2.Name\n",
      "Fp3_n[\"Name\"] = Fp3.Name\n",
      "Fp4_n[\"Name\"] = Fp4.Name\n",
      "Fp5_n[\"Name\"] = Fp5.Name\n",
      "Fp6_n[\"Name\"] = Fp6.Name\n",
      "Fp7_n[\"Name\"] = Fp7.Name\n",
      "Fp8_n[\"Name\"] = Fp8.Name\n",
      "Fp9_n[\"Name\"] = Fp9.Name\n",
      "Fp10_n[\"Name\"] = Fp10.Name\n",
      "Fp11_n[\"Name\"] = Fp11.Name\n",
      "Fp12_n[\"Name\"] = Fp12.Name\n",
      "141/16:\n",
      "Fp1_n.set_index('Name', drop=True)\n",
      "Fp2_n.set_index('Name', drop=True)\n",
      "Fp3_n.set_index('Name', drop=True)\n",
      "Fp4_n.set_index('Name', drop=True)\n",
      "Fp5_n.set_index('Name', drop=True)\n",
      "Fp6_n.set_index('Name', drop=True)\n",
      "Fp7_n.set_index('Name', drop=True)\n",
      "Fp8_n.set_index('Name', drop=True)\n",
      "Fp9_n.set_index('Name', drop=True)\n",
      "Fp10_n.set_index('Name', drop=True)\n",
      "Fp11_n.set_index('Name', drop=True)\n",
      "Fp12_n.set_index('Name', drop=True)\n",
      "141/17:\n",
      "Fp1_n .to_csv('Fp_normalized/Fingerprinter.csv'               , sep=',' ,index=False)\n",
      "Fp2_n .to_csv('Fp_normalized/ExtendedFingerprinter.csv'       , sep=',' ,index=False)\n",
      "Fp3_n .to_csv('Fp_normalized/EStateFingerprinter.csv'         , sep=',' ,index=False)\n",
      "Fp4_n .to_csv('Fp_normalized/GraphOnlyFingerprinter.csv'      , sep=',' ,index=False)\n",
      "Fp5_n .to_csv('Fp_normalized/MACCSFingerprinter.csv'          , sep=',' ,index=False)\n",
      "Fp6_n .to_csv('Fp_normalized/PubchemFingerprinter.csv'        , sep=',' ,index=False)\n",
      "Fp7_n .to_csv('Fp_normalized/SubstructureFingerprinter.csv'   , sep=',' ,index=False)\n",
      "Fp8_n .to_csv('Fp_normalized/SubstructureFingerprintCount.csv', sep=',' ,index=False)\n",
      "Fp9_n .to_csv('Fp_normalized/KlekotaRothFingerprinter.csv'    , sep=',' ,index=False)\n",
      "Fp10_n.to_csv('Fp_normalized/KlekotaRothFingerprintCount.csv' , sep=',' ,index=False)\n",
      "Fp11_n.to_csv('Fp_normalized/AtomPairs2DFingerprinter.csv'    , sep=',' ,index=False)\n",
      "Fp12_n.to_csv('Fp_normalized/AtomPairs2DFingerprintCount.csv' , sep=',' ,index=False)\n",
      "141/18:\n",
      "Fp1_n.set_index('Name', drop=True)\n",
      "Fp2_n.set_index('Name', drop=True)\n",
      "Fp3_n.set_index('Name', drop=True)\n",
      "Fp4_n.set_index('Name', drop=True)\n",
      "Fp5_n.set_index('Name', drop=True)\n",
      "Fp6_n.set_index('Name', drop=True)\n",
      "Fp7_n.set_index('Name', drop=True)\n",
      "Fp8_n.set_index('Name', drop=True)\n",
      "Fp9_n.set_index('Name', drop=True)\n",
      "Fp10_n.set_index('Name', drop=True)\n",
      "Fp11_n.set_index('Name', drop=True)\n",
      "Fp12_n.set_index('Name', drop=True)\n",
      "141/19: Fp12\n",
      "141/20: Fp12_n\n",
      "141/21: Fp1_n.set_index('Name', drop=True)\n",
      "141/22: Fp2_n.set_index('Name', drop=True)\n",
      "141/23:\n",
      "Fp1_n.set_index('Name', drop=True),\n",
      "Fp2_n.set_index('Name', drop=True)\n",
      "Fp3_n.set_index('Name', drop=True)\n",
      "Fp4_n.set_index('Name', drop=True)\n",
      "Fp5_n.set_index('Name', drop=True)\n",
      "Fp6_n.set_index('Name', drop=True)\n",
      "Fp7_n.set_index('Name', drop=True)\n",
      "Fp8_n.set_index('Name', drop=True)\n",
      "Fp9_n.set_index('Name', drop=True)\n",
      "Fp10_n.set_index('Name', drop=True)\n",
      "Fp11_n.set_index('Name', drop=True)\n",
      "Fp12_n.set_index('Name', drop=True)\n",
      "141/24:\n",
      "Fp1_n.set_index('Name', drop=True)\n",
      "Fp2_n.set_index('Name', drop=True)\n",
      "Fp3_n.set_index('Name', drop=True)\n",
      "Fp4_n.set_index('Name', drop=True)\n",
      "Fp5_n.set_index('Name', drop=True)\n",
      "Fp6_n.set_index('Name', drop=True)\n",
      "Fp7_n.set_index('Name', drop=True)\n",
      "Fp8_n.set_index('Name', drop=True)\n",
      "Fp9_n.set_index('Name', drop=True)\n",
      "Fp10_n.set_index('Name', drop=True)\n",
      "Fp11_n.set_index('Name', drop=True)\n",
      "Fp12_n.set_index('Name', drop=True)\n",
      "141/25: Fp12_n\n",
      "141/26: Fp2_n\n",
      "141/27: Fp1_n\n",
      "141/28: Fp1_n.set_index('Name', drop=True)\n",
      "141/29: Fp2_n.set_index('Name', drop=True)\n",
      "141/30:\n",
      "Fp2_n.set_index('Name', drop=True)\n",
      "Fp3_n.set_index('Name', drop=True)\n",
      "141/31:\n",
      "Fp2_n.set_index('Name', drop=True)\n",
      "Fp3_n.set_index('Name', drop=True)\n",
      "\n",
      "Fp3_n\n",
      "141/32: Fp2_n.set_index('Name', drop=True)\n",
      "141/33: Fp3_n.set_index('Name', drop=True)\n",
      "141/34: Fp1_n\n",
      "141/35:\n",
      "Fp1_n = Fp1_n.set_index('Name', drop=True)\n",
      "Fp2_n = Fp2_n.set_index('Name', drop=True)\n",
      "Fp3_n = Fp3_n.set_index('Name', drop=True)\n",
      "Fp4_n = Fp4_n.set_index('Name', drop=True)\n",
      "Fp5_n = Fp5_n.set_index('Name', drop=True)\n",
      "Fp6_n = Fp6_n.set_index('Name', drop=True)\n",
      "Fp7_n = Fp7_n.set_index('Name', drop=True)\n",
      "Fp8_n = Fp8_n.set_index('Name', drop=True)\n",
      "Fp9_n = Fp9_n.set_index('Name', drop=True)\n",
      "Fp10_n = Fp10_n.set_index('Name', drop=True)\n",
      "Fp11_n = Fp11_n.set_index('Name', drop=True)\n",
      "Fp12_n = Fp12_n.set_index('Name', drop=True)\n",
      "141/36:\n",
      "Fp1_n .to_csv('Fp_normalized/Fingerprinter.csv'               , sep=',' ,index=False)\n",
      "Fp2_n .to_csv('Fp_normalized/ExtendedFingerprinter.csv'       , sep=',' ,index=False)\n",
      "Fp3_n .to_csv('Fp_normalized/EStateFingerprinter.csv'         , sep=',' ,index=False)\n",
      "Fp4_n .to_csv('Fp_normalized/GraphOnlyFingerprinter.csv'      , sep=',' ,index=False)\n",
      "Fp5_n .to_csv('Fp_normalized/MACCSFingerprinter.csv'          , sep=',' ,index=False)\n",
      "Fp6_n .to_csv('Fp_normalized/PubchemFingerprinter.csv'        , sep=',' ,index=False)\n",
      "Fp7_n .to_csv('Fp_normalized/SubstructureFingerprinter.csv'   , sep=',' ,index=False)\n",
      "Fp8_n .to_csv('Fp_normalized/SubstructureFingerprintCount.csv', sep=',' ,index=False)\n",
      "Fp9_n .to_csv('Fp_normalized/KlekotaRothFingerprinter.csv'    , sep=',' ,index=False)\n",
      "Fp10_n.to_csv('Fp_normalized/KlekotaRothFingerprintCount.csv' , sep=',' ,index=False)\n",
      "Fp11_n.to_csv('Fp_normalized/AtomPairs2DFingerprinter.csv'    , sep=',' ,index=False)\n",
      "Fp12_n.to_csv('Fp_normalized/AtomPairs2DFingerprintCount.csv' , sep=',' ,index=False)\n",
      "141/37:\n",
      "Fp1_n = Fp1_n.set_index('Name', drop=True)\n",
      "Fp2_n = Fp2_n.set_index('Name', drop=True)\n",
      "Fp3_n = Fp3_n.set_index('Name', drop=True)\n",
      "Fp4_n = Fp4_n.set_index('Name', drop=True)\n",
      "Fp5_n = Fp5_n.set_index('Name', drop=True)\n",
      "Fp6_n = Fp6_n.set_index('Name', drop=True)\n",
      "Fp7_n = Fp7_n.set_index('Name', drop=True)\n",
      "Fp8_n = Fp8_n.set_index('Name', drop=True)\n",
      "Fp9_n = Fp9_n.set_index('Name', drop=True)\n",
      "Fp10_n = Fp10_n.set_index('Name', drop=True)\n",
      "Fp11_n = Fp11_n.set_index('Name', drop=True)\n",
      "Fp12_n = Fp12_n.set_index('Name', drop=True)\n",
      "141/38:\n",
      "Fp1_n[\"Name\"] = Fp1.Name\n",
      "Fp2_n[\"Name\"] = Fp2.Name\n",
      "Fp3_n[\"Name\"] = Fp3.Name\n",
      "Fp4_n[\"Name\"] = Fp4.Name\n",
      "Fp5_n[\"Name\"] = Fp5.Name\n",
      "Fp6_n[\"Name\"] = Fp6.Name\n",
      "Fp7_n[\"Name\"] = Fp7.Name\n",
      "Fp8_n[\"Name\"] = Fp8.Name\n",
      "Fp9_n[\"Name\"] = Fp9.Name\n",
      "Fp10_n[\"Name\"] = Fp10.Name\n",
      "Fp11_n[\"Name\"] = Fp11.Name\n",
      "Fp12_n[\"Name\"] = Fp12.Name\n",
      "141/39:\n",
      "Fp1_n = Fp1_n.set_index('Name', drop=True)\n",
      "Fp2_n = Fp2_n.set_index('Name', drop=True)\n",
      "Fp3_n = Fp3_n.set_index('Name', drop=True)\n",
      "Fp4_n = Fp4_n.set_index('Name', drop=True)\n",
      "Fp5_n = Fp5_n.set_index('Name', drop=True)\n",
      "Fp6_n = Fp6_n.set_index('Name', drop=True)\n",
      "Fp7_n = Fp7_n.set_index('Name', drop=True)\n",
      "Fp8_n = Fp8_n.set_index('Name', drop=True)\n",
      "Fp9_n = Fp9_n.set_index('Name', drop=True)\n",
      "Fp10_n = Fp10_n.set_index('Name', drop=True)\n",
      "Fp11_n = Fp11_n.set_index('Name', drop=True)\n",
      "Fp12_n = Fp12_n.set_index('Name', drop=True)\n",
      "141/40:\n",
      "Fp1_n .to_csv('Fp_normalized/Fingerprinter.csv'               , sep=',' ,index=False)\n",
      "Fp2_n .to_csv('Fp_normalized/ExtendedFingerprinter.csv'       , sep=',' ,index=False)\n",
      "Fp3_n .to_csv('Fp_normalized/EStateFingerprinter.csv'         , sep=',' ,index=False)\n",
      "Fp4_n .to_csv('Fp_normalized/GraphOnlyFingerprinter.csv'      , sep=',' ,index=False)\n",
      "Fp5_n .to_csv('Fp_normalized/MACCSFingerprinter.csv'          , sep=',' ,index=False)\n",
      "Fp6_n .to_csv('Fp_normalized/PubchemFingerprinter.csv'        , sep=',' ,index=False)\n",
      "Fp7_n .to_csv('Fp_normalized/SubstructureFingerprinter.csv'   , sep=',' ,index=False)\n",
      "Fp8_n .to_csv('Fp_normalized/SubstructureFingerprintCount.csv', sep=',' ,index=False)\n",
      "Fp9_n .to_csv('Fp_normalized/KlekotaRothFingerprinter.csv'    , sep=',' ,index=False)\n",
      "Fp10_n.to_csv('Fp_normalized/KlekotaRothFingerprintCount.csv' , sep=',' ,index=False)\n",
      "Fp11_n.to_csv('Fp_normalized/AtomPairs2DFingerprinter.csv'    , sep=',' ,index=False)\n",
      "Fp12_n.to_csv('Fp_normalized/AtomPairs2DFingerprintCount.csv' , sep=',' ,index=False)\n",
      "141/41: Fp12_n\n",
      "141/42:\n",
      "path = r'Fingerprint/'\n",
      "\n",
      "Fp1  = pd.read_csv(path + 'Fingerprinter.csv'               , header = 0)\n",
      "Fp2  = pd.read_csv(path + 'ExtendedFingerprinter.csv'       , header = 0)\n",
      "Fp3  = pd.read_csv(path + 'EStateFingerprinter.csv'         , header = 0)\n",
      "Fp4  = pd.read_csv(path + 'GraphOnlyFingerprinter.csv'      , header = 0)\n",
      "Fp5  = pd.read_csv(path + 'MACCSFingerprinter.csv'          , header = 0)\n",
      "Fp6  = pd.read_csv(path + 'PubchemFingerprinter.csv'        , header = 0)\n",
      "Fp7  = pd.read_csv(path + 'SubstructureFingerprinter.csv'   , header = 0)\n",
      "Fp8  = pd.read_csv(path + 'SubstructureFingerprintCount.csv', header = 0)\n",
      "Fp9  = pd.read_csv(path + 'KlekotaRothFingerprinter.csv'    , header = 0)\n",
      "Fp10 = pd.read_csv(path + 'KlekotaRothFingerprintCount.csv' , header = 0)\n",
      "Fp11 = pd.read_csv(path + 'AtomPairs2DFingerprinter.csv'    , header = 0)\n",
      "Fp12 = pd.read_csv(path + 'AtomPairs2DFingerprintCount.csv' , header = 0)\n",
      "141/43: Fp1.shape\n",
      "141/44:\n",
      "df = pd.read_csv('df6_lipinski.csv')\n",
      "df\n",
      "141/45: name = df['compound']\n",
      "141/46: name.shape\n",
      "141/47:\n",
      "Fp1.Name = name\n",
      "Fp2.Name = name\n",
      "Fp3.Name = name\n",
      "Fp4.Name = name\n",
      "Fp5.Name = name\n",
      "Fp6.Name = name\n",
      "Fp7.Name = name\n",
      "Fp8.Name = name\n",
      "Fp9.Name = name\n",
      "Fp10.Name = name\n",
      "Fp11.Name = name\n",
      "Fp12.Name = name\n",
      "141/48: Fp12\n",
      "141/49:\n",
      "Fpp1 = Fp1.drop('Name', axis=1)\n",
      "Fpp2 = Fp2.drop('Name', axis=1)\n",
      "Fpp3 = Fp3.drop('Name', axis=1)\n",
      "Fpp4 = Fp4.drop('Name', axis=1)\n",
      "Fpp5 = Fp5.drop('Name', axis=1)\n",
      "Fpp6 = Fp6.drop('Name', axis=1)\n",
      "Fpp7 = Fp7.drop('Name', axis=1)\n",
      "Fpp8 = Fp8.drop('Name', axis=1)\n",
      "Fpp9 = Fp9.drop('Name', axis=1)\n",
      "Fpp10 = Fp10.drop('Name', axis=1)\n",
      "Fpp11 = Fp11.drop('Name', axis=1)\n",
      "Fpp12 = Fp12.drop('Name', axis=1)\n",
      "141/50:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)\n",
      "    Fp_normalized\n",
      "    \n",
      "    return Fp_normalized\n",
      "141/51:\n",
      "Fp1_n  = normalized (Fpp1 )\n",
      "Fp2_n  = normalized (Fpp2 )\n",
      "Fp3_n  = normalized (Fpp3 )\n",
      "Fp4_n  = normalized (Fpp4 )\n",
      "Fp5_n  = normalized (Fpp5 )\n",
      "Fp6_n  = normalized (Fpp6 )\n",
      "Fp7_n  = normalized (Fpp7 )\n",
      "Fp8_n  = normalized (Fpp8 )\n",
      "Fp9_n  = normalized (Fpp9 )\n",
      "Fp10_n = normalized (Fpp10)\n",
      "Fp11_n = normalized (Fpp11)\n",
      "Fp12_n = normalized (Fpp12)\n",
      "141/52: Fp1_n\n",
      "141/53:\n",
      "Fp1_n[\"Name\"] = Fp1.Name\n",
      "Fp2_n[\"Name\"] = Fp2.Name\n",
      "Fp3_n[\"Name\"] = Fp3.Name\n",
      "Fp4_n[\"Name\"] = Fp4.Name\n",
      "Fp5_n[\"Name\"] = Fp5.Name\n",
      "Fp6_n[\"Name\"] = Fp6.Name\n",
      "Fp7_n[\"Name\"] = Fp7.Name\n",
      "Fp8_n[\"Name\"] = Fp8.Name\n",
      "Fp9_n[\"Name\"] = Fp9.Name\n",
      "Fp10_n[\"Name\"] = Fp10.Name\n",
      "Fp11_n[\"Name\"] = Fp11.Name\n",
      "Fp12_n[\"Name\"] = Fp12.Name\n",
      "141/54:\n",
      "Fp1_n = Fp1_n.set_index('Name', drop=True)\n",
      "Fp2_n = Fp2_n.set_index('Name', drop=True)\n",
      "Fp3_n = Fp3_n.set_index('Name', drop=True)\n",
      "Fp4_n = Fp4_n.set_index('Name', drop=True)\n",
      "Fp5_n = Fp5_n.set_index('Name', drop=True)\n",
      "Fp6_n = Fp6_n.set_index('Name', drop=True)\n",
      "Fp7_n = Fp7_n.set_index('Name', drop=True)\n",
      "Fp8_n = Fp8_n.set_index('Name', drop=True)\n",
      "Fp9_n = Fp9_n.set_index('Name', drop=True)\n",
      "Fp10_n = Fp10_n.set_index('Name', drop=True)\n",
      "Fp11_n = Fp11_n.set_index('Name', drop=True)\n",
      "Fp12_n = Fp12_n.set_index('Name', drop=True)\n",
      "141/55:\n",
      "Fp1_n .to_csv('Fp_normalized/Fingerprinter.csv'               , sep=',' ,index=False)\n",
      "Fp2_n .to_csv('Fp_normalized/ExtendedFingerprinter.csv'       , sep=',' ,index=False)\n",
      "Fp3_n .to_csv('Fp_normalized/EStateFingerprinter.csv'         , sep=',' ,index=False)\n",
      "Fp4_n .to_csv('Fp_normalized/GraphOnlyFingerprinter.csv'      , sep=',' ,index=False)\n",
      "Fp5_n .to_csv('Fp_normalized/MACCSFingerprinter.csv'          , sep=',' ,index=False)\n",
      "Fp6_n .to_csv('Fp_normalized/PubchemFingerprinter.csv'        , sep=',' ,index=False)\n",
      "Fp7_n .to_csv('Fp_normalized/SubstructureFingerprinter.csv'   , sep=',' ,index=False)\n",
      "Fp8_n .to_csv('Fp_normalized/SubstructureFingerprintCount.csv', sep=',' ,index=False)\n",
      "Fp9_n .to_csv('Fp_normalized/KlekotaRothFingerprinter.csv'    , sep=',' ,index=False)\n",
      "Fp10_n.to_csv('Fp_normalized/KlekotaRothFingerprintCount.csv' , sep=',' ,index=False)\n",
      "Fp11_n.to_csv('Fp_normalized/AtomPairs2DFingerprinter.csv'    , sep=',' ,index=False)\n",
      "Fp12_n.to_csv('Fp_normalized/AtomPairs2DFingerprintCount.csv' , sep=',' ,index=False)\n",
      "141/56: Fp12_n\n",
      "141/57:\n",
      "Fp1_n[\"Name\"] = Fp1.Name\n",
      "Fp2_n[\"Name\"] = Fp2.Name\n",
      "Fp3_n[\"Name\"] = Fp3.Name\n",
      "Fp4_n[\"Name\"] = Fp4.Name\n",
      "Fp5_n[\"Name\"] = Fp5.Name\n",
      "Fp6_n[\"Name\"] = Fp6.Name\n",
      "Fp7_n[\"Name\"] = Fp7.Name\n",
      "Fp8_n[\"Name\"] = Fp8.Name\n",
      "Fp9_n[\"Name\"] = Fp9.Name\n",
      "Fp10_n[\"Name\"] = Fp10.Name\n",
      "Fp11_n[\"Name\"] = Fp11.Name\n",
      "Fp12_n[\"Name\"] = Fp12.Name\n",
      "141/58: Fp12_n\n",
      "141/59:\n",
      "Fp1_n = Fp1_n.set_index('Name', drop=True)\n",
      "Fp2_n = Fp2_n.set_index('Name', drop=True)\n",
      "Fp3_n = Fp3_n.set_index('Name', drop=True)\n",
      "Fp4_n = Fp4_n.set_index('Name', drop=True)\n",
      "Fp5_n = Fp5_n.set_index('Name', drop=True)\n",
      "Fp6_n = Fp6_n.set_index('Name', drop=True)\n",
      "Fp7_n = Fp7_n.set_index('Name', drop=True)\n",
      "Fp8_n = Fp8_n.set_index('Name', drop=True)\n",
      "Fp9_n = Fp9_n.set_index('Name', drop=True)\n",
      "Fp10_n = Fp10_n.set_index('Name', drop=True)\n",
      "Fp11_n = Fp11_n.set_index('Name', drop=True)\n",
      "Fp12_n = Fp12_n.set_index('Name', drop=True)\n",
      "141/60: Fp12_n\n",
      "141/61: Fp1_n\n",
      "141/62:\n",
      "Fp1_n[\"Name\"] = Fp1.Name\n",
      "Fp2_n[\"Name\"] = Fp2.Name\n",
      "Fp3_n[\"Name\"] = Fp3.Name\n",
      "Fp4_n[\"Name\"] = Fp4.Name\n",
      "Fp5_n[\"Name\"] = Fp5.Name\n",
      "Fp6_n[\"Name\"] = Fp6.Name\n",
      "Fp7_n[\"Name\"] = Fp7.Name\n",
      "Fp8_n[\"Name\"] = Fp8.Name\n",
      "Fp9_n[\"Name\"] = Fp9.Name\n",
      "Fp10_n[\"Name\"] = Fp10.Name\n",
      "Fp11_n[\"Name\"] = Fp11.Name\n",
      "Fp12_n[\"Name\"] = Fp12.Name\n",
      "141/63: Fp1_n\n",
      "141/64:\n",
      "path = r'Fingerprint/'\n",
      "\n",
      "Fp1  = pd.read_csv(path + 'Fingerprinter.csv'               , header = 0)\n",
      "Fp2  = pd.read_csv(path + 'ExtendedFingerprinter.csv'       , header = 0)\n",
      "Fp3  = pd.read_csv(path + 'EStateFingerprinter.csv'         , header = 0)\n",
      "Fp4  = pd.read_csv(path + 'GraphOnlyFingerprinter.csv'      , header = 0)\n",
      "Fp5  = pd.read_csv(path + 'MACCSFingerprinter.csv'          , header = 0)\n",
      "Fp6  = pd.read_csv(path + 'PubchemFingerprinter.csv'        , header = 0)\n",
      "Fp7  = pd.read_csv(path + 'SubstructureFingerprinter.csv'   , header = 0)\n",
      "Fp8  = pd.read_csv(path + 'SubstructureFingerprintCount.csv', header = 0)\n",
      "Fp9  = pd.read_csv(path + 'KlekotaRothFingerprinter.csv'    , header = 0)\n",
      "Fp10 = pd.read_csv(path + 'KlekotaRothFingerprintCount.csv' , header = 0)\n",
      "Fp11 = pd.read_csv(path + 'AtomPairs2DFingerprinter.csv'    , header = 0)\n",
      "Fp12 = pd.read_csv(path + 'AtomPairs2DFingerprintCount.csv' , header = 0)\n",
      "141/65: Fp1.shape\n",
      "141/66:\n",
      "df = pd.read_csv('df6_lipinski.csv')\n",
      "df\n",
      "141/67: name = df['compound']\n",
      "141/68: name.shape\n",
      "141/69:\n",
      "Fp1.Name = name\n",
      "Fp2.Name = name\n",
      "Fp3.Name = name\n",
      "Fp4.Name = name\n",
      "Fp5.Name = name\n",
      "Fp6.Name = name\n",
      "Fp7.Name = name\n",
      "Fp8.Name = name\n",
      "Fp9.Name = name\n",
      "Fp10.Name = name\n",
      "Fp11.Name = name\n",
      "Fp12.Name = name\n",
      "141/70:\n",
      "Fpp1 = Fp1.drop('Name', axis=1)\n",
      "Fpp2 = Fp2.drop('Name', axis=1)\n",
      "Fpp3 = Fp3.drop('Name', axis=1)\n",
      "Fpp4 = Fp4.drop('Name', axis=1)\n",
      "Fpp5 = Fp5.drop('Name', axis=1)\n",
      "Fpp6 = Fp6.drop('Name', axis=1)\n",
      "Fpp7 = Fp7.drop('Name', axis=1)\n",
      "Fpp8 = Fp8.drop('Name', axis=1)\n",
      "Fpp9 = Fp9.drop('Name', axis=1)\n",
      "Fpp10 = Fp10.drop('Name', axis=1)\n",
      "Fpp11 = Fp11.drop('Name', axis=1)\n",
      "Fpp12 = Fp12.drop('Name', axis=1)\n",
      "141/71:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)\n",
      "    Fp_normalized\n",
      "    \n",
      "    return Fp_normalized\n",
      "141/72:\n",
      "Fp1_n  = normalized (Fpp1 )\n",
      "Fp2_n  = normalized (Fpp2 )\n",
      "Fp3_n  = normalized (Fpp3 )\n",
      "Fp4_n  = normalized (Fpp4 )\n",
      "Fp5_n  = normalized (Fpp5 )\n",
      "Fp6_n  = normalized (Fpp6 )\n",
      "Fp7_n  = normalized (Fpp7 )\n",
      "Fp8_n  = normalized (Fpp8 )\n",
      "Fp9_n  = normalized (Fpp9 )\n",
      "Fp10_n = normalized (Fpp10)\n",
      "Fp11_n = normalized (Fpp11)\n",
      "Fp12_n = normalized (Fpp12)\n",
      "141/73: Fp1_n\n",
      "141/74:\n",
      "Fp1_n[\"Name\"] = Fp1.Name\n",
      "Fp2_n[\"Name\"] = Fp2.Name\n",
      "Fp3_n[\"Name\"] = Fp3.Name\n",
      "Fp4_n[\"Name\"] = Fp4.Name\n",
      "Fp5_n[\"Name\"] = Fp5.Name\n",
      "Fp6_n[\"Name\"] = Fp6.Name\n",
      "Fp7_n[\"Name\"] = Fp7.Name\n",
      "Fp8_n[\"Name\"] = Fp8.Name\n",
      "Fp9_n[\"Name\"] = Fp9.Name\n",
      "Fp10_n[\"Name\"] = Fp10.Name\n",
      "Fp11_n[\"Name\"] = Fp11.Name\n",
      "Fp12_n[\"Name\"] = Fp12.Name\n",
      "141/75: Fp1_n\n",
      "141/76: Fp1_n = Fp1_n.set_index('Name')\n",
      "141/77: Fp1_n\n",
      "141/78:\n",
      "Fp1_n = Fp1_n.set_index('Name')\n",
      "Fp2_n = Fp2_n.set_index('Name')\n",
      "Fp3_n = Fp3_n.set_index('Name')\n",
      "Fp4_n = Fp4_n.set_index('Name')\n",
      "Fp5_n = Fp5_n.set_index('Name')\n",
      "Fp6_n = Fp6_n.set_index('Name')\n",
      "Fp7_n = Fp7_n.set_index('Name')\n",
      "Fp8_n = Fp8_n.set_index('Name')\n",
      "Fp9_n = Fp9_n.set_index('Name')\n",
      "Fp10_n = Fp10_n.set_index('Name')\n",
      "Fp11_n = Fp11_n.set_index('Name')\n",
      "Fp12_n = Fp12_n.set_index('Name')\n",
      "141/79:\n",
      "Fp1_n[\"Name\"] = Fp1.Name\n",
      "Fp2_n[\"Name\"] = Fp2.Name\n",
      "Fp3_n[\"Name\"] = Fp3.Name\n",
      "Fp4_n[\"Name\"] = Fp4.Name\n",
      "Fp5_n[\"Name\"] = Fp5.Name\n",
      "Fp6_n[\"Name\"] = Fp6.Name\n",
      "Fp7_n[\"Name\"] = Fp7.Name\n",
      "Fp8_n[\"Name\"] = Fp8.Name\n",
      "Fp9_n[\"Name\"] = Fp9.Name\n",
      "Fp10_n[\"Name\"] = Fp10.Name\n",
      "Fp11_n[\"Name\"] = Fp11.Name\n",
      "Fp12_n[\"Name\"] = Fp12.Name\n",
      "141/80: Fp1_n\n",
      "141/81:\n",
      "Fp1_n = Fp1_n.set_index('Name')\n",
      "Fp2_n = Fp2_n.set_index('Name')\n",
      "Fp3_n = Fp3_n.set_index('Name')\n",
      "Fp4_n = Fp4_n.set_index('Name')\n",
      "Fp5_n = Fp5_n.set_index('Name')\n",
      "Fp6_n = Fp6_n.set_index('Name')\n",
      "Fp7_n = Fp7_n.set_index('Name')\n",
      "Fp8_n = Fp8_n.set_index('Name')\n",
      "Fp9_n = Fp9_n.set_index('Name')\n",
      "Fp10_n = Fp10_n.set_index('Name')\n",
      "Fp11_n = Fp11_n.set_index('Name')\n",
      "Fp12_n = Fp12_n.set_index('Name')\n",
      "141/82: Fp1_n\n",
      "141/83:\n",
      "Fp1_n[\"Name\"] = Fp1.Name\n",
      "Fp2_n[\"Name\"] = Fp2.Name\n",
      "Fp3_n[\"Name\"] = Fp3.Name\n",
      "Fp4_n[\"Name\"] = Fp4.Name\n",
      "Fp5_n[\"Name\"] = Fp5.Name\n",
      "Fp6_n[\"Name\"] = Fp6.Name\n",
      "Fp7_n[\"Name\"] = Fp7.Name\n",
      "Fp8_n[\"Name\"] = Fp8.Name\n",
      "Fp9_n[\"Name\"] = Fp9.Name\n",
      "Fp10_n[\"Name\"] = Fp10.Name\n",
      "Fp11_n[\"Name\"] = Fp11.Name\n",
      "Fp12_n[\"Name\"] = Fp12.Name\n",
      "141/84:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "141/85:\n",
      "path = r'Fingerprint/'\n",
      "\n",
      "Fp1  = pd.read_csv(path + 'Fingerprinter.csv'               , header = 0)\n",
      "Fp2  = pd.read_csv(path + 'ExtendedFingerprinter.csv'       , header = 0)\n",
      "Fp3  = pd.read_csv(path + 'EStateFingerprinter.csv'         , header = 0)\n",
      "Fp4  = pd.read_csv(path + 'GraphOnlyFingerprinter.csv'      , header = 0)\n",
      "Fp5  = pd.read_csv(path + 'MACCSFingerprinter.csv'          , header = 0)\n",
      "Fp6  = pd.read_csv(path + 'PubchemFingerprinter.csv'        , header = 0)\n",
      "Fp7  = pd.read_csv(path + 'SubstructureFingerprinter.csv'   , header = 0)\n",
      "Fp8  = pd.read_csv(path + 'SubstructureFingerprintCount.csv', header = 0)\n",
      "Fp9  = pd.read_csv(path + 'KlekotaRothFingerprinter.csv'    , header = 0)\n",
      "Fp10 = pd.read_csv(path + 'KlekotaRothFingerprintCount.csv' , header = 0)\n",
      "Fp11 = pd.read_csv(path + 'AtomPairs2DFingerprinter.csv'    , header = 0)\n",
      "Fp12 = pd.read_csv(path + 'AtomPairs2DFingerprintCount.csv' , header = 0)\n",
      "141/86: Fp1.shape\n",
      "141/87:\n",
      "df = pd.read_csv('df6_lipinski.csv')\n",
      "df\n",
      "141/88: name = df['compound']\n",
      "141/89:\n",
      "path = r'Fingerprint/'\n",
      "\n",
      "Fp1  = pd.read_csv(path + 'Fingerprinter.csv'               , header = 0)\n",
      "Fp2  = pd.read_csv(path + 'ExtendedFingerprinter.csv'       , header = 0)\n",
      "Fp3  = pd.read_csv(path + 'EStateFingerprinter.csv'         , header = 0)\n",
      "Fp4  = pd.read_csv(path + 'GraphOnlyFingerprinter.csv'      , header = 0)\n",
      "Fp5  = pd.read_csv(path + 'MACCSFingerprinter.csv'          , header = 0)\n",
      "Fp6  = pd.read_csv(path + 'PubchemFingerprinter.csv'        , header = 0)\n",
      "Fp7  = pd.read_csv(path + 'SubstructureFingerprinter.csv'   , header = 0)\n",
      "Fp8  = pd.read_csv(path + 'SubstructureFingerprintCount.csv', header = 0)\n",
      "Fp9  = pd.read_csv(path + 'KlekotaRothFingerprinter.csv'    , header = 0)\n",
      "Fp10 = pd.read_csv(path + 'KlekotaRothFingerprintCount.csv' , header = 0)\n",
      "Fp11 = pd.read_csv(path + 'AtomPairs2DFingerprinter.csv'    , header = 0)\n",
      "Fp12 = pd.read_csv(path + 'AtomPairs2DFingerprintCount.csv' , header = 0)\n",
      "141/90: Fp1.shape\n",
      "141/91:\n",
      "df = pd.read_csv('df6_lipinski.csv')\n",
      "df\n",
      "141/92: name = df['compound']\n",
      "141/93: name.shape\n",
      "141/94:\n",
      "Fp1.Name = name\n",
      "Fp2.Name = name\n",
      "Fp3.Name = name\n",
      "Fp4.Name = name\n",
      "Fp5.Name = name\n",
      "Fp6.Name = name\n",
      "Fp7.Name = name\n",
      "Fp8.Name = name\n",
      "Fp9.Name = name\n",
      "Fp10.Name = name\n",
      "Fp11.Name = name\n",
      "Fp12.Name = name\n",
      "141/95:\n",
      "Fpp1 = Fp1.drop('Name', axis=1)\n",
      "Fpp2 = Fp2.drop('Name', axis=1)\n",
      "Fpp3 = Fp3.drop('Name', axis=1)\n",
      "Fpp4 = Fp4.drop('Name', axis=1)\n",
      "Fpp5 = Fp5.drop('Name', axis=1)\n",
      "Fpp6 = Fp6.drop('Name', axis=1)\n",
      "Fpp7 = Fp7.drop('Name', axis=1)\n",
      "Fpp8 = Fp8.drop('Name', axis=1)\n",
      "Fpp9 = Fp9.drop('Name', axis=1)\n",
      "Fpp10 = Fp10.drop('Name', axis=1)\n",
      "Fpp11 = Fp11.drop('Name', axis=1)\n",
      "Fpp12 = Fp12.drop('Name', axis=1)\n",
      "141/96:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)\n",
      "    Fp_normalized\n",
      "    \n",
      "    return Fp_normalized\n",
      "141/97:\n",
      "Fp1_n  = normalized (Fpp1 )\n",
      "Fp2_n  = normalized (Fpp2 )\n",
      "Fp3_n  = normalized (Fpp3 )\n",
      "Fp4_n  = normalized (Fpp4 )\n",
      "Fp5_n  = normalized (Fpp5 )\n",
      "Fp6_n  = normalized (Fpp6 )\n",
      "Fp7_n  = normalized (Fpp7 )\n",
      "Fp8_n  = normalized (Fpp8 )\n",
      "Fp9_n  = normalized (Fpp9 )\n",
      "Fp10_n = normalized (Fpp10)\n",
      "Fp11_n = normalized (Fpp11)\n",
      "Fp12_n = normalized (Fpp12)\n",
      "141/98: Fp1_n\n",
      "141/99:\n",
      "Fp1_n[\"Name\"] = Fp1.Name\n",
      "Fp2_n[\"Name\"] = Fp2.Name\n",
      "Fp3_n[\"Name\"] = Fp3.Name\n",
      "Fp4_n[\"Name\"] = Fp4.Name\n",
      "Fp5_n[\"Name\"] = Fp5.Name\n",
      "Fp6_n[\"Name\"] = Fp6.Name\n",
      "Fp7_n[\"Name\"] = Fp7.Name\n",
      "Fp8_n[\"Name\"] = Fp8.Name\n",
      "Fp9_n[\"Name\"] = Fp9.Name\n",
      "Fp10_n[\"Name\"] = Fp10.Name\n",
      "Fp11_n[\"Name\"] = Fp11.Name\n",
      "Fp12_n[\"Name\"] = Fp12.Name\n",
      "141/100: Fp1_n\n",
      "141/101:\n",
      "Fp1_n = Fp1_n.set_index('Name')\n",
      "Fp2_n = Fp2_n.set_index('Name')\n",
      "Fp3_n = Fp3_n.set_index('Name')\n",
      "Fp4_n = Fp4_n.set_index('Name')\n",
      "Fp5_n = Fp5_n.set_index('Name')\n",
      "Fp6_n = Fp6_n.set_index('Name')\n",
      "Fp7_n = Fp7_n.set_index('Name')\n",
      "Fp8_n = Fp8_n.set_index('Name')\n",
      "Fp9_n = Fp9_n.set_index('Name')\n",
      "Fp10_n = Fp10_n.set_index('Name')\n",
      "Fp11_n = Fp11_n.set_index('Name')\n",
      "Fp12_n = Fp12_n.set_index('Name')\n",
      "141/102: Fp1_n\n",
      "141/103:\n",
      "Fp1_n .to_csv('Fp_normalized/Fingerprinter.csv'               , sep=',' ,index=False)\n",
      "Fp2_n .to_csv('Fp_normalized/ExtendedFingerprinter.csv'       , sep=',' ,index=False)\n",
      "Fp3_n .to_csv('Fp_normalized/EStateFingerprinter.csv'         , sep=',' ,index=False)\n",
      "Fp4_n .to_csv('Fp_normalized/GraphOnlyFingerprinter.csv'      , sep=',' ,index=False)\n",
      "Fp5_n .to_csv('Fp_normalized/MACCSFingerprinter.csv'          , sep=',' ,index=False)\n",
      "Fp6_n .to_csv('Fp_normalized/PubchemFingerprinter.csv'        , sep=',' ,index=False)\n",
      "Fp7_n .to_csv('Fp_normalized/SubstructureFingerprinter.csv'   , sep=',' ,index=False)\n",
      "Fp8_n .to_csv('Fp_normalized/SubstructureFingerprintCount.csv', sep=',' ,index=False)\n",
      "Fp9_n .to_csv('Fp_normalized/KlekotaRothFingerprinter.csv'    , sep=',' ,index=False)\n",
      "Fp10_n.to_csv('Fp_normalized/KlekotaRothFingerprintCount.csv' , sep=',' ,index=False)\n",
      "Fp11_n.to_csv('Fp_normalized/AtomPairs2DFingerprinter.csv'    , sep=',' ,index=False)\n",
      "Fp12_n.to_csv('Fp_normalized/AtomPairs2DFingerprintCount.csv' , sep=',' ,index=False)\n",
      "141/104: Fp1_n\n",
      "141/105: Fp2_n\n",
      "141/106: raw1  = df.merge(Fp1_n , on='Name', how='outer')\n",
      "141/107: df.Name = name\n",
      "141/108: df.Name = Name\n",
      "141/109: df.Name = name\n",
      "141/110:\n",
      "df.Name = name\n",
      "df\n",
      "141/111:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df2\n",
      "141/112:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df2.drop('Unnamed: 0')\n",
      "141/113:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df2.drop('Unnamed: 0', axis=1)\n",
      "141/114:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df2 = df2.drop('Unnamed: 0', axis=1)\n",
      "df3 = df2.set_index('Name')\n",
      "141/115:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df2 = df2.drop('Unnamed: 0', axis=1)\n",
      "df3 = df2.set_index('Name')\n",
      "df3\n",
      "141/116: raw1  = df.merge(Fp1_n , on='Name', how='outer')\n",
      "141/117: raw1  = df.merge(Fp1_n, df3 , on='Name', how='outer')\n",
      "141/118: raw1\n",
      "141/119: raw1  = df.merge(Fp1_n, df3 , on='Name', how='outer')\n",
      "141/120: raw1\n",
      "141/121: raw1  = df3.merge(Fp1_n, on='Name', how='outer')\n",
      "141/122: raw1\n",
      "141/123: df4 = df3['LogKp']\n",
      "141/124: LogKp = df3['LogKp']\n",
      "141/125:\n",
      "LogKp = df3['LogKp']\n",
      "LogKp\n",
      "141/126: raw1  = LogKp.merge(Fp1_n, on='Name', how='outer')\n",
      "141/127:\n",
      "LogKp = Datafram.df3['LogKp']\n",
      "LogKp\n",
      "141/128:\n",
      "LogKp = df3['LogKp'].Dataframe\n",
      "LogKp\n",
      "141/129:\n",
      "LogKp = df3['LogKp']\n",
      "LogKp = LogKp.to_frame\n",
      "141/130:\n",
      "LogKp = df3['LogKp']\n",
      "LogKp = LogKp.to_frame\n",
      "LogKp\n",
      "141/131:\n",
      "LogKp = df3['LogKp']\n",
      "LogKp = LogKp.to_frame()\n",
      "LogKp\n",
      "141/132: raw1  = LogKp.merge(Fp1_n, on='Name', how='outer')\n",
      "141/133:\n",
      "raw1  = LogKp.merge(Fp1_n, on='Name', how='outer')\n",
      "raw2  = LogKp.merge(Fp2_n, on='Name', how='outer')\n",
      "raw3  = LogKp.merge(Fp3_n, on='Name', how='outer')\n",
      "raw4  = LogKp.merge(Fp4_n, on='Name', how='outer')\n",
      "raw5  = LogKp.merge(Fp5_n, on='Name', how='outer')\n",
      "raw6  = LogKp.merge(Fp6_n, on='Name', how='outer')\n",
      "raw7  = LogKp.merge(Fp7_n, on='Name', how='outer')\n",
      "raw8  = LogKp.merge(Fp8_n, on='Name', how='outer')\n",
      "raw9  = LogKp.merge(Fp9_n, on='Name', how='outer')\n",
      "raw10  = LogKp.merge(Fp10_n, on='Name', how='outer')\n",
      "raw11  = LogKp.merge(Fp11_n, on='Name', how='outer')\n",
      "raw12  = LogKp.merge(Fp12_n, on='Name', how='outer')\n",
      "141/134: raw1\n",
      "141/135:\n",
      "raw1 .to_csv('QSAR/Kp_Fingerprinter.csv'               , sep=',' ,index=True)\n",
      "raw2 .to_csv('QSAR/Kp_ExtendedFingerprinter.csv'       , sep=',' ,index=True)\n",
      "raw3 .to_csv('QSAR/Kp_EStateFingerprinter.csv'         , sep=',' ,index=True)\n",
      "raw4 .to_csv('QSAR/Kp_GraphOnlyFingerprinter.csv'      , sep=',' ,index=True)\n",
      "raw5 .to_csv('QSAR/Kp_MACCSFingerprinter.csv'          , sep=',' ,index=True)\n",
      "raw6 .to_csv('QSAR/Kp_PubchemFingerprinter.csv'        , sep=',' ,index=True)\n",
      "raw7 .to_csv('QSAR/Kp_SubstructureFingerprinter.csv'   , sep=',' ,index=True)\n",
      "raw8 .to_csv('QSAR/Kp_SubstructureFingerprintCount.csv', sep=',' ,index=True)\n",
      "raw9 .to_csv('QSAR/Kp_KlekotaRothFingerprinter.csv'    , sep=',' ,index=True)\n",
      "raw10.to_csv('QSAR/Kp_KlekotaRothFingerprintCount.csv' , sep=',' ,index=True)\n",
      "raw11.to_csv('QSAR/Kp_AtomPairs2DFingerprinter.csv'    , sep=',' ,index=True)\n",
      "raw12.to_csv('QSAR/Kp_AtomPairs2DFingerprintCount.csv' , sep=',' ,index=True)\n",
      "141/136:\n",
      "print (len(raw1 ),len(raw2 ),len(raw3 ),len(raw4 ),len(raw5 )\n",
      "      ,len(raw6 ),len(raw7 ),len(raw8 ),len(raw9 ),len(raw10)\n",
      "      ,len(raw11),len(raw12))\n",
      "141/137:\n",
      "print (len(raw1 .columns),len(raw2 .columns),len(raw3 .columns),len(raw4 .columns),len(raw5 )\n",
      "      ,len(raw6 .columns),len(raw7 .columns),len(raw8 .columns),len(raw9 .columns),len(raw10)\n",
      "      ,len(raw11.columns),len(raw12.columns))\n",
      "141/138:\n",
      "import numpy as np\n",
      "\n",
      "def Remove_useless_descriptor(df, threshold):\n",
      "    \n",
      "    des1 = len(df.columns) \n",
      "    \n",
      "    h = df.columns.tolist()\n",
      "    df = df.as_matrix().astype(np.float)\n",
      "    df = np.array(df)\n",
      "\n",
      "    STDEV = np.std(df, axis=0)\n",
      "    idx = [idx for idx, val in enumerate(STDEV) if val > threshold]\n",
      "    df2 = df[:,idx]\n",
      "    hx = np.array(h)[idx]\n",
      "    \n",
      "    df = pd.DataFrame(df2, columns=[hx])\n",
      "    \n",
      "    des2 = len(df.columns)\n",
      "    \n",
      "    print 'from Remove useless descriptor'\n",
      "    print \"The initial set of \" + str(des1) + \\\n",
      "          \" descriptors has been reduced to \" + str(des2) + \" descriptors.\"\n",
      "    \n",
      "    return df, des1, des2\n",
      "141/139:\n",
      "import numpy as np\n",
      "\n",
      "def Remove_useless_descriptor(df, threshold):\n",
      "    \n",
      "    des1 = len(df.columns) \n",
      "    \n",
      "    h = df.columns.tolist()\n",
      "    df = df.as_matrix().astype(np.float)\n",
      "    df = np.array(df)\n",
      "\n",
      "    STDEV = np.std(df, axis=0)\n",
      "    idx = [idx for idx, val in enumerate(STDEV) if val > threshold]\n",
      "    df2 = df[:,idx]\n",
      "    hx = np.array(h)[idx]\n",
      "    \n",
      "    df = pd.DataFrame(df2, columns=[hx])\n",
      "    \n",
      "    des2 = len(df.columns)\n",
      "    \n",
      "    print('from Remove useless descriptor')\n",
      "    print(\"The initial set of \" + str(des1) + \\\n",
      "          \" descriptors has been reduced to \" + str(des2) + \" descriptors.\")\n",
      "    \n",
      "    return df, des1, des2\n",
      "141/140:\n",
      "from scipy import stats\n",
      "\n",
      "def correlation(df, threshold):\n",
      "\n",
      "    des3 = len(df.columns) \n",
      "    corr = stats.pearsonr\n",
      "    col_corr = set() # Set of all the names of deleted columns\n",
      "    corr_matrix = df.corr()\n",
      "    for i in range(len(corr_matrix.columns)):\n",
      "        for j in range(i):\n",
      "            if corr_matrix.iloc[i, j] >= threshold:\n",
      "                colname = corr_matrix.columns[i] # getting the name of column\n",
      "                col_corr.add(colname)\n",
      "                if colname in df.columns:\n",
      "                    del df[colname] # deleting the column from the dataset\n",
      "    des4 = len(df.columns) \n",
      "\n",
      "    print 'from Remove correlation'\n",
      "    print (\"The initial set of \" + str(des3) + ' descriptors'+ \n",
      "           \" has been reduced to \" + str(des4) + \" descriptors.\")\n",
      "\n",
      "    return df, des3, des4\n",
      "141/141:\n",
      "from scipy import stats\n",
      "\n",
      "def correlation(df, threshold):\n",
      "\n",
      "    des3 = len(df.columns) \n",
      "    corr = stats.pearsonr\n",
      "    col_corr = set() # Set of all the names of deleted columns\n",
      "    corr_matrix = df.corr()\n",
      "    for i in range(len(corr_matrix.columns)):\n",
      "        for j in range(i):\n",
      "            if corr_matrix.iloc[i, j] >= threshold:\n",
      "                colname = corr_matrix.columns[i] # getting the name of column\n",
      "                col_corr.add(colname)\n",
      "                if colname in df.columns:\n",
      "                    del df[colname] # deleting the column from the dataset\n",
      "    des4 = len(df.columns) \n",
      "\n",
      "    print ('from Remove correlation')\n",
      "    print (\"The initial set of \" + str(des3) + ' descriptors'+ \n",
      "           \" has been reduced to \" + str(des4) + \" descriptors.\")\n",
      "\n",
      "    return df, des3, des4\n",
      "141/142:\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.cross_validation import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn import cross_validation\n",
      "from collections import defaultdict\n",
      "\n",
      "def build_model(X, Y, seed, hx, f):\n",
      "    \n",
      "    \n",
      "    #Data split using 70/30 ratio\n",
      "    X_internal, X_external, Y_internal, Y_external = train_test_split(X,\n",
      "                                                        Y, test_size=0.3,\n",
      "                                                        random_state=seed)\n",
      "\n",
      "    # Training set\n",
      "    rf = RandomForestRegressor(n_estimators=400, \n",
      "                               max_features='sqrt', \n",
      "                               min_samples_leaf = 1,\n",
      "                               random_state = 13,\n",
      "                               n_jobs=-1)\n",
      "    \n",
      "    rf.fit(X_internal,Y_internal)\n",
      "    prediction = rf.predict(X_internal)\n",
      "    \n",
      "    # Cross-validation\n",
      "    cv = cross_validation.cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)\n",
      "    \n",
      "    # External set  \n",
      "    prediction_external = rf.predict(X_external)\n",
      "    \n",
      "    #print result from each seed    \n",
      "    R2_train.append(r2_score(Y_internal, prediction))\n",
      "    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))\n",
      "    Q2_CV.append(r2_score(Y_internal, cv))\n",
      "    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))\n",
      "    Q2_External.append(r2_score(Y_external, prediction_external))\n",
      "    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))\n",
      "    \n",
      "    #Feature Importance\n",
      "    Feature = hx[:]\n",
      "    feature_importance = rf.feature_importances_\n",
      "    importances = 100.0 * (feature_importance / feature_importance.max()) #index\n",
      "    \n",
      "    for i, fx in enumerate(Feature):\n",
      "        importances_dict[fx].append(importances[i])\n",
      "    \n",
      "    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict\n",
      "141/143:\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.cross_validation import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn import cross_validation\n",
      "from collections import defaultdict\n",
      "\n",
      "def build_model(X, Y, seed, hx, f):\n",
      "    \n",
      "    \n",
      "    #Data split using 70/30 ratio\n",
      "    X_internal, X_external, Y_internal, Y_external = train_test_split(X,\n",
      "                                                        Y, test_size=0.3,\n",
      "                                                        random_state=seed)\n",
      "\n",
      "    # Training set\n",
      "    rf = RandomForestRegressor(n_estimators=400, \n",
      "                               max_features='sqrt', \n",
      "                               min_samples_leaf = 1,\n",
      "                               random_state = 13,\n",
      "                               n_jobs=-1)\n",
      "    \n",
      "    rf.fit(X_internal,Y_internal)\n",
      "    prediction = rf.predict(X_internal)\n",
      "    \n",
      "    # Cross-validation\n",
      "    cv = cross_validation.cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)\n",
      "    \n",
      "    # External set  \n",
      "    prediction_external = rf.predict(X_external)\n",
      "    \n",
      "    #print result from each seed    \n",
      "    R2_train.append(r2_score(Y_internal, prediction))\n",
      "    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))\n",
      "    Q2_CV.append(r2_score(Y_internal, cv))\n",
      "    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))\n",
      "    Q2_External.append(r2_score(Y_external, prediction_external))\n",
      "    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))\n",
      "    \n",
      "    #Feature Importance\n",
      "    Feature = hx[:]\n",
      "    feature_importance = rf.feature_importances_\n",
      "    importances = 100.0 * (feature_importance / feature_importance.max()) #index\n",
      "    \n",
      "    for i, fx in enumerate(Feature):\n",
      "        importances_dict[fx].append(importances[i])\n",
      "    \n",
      "    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict\n",
      "141/144:\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn import cross_validation\n",
      "from collections import defaultdict\n",
      "\n",
      "def build_model(X, Y, seed, hx, f):\n",
      "    \n",
      "    \n",
      "    #Data split using 70/30 ratio\n",
      "    X_internal, X_external, Y_internal, Y_external = train_test_split(X,\n",
      "                                                        Y, test_size=0.3,\n",
      "                                                        random_state=seed)\n",
      "\n",
      "    # Training set\n",
      "    rf = RandomForestRegressor(n_estimators=400, \n",
      "                               max_features='sqrt', \n",
      "                               min_samples_leaf = 1,\n",
      "                               random_state = 13,\n",
      "                               n_jobs=-1)\n",
      "    \n",
      "    rf.fit(X_internal,Y_internal)\n",
      "    prediction = rf.predict(X_internal)\n",
      "    \n",
      "    # Cross-validation\n",
      "    cv = cross_validation.cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)\n",
      "    \n",
      "    # External set  \n",
      "    prediction_external = rf.predict(X_external)\n",
      "    \n",
      "    #print result from each seed    \n",
      "    R2_train.append(r2_score(Y_internal, prediction))\n",
      "    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))\n",
      "    Q2_CV.append(r2_score(Y_internal, cv))\n",
      "    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))\n",
      "    Q2_External.append(r2_score(Y_external, prediction_external))\n",
      "    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))\n",
      "    \n",
      "    #Feature Importance\n",
      "    Feature = hx[:]\n",
      "    feature_importance = rf.feature_importances_\n",
      "    importances = 100.0 * (feature_importance / feature_importance.max()) #index\n",
      "    \n",
      "    for i, fx in enumerate(Feature):\n",
      "        importances_dict[fx].append(importances[i])\n",
      "    \n",
      "    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict\n",
      "141/145:\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn import cross_validate, cross_val_predict\n",
      "from collections import defaultdict\n",
      "\n",
      "def build_model(X, Y, seed, hx, f):\n",
      "    \n",
      "    \n",
      "    #Data split using 70/30 ratio\n",
      "    X_internal, X_external, Y_internal, Y_external = train_test_split(X,\n",
      "                                                        Y, test_size=0.3,\n",
      "                                                        random_state=seed)\n",
      "\n",
      "    # Training set\n",
      "    rf = RandomForestRegressor(n_estimators=400, \n",
      "                               max_features='sqrt', \n",
      "                               min_samples_leaf = 1,\n",
      "                               random_state = 13,\n",
      "                               n_jobs=-1)\n",
      "    \n",
      "    rf.fit(X_internal,Y_internal)\n",
      "    prediction = rf.predict(X_internal)\n",
      "    \n",
      "    # Cross-validation\n",
      "    cv = cross_validation.cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)\n",
      "    \n",
      "    # External set  \n",
      "    prediction_external = rf.predict(X_external)\n",
      "    \n",
      "    #print result from each seed    \n",
      "    R2_train.append(r2_score(Y_internal, prediction))\n",
      "    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))\n",
      "    Q2_CV.append(r2_score(Y_internal, cv))\n",
      "    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))\n",
      "    Q2_External.append(r2_score(Y_external, prediction_external))\n",
      "    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))\n",
      "    \n",
      "    #Feature Importance\n",
      "    Feature = hx[:]\n",
      "    feature_importance = rf.feature_importances_\n",
      "    importances = 100.0 * (feature_importance / feature_importance.max()) #index\n",
      "    \n",
      "    for i, fx in enumerate(Feature):\n",
      "        importances_dict[fx].append(importances[i])\n",
      "    \n",
      "    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict\n",
      "141/146:\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn import cross_val_predict, cross_validate\n",
      "from collections import defaultdict\n",
      "\n",
      "def build_model(X, Y, seed, hx, f):\n",
      "    \n",
      "    \n",
      "    #Data split using 70/30 ratio\n",
      "    X_internal, X_external, Y_internal, Y_external = train_test_split(X,\n",
      "                                                        Y, test_size=0.3,\n",
      "                                                        random_state=seed)\n",
      "\n",
      "    # Training set\n",
      "    rf = RandomForestRegressor(n_estimators=400, \n",
      "                               max_features='sqrt', \n",
      "                               min_samples_leaf = 1,\n",
      "                               random_state = 13,\n",
      "                               n_jobs=-1)\n",
      "    \n",
      "    rf.fit(X_internal,Y_internal)\n",
      "    prediction = rf.predict(X_internal)\n",
      "    \n",
      "    # Cross-validation\n",
      "    cv = cross_validation.cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)\n",
      "    \n",
      "    # External set  \n",
      "    prediction_external = rf.predict(X_external)\n",
      "    \n",
      "    #print result from each seed    \n",
      "    R2_train.append(r2_score(Y_internal, prediction))\n",
      "    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))\n",
      "    Q2_CV.append(r2_score(Y_internal, cv))\n",
      "    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))\n",
      "    Q2_External.append(r2_score(Y_external, prediction_external))\n",
      "    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))\n",
      "    \n",
      "    #Feature Importance\n",
      "    Feature = hx[:]\n",
      "    feature_importance = rf.feature_importances_\n",
      "    importances = 100.0 * (feature_importance / feature_importance.max()) #index\n",
      "    \n",
      "    for i, fx in enumerate(Feature):\n",
      "        importances_dict[fx].append(importances[i])\n",
      "    \n",
      "    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict\n",
      "141/147:\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "\n",
      "def build_model(X, Y, seed, hx, f):\n",
      "    \n",
      "    \n",
      "    #Data split using 70/30 ratio\n",
      "    X_internal, X_external, Y_internal, Y_external = train_test_split(X,\n",
      "                                                        Y, test_size=0.3,\n",
      "                                                        random_state=seed)\n",
      "\n",
      "    # Training set\n",
      "    rf = RandomForestRegressor(n_estimators=400, \n",
      "                               max_features='sqrt', \n",
      "                               min_samples_leaf = 1,\n",
      "                               random_state = 13,\n",
      "                               n_jobs=-1)\n",
      "    \n",
      "    rf.fit(X_internal,Y_internal)\n",
      "    prediction = rf.predict(X_internal)\n",
      "    \n",
      "    # Cross-validation\n",
      "    cv = cross_validation.cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)\n",
      "    \n",
      "    # External set  \n",
      "    prediction_external = rf.predict(X_external)\n",
      "    \n",
      "    #print result from each seed    \n",
      "    R2_train.append(r2_score(Y_internal, prediction))\n",
      "    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))\n",
      "    Q2_CV.append(r2_score(Y_internal, cv))\n",
      "    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))\n",
      "    Q2_External.append(r2_score(Y_external, prediction_external))\n",
      "    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))\n",
      "    \n",
      "    #Feature Importance\n",
      "    Feature = hx[:]\n",
      "    feature_importance = rf.feature_importances_\n",
      "    importances = 100.0 * (feature_importance / feature_importance.max()) #index\n",
      "    \n",
      "    for i, fx in enumerate(Feature):\n",
      "        importances_dict[fx].append(importances[i])\n",
      "    \n",
      "    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict\n",
      "141/148:\n",
      "from copy import deepcopy\n",
      "\n",
      "def Y_scrambling(X_internal, X_external, Y_internal, Y_external):\n",
      "    # Do the Y-scrambling. Loop over the actual learning for 100 times.\n",
      "    for randomseedcounter in range(1,101):\n",
      "        y_train_scrambled = deepcopy(Y_internal)\n",
      "        X_train_scrambled = deepcopy(X_internal)\n",
      "        np.random.shuffle(y_train_scrambled)\n",
      "        np.random.shuffle(X_train_scrambled)\n",
      "\n",
      "        # training was done on \"scrambled\" data - prediction on test set\n",
      "        RF_scrambled         = RandomForestRegressor()\n",
      "        RF_scrambled         = RF_scrambled.fit(X_internal,y_train_scrambled)\n",
      "        y_predict_scrambled  = RF_scrambled.predict(X_external)\n",
      "    \n",
      "        acclist_predictionOnTest_scrambledtrain.append((RF_scrambled.score(X_external,Y_external))**2)\n",
      "    \n",
      "        # training was done on \"scrambled\" data - prediction on train set\n",
      "        y_predict_scrambled_predictTrain  = RF_scrambled.predict(X_internal)\n",
      "    \n",
      "        acclist_predictionOnTrain_scrambledtrain.append((RF_scrambled.score(X_internal,Y_internal))**2)\n",
      "        \n",
      "        r2 = pd.DataFrame(acclist_predictionOnTrain_scrambledtrain, columns=['R2'])\n",
      "        q2 = pd.DataFrame(acclist_predictionOnTest_scrambledtrain, columns=['Q2'])\n",
      "\n",
      "        #result = pd.concat([r2, q2], axis=1, join='inner').to_csv(f+\"_Y_scrambling.csv\", header=False, index=False)\n",
      "        \n",
      "        \n",
      "    return acclist_predictionOnTest_scrambledtrain,acclist_predictionOnTrain_scrambledtrain\n",
      "141/149:\n",
      "def mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, importances_dict):\n",
      "    R2_train_mean = np.mean(R2_train)\n",
      "    RMSE_train_mean = np.mean(RMSE_train)\n",
      "    Q2_CV_mean = np.mean(Q2_CV)\n",
      "    RMSE_CV_mean = np.mean(RMSE_CV)\n",
      "    Q2_External_mean = np.mean(Q2_External)\n",
      "    RMSE_External_mean = np.mean(RMSE_External)\n",
      "    importances_mean0 = {}\n",
      "    for fx in importances_dict:\n",
      "        importances_mean0[fx] = np.mean(importances_dict[fx])\n",
      "    importances_mean = sorted([(k,v) for k,v in importances_mean0.iteritems()],\n",
      "                                    key=lambda x: x[1], reverse=True)\n",
      "    \n",
      "    #predictionOnTest_mean = np.mean(acclist_predictionOnTest_scrambledtrain)\n",
      "    #predictionOnTrain_mean = np.mean(acclist_predictionOnTrain_scrambledtrain)\n",
      "    \n",
      "    return R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \\\n",
      "            RMSE_External_mean, importances_mean\n",
      "141/150:\n",
      "def std(R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, importances_dict):\n",
      "    R2_train_std = np.std(R2_train)\n",
      "    RMSE_train_std = np.std(RMSE_train)\n",
      "    Q2_CV_std = np.std(Q2_CV)\n",
      "    RMSE_CV_std = np.std(RMSE_CV)\n",
      "    Q2_External_std = np.std(Q2_External)\n",
      "    RMSE_External_std = np.std(RMSE_External)\n",
      "    importances_std0 = {}\n",
      "    for fx in importances_dict:\n",
      "        importances_std0[fx] = np.std(importances_dict[fx])\n",
      "    importances_std = sorted([(k,v) for k,v in importances_std0.iteritems()],\n",
      "                                    key=lambda x: x[1], reverse=True)\n",
      "    #predictionOnTest_std = np.std(acclist_predictionOnTest_scrambledtrain)\n",
      "    #predictionOnTrain_std = np.std(acclist_predictionOnTrain_scrambledtrain)\n",
      "    \n",
      "    return R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\\\n",
      "            RMSE_External_std, importances_std\n",
      "141/151:\n",
      "def print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \n",
      "           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, \n",
      "           RMSE_External_std, X_internal, X_external, X, f, des1, des2 ):\n",
      "    #outfile = open('all_output.csv', 'a')\n",
      "    name = [os.path.basename(f)]\n",
      "    print_out = name\n",
      "    print_out = [name.replace('.smi','') for name in print_out]\n",
      "    \n",
      "    print >> outfile, '%s,%d,%d,%d,%d,%0.4f,%0.4f,%0.4f,%0.4f,%d,%0.4f,%0.4f,%0.4f,%0.4f,%d,%0.4f,%0.4f,%0.4f,%0.4f' \\\n",
      "                                     % (print_out, len(X_internal),\n",
      "                                        des1, des2, des4,\n",
      "                                        R2_train_mean, R2_train_std,\n",
      "                                        RMSE_train_mean, RMSE_train_std,\n",
      "                                        len(X_internal),\n",
      "                                        Q2_CV_mean, Q2_CV_std,\n",
      "                                        RMSE_CV_mean, RMSE_CV_std,\n",
      "                                        len(X_external),\n",
      "                                        Q2_External_mean, Q2_External_std,\n",
      "                                        RMSE_External_mean, RMSE_External_std,)\n",
      " \n",
      "    print ('\\nTraining set\\n------------')\n",
      "    print ('N: ' + (str(len(X_internal))))\n",
      "    print ('R2: %0.4f'%(R2_train_mean))\n",
      "    print ('std_R2: %0.4f'%(R2_train_std))\n",
      "    print ('RMSE: %0.4f'%(RMSE_train_mean))\n",
      "    print ('std_RMSE: %0.4f'%(RMSE_train_std))\n",
      "\n",
      "    print ('\\nCross-validation set\\n------------')\n",
      "    print 'N: ' + (str(len(X_internal)))\n",
      "    print 'Q2: %0.4f'%(Q2_CV_mean)\n",
      "    print 'std_Q2: %0.4f'%(Q2_CV_std)\n",
      "    print 'RMSE: %0.4f'%(RMSE_CV_mean)\n",
      "    print 'std_RMSE: %0.4f'%(RMSE_CV_std)\n",
      "\n",
      "    print '\\nExternal set\\n------------'\n",
      "    print 'N: ' + (str(len(X_external)))\n",
      "    print 'Q2_EXt: %0.4f'%(Q2_External_mean)\n",
      "    print 'std_Q2_EXt: %0.4f'%(Q2_External_std)\n",
      "    print 'RMSE: %0.4f'%(RMSE_External_mean)\n",
      "    print 'std_RMSE: %0.4f'%(RMSE_External_std)\n",
      "141/152:\n",
      "def print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \n",
      "           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, \n",
      "           RMSE_External_std, X_internal, X_external, X, f, des1, des2 ):\n",
      "    #outfile = open('all_output.csv', 'a')\n",
      "    name = [os.path.basename(f)]\n",
      "    print_out = name\n",
      "    print_out = [name.replace('.smi','') for name in print_out]\n",
      "    \n",
      "    print >> outfile, '%s,%d,%d,%d,%d,%0.4f,%0.4f,%0.4f,%0.4f,%d,%0.4f,%0.4f,%0.4f,%0.4f,%d,%0.4f,%0.4f,%0.4f,%0.4f' \\\n",
      "                                     % (print_out, len(X_internal),\n",
      "                                        des1, des2, des4,\n",
      "                                        R2_train_mean, R2_train_std,\n",
      "                                        RMSE_train_mean, RMSE_train_std,\n",
      "                                        len(X_internal),\n",
      "                                        Q2_CV_mean, Q2_CV_std,\n",
      "                                        RMSE_CV_mean, RMSE_CV_std,\n",
      "                                        len(X_external),\n",
      "                                        Q2_External_mean, Q2_External_std,\n",
      "                                        RMSE_External_mean, RMSE_External_std,)\n",
      " \n",
      "    print ('\\nTraining set\\n------------')\n",
      "    print ('N: ' + (str(len(X_internal))))\n",
      "    print ('R2: %0.4f'%(R2_train_mean))\n",
      "    print ('std_R2: %0.4f'%(R2_train_std))\n",
      "    print ('RMSE: %0.4f'%(RMSE_train_mean))\n",
      "    print ('std_RMSE: %0.4f'%(RMSE_train_std))\n",
      "\n",
      "    print ('\\nCross-validation set\\n------------')\n",
      "    print ('N: ' + (str(len(X_internal))))\n",
      "    print ('Q2: %0.4f'%(Q2_CV_mean))\n",
      "    print ('std_Q2: %0.4f'%(Q2_CV_std))\n",
      "    print ('RMSE: %0.4f'%(RMSE_CV_mean))\n",
      "    print ('std_RMSE: %0.4f'%(RMSE_CV_std))\n",
      "\n",
      "    print ('\\nExternal set\\n------------')\n",
      "    print ('N: ' + (str(len(X_external))))\n",
      "    print ('Q2_EXt: %0.4f'%(Q2_External_mean))\n",
      "    print ('std_Q2_EXt: %0.4f'%(Q2_External_std))\n",
      "    print ('RMSE: %0.4f'%(RMSE_External_mean))\n",
      "    print ('std_RMSE: %0.4f'%(RMSE_External_std))\n",
      "141/153:\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.ticker import MultipleLocator\n",
      "import pylab as py \n",
      "import statsmodels.api as sm\n",
      "from statsmodels.stats.outliers_influence import summary_table\n",
      "\n",
      "def plot_model (f, X_internal, X_external, Y_internal, Y_external,\n",
      "                R2_train_mean, Q2_External_mean,\n",
      "                importances_mean, importances_std, Feature, prediction, \n",
      "                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain):\n",
      "    \n",
      "    # Prepare plot\n",
      "    m  = rf.fit(X_internal,Y_internal)\n",
      "    cm = plt.cm.RdBu\n",
      "    cv = cross_validation.cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)\n",
      "\n",
      "    fig_size = plt.rcParams[\"figure.figsize\"]\n",
      "    fig_size[0]=5\n",
      "    fig_size[1]=5\n",
      "\n",
      "    # Train Set\n",
      "    x_train = np.array(Y_internal)\n",
      "    y_train = m.predict(X_internal).flatten()\n",
      "    py.scatter(x_train, y_train, s=50, marker='.', alpha=0.3,\n",
      "            c='g', cmap=cm ,edgecolors='g')\n",
      "                 #label=r\"$R^{2}_{Tr}$ = %.4f\" % R2_train_mean)\n",
      "    \n",
      "    \n",
      "    # CV Set\n",
      "    np.array(cv)\n",
      "    x_test = np.array(Y_internal)\n",
      "    y_test = cv\n",
      "    py.scatter(x_test, y_test, s=50, marker='.', alpha=0.3,\n",
      "            c='b', cmap=cm ,edgecolors='b') \n",
      "                 #label=r\"$Q^{2}_{Ext}$ = %.4f\" % Q2_External_mean)\n",
      "        #2SD line\n",
      "    X = sm.add_constant(x_test)\n",
      "    res = sm.OLS(y_test, X).fit()\n",
      "\n",
      "    st, data, ss2 = summary_table(res, alpha=0.05)\n",
      "    fittedvalues = data[:,2]\n",
      "        #predict_mean_se  = data[:,3]\n",
      "    predict_mean_ci_low, predict_mean_ci_upp = data[:,4:6].T\n",
      "    predict_ci_low, predict_ci_upp = data[:,6:8].T\n",
      "        #2SD line\n",
      "    plt.plot(X, predict_ci_low, '--b', linewidth=0.5, alpha=0.5)\n",
      "    plt.plot(X, predict_ci_upp, '--b', linewidth=0.5, alpha=0.5)\n",
      "        \n",
      "    # External Set\n",
      "    x_test = np.array(Y_external)\n",
      "    y_test = m.predict(X_external).flatten()\n",
      "    py.scatter(x_test, y_test, s=50, marker='.', alpha=0.3,\n",
      "            c='r', cmap=cm ,edgecolors='r') \n",
      "                 #label=r\"$Q^{2}_{Ext}$ = %.4f\" % Q2_External_mean)\n",
      "    \n",
      "    \n",
      "    #py.plot(x_train, np.polyval(np.polyfit(x_train,y_train,1), x_train), '--r') #mean line\n",
      "    plt.legend(loc=2,prop={'size':6})\n",
      "    plt.xlabel(\"Experimental $pIC_{50}$ values\", fontsize=10)\n",
      "    plt.ylabel(\"Predicted $pIC_{50}$ values\", fontsize=10)\n",
      "        \n",
      "    min_axis = np.min(np.concatenate([Y_internal, prediction], axis=0))\n",
      "    max_axis = np.max(np.concatenate([Y_internal, prediction], axis=0))\n",
      "    plt.xlim([(min_axis*0.9),(max_axis*1.05)])\n",
      "    plt.ylim([(min_axis*0.9),(max_axis*1.05)])\n",
      "    plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "    \n",
      "    # Save plot to file\n",
      "    plt.savefig(f+'_compair_Fp.pdf', dpi=300)\n",
      "    plt.show()\n",
      "    \n",
      "    # Y-scrambling plot\n",
      "    py.scatter(Q2_CV_mean, R2_train_mean, s=100, marker='.', alpha=0.3,\n",
      "            c='b', cmap=cm ,edgecolors='b') \n",
      "            \n",
      "    py.scatter(acclist_predictionOnTest_scrambledtrain,acclist_predictionOnTrain_scrambledtrain, \\\n",
      "               s=100, marker='.', alpha=0.3, c='r', cmap=cm ,edgecolors='r')\n",
      "\n",
      "    plt.legend(loc=2,prop={'size':6})\n",
      "    plt.xlabel(\"$Q^{2}$\", fontsize=10)\n",
      "    plt.ylabel(\"$R^{2}$\", fontsize=10)\n",
      "    plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "    \n",
      "    plt.ylim([0, 1])\n",
      "    plt.xlim([0, 1])\n",
      "    plt.axhline(0.5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "    plt.axvline(0.5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "    \n",
      "    plt.savefig(f+'_Y_scrambling.pdf', dpi=300)\n",
      "\n",
      "    plt.show()\n",
      "\n",
      "    #Feature Importance \n",
      "    fig_size = plt.rcParams[\"figure.figsize\"]\n",
      "    fig_size[0]=5\n",
      "    fig_size[1]=10\n",
      "\n",
      "    barlist = plt.barh(range(20), [x[1] for x in importances_mean[:20]], \n",
      "          color=\"g\", xerr=[x[1] for x in importances_std[:20]], align=\"center\", \\\n",
      "                       error_kw=dict(ecolor='gray', lw=2, capsize=5, capthick=2))\n",
      "\n",
      "    print (str(\"top10\"), [x[0] for x in importances_mean[:10]])\n",
      "    print ('')\n",
      "    print (str(\"top20\"), [x[0] for x in importances_mean[:20]])\n",
      "    print ('')\n",
      "    print (str(\"top30\"), [x[0] for x in importances_mean[:30]])\n",
      "    print ('')\n",
      "    print (str(\"top40\"), [x[0] for x in importances_mean[:40]])\n",
      "    print ('')\n",
      "    print (str(\"top50\"), [x[0] for x in importances_mean[:50]])\n",
      "    \n",
      "    plt.yticks(range(20), [x[0] for x in importances_mean[:20]])\n",
      "    plt.ylim([-1, 20])\n",
      "    \n",
      "    plt.xlabel(r\"$\\bf{Gini}$\" + \" \"+ r\"$\\bf{index}$\", fontsize=12)\n",
      "    ax = plt.gca()\n",
      "    ax.invert_yaxis()\n",
      "    plt.tight_layout(pad=2.0, w_pad=0.7, h_pad=2.0)\n",
      "    plt.savefig(f+'_Feature_importances.pdf', dpi=300)\n",
      "    plt.show()\n",
      "141/154:\n",
      "import glob # to read multiple files \n",
      "from rdkit import Chem, DataStructs\n",
      "from rdkit.Chem import AllChem\n",
      "import os\n",
      "\n",
      "%config InlineBackend.figure_format = 'retina'\n",
      "\n",
      "! rm Result/CHEMBL206_IC50_Revision.csv\n",
      "\n",
      "outfile = open('Result/Result_LogKp.csv', 'a')\n",
      "\n",
      "print >> outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\\\n",
      "                    'R2_train,R2_train_std,' + \\\n",
      "                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \\\n",
      "                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std'\n",
      "\n",
      "path = r'QSAR/'\n",
      "\n",
      "for f in glob.glob(path + '*.csv'):\n",
      "    df = pd.read_csv(f)\n",
      "    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))\n",
      "    df = df.fillna(method='ffill')\n",
      "    Y = df[\"LogKp\"].as_matrix().astype(np.float)\n",
      "    data = df.ix[:,2:]\n",
      "    \n",
      "    print '\\n\\n************************************************************************************'\n",
      "    print ''\n",
      "    print (f)\n",
      "    print ''\n",
      "    \n",
      "    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%\n",
      "    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7\n",
      "\n",
      "    h = data.columns.tolist()\n",
      "    hx = np.array(h)\n",
      "\n",
      "    data = data.as_matrix().astype(np.float)\n",
      "    X = np.array(data)\n",
      "    \n",
      "    # Prepare empty lists to plot QSAR model\n",
      "    R2_train = []\n",
      "    RMSE_train = []\n",
      "    Q2_CV = []\n",
      "    RMSE_CV = []\n",
      "    Q2_External = []\n",
      "    RMSE_External = []\n",
      "    importances_dict = defaultdict(list)\n",
      "    \n",
      "    # Prepare empty lists to plot the performance of accuracy.\n",
      "    acclist_realRF                          = []\n",
      "    acclist_realRF_predictTrain             = []\n",
      "    acclist_predictionOnTest_scrambledtrain  = []\n",
      "    acclist_predictionOnTrain_scrambledtrain = []\n",
      "        \n",
      "    for i in range(10):\n",
      "        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)\n",
      "            \n",
      "    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \\\n",
      "                                                                        X_internal, X_external, Y_internal, Y_external)\n",
      "    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \\\n",
      "        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\\\n",
      "        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \n",
      "           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, \n",
      "           RMSE_External_std, X_internal, X_external, X, f, des1, des2)\n",
      "    plot_model(f, X_internal, X_external, Y_internal, Y_external,\n",
      "                R2_train_mean, Q2_External_mean,\n",
      "                importances_mean, importances_std, Feature, prediction,\n",
      "                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)\n",
      "    \n",
      "outfile.close()\n",
      "    #END\n",
      "141/155:\n",
      "import glob # to read multiple files \n",
      "from rdkit import Chem, DataStructs\n",
      "from rdkit.Chem import AllChem\n",
      "import os\n",
      "\n",
      "%config InlineBackend.figure_format = 'retina'\n",
      "\n",
      "! rm Result/CHEMBL206_IC50_Revision.csv\n",
      "\n",
      "outfile = open('Result/Result_LogKp.csv', 'a')\n",
      "\n",
      "print >> outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\\\n",
      "                    'R2_train,R2_train_std,' + \\\n",
      "                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \\\n",
      "                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std'\n",
      "\n",
      "path = r'QSAR/'\n",
      "\n",
      "for f in glob.glob(path + '*.csv'):\n",
      "    df = pd.read_csv(f)\n",
      "    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))\n",
      "    df = df.fillna(method='ffill')\n",
      "    Y = df[\"LogKp\"].as_matrix().astype(np.float)\n",
      "    data = df.ix[:,2:]\n",
      "    \n",
      "    print ('\\n\\n************************************************************************************')\n",
      "    print ('')\n",
      "    print (f)\n",
      "    print ('')\n",
      "    \n",
      "    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%\n",
      "    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7\n",
      "\n",
      "    h = data.columns.tolist()\n",
      "    hx = np.array(h)\n",
      "\n",
      "    data = data.as_matrix().astype(np.float)\n",
      "    X = np.array(data)\n",
      "    \n",
      "    # Prepare empty lists to plot QSAR model\n",
      "    R2_train = []\n",
      "    RMSE_train = []\n",
      "    Q2_CV = []\n",
      "    RMSE_CV = []\n",
      "    Q2_External = []\n",
      "    RMSE_External = []\n",
      "    importances_dict = defaultdict(list)\n",
      "    \n",
      "    # Prepare empty lists to plot the performance of accuracy.\n",
      "    acclist_realRF                          = []\n",
      "    acclist_realRF_predictTrain             = []\n",
      "    acclist_predictionOnTest_scrambledtrain  = []\n",
      "    acclist_predictionOnTrain_scrambledtrain = []\n",
      "        \n",
      "    for i in range(10):\n",
      "        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)\n",
      "            \n",
      "    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \\\n",
      "                                                                        X_internal, X_external, Y_internal, Y_external)\n",
      "    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \\\n",
      "        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\\\n",
      "        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \n",
      "           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, \n",
      "           RMSE_External_std, X_internal, X_external, X, f, des1, des2)\n",
      "    plot_model(f, X_internal, X_external, Y_internal, Y_external,\n",
      "                R2_train_mean, Q2_External_mean,\n",
      "                importances_mean, importances_std, Feature, prediction,\n",
      "                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)\n",
      "    \n",
      "outfile.close()\n",
      "    #END\n",
      "141/156:\n",
      "import glob # to read multiple files \n",
      "from rdkit import Chem, DataStructs\n",
      "from rdkit.Chem import AllChem\n",
      "import os\n",
      "\n",
      "%config InlineBackend.figure_format = 'retina'\n",
      "\n",
      "! rm Result/Result_LogKp.csv\n",
      "\n",
      "outfile = open('Result/Result_LogKp.csv', 'a')\n",
      "\n",
      "print >> outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\\\n",
      "                    'R2_train,R2_train_std,' + \\\n",
      "                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \\\n",
      "                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std'\n",
      "\n",
      "path = r'QSAR/'\n",
      "\n",
      "for f in glob.glob(path + '*.csv'):\n",
      "    df = pd.read_csv(f)\n",
      "    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))\n",
      "    df = df.fillna(method='ffill')\n",
      "    Y = df[\"LogKp\"].as_matrix().astype(np.float)\n",
      "    data = df.ix[:,2:]\n",
      "    \n",
      "    print ('\\n\\n************************************************************************************')\n",
      "    print ('')\n",
      "    print (f)\n",
      "    print ('')\n",
      "    \n",
      "    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%\n",
      "    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7\n",
      "\n",
      "    h = data.columns.tolist()\n",
      "    hx = np.array(h)\n",
      "\n",
      "    data = data.as_matrix().astype(np.float)\n",
      "    X = np.array(data)\n",
      "    \n",
      "    # Prepare empty lists to plot QSAR model\n",
      "    R2_train = []\n",
      "    RMSE_train = []\n",
      "    Q2_CV = []\n",
      "    RMSE_CV = []\n",
      "    Q2_External = []\n",
      "    RMSE_External = []\n",
      "    importances_dict = defaultdict(list)\n",
      "    \n",
      "    # Prepare empty lists to plot the performance of accuracy.\n",
      "    acclist_realRF                          = []\n",
      "    acclist_realRF_predictTrain             = []\n",
      "    acclist_predictionOnTest_scrambledtrain  = []\n",
      "    acclist_predictionOnTrain_scrambledtrain = []\n",
      "        \n",
      "    for i in range(10):\n",
      "        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)\n",
      "            \n",
      "    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \\\n",
      "                                                                        X_internal, X_external, Y_internal, Y_external)\n",
      "    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \\\n",
      "        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\\\n",
      "        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \n",
      "           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, \n",
      "           RMSE_External_std, X_internal, X_external, X, f, des1, des2)\n",
      "    plot_model(f, X_internal, X_external, Y_internal, Y_external,\n",
      "                R2_train_mean, Q2_External_mean,\n",
      "                importances_mean, importances_std, Feature, prediction,\n",
      "                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)\n",
      "    \n",
      "outfile.close()\n",
      "    #END\n",
      "141/157:\n",
      "import glob # to read multiple files \n",
      "from rdkit import Chem, DataStructs\n",
      "from rdkit.Chem import AllChem\n",
      "import os\n",
      "\n",
      "%config InlineBackend.figure_format = 'retina'\n",
      "\n",
      "! rm Result/Result_LogKp.csv\n",
      "\n",
      "outfile = open('Result/Result_LogKp.csv', 'a')\n",
      "\n",
      "print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\\\n",
      "                    'R2_train,R2_train_std,' + \\\n",
      "                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \\\n",
      "                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')\n",
      "\n",
      "path = r'QSAR/'\n",
      "\n",
      "for f in glob.glob(path + '*.csv'):\n",
      "    df = pd.read_csv(f)\n",
      "    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))\n",
      "    df = df.fillna(method='ffill')\n",
      "    Y = df[\"LogKp\"].as_matrix().astype(np.float)\n",
      "    data = df.ix[:,2:]\n",
      "    \n",
      "    print ('\\n\\n************************************************************************************')\n",
      "    print ('')\n",
      "    print (f)\n",
      "    print ('')\n",
      "    \n",
      "    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%\n",
      "    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7\n",
      "\n",
      "    h = data.columns.tolist()\n",
      "    hx = np.array(h)\n",
      "\n",
      "    data = data.as_matrix().astype(np.float)\n",
      "    X = np.array(data)\n",
      "    \n",
      "    # Prepare empty lists to plot QSAR model\n",
      "    R2_train = []\n",
      "    RMSE_train = []\n",
      "    Q2_CV = []\n",
      "    RMSE_CV = []\n",
      "    Q2_External = []\n",
      "    RMSE_External = []\n",
      "    importances_dict = defaultdict(list)\n",
      "    \n",
      "    # Prepare empty lists to plot the performance of accuracy.\n",
      "    acclist_realRF                          = []\n",
      "    acclist_realRF_predictTrain             = []\n",
      "    acclist_predictionOnTest_scrambledtrain  = []\n",
      "    acclist_predictionOnTrain_scrambledtrain = []\n",
      "        \n",
      "    for i in range(10):\n",
      "        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)\n",
      "            \n",
      "    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \\\n",
      "                                                                        X_internal, X_external, Y_internal, Y_external)\n",
      "    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \\\n",
      "        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\\\n",
      "        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \n",
      "           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, \n",
      "           RMSE_External_std, X_internal, X_external, X, f, des1, des2)\n",
      "    plot_model(f, X_internal, X_external, Y_internal, Y_external,\n",
      "                R2_train_mean, Q2_External_mean,\n",
      "                importances_mean, importances_std, Feature, prediction,\n",
      "                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)\n",
      "    \n",
      "outfile.close()\n",
      "    #END\n",
      "141/158:\n",
      "import glob # to read multiple files \n",
      "from rdkit import Chem, DataStructs\n",
      "from rdkit.Chem import AllChem\n",
      "import os\n",
      "\n",
      "%config InlineBackend.figure_format = 'retina'\n",
      "\n",
      "! rm Result/Result_LogKp.csv\n",
      "\n",
      "outfile = open('Result/Result_LogKp.csv', 'a')\n",
      "\n",
      "print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\\\n",
      "                    'R2_train,R2_train_std,' + \\\n",
      "                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \\\n",
      "                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')\n",
      "\n",
      "path = r'QSAR/'\n",
      "\n",
      "for f in glob.glob(path + '*.csv'):\n",
      "    df = pd.read_csv(f)\n",
      "    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))\n",
      "    df = df.fillna(method='ffill')\n",
      "    Y = df[\"LogKp\"].astype(np.float)\n",
      "    data = df.ix[:,2:]\n",
      "    \n",
      "    print ('\\n\\n************************************************************************************')\n",
      "    print ('')\n",
      "    print (f)\n",
      "    print ('')\n",
      "    \n",
      "    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%\n",
      "    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7\n",
      "\n",
      "    h = data.columns.tolist()\n",
      "    hx = np.array(h)\n",
      "\n",
      "    data = data.as_matrix().astype(np.float)\n",
      "    X = np.array(data)\n",
      "    \n",
      "    # Prepare empty lists to plot QSAR model\n",
      "    R2_train = []\n",
      "    RMSE_train = []\n",
      "    Q2_CV = []\n",
      "    RMSE_CV = []\n",
      "    Q2_External = []\n",
      "    RMSE_External = []\n",
      "    importances_dict = defaultdict(list)\n",
      "    \n",
      "    # Prepare empty lists to plot the performance of accuracy.\n",
      "    acclist_realRF                          = []\n",
      "    acclist_realRF_predictTrain             = []\n",
      "    acclist_predictionOnTest_scrambledtrain  = []\n",
      "    acclist_predictionOnTrain_scrambledtrain = []\n",
      "        \n",
      "    for i in range(10):\n",
      "        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)\n",
      "            \n",
      "    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \\\n",
      "                                                                        X_internal, X_external, Y_internal, Y_external)\n",
      "    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \\\n",
      "        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\\\n",
      "        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \n",
      "           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, \n",
      "           RMSE_External_std, X_internal, X_external, X, f, des1, des2)\n",
      "    plot_model(f, X_internal, X_external, Y_internal, Y_external,\n",
      "                R2_train_mean, Q2_External_mean,\n",
      "                importances_mean, importances_std, Feature, prediction,\n",
      "                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)\n",
      "    \n",
      "outfile.close()\n",
      "    #END\n",
      "141/159:\n",
      "import glob # to read multiple files \n",
      "from rdkit import Chem, DataStructs\n",
      "from rdkit.Chem import AllChem\n",
      "import os\n",
      "\n",
      "%config InlineBackend.figure_format = 'retina'\n",
      "\n",
      "! rm Result/Result_LogKp.csv\n",
      "\n",
      "outfile = open('Result/Result_LogKp.csv', 'a')\n",
      "\n",
      "print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\\\n",
      "                    'R2_train,R2_train_std,' + \\\n",
      "                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \\\n",
      "                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')\n",
      "\n",
      "path = r'QSAR/'\n",
      "\n",
      "for f in glob.glob(path + '*.csv'):\n",
      "    df = pd.read_csv(f)\n",
      "    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))\n",
      "    df = df.fillna(method='ffill')\n",
      "    Y = df[\"LogKp\"].astype(np.float)\n",
      "    data = df\n",
      "    \n",
      "    print ('\\n\\n************************************************************************************')\n",
      "    print ('')\n",
      "    print (f)\n",
      "    print ('')\n",
      "    \n",
      "    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%\n",
      "    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7\n",
      "\n",
      "    h = data.columns.tolist()\n",
      "    hx = np.array(h)\n",
      "\n",
      "    data = data.as_matrix().astype(np.float)\n",
      "    X = np.array(data)\n",
      "    \n",
      "    # Prepare empty lists to plot QSAR model\n",
      "    R2_train = []\n",
      "    RMSE_train = []\n",
      "    Q2_CV = []\n",
      "    RMSE_CV = []\n",
      "    Q2_External = []\n",
      "    RMSE_External = []\n",
      "    importances_dict = defaultdict(list)\n",
      "    \n",
      "    # Prepare empty lists to plot the performance of accuracy.\n",
      "    acclist_realRF                          = []\n",
      "    acclist_realRF_predictTrain             = []\n",
      "    acclist_predictionOnTest_scrambledtrain  = []\n",
      "    acclist_predictionOnTrain_scrambledtrain = []\n",
      "        \n",
      "    for i in range(10):\n",
      "        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)\n",
      "            \n",
      "    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \\\n",
      "                                                                        X_internal, X_external, Y_internal, Y_external)\n",
      "    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \\\n",
      "        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\\\n",
      "        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \n",
      "           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, \n",
      "           RMSE_External_std, X_internal, X_external, X, f, des1, des2)\n",
      "    plot_model(f, X_internal, X_external, Y_internal, Y_external,\n",
      "                R2_train_mean, Q2_External_mean,\n",
      "                importances_mean, importances_std, Feature, prediction,\n",
      "                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)\n",
      "    \n",
      "outfile.close()\n",
      "    #END\n",
      "141/160:\n",
      "import numpy as np\n",
      "\n",
      "def Remove_useless_descriptor(df, threshold):\n",
      "    \n",
      "    des1 = len(df.columns) \n",
      "    \n",
      "    h = df.columns.tolist()\n",
      "    df = df..astype(np.float)\n",
      "    df = np.array(df)\n",
      "\n",
      "    STDEV = np.std(df, axis=0)\n",
      "    idx = [idx for idx, val in enumerate(STDEV) if val > threshold]\n",
      "    df2 = df[:,idx]\n",
      "    hx = np.array(h)[idx]\n",
      "    \n",
      "    df = pd.DataFrame(df2, columns=[hx])\n",
      "    \n",
      "    des2 = len(df.columns)\n",
      "    \n",
      "    print('from Remove useless descriptor')\n",
      "    print(\"The initial set of \" + str(des1) + \\\n",
      "          \" descriptors has been reduced to \" + str(des2) + \" descriptors.\")\n",
      "    \n",
      "    return df, des1, des2\n",
      "141/161:\n",
      "import numpy as np\n",
      "\n",
      "def Remove_useless_descriptor(df, threshold):\n",
      "    \n",
      "    des1 = len(df.columns) \n",
      "    \n",
      "    h = df.columns.tolist()\n",
      "    df = df.astype(np.float)\n",
      "    df = np.array(df)\n",
      "\n",
      "    STDEV = np.std(df, axis=0)\n",
      "    idx = [idx for idx, val in enumerate(STDEV) if val > threshold]\n",
      "    df2 = df[:,idx]\n",
      "    hx = np.array(h)[idx]\n",
      "    \n",
      "    df = pd.DataFrame(df2, columns=[hx])\n",
      "    \n",
      "    des2 = len(df.columns)\n",
      "    \n",
      "    print('from Remove useless descriptor')\n",
      "    print(\"The initial set of \" + str(des1) + \\\n",
      "          \" descriptors has been reduced to \" + str(des2) + \" descriptors.\")\n",
      "    \n",
      "    return df, des1, des2\n",
      "141/162:\n",
      "import glob # to read multiple files \n",
      "from rdkit import Chem, DataStructs\n",
      "from rdkit.Chem import AllChem\n",
      "import os\n",
      "\n",
      "%config InlineBackend.figure_format = 'retina'\n",
      "\n",
      "! rm Result/Result_LogKp.csv\n",
      "\n",
      "outfile = open('Result/Result_LogKp.csv', 'a')\n",
      "\n",
      "print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\\\n",
      "                    'R2_train,R2_train_std,' + \\\n",
      "                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \\\n",
      "                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')\n",
      "\n",
      "path = r'QSAR/'\n",
      "\n",
      "for f in glob.glob(path + '*.csv'):\n",
      "    df = pd.read_csv(f)\n",
      "    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))\n",
      "    df = df.fillna(method='ffill')\n",
      "    Y = df[\"LogKp\"].astype(np.float)\n",
      "    data = df\n",
      "    \n",
      "    print ('\\n\\n************************************************************************************')\n",
      "    print ('')\n",
      "    print (f)\n",
      "    print ('')\n",
      "    \n",
      "    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%\n",
      "    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7\n",
      "\n",
      "    h = data.columns.tolist()\n",
      "    hx = np.array(h)\n",
      "\n",
      "    data = data.as_matrix().astype(np.float)\n",
      "    X = np.array(data)\n",
      "    \n",
      "    # Prepare empty lists to plot QSAR model\n",
      "    R2_train = []\n",
      "    RMSE_train = []\n",
      "    Q2_CV = []\n",
      "    RMSE_CV = []\n",
      "    Q2_External = []\n",
      "    RMSE_External = []\n",
      "    importances_dict = defaultdict(list)\n",
      "    \n",
      "    # Prepare empty lists to plot the performance of accuracy.\n",
      "    acclist_realRF                          = []\n",
      "    acclist_realRF_predictTrain             = []\n",
      "    acclist_predictionOnTest_scrambledtrain  = []\n",
      "    acclist_predictionOnTrain_scrambledtrain = []\n",
      "        \n",
      "    for i in range(10):\n",
      "        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)\n",
      "            \n",
      "    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \\\n",
      "                                                                        X_internal, X_external, Y_internal, Y_external)\n",
      "    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \\\n",
      "        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\\\n",
      "        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \n",
      "           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, \n",
      "           RMSE_External_std, X_internal, X_external, X, f, des1, des2)\n",
      "    plot_model(f, X_internal, X_external, Y_internal, Y_external,\n",
      "                R2_train_mean, Q2_External_mean,\n",
      "                importances_mean, importances_std, Feature, prediction,\n",
      "                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)\n",
      "    \n",
      "outfile.close()\n",
      "    #END\n",
      "141/163:\n",
      "import numpy as np\n",
      "\n",
      "def Remove_useless_descriptor(df, threshold):\n",
      "    \n",
      "    des1 = len(df.columns) \n",
      "    \n",
      "    h = df.columns.tolist()\n",
      "    df = df.as_matrix().astype(np.float)\n",
      "    df = np.array(df)\n",
      "\n",
      "    STDEV = np.std(df, axis=0)\n",
      "    idx = [idx for idx, val in enumerate(STDEV) if val > threshold]\n",
      "    df2 = df[:,idx]\n",
      "    hx = np.array(h)[idx]\n",
      "    \n",
      "    df = pd.DataFrame(df2, columns=[hx])\n",
      "    \n",
      "    des2 = len(df.columns)\n",
      "    \n",
      "    print('from Remove useless descriptor')\n",
      "    print(\"The initial set of \" + str(des1) + \\\n",
      "          \" descriptors has been reduced to \" + str(des2) + \" descriptors.\")\n",
      "    \n",
      "    return df, des1, des2\n",
      "141/164:\n",
      "import glob # to read multiple files \n",
      "from rdkit import Chem, DataStructs\n",
      "from rdkit.Chem import AllChem\n",
      "import os\n",
      "\n",
      "%config InlineBackend.figure_format = 'retina'\n",
      "\n",
      "! rm Result/Result_LogKp.csv\n",
      "\n",
      "outfile = open('Result/Result_LogKp.csv', 'a')\n",
      "\n",
      "print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\\\n",
      "                    'R2_train,R2_train_std,' + \\\n",
      "                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \\\n",
      "                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')\n",
      "\n",
      "path = r'QSAR/'\n",
      "\n",
      "for f in glob.glob(path + '*.csv'):\n",
      "    df = pd.read_csv(f)\n",
      "    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))\n",
      "    df = df.fillna(method='ffill')\n",
      "    Y = df[\"LogKp\"].astype(np.float)\n",
      "    data = df\n",
      "    \n",
      "    print ('\\n\\n************************************************************************************')\n",
      "    print ('')\n",
      "    print (f)\n",
      "    print ('')\n",
      "    \n",
      "    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%\n",
      "    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7\n",
      "\n",
      "    h = data.columns.tolist()\n",
      "    hx = np.array(h)\n",
      "\n",
      "    data = data.as_matrix().astype(np.float)\n",
      "    X = np.array(data)\n",
      "    \n",
      "    # Prepare empty lists to plot QSAR model\n",
      "    R2_train = []\n",
      "    RMSE_train = []\n",
      "    Q2_CV = []\n",
      "    RMSE_CV = []\n",
      "    Q2_External = []\n",
      "    RMSE_External = []\n",
      "    importances_dict = defaultdict(list)\n",
      "    \n",
      "    # Prepare empty lists to plot the performance of accuracy.\n",
      "    acclist_realRF                          = []\n",
      "    acclist_realRF_predictTrain             = []\n",
      "    acclist_predictionOnTest_scrambledtrain  = []\n",
      "    acclist_predictionOnTrain_scrambledtrain = []\n",
      "        \n",
      "    for i in range(10):\n",
      "        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)\n",
      "            \n",
      "    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \\\n",
      "                                                                        X_internal, X_external, Y_internal, Y_external)\n",
      "    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \\\n",
      "        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\\\n",
      "        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \n",
      "           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, \n",
      "           RMSE_External_std, X_internal, X_external, X, f, des1, des2)\n",
      "    plot_model(f, X_internal, X_external, Y_internal, Y_external,\n",
      "                R2_train_mean, Q2_External_mean,\n",
      "                importances_mean, importances_std, Feature, prediction,\n",
      "                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)\n",
      "    \n",
      "outfile.close()\n",
      "    #END\n",
      "141/165:\n",
      "import numpy as np\n",
      "\n",
      "def Remove_useless_descriptor(df, threshold):\n",
      "    \n",
      "    des1 = len(df.columns) \n",
      "    \n",
      "    h = df.columns.tolist()\n",
      "    df = df.values.astype(np.float)\n",
      "    df = np.array(df)\n",
      "\n",
      "    STDEV = np.std(df, axis=0)\n",
      "    idx = [idx for idx, val in enumerate(STDEV) if val > threshold]\n",
      "    df2 = df[:,idx]\n",
      "    hx = np.array(h)[idx]\n",
      "    \n",
      "    df = pd.DataFrame(df2, columns=[hx])\n",
      "    \n",
      "    des2 = len(df.columns)\n",
      "    \n",
      "    print('from Remove useless descriptor')\n",
      "    print(\"The initial set of \" + str(des1) + \\\n",
      "          \" descriptors has been reduced to \" + str(des2) + \" descriptors.\")\n",
      "    \n",
      "    return df, des1, des2\n",
      "141/166:\n",
      "import glob # to read multiple files \n",
      "from rdkit import Chem, DataStructs\n",
      "from rdkit.Chem import AllChem\n",
      "import os\n",
      "\n",
      "%config InlineBackend.figure_format = 'retina'\n",
      "\n",
      "! rm Result/Result_LogKp.csv\n",
      "\n",
      "outfile = open('Result/Result_LogKp.csv', 'a')\n",
      "\n",
      "print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\\\n",
      "                    'R2_train,R2_train_std,' + \\\n",
      "                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \\\n",
      "                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')\n",
      "\n",
      "path = r'QSAR/'\n",
      "\n",
      "for f in glob.glob(path + '*.csv'):\n",
      "    df = pd.read_csv(f)\n",
      "    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))\n",
      "    df = df.fillna(method='ffill')\n",
      "    Y = df[\"LogKp\"].values.astype(np.float)\n",
      "    data = df\n",
      "    \n",
      "    print ('\\n\\n************************************************************************************')\n",
      "    print ('')\n",
      "    print (f)\n",
      "    print ('')\n",
      "    \n",
      "    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%\n",
      "    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7\n",
      "\n",
      "    h = data.columns.tolist()\n",
      "    hx = np.array(h)\n",
      "\n",
      "    data = data.values.astype(np.float)\n",
      "    X = np.array(data)\n",
      "    \n",
      "    # Prepare empty lists to plot QSAR model\n",
      "    R2_train = []\n",
      "    RMSE_train = []\n",
      "    Q2_CV = []\n",
      "    RMSE_CV = []\n",
      "    Q2_External = []\n",
      "    RMSE_External = []\n",
      "    importances_dict = defaultdict(list)\n",
      "    \n",
      "    # Prepare empty lists to plot the performance of accuracy.\n",
      "    acclist_realRF                          = []\n",
      "    acclist_realRF_predictTrain             = []\n",
      "    acclist_predictionOnTest_scrambledtrain  = []\n",
      "    acclist_predictionOnTrain_scrambledtrain = []\n",
      "        \n",
      "    for i in range(10):\n",
      "        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)\n",
      "            \n",
      "    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \\\n",
      "                                                                        X_internal, X_external, Y_internal, Y_external)\n",
      "    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \\\n",
      "        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\\\n",
      "        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \n",
      "           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, \n",
      "           RMSE_External_std, X_internal, X_external, X, f, des1, des2)\n",
      "    plot_model(f, X_internal, X_external, Y_internal, Y_external,\n",
      "                R2_train_mean, Q2_External_mean,\n",
      "                importances_mean, importances_std, Feature, prediction,\n",
      "                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)\n",
      "    \n",
      "outfile.close()\n",
      "    #END\n",
      "142/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "df0 = pd.read_csv('model/ER_alpha_Train_final.csv', header = 0)\n",
      "142/2: len(df0)\n",
      "142/3: df0.tail(2)\n",
      "142/4: df = df0.drop(['STATUS.1'], axis=1)\n",
      "142/5: df = df[['chemblId','pIC50']]\n",
      "142/6: df.head()\n",
      "142/7:\n",
      "path = r'Fingerprint/'\n",
      "\n",
      "Fp1  = pd.read_csv(path + 'Train_ER_alpha-Fingerprinter.csv'               , header = 0)\n",
      "Fp2  = pd.read_csv(path + 'Train_ER_alpha-ExtendedFingerprinter.csv'       , header = 0)\n",
      "Fp3  = pd.read_csv(path + 'Train_ER_alpha-EStateFingerprinter.csv'         , header = 0)\n",
      "Fp4  = pd.read_csv(path + 'Train_ER_alpha-GraphOnlyFingerprinter.csv'      , header = 0)\n",
      "Fp5  = pd.read_csv(path + 'Train_ER_alpha-MACCSFingerprinter.csv'          , header = 0)\n",
      "Fp6  = pd.read_csv(path + 'Train_ER_alpha-PubchemFingerprinter.csv'        , header = 0)\n",
      "Fp7  = pd.read_csv(path + 'Train_ER_alpha-SubstructureFingerprinter.csv'   , header = 0)\n",
      "Fp8  = pd.read_csv(path + 'Train_ER_alpha-SubstructureFingerprintCount.csv', header = 0)\n",
      "Fp9  = pd.read_csv(path + 'Train_ER_alpha-KlekotaRothFingerprinter.csv'    , header = 0)\n",
      "Fp10 = pd.read_csv(path + 'Train_ER_alpha-KlekotaRothFingerprintCount.csv' , header = 0)\n",
      "Fp11 = pd.read_csv(path + 'Train_ER_alpha-AtomPairs2DFingerprinter.csv'    , header = 0)\n",
      "Fp12 = pd.read_csv(path + 'Train_ER_alpha-AtomPairs2DFingerprintCount.csv' , header = 0)\n",
      "142/8:\n",
      "Fp1 = Fp1.rename(columns = {'Name':'chemblId'})\n",
      "Fp2 = Fp2.rename(columns = {'Name':'chemblId'})\n",
      "Fp3 = Fp3.rename(columns = {'Name':'chemblId'})\n",
      "Fp4 = Fp4.rename(columns = {'Name':'chemblId'})\n",
      "Fp5 = Fp5.rename(columns = {'Name':'chemblId'})\n",
      "Fp6 = Fp6.rename(columns = {'Name':'chemblId'})\n",
      "Fp7 = Fp7.rename(columns = {'Name':'chemblId'})\n",
      "Fp8 = Fp8.rename(columns = {'Name':'chemblId'})\n",
      "Fp9 = Fp9.rename(columns = {'Name':'chemblId'})\n",
      "Fp10= Fp10.rename(columns = {'Name':'chemblId'})\n",
      "Fp11= Fp11.rename(columns = {'Name':'chemblId'})\n",
      "Fp12= Fp12.rename(columns = {'Name':'chemblId'})\n",
      "142/9:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    chemblId = Fp.chemblId\n",
      "    Fp_ix = Fp.ix[:,1:]\n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp_ix)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled)\n",
      "    Fp_normalized\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp_ix.columns)\n",
      "    Fp_normalized['chemblId'] = chemblId\n",
      "    \n",
      "    return Fp_normalized\n",
      "142/10:\n",
      "Fp1_n  = normalized (Fp1 )\n",
      "Fp2_n  = normalized (Fp2 )\n",
      "Fp3_n  = normalized (Fp3 )\n",
      "Fp4_n  = normalized (Fp4 )\n",
      "Fp5_n  = normalized (Fp5 )\n",
      "Fp6_n  = normalized (Fp6 )\n",
      "Fp7_n  = normalized (Fp7 )\n",
      "Fp8_n  = normalized (Fp8 )\n",
      "Fp9_n  = normalized (Fp9 )\n",
      "Fp10_n = normalized (Fp10)\n",
      "Fp11_n = normalized (Fp11)\n",
      "Fp12_n = normalized (Fp12)\n",
      "141/167:\n",
      "raw1 .to_csv('QSAR/Kp_Fingerprinter.csv'               , sep=',' ,index=False)\n",
      "raw2 .to_csv('QSAR/Kp_ExtendedFingerprinter.csv'       , sep=',' ,index=True)\n",
      "raw3 .to_csv('QSAR/Kp_EStateFingerprinter.csv'         , sep=',' ,index=True)\n",
      "raw4 .to_csv('QSAR/Kp_GraphOnlyFingerprinter.csv'      , sep=',' ,index=True)\n",
      "raw5 .to_csv('QSAR/Kp_MACCSFingerprinter.csv'          , sep=',' ,index=True)\n",
      "raw6 .to_csv('QSAR/Kp_PubchemFingerprinter.csv'        , sep=',' ,index=True)\n",
      "raw7 .to_csv('QSAR/Kp_SubstructureFingerprinter.csv'   , sep=',' ,index=True)\n",
      "raw8 .to_csv('QSAR/Kp_SubstructureFingerprintCount.csv', sep=',' ,index=True)\n",
      "raw9 .to_csv('QSAR/Kp_KlekotaRothFingerprinter.csv'    , sep=',' ,index=True)\n",
      "raw10.to_csv('QSAR/Kp_KlekotaRothFingerprintCount.csv' , sep=',' ,index=True)\n",
      "raw11.to_csv('QSAR/Kp_AtomPairs2DFingerprinter.csv'    , sep=',' ,index=True)\n",
      "raw12.to_csv('QSAR/Kp_AtomPairs2DFingerprintCount.csv' , sep=',' ,index=True)\n",
      "141/168:\n",
      "import glob # to read multiple files \n",
      "from rdkit import Chem, DataStructs\n",
      "from rdkit.Chem import AllChem\n",
      "import os\n",
      "\n",
      "%config InlineBackend.figure_format = 'retina'\n",
      "\n",
      "! rm Result/Result_LogKp.csv\n",
      "\n",
      "outfile = open('Result/Result_LogKp.csv', 'a')\n",
      "\n",
      "print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\\\n",
      "                    'R2_train,R2_train_std,' + \\\n",
      "                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \\\n",
      "                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')\n",
      "\n",
      "path = r'QSAR/'\n",
      "\n",
      "for f in glob.glob(path + '*.csv'):\n",
      "    df = pd.read_csv(f)\n",
      "    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))\n",
      "    df = df.fillna(method='ffill')\n",
      "    Y = df[\"LogKp\"].values.astype(np.float)\n",
      "    data = df\n",
      "    \n",
      "    print ('\\n\\n************************************************************************************')\n",
      "    print ('')\n",
      "    print (f)\n",
      "    print ('')\n",
      "    \n",
      "    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%\n",
      "    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7\n",
      "\n",
      "    h = data.columns.tolist()\n",
      "    hx = np.array(h)\n",
      "\n",
      "    data = data.values.astype(np.float)\n",
      "    X = np.array(data)\n",
      "    \n",
      "    # Prepare empty lists to plot QSAR model\n",
      "    R2_train = []\n",
      "    RMSE_train = []\n",
      "    Q2_CV = []\n",
      "    RMSE_CV = []\n",
      "    Q2_External = []\n",
      "    RMSE_External = []\n",
      "    importances_dict = defaultdict(list)\n",
      "    \n",
      "    # Prepare empty lists to plot the performance of accuracy.\n",
      "    acclist_realRF                          = []\n",
      "    acclist_realRF_predictTrain             = []\n",
      "    acclist_predictionOnTest_scrambledtrain  = []\n",
      "    acclist_predictionOnTrain_scrambledtrain = []\n",
      "        \n",
      "    for i in range(10):\n",
      "        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)\n",
      "            \n",
      "    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \\\n",
      "                                                                        X_internal, X_external, Y_internal, Y_external)\n",
      "    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \\\n",
      "        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\\\n",
      "        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \n",
      "           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, \n",
      "           RMSE_External_std, X_internal, X_external, X, f, des1, des2)\n",
      "    plot_model(f, X_internal, X_external, Y_internal, Y_external,\n",
      "                R2_train_mean, Q2_External_mean,\n",
      "                importances_mean, importances_std, Feature, prediction,\n",
      "                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)\n",
      "    \n",
      "outfile.close()\n",
      "    #END\n",
      "141/169:\n",
      "raw1 .to_csv('QSAR/Kp_Fingerprinter.csv'               , sep=',' ,index=False)\n",
      "raw2 .to_csv('QSAR/Kp_ExtendedFingerprinter.csv'       , sep=',' ,index=True)\n",
      "raw3 .to_csv('QSAR/Kp_EStateFingerprinter.csv'         , sep=',' ,index=True)\n",
      "raw4 .to_csv('QSAR/Kp_GraphOnlyFingerprinter.csv'      , sep=',' ,index=True)\n",
      "raw5 .to_csv('QSAR/Kp_MACCSFingerprinter.csv'          , sep=',' ,index=True)\n",
      "raw6 .to_csv('QSAR/Kp_PubchemFingerprinter.csv'        , sep=',' ,index=True)\n",
      "raw7 .to_csv('QSAR/Kp_SubstructureFingerprinter.csv'   , sep=',' ,index=False)\n",
      "raw8 .to_csv('QSAR/Kp_SubstructureFingerprintCount.csv', sep=',' ,index=True)\n",
      "raw9 .to_csv('QSAR/Kp_KlekotaRothFingerprinter.csv'    , sep=',' ,index=True)\n",
      "raw10.to_csv('QSAR/Kp_KlekotaRothFingerprintCount.csv' , sep=',' ,index=True)\n",
      "raw11.to_csv('QSAR/Kp_AtomPairs2DFingerprinter.csv'    , sep=',' ,index=True)\n",
      "raw12.to_csv('QSAR/Kp_AtomPairs2DFingerprintCount.csv' , sep=',' ,index=True)\n",
      "141/170:\n",
      "import glob # to read multiple files \n",
      "from rdkit import Chem, DataStructs\n",
      "from rdkit.Chem import AllChem\n",
      "import os\n",
      "\n",
      "%config InlineBackend.figure_format = 'retina'\n",
      "\n",
      "! rm Result/Result_LogKp.csv\n",
      "\n",
      "outfile = open('Result/Result_LogKp.csv', 'a')\n",
      "\n",
      "print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\\\n",
      "                    'R2_train,R2_train_std,' + \\\n",
      "                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \\\n",
      "                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')\n",
      "\n",
      "path = r'QSAR/'\n",
      "\n",
      "for f in glob.glob(path + '*.csv'):\n",
      "    df = pd.read_csv(f)\n",
      "    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))\n",
      "    df = df.fillna(method='ffill')\n",
      "    Y = df[\"LogKp\"].values.astype(np.float)\n",
      "    data = df\n",
      "    \n",
      "    print ('\\n\\n************************************************************************************')\n",
      "    print ('')\n",
      "    print (f)\n",
      "    print ('')\n",
      "    \n",
      "    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%\n",
      "    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7\n",
      "\n",
      "    h = data.columns.tolist()\n",
      "    hx = np.array(h)\n",
      "\n",
      "    data = data.values.astype(np.float)\n",
      "    X = np.array(data)\n",
      "    \n",
      "    # Prepare empty lists to plot QSAR model\n",
      "    R2_train = []\n",
      "    RMSE_train = []\n",
      "    Q2_CV = []\n",
      "    RMSE_CV = []\n",
      "    Q2_External = []\n",
      "    RMSE_External = []\n",
      "    importances_dict = defaultdict(list)\n",
      "    \n",
      "    # Prepare empty lists to plot the performance of accuracy.\n",
      "    acclist_realRF                          = []\n",
      "    acclist_realRF_predictTrain             = []\n",
      "    acclist_predictionOnTest_scrambledtrain  = []\n",
      "    acclist_predictionOnTrain_scrambledtrain = []\n",
      "        \n",
      "    for i in range(10):\n",
      "        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)\n",
      "            \n",
      "    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \\\n",
      "                                                                        X_internal, X_external, Y_internal, Y_external)\n",
      "    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \\\n",
      "        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\\\n",
      "        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \n",
      "           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, \n",
      "           RMSE_External_std, X_internal, X_external, X, f, des1, des2)\n",
      "    plot_model(f, X_internal, X_external, Y_internal, Y_external,\n",
      "                R2_train_mean, Q2_External_mean,\n",
      "                importances_mean, importances_std, Feature, prediction,\n",
      "                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)\n",
      "    \n",
      "outfile.close()\n",
      "    #END\n",
      "141/171:\n",
      "raw1 .to_csv('QSAR/Kp_Fingerprinter.csv'               , sep=',' ,index=False)\n",
      "raw2 .to_csv('QSAR/Kp_ExtendedFingerprinter.csv'       , sep=',' ,index=False)\n",
      "raw3 .to_csv('QSAR/Kp_EStateFingerprinter.csv'         , sep=',' ,index=False)\n",
      "raw4 .to_csv('QSAR/Kp_GraphOnlyFingerprinter.csv'      , sep=',' ,index=False)\n",
      "raw5 .to_csv('QSAR/Kp_MACCSFingerprinter.csv'          , sep=',' ,index=False)\n",
      "raw6 .to_csv('QSAR/Kp_PubchemFingerprinter.csv'        , sep=',' ,index=False)\n",
      "raw7 .to_csv('QSAR/Kp_SubstructureFingerprinter.csv'   , sep=',' ,index=False)\n",
      "raw8 .to_csv('QSAR/Kp_SubstructureFingerprintCount.csv', sep=',' ,index=False)\n",
      "raw9 .to_csv('QSAR/Kp_KlekotaRothFingerprinter.csv'    , sep=',' ,index=False)\n",
      "raw10.to_csv('QSAR/Kp_KlekotaRothFingerprintCount.csv' , sep=',' ,index=False)\n",
      "raw11.to_csv('QSAR/Kp_AtomPairs2DFingerprinter.csv'    , sep=',' ,index=False)\n",
      "raw12.to_csv('QSAR/Kp_AtomPairs2DFingerprintCount.csv' , sep=',' ,index=False)\n",
      "141/172:\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "\n",
      "def build_model(X, Y, seed, hx, f):\n",
      "    \n",
      "    \n",
      "    #Data split using 70/30 ratio\n",
      "    X_internal, X_external, Y_internal, Y_external = train_test_split(X,\n",
      "                                                        Y, test_size=0.3,\n",
      "                                                        random_state=seed)\n",
      "\n",
      "    # Training set\n",
      "    rf = RandomForestRegressor(n_estimators=400, \n",
      "                               max_features='sqrt', \n",
      "                               min_samples_leaf = 1,\n",
      "                               random_state = 13,\n",
      "                               n_jobs=-1)\n",
      "    \n",
      "    rf.fit(X_internal,Y_internal)\n",
      "    prediction = rf.predict(X_internal)\n",
      "    \n",
      "    # Cross-validation\n",
      "    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)\n",
      "    \n",
      "    # External set  \n",
      "    prediction_external = rf.predict(X_external)\n",
      "    \n",
      "    #print result from each seed    \n",
      "    R2_train.append(r2_score(Y_internal, prediction))\n",
      "    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))\n",
      "    Q2_CV.append(r2_score(Y_internal, cv))\n",
      "    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))\n",
      "    Q2_External.append(r2_score(Y_external, prediction_external))\n",
      "    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))\n",
      "    \n",
      "    #Feature Importance\n",
      "    Feature = hx[:]\n",
      "    feature_importance = rf.feature_importances_\n",
      "    importances = 100.0 * (feature_importance / feature_importance.max()) #index\n",
      "    \n",
      "    for i, fx in enumerate(Feature):\n",
      "        importances_dict[fx].append(importances[i])\n",
      "    \n",
      "    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict\n",
      "141/173:\n",
      "import glob # to read multiple files \n",
      "from rdkit import Chem, DataStructs\n",
      "from rdkit.Chem import AllChem\n",
      "import os\n",
      "\n",
      "%config InlineBackend.figure_format = 'retina'\n",
      "\n",
      "! rm Result/Result_LogKp.csv\n",
      "\n",
      "outfile = open('Result/Result_LogKp.csv', 'a')\n",
      "\n",
      "print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\\\n",
      "                    'R2_train,R2_train_std,' + \\\n",
      "                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \\\n",
      "                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')\n",
      "\n",
      "path = r'QSAR/'\n",
      "\n",
      "for f in glob.glob(path + '*.csv'):\n",
      "    df = pd.read_csv(f)\n",
      "    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))\n",
      "    df = df.fillna(method='ffill')\n",
      "    Y = df[\"LogKp\"].values.astype(np.float)\n",
      "    data = df\n",
      "    \n",
      "    print ('\\n\\n************************************************************************************')\n",
      "    print ('')\n",
      "    print (f)\n",
      "    print ('')\n",
      "    \n",
      "    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%\n",
      "    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7\n",
      "\n",
      "    h = data.columns.tolist()\n",
      "    hx = np.array(h)\n",
      "\n",
      "    data = data.values.astype(np.float)\n",
      "    X = np.array(data)\n",
      "    \n",
      "    # Prepare empty lists to plot QSAR model\n",
      "    R2_train = []\n",
      "    RMSE_train = []\n",
      "    Q2_CV = []\n",
      "    RMSE_CV = []\n",
      "    Q2_External = []\n",
      "    RMSE_External = []\n",
      "    importances_dict = defaultdict(list)\n",
      "    \n",
      "    # Prepare empty lists to plot the performance of accuracy.\n",
      "    acclist_realRF                          = []\n",
      "    acclist_realRF_predictTrain             = []\n",
      "    acclist_predictionOnTest_scrambledtrain  = []\n",
      "    acclist_predictionOnTrain_scrambledtrain = []\n",
      "        \n",
      "    for i in range(10):\n",
      "        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)\n",
      "            \n",
      "    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \\\n",
      "                                                                        X_internal, X_external, Y_internal, Y_external)\n",
      "    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \\\n",
      "        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\\\n",
      "        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \n",
      "           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, \n",
      "           RMSE_External_std, X_internal, X_external, X, f, des1, des2)\n",
      "    plot_model(f, X_internal, X_external, Y_internal, Y_external,\n",
      "                R2_train_mean, Q2_External_mean,\n",
      "                importances_mean, importances_std, Feature, prediction,\n",
      "                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)\n",
      "    \n",
      "outfile.close()\n",
      "    #END\n",
      "141/174:\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "\n",
      "def build_model(X, Y, seed, hx, f):\n",
      "    \n",
      "    \n",
      "    #Data split using 70/30 ratio\n",
      "    X_internal, X_external, Y_internal, Y_external = train_test_split(X,\n",
      "                                                        Y, test_size=0.3,\n",
      "                                                        random_state=seed)\n",
      "\n",
      "    # Training set\n",
      "    rf = RandomForestRegressor(n_estimators=400, \n",
      "                               max_features='sqrt', \n",
      "                               min_samples_leaf = 1,\n",
      "                               random_state = 13,\n",
      "                               n_jobs=-1)\n",
      "    \n",
      "    rf.fit(X_internal,Y_internal)\n",
      "    prediction = rf.predict(X_internal)\n",
      "    \n",
      "    # Cross-validation\n",
      "    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)\n",
      "    \n",
      "    # External set  \n",
      "    prediction_external = rf.predict(X_external)\n",
      "    \n",
      "    #print result from each seed    \n",
      "    R2_train.append(r2_score(Y_internal, prediction))\n",
      "    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))\n",
      "    Q2_CV.append(r2_score(Y_internal, cv))\n",
      "    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))\n",
      "    Q2_External.append(r2_score(Y_external, prediction_external))\n",
      "    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))\n",
      "    \n",
      "    #Feature Importance\n",
      "    Feature = hx[:]\n",
      "    feature_importance = rf.feature_importances_\n",
      "    importances = 100.0 * (feature_importance / feature_importance.max()) #index\n",
      "    \n",
      "    for i, fx in enumerate(Feature):\n",
      "        importances[fx].append(importances[i])\n",
      "    \n",
      "    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict\n",
      "141/175:\n",
      "import glob # to read multiple files \n",
      "from rdkit import Chem, DataStructs\n",
      "from rdkit.Chem import AllChem\n",
      "import os\n",
      "\n",
      "%config InlineBackend.figure_format = 'retina'\n",
      "\n",
      "! rm Result/Result_LogKp.csv\n",
      "\n",
      "outfile = open('Result/Result_LogKp.csv', 'a')\n",
      "\n",
      "print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\\\n",
      "                    'R2_train,R2_train_std,' + \\\n",
      "                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \\\n",
      "                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')\n",
      "\n",
      "path = r'QSAR/'\n",
      "\n",
      "for f in glob.glob(path + '*.csv'):\n",
      "    df = pd.read_csv(f)\n",
      "    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))\n",
      "    df = df.fillna(method='ffill')\n",
      "    Y = df[\"LogKp\"].values.astype(np.float)\n",
      "    data = df\n",
      "    \n",
      "    print ('\\n\\n************************************************************************************')\n",
      "    print ('')\n",
      "    print (f)\n",
      "    print ('')\n",
      "    \n",
      "    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%\n",
      "    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7\n",
      "\n",
      "    h = data.columns.tolist()\n",
      "    hx = np.array(h)\n",
      "\n",
      "    data = data.values.astype(np.float)\n",
      "    X = np.array(data)\n",
      "    \n",
      "    # Prepare empty lists to plot QSAR model\n",
      "    R2_train = []\n",
      "    RMSE_train = []\n",
      "    Q2_CV = []\n",
      "    RMSE_CV = []\n",
      "    Q2_External = []\n",
      "    RMSE_External = []\n",
      "    importances_dict = defaultdict(list)\n",
      "    \n",
      "    # Prepare empty lists to plot the performance of accuracy.\n",
      "    acclist_realRF                          = []\n",
      "    acclist_realRF_predictTrain             = []\n",
      "    acclist_predictionOnTest_scrambledtrain  = []\n",
      "    acclist_predictionOnTrain_scrambledtrain = []\n",
      "        \n",
      "    for i in range(10):\n",
      "        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)\n",
      "            \n",
      "    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \\\n",
      "                                                                        X_internal, X_external, Y_internal, Y_external)\n",
      "    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \\\n",
      "        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\\\n",
      "        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \n",
      "           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, \n",
      "           RMSE_External_std, X_internal, X_external, X, f, des1, des2)\n",
      "    plot_model(f, X_internal, X_external, Y_internal, Y_external,\n",
      "                R2_train_mean, Q2_External_mean,\n",
      "                importances_mean, importances_std, Feature, prediction,\n",
      "                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)\n",
      "    \n",
      "outfile.close()\n",
      "    #END\n",
      "141/176:\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "\n",
      "def build_model(X, Y, seed, hx, f):\n",
      "    \n",
      "    \n",
      "    #Data split using 70/30 ratio\n",
      "    X_internal, X_external, Y_internal, Y_external = train_test_split(X,\n",
      "                                                        Y, test_size=0.3,\n",
      "                                                        random_state=seed)\n",
      "\n",
      "    # Training set\n",
      "    rf = RandomForestRegressor(n_estimators=400, \n",
      "                               max_features='sqrt', \n",
      "                               min_samples_leaf = 1,\n",
      "                               random_state = 13,\n",
      "                               n_jobs=-1)\n",
      "    \n",
      "    rf.fit(X_internal,Y_internal)\n",
      "    prediction = rf.predict(X_internal)\n",
      "    \n",
      "    # Cross-validation\n",
      "    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)\n",
      "    \n",
      "    # External set  \n",
      "    prediction_external = rf.predict(X_external)\n",
      "    \n",
      "    #print result from each seed    \n",
      "    R2_train.append(r2_score(Y_internal, prediction))\n",
      "    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))\n",
      "    Q2_CV.append(r2_score(Y_internal, cv))\n",
      "    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))\n",
      "    Q2_External.append(r2_score(Y_external, prediction_external))\n",
      "    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))\n",
      "    \n",
      "    #Feature Importance\n",
      "    Feature = hx[:]\n",
      "    feature_importance = rf.feature_importances_\n",
      "    importances = 100.0 * (feature_importance / feature_importance.max()) #index\n",
      "    \n",
      "    for i, fx in enumerate(Feature):\n",
      "        importances_dict[fx].append(importances_dict[i])\n",
      "    \n",
      "    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict\n",
      "141/177:\n",
      "import glob # to read multiple files \n",
      "from rdkit import Chem, DataStructs\n",
      "from rdkit.Chem import AllChem\n",
      "import os\n",
      "\n",
      "%config InlineBackend.figure_format = 'retina'\n",
      "\n",
      "! rm Result/Result_LogKp.csv\n",
      "\n",
      "outfile = open('Result/Result_LogKp.csv', 'a')\n",
      "\n",
      "print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\\\n",
      "                    'R2_train,R2_train_std,' + \\\n",
      "                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \\\n",
      "                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')\n",
      "\n",
      "path = r'QSAR/'\n",
      "\n",
      "for f in glob.glob(path + '*.csv'):\n",
      "    df = pd.read_csv(f)\n",
      "    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))\n",
      "    df = df.fillna(method='ffill')\n",
      "    Y = df[\"LogKp\"].values.astype(np.float)\n",
      "    data = df\n",
      "    \n",
      "    print ('\\n\\n************************************************************************************')\n",
      "    print ('')\n",
      "    print (f)\n",
      "    print ('')\n",
      "    \n",
      "    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%\n",
      "    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7\n",
      "\n",
      "    h = data.columns.tolist()\n",
      "    hx = np.array(h)\n",
      "\n",
      "    data = data.values.astype(np.float)\n",
      "    X = np.array(data)\n",
      "    \n",
      "    # Prepare empty lists to plot QSAR model\n",
      "    R2_train = []\n",
      "    RMSE_train = []\n",
      "    Q2_CV = []\n",
      "    RMSE_CV = []\n",
      "    Q2_External = []\n",
      "    RMSE_External = []\n",
      "    importances_dict = defaultdict(list)\n",
      "    \n",
      "    # Prepare empty lists to plot the performance of accuracy.\n",
      "    acclist_realRF                          = []\n",
      "    acclist_realRF_predictTrain             = []\n",
      "    acclist_predictionOnTest_scrambledtrain  = []\n",
      "    acclist_predictionOnTrain_scrambledtrain = []\n",
      "        \n",
      "    for i in range(10):\n",
      "        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)\n",
      "            \n",
      "    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \\\n",
      "                                                                        X_internal, X_external, Y_internal, Y_external)\n",
      "    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \\\n",
      "        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\\\n",
      "        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \n",
      "           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, \n",
      "           RMSE_External_std, X_internal, X_external, X, f, des1, des2)\n",
      "    plot_model(f, X_internal, X_external, Y_internal, Y_external,\n",
      "                R2_train_mean, Q2_External_mean,\n",
      "                importances_mean, importances_std, Feature, prediction,\n",
      "                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)\n",
      "    \n",
      "outfile.close()\n",
      "    #END\n",
      "141/178:\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "\n",
      "def build_model(X, Y, seed, hx, f):\n",
      "    \n",
      "    \n",
      "    #Data split using 70/30 ratio\n",
      "    X_internal, X_external, Y_internal, Y_external = train_test_split(X,\n",
      "                                                        Y, test_size=0.3,\n",
      "                                                        random_state=seed)\n",
      "\n",
      "    # Training set\n",
      "    rf = RandomForestRegressor(n_estimators=400, \n",
      "                               max_features='sqrt', \n",
      "                               min_samples_leaf = 1,\n",
      "                               random_state = 13,\n",
      "                               n_jobs=-1)\n",
      "    \n",
      "    rf.fit(X_internal,Y_internal)\n",
      "    prediction = rf.predict(X_internal)\n",
      "    \n",
      "    # Cross-validation\n",
      "    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)\n",
      "    \n",
      "    # External set  \n",
      "    prediction_external = rf.predict(X_external)\n",
      "    \n",
      "    #print result from each seed    \n",
      "    R2_train.append(r2_score(Y_internal, prediction))\n",
      "    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))\n",
      "    Q2_CV.append(r2_score(Y_internal, cv))\n",
      "    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))\n",
      "    Q2_External.append(r2_score(Y_external, prediction_external))\n",
      "    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))\n",
      "    \n",
      "    #Feature Importance\n",
      "    Feature = hx[:]\n",
      "    feature_importance = rf.feature_importances_\n",
      "    importances = 100.0 * (feature_importance / feature_importance.max()) #index\n",
      "    \n",
      "    for i, fx in enumerate(Feature):\n",
      "        importances[fx].append(importances_dict[i])\n",
      "    \n",
      "    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict\n",
      "141/179:\n",
      "import glob # to read multiple files \n",
      "from rdkit import Chem, DataStructs\n",
      "from rdkit.Chem import AllChem\n",
      "import os\n",
      "\n",
      "%config InlineBackend.figure_format = 'retina'\n",
      "\n",
      "! rm Result/Result_LogKp.csv\n",
      "\n",
      "outfile = open('Result/Result_LogKp.csv', 'a')\n",
      "\n",
      "print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\\\n",
      "                    'R2_train,R2_train_std,' + \\\n",
      "                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \\\n",
      "                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')\n",
      "\n",
      "path = r'QSAR/'\n",
      "\n",
      "for f in glob.glob(path + '*.csv'):\n",
      "    df = pd.read_csv(f)\n",
      "    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))\n",
      "    df = df.fillna(method='ffill')\n",
      "    Y = df[\"LogKp\"].values.astype(np.float)\n",
      "    data = df\n",
      "    \n",
      "    print ('\\n\\n************************************************************************************')\n",
      "    print ('')\n",
      "    print (f)\n",
      "    print ('')\n",
      "    \n",
      "    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%\n",
      "    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7\n",
      "\n",
      "    h = data.columns.tolist()\n",
      "    hx = np.array(h)\n",
      "\n",
      "    data = data.values.astype(np.float)\n",
      "    X = np.array(data)\n",
      "    \n",
      "    # Prepare empty lists to plot QSAR model\n",
      "    R2_train = []\n",
      "    RMSE_train = []\n",
      "    Q2_CV = []\n",
      "    RMSE_CV = []\n",
      "    Q2_External = []\n",
      "    RMSE_External = []\n",
      "    importances_dict = defaultdict(list)\n",
      "    \n",
      "    # Prepare empty lists to plot the performance of accuracy.\n",
      "    acclist_realRF                          = []\n",
      "    acclist_realRF_predictTrain             = []\n",
      "    acclist_predictionOnTest_scrambledtrain  = []\n",
      "    acclist_predictionOnTrain_scrambledtrain = []\n",
      "        \n",
      "    for i in range(10):\n",
      "        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)\n",
      "            \n",
      "    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \\\n",
      "                                                                        X_internal, X_external, Y_internal, Y_external)\n",
      "    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \\\n",
      "        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\\\n",
      "        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \n",
      "           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, \n",
      "           RMSE_External_std, X_internal, X_external, X, f, des1, des2)\n",
      "    plot_model(f, X_internal, X_external, Y_internal, Y_external,\n",
      "                R2_train_mean, Q2_External_mean,\n",
      "                importances_mean, importances_std, Feature, prediction,\n",
      "                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)\n",
      "    \n",
      "outfile.close()\n",
      "    #END\n",
      "141/180:\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "\n",
      "def build_model(X, Y, seed, hx, f):\n",
      "    \n",
      "    \n",
      "    #Data split using 70/30 ratio\n",
      "    X_internal, X_external, Y_internal, Y_external = train_test_split(X,\n",
      "                                                        Y, test_size=0.3,\n",
      "                                                        random_state=seed)\n",
      "\n",
      "    # Training set\n",
      "    rf = RandomForestRegressor(n_estimators=400, \n",
      "                               max_features='sqrt', \n",
      "                               min_samples_leaf = 1,\n",
      "                               random_state = 13,\n",
      "                               n_jobs=-1)\n",
      "    \n",
      "    rf.fit(X_internal,Y_internal)\n",
      "    prediction = rf.predict(X_internal)\n",
      "    \n",
      "    # Cross-validation\n",
      "    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)\n",
      "    \n",
      "    # External set  \n",
      "    prediction_external = rf.predict(X_external)\n",
      "    \n",
      "    #print result from each seed    \n",
      "    R2_train.append(r2_score(Y_internal, prediction))\n",
      "    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))\n",
      "    Q2_CV.append(r2_score(Y_internal, cv))\n",
      "    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))\n",
      "    Q2_External.append(r2_score(Y_external, prediction_external))\n",
      "    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))\n",
      "    \n",
      "    #Feature Importance\n",
      "    Feature = hx[:]\n",
      "    feature_importance = rf.feature_importances_\n",
      "    importances = 100.0 * (feature_importance / feature_importance.max()) #index\n",
      "    \n",
      "    for i, fx in enumerate(Feature):\n",
      "        importances[fx].append(importances_dict[i]).astype(int)\n",
      "    \n",
      "    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict\n",
      "141/181:\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "\n",
      "def build_model(X, Y, seed, hx, f):\n",
      "    \n",
      "    \n",
      "    #Data split using 70/30 ratio\n",
      "    X_internal, X_external, Y_internal, Y_external = train_test_split(X,\n",
      "                                                        Y, test_size=0.3,\n",
      "                                                        random_state=seed)\n",
      "\n",
      "    # Training set\n",
      "    rf = RandomForestRegressor(n_estimators=400, \n",
      "                               max_features='sqrt', \n",
      "                               min_samples_leaf = 1,\n",
      "                               random_state = 13,\n",
      "                               n_jobs=-1)\n",
      "    \n",
      "    rf.fit(X_internal,Y_internal)\n",
      "    prediction = rf.predict(X_internal)\n",
      "    \n",
      "    # Cross-validation\n",
      "    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)\n",
      "    \n",
      "    # External set  \n",
      "    prediction_external = rf.predict(X_external)\n",
      "    \n",
      "    #print result from each seed    \n",
      "    R2_train.append(r2_score(Y_internal, prediction))\n",
      "    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))\n",
      "    Q2_CV.append(r2_score(Y_internal, cv))\n",
      "    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))\n",
      "    Q2_External.append(r2_score(Y_external, prediction_external))\n",
      "    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))\n",
      "    \n",
      "    #Feature Importance\n",
      "    Feature = hx[:]\n",
      "    feature_importance = rf.feature_importances_\n",
      "    importances = 100.0 * (feature_importance / feature_importance.max()) #index\n",
      "    \n",
      "    for i, fx in enumerate(Feature):\n",
      "        importances[fx].astype(int).append(importances_dict[i])\n",
      "    \n",
      "    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict\n",
      "141/182:\n",
      "import glob # to read multiple files \n",
      "from rdkit import Chem, DataStructs\n",
      "from rdkit.Chem import AllChem\n",
      "import os\n",
      "\n",
      "%config InlineBackend.figure_format = 'retina'\n",
      "\n",
      "! rm Result/Result_LogKp.csv\n",
      "\n",
      "outfile = open('Result/Result_LogKp.csv', 'a')\n",
      "\n",
      "print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\\\n",
      "                    'R2_train,R2_train_std,' + \\\n",
      "                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \\\n",
      "                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')\n",
      "\n",
      "path = r'QSAR/'\n",
      "\n",
      "for f in glob.glob(path + '*.csv'):\n",
      "    df = pd.read_csv(f)\n",
      "    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))\n",
      "    df = df.fillna(method='ffill')\n",
      "    Y = df[\"LogKp\"].values.astype(np.float)\n",
      "    data = df\n",
      "    \n",
      "    print ('\\n\\n************************************************************************************')\n",
      "    print ('')\n",
      "    print (f)\n",
      "    print ('')\n",
      "    \n",
      "    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%\n",
      "    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7\n",
      "\n",
      "    h = data.columns.tolist()\n",
      "    hx = np.array(h)\n",
      "\n",
      "    data = data.values.astype(np.float)\n",
      "    X = np.array(data)\n",
      "    \n",
      "    # Prepare empty lists to plot QSAR model\n",
      "    R2_train = []\n",
      "    RMSE_train = []\n",
      "    Q2_CV = []\n",
      "    RMSE_CV = []\n",
      "    Q2_External = []\n",
      "    RMSE_External = []\n",
      "    importances_dict = defaultdict(list)\n",
      "    \n",
      "    # Prepare empty lists to plot the performance of accuracy.\n",
      "    acclist_realRF                          = []\n",
      "    acclist_realRF_predictTrain             = []\n",
      "    acclist_predictionOnTest_scrambledtrain  = []\n",
      "    acclist_predictionOnTrain_scrambledtrain = []\n",
      "        \n",
      "    for i in range(10):\n",
      "        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)\n",
      "            \n",
      "    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \\\n",
      "                                                                        X_internal, X_external, Y_internal, Y_external)\n",
      "    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \\\n",
      "        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\\\n",
      "        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \n",
      "           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, \n",
      "           RMSE_External_std, X_internal, X_external, X, f, des1, des2)\n",
      "    plot_model(f, X_internal, X_external, Y_internal, Y_external,\n",
      "                R2_train_mean, Q2_External_mean,\n",
      "                importances_mean, importances_std, Feature, prediction,\n",
      "                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)\n",
      "    \n",
      "outfile.close()\n",
      "    #END\n",
      "141/183:\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "\n",
      "def build_model(X, Y, seed, hx, f):\n",
      "    \n",
      "    \n",
      "    #Data split using 70/30 ratio\n",
      "    X_internal, X_external, Y_internal, Y_external = train_test_split(X,\n",
      "                                                        Y, test_size=0.3,\n",
      "                                                        random_state=seed)\n",
      "\n",
      "    # Training set\n",
      "    rf = RandomForestRegressor(n_estimators=400, \n",
      "                               max_features='sqrt', \n",
      "                               min_samples_leaf = 1,\n",
      "                               random_state = 13,\n",
      "                               n_jobs=-1)\n",
      "    \n",
      "    rf.fit(X_internal,Y_internal)\n",
      "    prediction = rf.predict(X_internal)\n",
      "    \n",
      "    # Cross-validation\n",
      "    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)\n",
      "    \n",
      "    # External set  \n",
      "    prediction_external = rf.predict(X_external)\n",
      "    \n",
      "    #print result from each seed    \n",
      "    R2_train.append(r2_score(Y_internal, prediction))\n",
      "    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))\n",
      "    Q2_CV.append(r2_score(Y_internal, cv))\n",
      "    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))\n",
      "    Q2_External.append(r2_score(Y_external, prediction_external))\n",
      "    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))\n",
      "    \n",
      "    #Feature Importance\n",
      "    Feature = hx[:]\n",
      "    feature_importance = rf.feature_importances_\n",
      "    importances = 100.0 * (feature_importance / feature_importance.max()) #index\n",
      "    \n",
      "    for i, fx in enumerate(Feature):\n",
      "        int(importances_dict[fx]).append(importances_dict[i])\n",
      "    \n",
      "    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict\n",
      "141/184:\n",
      "import glob # to read multiple files \n",
      "from rdkit import Chem, DataStructs\n",
      "from rdkit.Chem import AllChem\n",
      "import os\n",
      "\n",
      "%config InlineBackend.figure_format = 'retina'\n",
      "\n",
      "! rm Result/Result_LogKp.csv\n",
      "\n",
      "outfile = open('Result/Result_LogKp.csv', 'a')\n",
      "\n",
      "print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\\\n",
      "                    'R2_train,R2_train_std,' + \\\n",
      "                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \\\n",
      "                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')\n",
      "\n",
      "path = r'QSAR/'\n",
      "\n",
      "for f in glob.glob(path + '*.csv'):\n",
      "    df = pd.read_csv(f)\n",
      "    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))\n",
      "    df = df.fillna(method='ffill')\n",
      "    Y = df[\"LogKp\"].values.astype(np.float)\n",
      "    data = df\n",
      "    \n",
      "    print ('\\n\\n************************************************************************************')\n",
      "    print ('')\n",
      "    print (f)\n",
      "    print ('')\n",
      "    \n",
      "    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%\n",
      "    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7\n",
      "\n",
      "    h = data.columns.tolist()\n",
      "    hx = np.array(h)\n",
      "\n",
      "    data = data.values.astype(np.float)\n",
      "    X = np.array(data)\n",
      "    \n",
      "    # Prepare empty lists to plot QSAR model\n",
      "    R2_train = []\n",
      "    RMSE_train = []\n",
      "    Q2_CV = []\n",
      "    RMSE_CV = []\n",
      "    Q2_External = []\n",
      "    RMSE_External = []\n",
      "    importances_dict = defaultdict(list)\n",
      "    \n",
      "    # Prepare empty lists to plot the performance of accuracy.\n",
      "    acclist_realRF                          = []\n",
      "    acclist_realRF_predictTrain             = []\n",
      "    acclist_predictionOnTest_scrambledtrain  = []\n",
      "    acclist_predictionOnTrain_scrambledtrain = []\n",
      "        \n",
      "    for i in range(10):\n",
      "        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)\n",
      "            \n",
      "    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \\\n",
      "                                                                        X_internal, X_external, Y_internal, Y_external)\n",
      "    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \\\n",
      "        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\\\n",
      "        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \n",
      "           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, \n",
      "           RMSE_External_std, X_internal, X_external, X, f, des1, des2)\n",
      "    plot_model(f, X_internal, X_external, Y_internal, Y_external,\n",
      "                R2_train_mean, Q2_External_mean,\n",
      "                importances_mean, importances_std, Feature, prediction,\n",
      "                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)\n",
      "    \n",
      "outfile.close()\n",
      "    #END\n",
      "141/185:\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "\n",
      "def build_model(X, Y, seed, hx, f):\n",
      "    \n",
      "    \n",
      "    #Data split using 70/30 ratio\n",
      "    X_internal, X_external, Y_internal, Y_external = train_test_split(X,\n",
      "                                                        Y, test_size=0.3,\n",
      "                                                        random_state=seed)\n",
      "\n",
      "    # Training set\n",
      "    rf = RandomForestRegressor(n_estimators=400, \n",
      "                               max_features='sqrt', \n",
      "                               min_samples_leaf = 1,\n",
      "                               random_state = 13,\n",
      "                               n_jobs=-1)\n",
      "    \n",
      "    rf.fit(X_internal,Y_internal)\n",
      "    prediction = rf.predict(X_internal)\n",
      "    \n",
      "    # Cross-validation\n",
      "    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)\n",
      "    \n",
      "    # External set  \n",
      "    prediction_external = rf.predict(X_external)\n",
      "    \n",
      "    #print result from each seed    \n",
      "    R2_train.append(r2_score(Y_internal, prediction))\n",
      "    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))\n",
      "    Q2_CV.append(r2_score(Y_internal, cv))\n",
      "    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))\n",
      "    Q2_External.append(r2_score(Y_external, prediction_external))\n",
      "    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))\n",
      "    \n",
      "    #Feature Importance\n",
      "    Feature = hx[:]\n",
      "    feature_importance = rf.feature_importances_\n",
      "    importances = 100.0 * (feature_importance / feature_importance.max()) #index\n",
      "    \n",
      "    for i, fx in enumerate(Feature):\n",
      "        int(importances_dict[fx]).append(importances[i])\n",
      "    \n",
      "    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict\n",
      "141/186:\n",
      "import glob # to read multiple files \n",
      "from rdkit import Chem, DataStructs\n",
      "from rdkit.Chem import AllChem\n",
      "import os\n",
      "\n",
      "%config InlineBackend.figure_format = 'retina'\n",
      "\n",
      "! rm Result/Result_LogKp.csv\n",
      "\n",
      "outfile = open('Result/Result_LogKp.csv', 'a')\n",
      "\n",
      "print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\\\n",
      "                    'R2_train,R2_train_std,' + \\\n",
      "                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \\\n",
      "                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')\n",
      "\n",
      "path = r'QSAR/'\n",
      "\n",
      "for f in glob.glob(path + '*.csv'):\n",
      "    df = pd.read_csv(f)\n",
      "    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))\n",
      "    df = df.fillna(method='ffill')\n",
      "    Y = df[\"LogKp\"].values.astype(np.float)\n",
      "    data = df\n",
      "    \n",
      "    print ('\\n\\n************************************************************************************')\n",
      "    print ('')\n",
      "    print (f)\n",
      "    print ('')\n",
      "    \n",
      "    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%\n",
      "    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7\n",
      "\n",
      "    h = data.columns.tolist()\n",
      "    hx = np.array(h)\n",
      "\n",
      "    data = data.values.astype(np.float)\n",
      "    X = np.array(data)\n",
      "    \n",
      "    # Prepare empty lists to plot QSAR model\n",
      "    R2_train = []\n",
      "    RMSE_train = []\n",
      "    Q2_CV = []\n",
      "    RMSE_CV = []\n",
      "    Q2_External = []\n",
      "    RMSE_External = []\n",
      "    importances_dict = defaultdict(list)\n",
      "    \n",
      "    # Prepare empty lists to plot the performance of accuracy.\n",
      "    acclist_realRF                          = []\n",
      "    acclist_realRF_predictTrain             = []\n",
      "    acclist_predictionOnTest_scrambledtrain  = []\n",
      "    acclist_predictionOnTrain_scrambledtrain = []\n",
      "        \n",
      "    for i in range(10):\n",
      "        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)\n",
      "            \n",
      "    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \\\n",
      "                                                                        X_internal, X_external, Y_internal, Y_external)\n",
      "    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \\\n",
      "        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\\\n",
      "        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \n",
      "           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, \n",
      "           RMSE_External_std, X_internal, X_external, X, f, des1, des2)\n",
      "    plot_model(f, X_internal, X_external, Y_internal, Y_external,\n",
      "                R2_train_mean, Q2_External_mean,\n",
      "                importances_mean, importances_std, Feature, prediction,\n",
      "                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)\n",
      "    \n",
      "outfile.close()\n",
      "    #END\n",
      "141/187:\n",
      "def print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \n",
      "           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, \n",
      "           RMSE_External_std, X_internal, X_external, X, f, des1, des2 ):\n",
      "    #outfile = open('all_output.csv', 'a')\n",
      "    name = [os.path.basename(f)]\n",
      "    print_out = name\n",
      "    print_out = [name.replace('.smi','') for name in print_out]\n",
      "    \n",
      "    print (outfile, '%s,%d,%d,%d,%d,%0.4f,%0.4f,%0.4f,%0.4f,%d,%0.4f,%0.4f,%0.4f,%0.4f,%d,%0.4f,%0.4f,%0.4f,%0.4f' \\\n",
      "                                     % (print_out, len(X_internal),\n",
      "                                        des1, des2, des4,\n",
      "                                        R2_train_mean, R2_train_std,\n",
      "                                        RMSE_train_mean, RMSE_train_std,\n",
      "                                        len(X_internal),\n",
      "                                        Q2_CV_mean, Q2_CV_std,\n",
      "                                        RMSE_CV_mean, RMSE_CV_std,\n",
      "                                        len(X_external),\n",
      "                                        Q2_External_mean, Q2_External_std,\n",
      "                                        RMSE_External_mean, RMSE_External_std,))\n",
      " \n",
      "    print ('\\nTraining set\\n------------')\n",
      "    print ('N: ' + (str(len(X_internal))))\n",
      "    print ('R2: %0.4f'%(R2_train_mean))\n",
      "    print ('std_R2: %0.4f'%(R2_train_std))\n",
      "    print ('RMSE: %0.4f'%(RMSE_train_mean))\n",
      "    print ('std_RMSE: %0.4f'%(RMSE_train_std))\n",
      "\n",
      "    print ('\\nCross-validation set\\n------------')\n",
      "    print ('N: ' + (str(len(X_internal))))\n",
      "    print ('Q2: %0.4f'%(Q2_CV_mean))\n",
      "    print ('std_Q2: %0.4f'%(Q2_CV_std))\n",
      "    print ('RMSE: %0.4f'%(RMSE_CV_mean))\n",
      "    print ('std_RMSE: %0.4f'%(RMSE_CV_std))\n",
      "\n",
      "    print ('\\nExternal set\\n------------')\n",
      "    print ('N: ' + (str(len(X_external))))\n",
      "    print ('Q2_EXt: %0.4f'%(Q2_External_mean))\n",
      "    print ('std_Q2_EXt: %0.4f'%(Q2_External_std))\n",
      "    print ('RMSE: %0.4f'%(RMSE_External_mean))\n",
      "    print ('std_RMSE: %0.4f'%(RMSE_External_std))\n",
      "141/188:\n",
      "def mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, importances_dict):\n",
      "    R2_train_mean = np.mean(R2_train)\n",
      "    RMSE_train_mean = np.mean(RMSE_train)\n",
      "    Q2_CV_mean = np.mean(Q2_CV)\n",
      "    RMSE_CV_mean = np.mean(RMSE_CV)\n",
      "    Q2_External_mean = np.mean(Q2_External)\n",
      "    RMSE_External_mean = np.mean(RMSE_External)\n",
      "    importances_mean0 = {}\n",
      "    for fx in importances_dict:\n",
      "        importances_mean0[fx] = np.mean(importances_dict[fx])\n",
      "    importances_mean = sorted([(k,v) for k,v in importances_mean0.iteritems()],\n",
      "                                    key=lambda x: x[1], reverse=True)\n",
      "    \n",
      "    #predictionOnTest_mean = np.mean(acclist_predictionOnTest_scrambledtrain)\n",
      "    #predictionOnTrain_mean = np.mean(acclist_predictionOnTrain_scrambledtrain)\n",
      "    \n",
      "    return R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \\\n",
      "            RMSE_External_mean, importances_mean\n",
      "141/189:\n",
      "from copy import deepcopy\n",
      "\n",
      "def Y_scrambling(X_internal, X_external, Y_internal, Y_external):\n",
      "    # Do the Y-scrambling. Loop over the actual learning for 100 times.\n",
      "    for randomseedcounter in range(1,101):\n",
      "        y_train_scrambled = deepcopy(Y_internal)\n",
      "        X_train_scrambled = deepcopy(X_internal)\n",
      "        np.random.shuffle(y_train_scrambled)\n",
      "        np.random.shuffle(X_train_scrambled)\n",
      "\n",
      "        # training was done on \"scrambled\" data - prediction on test set\n",
      "        RF_scrambled         = RandomForestRegressor()\n",
      "        RF_scrambled         = RF_scrambled.fit(X_internal,y_train_scrambled)\n",
      "        y_predict_scrambled  = RF_scrambled.predict(X_external)\n",
      "    \n",
      "        acclist_predictionOnTest_scrambledtrain.append((RF_scrambled.score(X_external,Y_external))**2)\n",
      "    \n",
      "        # training was done on \"scrambled\" data - prediction on train set\n",
      "        y_predict_scrambled_predictTrain  = RF_scrambled.predict(X_internal)\n",
      "    \n",
      "        acclist_predictionOnTrain_scrambledtrain.append((RF_scrambled.score(X_internal,Y_internal))**2)\n",
      "        \n",
      "        r2 = pd.DataFrame(acclist_predictionOnTrain_scrambledtrain, columns=['R2'])\n",
      "        q2 = pd.DataFrame(acclist_predictionOnTest_scrambledtrain, columns=['Q2'])\n",
      "\n",
      "        #result = pd.concat([r2, q2], axis=1, join='inner').to_csv(f+\"_Y_scrambling.csv\", header=False, index=False)\n",
      "        \n",
      "        \n",
      "    return acclist_predictionOnTest_scrambledtrain,acclist_predictionOnTrain_scrambledtrain\n",
      "141/190:\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "\n",
      "def build_model(X, Y, seed, hx, f):\n",
      "    \n",
      "    \n",
      "    #Data split using 70/30 ratio\n",
      "    X_internal, X_external, Y_internal, Y_external = train_test_split(X,\n",
      "                                                        Y, test_size=0.3,\n",
      "                                                        random_state=seed)\n",
      "\n",
      "    # Training set\n",
      "    rf = RandomForestRegressor(n_estimators=400, \n",
      "                               max_features='sqrt', \n",
      "                               min_samples_leaf = 1,\n",
      "                               random_state = 13,\n",
      "                               n_jobs=-1)\n",
      "    \n",
      "    rf.fit(X_internal,Y_internal)\n",
      "    prediction = rf.predict(X_internal)\n",
      "    \n",
      "    # Cross-validation\n",
      "    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)\n",
      "    \n",
      "    # External set  \n",
      "    prediction_external = rf.predict(X_external)\n",
      "    \n",
      "    #print result from each seed    \n",
      "    R2_train.append(r2_score(Y_internal, prediction))\n",
      "    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))\n",
      "    Q2_CV.append(r2_score(Y_internal, cv))\n",
      "    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))\n",
      "    Q2_External.append(r2_score(Y_external, prediction_external))\n",
      "    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))\n",
      "    \n",
      "    #Feature Importance\n",
      "    Feature = hx[:]\n",
      "    feature_importance = rf.feature_importances_\n",
      "    importances = 100.0 * (feature_importance / feature_importance.max()) #index\n",
      "    \n",
      "    for i, fx in enumerate(Feature):\n",
      "        importances_dict(fx).append(importances[i])\n",
      "    \n",
      "    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict\n",
      "141/191:\n",
      "import glob # to read multiple files \n",
      "from rdkit import Chem, DataStructs\n",
      "from rdkit.Chem import AllChem\n",
      "import os\n",
      "\n",
      "%config InlineBackend.figure_format = 'retina'\n",
      "\n",
      "! rm Result/Result_LogKp.csv\n",
      "\n",
      "outfile = open('Result/Result_LogKp.csv', 'a')\n",
      "\n",
      "print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\\\n",
      "                    'R2_train,R2_train_std,' + \\\n",
      "                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \\\n",
      "                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')\n",
      "\n",
      "path = r'QSAR/'\n",
      "\n",
      "for f in glob.glob(path + '*.csv'):\n",
      "    df = pd.read_csv(f)\n",
      "    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))\n",
      "    df = df.fillna(method='ffill')\n",
      "    Y = df[\"LogKp\"].values.astype(np.float)\n",
      "    data = df\n",
      "    \n",
      "    print ('\\n\\n************************************************************************************')\n",
      "    print ('')\n",
      "    print (f)\n",
      "    print ('')\n",
      "    \n",
      "    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%\n",
      "    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7\n",
      "\n",
      "    h = data.columns.tolist()\n",
      "    hx = np.array(h)\n",
      "\n",
      "    data = data.values.astype(np.float)\n",
      "    X = np.array(data)\n",
      "    \n",
      "    # Prepare empty lists to plot QSAR model\n",
      "    R2_train = []\n",
      "    RMSE_train = []\n",
      "    Q2_CV = []\n",
      "    RMSE_CV = []\n",
      "    Q2_External = []\n",
      "    RMSE_External = []\n",
      "    importances_dict = defaultdict(list)\n",
      "    \n",
      "    # Prepare empty lists to plot the performance of accuracy.\n",
      "    acclist_realRF                          = []\n",
      "    acclist_realRF_predictTrain             = []\n",
      "    acclist_predictionOnTest_scrambledtrain  = []\n",
      "    acclist_predictionOnTrain_scrambledtrain = []\n",
      "        \n",
      "    for i in range(10):\n",
      "        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)\n",
      "            \n",
      "    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \\\n",
      "                                                                        X_internal, X_external, Y_internal, Y_external)\n",
      "    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \\\n",
      "        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\\\n",
      "        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \n",
      "           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, \n",
      "           RMSE_External_std, X_internal, X_external, X, f, des1, des2)\n",
      "    plot_model(f, X_internal, X_external, Y_internal, Y_external,\n",
      "                R2_train_mean, Q2_External_mean,\n",
      "                importances_mean, importances_std, Feature, prediction,\n",
      "                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)\n",
      "    \n",
      "outfile.close()\n",
      "    #END\n",
      "141/192:\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "\n",
      "def build_model(X, Y, seed, hx, f):\n",
      "    \n",
      "    \n",
      "    #Data split using 70/30 ratio\n",
      "    X_internal, X_external, Y_internal, Y_external = train_test_split(X,\n",
      "                                                        Y, test_size=0.3,\n",
      "                                                        random_state=seed)\n",
      "\n",
      "    # Training set\n",
      "    rf = RandomForestRegressor(n_estimators=400, \n",
      "                               max_features='sqrt', \n",
      "                               min_samples_leaf = 1,\n",
      "                               random_state = 13,\n",
      "                               n_jobs=-1)\n",
      "    \n",
      "    rf.fit(X_internal,Y_internal)\n",
      "    prediction = rf.predict(X_internal)\n",
      "    \n",
      "    # Cross-validation\n",
      "    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)\n",
      "    \n",
      "    # External set  \n",
      "    prediction_external = rf.predict(X_external)\n",
      "    \n",
      "    #print result from each seed    \n",
      "    R2_train.append(r2_score(Y_internal, prediction))\n",
      "    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))\n",
      "    Q2_CV.append(r2_score(Y_internal, cv))\n",
      "    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))\n",
      "    Q2_External.append(r2_score(Y_external, prediction_external))\n",
      "    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))\n",
      "    \n",
      "    #Feature Importance\n",
      "    Feature = hx[:]\n",
      "    feature_importance = rf.feature_importances_\n",
      "    importances = 100.0 * (feature_importance / feature_importance.max()) #index\n",
      "    \n",
      "    for i, fx in enumerate(Feature):\n",
      "        importances_dict[fx] =  importances[i]\n",
      "    \n",
      "    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict\n",
      "141/193:\n",
      "import glob # to read multiple files \n",
      "from rdkit import Chem, DataStructs\n",
      "from rdkit.Chem import AllChem\n",
      "import os\n",
      "\n",
      "%config InlineBackend.figure_format = 'retina'\n",
      "\n",
      "! rm Result/Result_LogKp.csv\n",
      "\n",
      "outfile = open('Result/Result_LogKp.csv', 'a')\n",
      "\n",
      "print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\\\n",
      "                    'R2_train,R2_train_std,' + \\\n",
      "                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \\\n",
      "                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')\n",
      "\n",
      "path = r'QSAR/'\n",
      "\n",
      "for f in glob.glob(path + '*.csv'):\n",
      "    df = pd.read_csv(f)\n",
      "    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))\n",
      "    df = df.fillna(method='ffill')\n",
      "    Y = df[\"LogKp\"].values.astype(np.float)\n",
      "    data = df\n",
      "    \n",
      "    print ('\\n\\n************************************************************************************')\n",
      "    print ('')\n",
      "    print (f)\n",
      "    print ('')\n",
      "    \n",
      "    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%\n",
      "    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7\n",
      "\n",
      "    h = data.columns.tolist()\n",
      "    hx = np.array(h)\n",
      "\n",
      "    data = data.values.astype(np.float)\n",
      "    X = np.array(data)\n",
      "    \n",
      "    # Prepare empty lists to plot QSAR model\n",
      "    R2_train = []\n",
      "    RMSE_train = []\n",
      "    Q2_CV = []\n",
      "    RMSE_CV = []\n",
      "    Q2_External = []\n",
      "    RMSE_External = []\n",
      "    importances_dict = defaultdict(list)\n",
      "    \n",
      "    # Prepare empty lists to plot the performance of accuracy.\n",
      "    acclist_realRF                          = []\n",
      "    acclist_realRF_predictTrain             = []\n",
      "    acclist_predictionOnTest_scrambledtrain  = []\n",
      "    acclist_predictionOnTrain_scrambledtrain = []\n",
      "        \n",
      "    for i in range(10):\n",
      "        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)\n",
      "            \n",
      "    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \\\n",
      "                                                                        X_internal, X_external, Y_internal, Y_external)\n",
      "    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \\\n",
      "        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\\\n",
      "        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \n",
      "           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, \n",
      "           RMSE_External_std, X_internal, X_external, X, f, des1, des2)\n",
      "    plot_model(f, X_internal, X_external, Y_internal, Y_external,\n",
      "                R2_train_mean, Q2_External_mean,\n",
      "                importances_mean, importances_std, Feature, prediction,\n",
      "                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)\n",
      "    \n",
      "outfile.close()\n",
      "    #END\n",
      "141/194:\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.ticker import MultipleLocator\n",
      "import pylab as py \n",
      "import statsmodels.api as sm\n",
      "from statsmodels.stats.outliers_influence import summary_table\n",
      "\n",
      "def plot_model (f, X_internal, X_external, Y_internal, Y_external,\n",
      "                R2_train_mean, Q2_External_mean,\n",
      "                importances_mean, importances_std, Feature, prediction, \n",
      "                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain):\n",
      "    \n",
      "    # Prepare plot\n",
      "    m  = rf.fit(X_internal,Y_internal)\n",
      "    cm = plt.cm.RdBu\n",
      "    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)\n",
      "\n",
      "    fig_size = plt.rcParams[\"figure.figsize\"]\n",
      "    fig_size[0]=5\n",
      "    fig_size[1]=5\n",
      "\n",
      "    # Train Set\n",
      "    x_train = np.array(Y_internal)\n",
      "    y_train = m.predict(X_internal).flatten()\n",
      "    py.scatter(x_train, y_train, s=50, marker='.', alpha=0.3,\n",
      "            c='g', cmap=cm ,edgecolors='g')\n",
      "                 #label=r\"$R^{2}_{Tr}$ = %.4f\" % R2_train_mean)\n",
      "    \n",
      "    \n",
      "    # CV Set\n",
      "    np.array(cv)\n",
      "    x_test = np.array(Y_internal)\n",
      "    y_test = cv\n",
      "    py.scatter(x_test, y_test, s=50, marker='.', alpha=0.3,\n",
      "            c='b', cmap=cm ,edgecolors='b') \n",
      "                 #label=r\"$Q^{2}_{Ext}$ = %.4f\" % Q2_External_mean)\n",
      "        #2SD line\n",
      "    X = sm.add_constant(x_test)\n",
      "    res = sm.OLS(y_test, X).fit()\n",
      "\n",
      "    st, data, ss2 = summary_table(res, alpha=0.05)\n",
      "    fittedvalues = data[:,2]\n",
      "        #predict_mean_se  = data[:,3]\n",
      "    predict_mean_ci_low, predict_mean_ci_upp = data[:,4:6].T\n",
      "    predict_ci_low, predict_ci_upp = data[:,6:8].T\n",
      "        #2SD line\n",
      "    plt.plot(X, predict_ci_low, '--b', linewidth=0.5, alpha=0.5)\n",
      "    plt.plot(X, predict_ci_upp, '--b', linewidth=0.5, alpha=0.5)\n",
      "        \n",
      "    # External Set\n",
      "    x_test = np.array(Y_external)\n",
      "    y_test = m.predict(X_external).flatten()\n",
      "    py.scatter(x_test, y_test, s=50, marker='.', alpha=0.3,\n",
      "            c='r', cmap=cm ,edgecolors='r') \n",
      "                 #label=r\"$Q^{2}_{Ext}$ = %.4f\" % Q2_External_mean)\n",
      "    \n",
      "    \n",
      "    #py.plot(x_train, np.polyval(np.polyfit(x_train,y_train,1), x_train), '--r') #mean line\n",
      "    plt.legend(loc=2,prop={'size':6})\n",
      "    plt.xlabel(\"Experimental $pIC_{50}$ values\", fontsize=10)\n",
      "    plt.ylabel(\"Predicted $pIC_{50}$ values\", fontsize=10)\n",
      "        \n",
      "    min_axis = np.min(np.concatenate([Y_internal, prediction], axis=0))\n",
      "    max_axis = np.max(np.concatenate([Y_internal, prediction], axis=0))\n",
      "    plt.xlim([(min_axis*0.9),(max_axis*1.05)])\n",
      "    plt.ylim([(min_axis*0.9),(max_axis*1.05)])\n",
      "    plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "    \n",
      "    # Save plot to file\n",
      "    plt.savefig(f+'_compair_Fp.pdf', dpi=300)\n",
      "    plt.show()\n",
      "    \n",
      "    # Y-scrambling plot\n",
      "    py.scatter(Q2_CV_mean, R2_train_mean, s=100, marker='.', alpha=0.3,\n",
      "            c='b', cmap=cm ,edgecolors='b') \n",
      "            \n",
      "    py.scatter(acclist_predictionOnTest_scrambledtrain,acclist_predictionOnTrain_scrambledtrain, \\\n",
      "               s=100, marker='.', alpha=0.3, c='r', cmap=cm ,edgecolors='r')\n",
      "\n",
      "    plt.legend(loc=2,prop={'size':6})\n",
      "    plt.xlabel(\"$Q^{2}$\", fontsize=10)\n",
      "    plt.ylabel(\"$R^{2}$\", fontsize=10)\n",
      "    plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "    \n",
      "    plt.ylim([0, 1])\n",
      "    plt.xlim([0, 1])\n",
      "    plt.axhline(0.5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "    plt.axvline(0.5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "    \n",
      "    plt.savefig(f+'_Y_scrambling.pdf', dpi=300)\n",
      "\n",
      "    plt.show()\n",
      "\n",
      "    #Feature Importance \n",
      "    fig_size = plt.rcParams[\"figure.figsize\"]\n",
      "    fig_size[0]=5\n",
      "    fig_size[1]=10\n",
      "\n",
      "    barlist = plt.barh(range(20), [x[1] for x in importances_mean[:20]], \n",
      "          color=\"g\", xerr=[x[1] for x in importances_std[:20]], align=\"center\", \\\n",
      "                       error_kw=dict(ecolor='gray', lw=2, capsize=5, capthick=2))\n",
      "\n",
      "    print (str(\"top10\"), [x[0] for x in importances_mean[:10]])\n",
      "    print ('')\n",
      "    print (str(\"top20\"), [x[0] for x in importances_mean[:20]])\n",
      "    print ('')\n",
      "    print (str(\"top30\"), [x[0] for x in importances_mean[:30]])\n",
      "    print ('')\n",
      "    print (str(\"top40\"), [x[0] for x in importances_mean[:40]])\n",
      "    print ('')\n",
      "    print (str(\"top50\"), [x[0] for x in importances_mean[:50]])\n",
      "    \n",
      "    plt.yticks(range(20), [x[0] for x in importances_mean[:20]])\n",
      "    plt.ylim([-1, 20])\n",
      "    \n",
      "    plt.xlabel(r\"$\\bf{Gini}$\" + \" \"+ r\"$\\bf{index}$\", fontsize=12)\n",
      "    ax = plt.gca()\n",
      "    ax.invert_yaxis()\n",
      "    plt.tight_layout(pad=2.0, w_pad=0.7, h_pad=2.0)\n",
      "    plt.savefig(f+'_Feature_importances.pdf', dpi=300)\n",
      "    plt.show()\n",
      "141/195:\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "\n",
      "def build_model(X, Y, seed, hx, f):\n",
      "    \n",
      "    \n",
      "    #Data split using 70/30 ratio\n",
      "    X_internal, X_external, Y_internal, Y_external = train_test_split(X,\n",
      "                                                        Y, test_size=0.3,\n",
      "                                                        random_state=seed)\n",
      "\n",
      "    # Training set\n",
      "    rf = RandomForestRegressor(n_estimators=400, \n",
      "                               max_features='sqrt', \n",
      "                               min_samples_leaf = 1,\n",
      "                               random_state = 13,\n",
      "                               n_jobs=-1)\n",
      "    \n",
      "    rf.fit(X_internal,Y_internal)\n",
      "    prediction = rf.predict(X_internal)\n",
      "    \n",
      "    # Cross-validation\n",
      "    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)\n",
      "    \n",
      "    # External set  \n",
      "    prediction_external = rf.predict(X_external)\n",
      "    \n",
      "    #print result from each seed    \n",
      "    R2_train.append(r2_score(Y_internal, prediction))\n",
      "    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))\n",
      "    Q2_CV.append(r2_score(Y_internal, cv))\n",
      "    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))\n",
      "    Q2_External.append(r2_score(Y_external, prediction_external))\n",
      "    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))\n",
      "    \n",
      "    #Feature Importance\n",
      "    Feature = hx[:]\n",
      "    feature_importance = rf.feature_importances_\n",
      "    importances = 100.0 * (feature_importance / feature_importance.max()) #index\n",
      "    \n",
      "    for i, fx in enumerate(Feature):\n",
      "        importances[fx].append(importances[i])\n",
      "    \n",
      "    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict\n",
      "141/196:\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "\n",
      "def build_model(X, Y, seed, hx, f):\n",
      "    \n",
      "    \n",
      "    #Data split using 70/30 ratio\n",
      "    X_internal, X_external, Y_internal, Y_external = train_test_split(X,\n",
      "                                                        Y, test_size=0.3,\n",
      "                                                        random_state=seed)\n",
      "\n",
      "    # Training set\n",
      "    rf = RandomForestRegressor(n_estimators=400, \n",
      "                               max_features='sqrt', \n",
      "                               min_samples_leaf = 1,\n",
      "                               random_state = 13,\n",
      "                               n_jobs=-1)\n",
      "    \n",
      "    rf.fit(X_internal,Y_internal)\n",
      "    prediction = rf.predict(X_internal)\n",
      "    \n",
      "    # Cross-validation\n",
      "    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)\n",
      "    \n",
      "    # External set  \n",
      "    prediction_external = rf.predict(X_external)\n",
      "    \n",
      "    #print result from each seed    \n",
      "    R2_train.append(r2_score(Y_internal, prediction))\n",
      "    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))\n",
      "    Q2_CV.append(r2_score(Y_internal, cv))\n",
      "    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))\n",
      "    Q2_External.append(r2_score(Y_external, prediction_external))\n",
      "    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))\n",
      "    \n",
      "    #Feature Importance\n",
      "    Feature = hx[:]\n",
      "    feature_importance = rf.feature_importances_\n",
      "    importances = 100.0 * (feature_importance / feature_importance.max()) #index\n",
      "    \n",
      "    for i, fx in enumerate(Feature):\n",
      "        importances[fx].astype(np.float)\n",
      "        importances[fx].append(importances[i])\n",
      "    \n",
      "    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict\n",
      "141/197:\n",
      "import glob # to read multiple files \n",
      "from rdkit import Chem, DataStructs\n",
      "from rdkit.Chem import AllChem\n",
      "import os\n",
      "\n",
      "%config InlineBackend.figure_format = 'retina'\n",
      "\n",
      "! rm Result/Result_LogKp.csv\n",
      "\n",
      "outfile = open('Result/Result_LogKp.csv', 'a')\n",
      "\n",
      "print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\\\n",
      "                    'R2_train,R2_train_std,' + \\\n",
      "                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \\\n",
      "                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')\n",
      "\n",
      "path = r'QSAR/'\n",
      "\n",
      "for f in glob.glob(path + '*.csv'):\n",
      "    df = pd.read_csv(f)\n",
      "    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))\n",
      "    df = df.fillna(method='ffill')\n",
      "    Y = df[\"LogKp\"].values.astype(np.float)\n",
      "    data = df\n",
      "    \n",
      "    print ('\\n\\n************************************************************************************')\n",
      "    print ('')\n",
      "    print (f)\n",
      "    print ('')\n",
      "    \n",
      "    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%\n",
      "    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7\n",
      "\n",
      "    h = data.columns.tolist()\n",
      "    hx = np.array(h)\n",
      "\n",
      "    data = data.values.astype(np.float)\n",
      "    X = np.array(data)\n",
      "    \n",
      "    # Prepare empty lists to plot QSAR model\n",
      "    R2_train = []\n",
      "    RMSE_train = []\n",
      "    Q2_CV = []\n",
      "    RMSE_CV = []\n",
      "    Q2_External = []\n",
      "    RMSE_External = []\n",
      "    importances_dict = defaultdict(list)\n",
      "    \n",
      "    # Prepare empty lists to plot the performance of accuracy.\n",
      "    acclist_realRF                          = []\n",
      "    acclist_realRF_predictTrain             = []\n",
      "    acclist_predictionOnTest_scrambledtrain  = []\n",
      "    acclist_predictionOnTrain_scrambledtrain = []\n",
      "        \n",
      "    for i in range(10):\n",
      "        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)\n",
      "            \n",
      "    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \\\n",
      "                                                                        X_internal, X_external, Y_internal, Y_external)\n",
      "    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \\\n",
      "        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\\\n",
      "        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \n",
      "           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, \n",
      "           RMSE_External_std, X_internal, X_external, X, f, des1, des2)\n",
      "    plot_model(f, X_internal, X_external, Y_internal, Y_external,\n",
      "                R2_train_mean, Q2_External_mean,\n",
      "                importances_mean, importances_std, Feature, prediction,\n",
      "                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)\n",
      "    \n",
      "outfile.close()\n",
      "    #END\n",
      "141/198:\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "\n",
      "def build_model(X, Y, seed, hx, f):\n",
      "    \n",
      "    \n",
      "    #Data split using 70/30 ratio\n",
      "    X_internal, X_external, Y_internal, Y_external = train_test_split(X,\n",
      "                                                        Y, test_size=0.3,\n",
      "                                                        random_state=seed)\n",
      "\n",
      "    # Training set\n",
      "    rf = RandomForestRegressor(n_estimators=400, \n",
      "                               max_features='sqrt', \n",
      "                               min_samples_leaf = 1,\n",
      "                               random_state = 13,\n",
      "                               n_jobs=-1)\n",
      "    \n",
      "    rf.fit(X_internal,Y_internal)\n",
      "    prediction = rf.predict(X_internal)\n",
      "    \n",
      "    # Cross-validation\n",
      "    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)\n",
      "    \n",
      "    # External set  \n",
      "    prediction_external = rf.predict(X_external)\n",
      "    \n",
      "    #print result from each seed    \n",
      "    R2_train.append(r2_score(Y_internal, prediction))\n",
      "    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))\n",
      "    Q2_CV.append(r2_score(Y_internal, cv))\n",
      "    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))\n",
      "    Q2_External.append(r2_score(Y_external, prediction_external))\n",
      "    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))\n",
      "    \n",
      "    #Feature Importance\n",
      "    Feature = hx[:]\n",
      "    feature_importance = rf.feature_importances_\n",
      "    importances = 100.0 * (feature_importance / feature_importance.max()) #index\n",
      "    \n",
      "    for i, fx in enumerate(Feature):\n",
      "        importances[fx].astype(int)\n",
      "        importances[fx].append(importances[i])\n",
      "    \n",
      "    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict\n",
      "141/199:\n",
      "import glob # to read multiple files \n",
      "from rdkit import Chem, DataStructs\n",
      "from rdkit.Chem import AllChem\n",
      "import os\n",
      "\n",
      "%config InlineBackend.figure_format = 'retina'\n",
      "\n",
      "! rm Result/Result_LogKp.csv\n",
      "\n",
      "outfile = open('Result/Result_LogKp.csv', 'a')\n",
      "\n",
      "print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\\\n",
      "                    'R2_train,R2_train_std,' + \\\n",
      "                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \\\n",
      "                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')\n",
      "\n",
      "path = r'QSAR/'\n",
      "\n",
      "for f in glob.glob(path + '*.csv'):\n",
      "    df = pd.read_csv(f)\n",
      "    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))\n",
      "    df = df.fillna(method='ffill')\n",
      "    Y = df[\"LogKp\"].values.astype(np.float)\n",
      "    data = df\n",
      "    \n",
      "    print ('\\n\\n************************************************************************************')\n",
      "    print ('')\n",
      "    print (f)\n",
      "    print ('')\n",
      "    \n",
      "    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%\n",
      "    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7\n",
      "\n",
      "    h = data.columns.tolist()\n",
      "    hx = np.array(h)\n",
      "\n",
      "    data = data.values.astype(np.float)\n",
      "    X = np.array(data)\n",
      "    \n",
      "    # Prepare empty lists to plot QSAR model\n",
      "    R2_train = []\n",
      "    RMSE_train = []\n",
      "    Q2_CV = []\n",
      "    RMSE_CV = []\n",
      "    Q2_External = []\n",
      "    RMSE_External = []\n",
      "    importances_dict = defaultdict(list)\n",
      "    \n",
      "    # Prepare empty lists to plot the performance of accuracy.\n",
      "    acclist_realRF                          = []\n",
      "    acclist_realRF_predictTrain             = []\n",
      "    acclist_predictionOnTest_scrambledtrain  = []\n",
      "    acclist_predictionOnTrain_scrambledtrain = []\n",
      "        \n",
      "    for i in range(10):\n",
      "        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)\n",
      "            \n",
      "    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \\\n",
      "                                                                        X_internal, X_external, Y_internal, Y_external)\n",
      "    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \\\n",
      "        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\\\n",
      "        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \n",
      "           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, \n",
      "           RMSE_External_std, X_internal, X_external, X, f, des1, des2)\n",
      "    plot_model(f, X_internal, X_external, Y_internal, Y_external,\n",
      "                R2_train_mean, Q2_External_mean,\n",
      "                importances_mean, importances_std, Feature, prediction,\n",
      "                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)\n",
      "    \n",
      "outfile.close()\n",
      "    #END\n",
      "141/200:\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "\n",
      "def build_model(X, Y, seed, hx, f):\n",
      "    \n",
      "    \n",
      "    #Data split using 70/30 ratio\n",
      "    X_internal, X_external, Y_internal, Y_external = train_test_split(X,\n",
      "                                                        Y, test_size=0.3,\n",
      "                                                        random_state=seed)\n",
      "\n",
      "    # Training set\n",
      "    rf = RandomForestRegressor(n_estimators=400, \n",
      "                               max_features='sqrt', \n",
      "                               min_samples_leaf = 1,\n",
      "                               random_state = 13,\n",
      "                               n_jobs=-1)\n",
      "    \n",
      "    rf.fit(X_internal,Y_internal)\n",
      "    prediction = rf.predict(X_internal)\n",
      "    \n",
      "    # Cross-validation\n",
      "    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)\n",
      "    \n",
      "    # External set  \n",
      "    prediction_external = rf.predict(X_external)\n",
      "    \n",
      "    #print result from each seed    \n",
      "    R2_train.append(r2_score(Y_internal, prediction))\n",
      "    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))\n",
      "    Q2_CV.append(r2_score(Y_internal, cv))\n",
      "    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))\n",
      "    Q2_External.append(r2_score(Y_external, prediction_external))\n",
      "    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))\n",
      "    \n",
      "    #Feature Importance\n",
      "    Feature = hx[:]\n",
      "    feature_importance = rf.feature_importances_\n",
      "    importances = 100.0 * (feature_importance / feature_importance.max()) #index\n",
      "    \n",
      "    for i, fx in enumerate(Feature):\n",
      "        importances_dict[fx] = importances[i]\n",
      "    \n",
      "    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict\n",
      "141/201:\n",
      "import glob # to read multiple files \n",
      "from rdkit import Chem, DataStructs\n",
      "from rdkit.Chem import AllChem\n",
      "import os\n",
      "\n",
      "%config InlineBackend.figure_format = 'retina'\n",
      "\n",
      "! rm Result/Result_LogKp.csv\n",
      "\n",
      "outfile = open('Result/Result_LogKp.csv', 'a')\n",
      "\n",
      "print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\\\n",
      "                    'R2_train,R2_train_std,' + \\\n",
      "                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \\\n",
      "                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')\n",
      "\n",
      "path = r'QSAR/'\n",
      "\n",
      "for f in glob.glob(path + '*.csv'):\n",
      "    df = pd.read_csv(f)\n",
      "    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))\n",
      "    df = df.fillna(method='ffill')\n",
      "    Y = df[\"LogKp\"].values.astype(np.float)\n",
      "    data = df\n",
      "    \n",
      "    print ('\\n\\n************************************************************************************')\n",
      "    print ('')\n",
      "    print (f)\n",
      "    print ('')\n",
      "    \n",
      "    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%\n",
      "    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7\n",
      "\n",
      "    h = data.columns.tolist()\n",
      "    hx = np.array(h)\n",
      "\n",
      "    data = data.values.astype(np.float)\n",
      "    X = np.array(data)\n",
      "    \n",
      "    # Prepare empty lists to plot QSAR model\n",
      "    R2_train = []\n",
      "    RMSE_train = []\n",
      "    Q2_CV = []\n",
      "    RMSE_CV = []\n",
      "    Q2_External = []\n",
      "    RMSE_External = []\n",
      "    importances_dict = defaultdict(list)\n",
      "    \n",
      "    # Prepare empty lists to plot the performance of accuracy.\n",
      "    acclist_realRF                          = []\n",
      "    acclist_realRF_predictTrain             = []\n",
      "    acclist_predictionOnTest_scrambledtrain  = []\n",
      "    acclist_predictionOnTrain_scrambledtrain = []\n",
      "        \n",
      "    for i in range(10):\n",
      "        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)\n",
      "            \n",
      "    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \\\n",
      "                                                                        X_internal, X_external, Y_internal, Y_external)\n",
      "    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \\\n",
      "        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\\\n",
      "        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \n",
      "           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, \n",
      "           RMSE_External_std, X_internal, X_external, X, f, des1, des2)\n",
      "    plot_model(f, X_internal, X_external, Y_internal, Y_external,\n",
      "                R2_train_mean, Q2_External_mean,\n",
      "                importances_mean, importances_std, Feature, prediction,\n",
      "                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)\n",
      "    \n",
      "outfile.close()\n",
      "    #END\n",
      "141/202:\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "\n",
      "def build_model(X, Y, seed, hx, f):\n",
      "    \n",
      "    \n",
      "    #Data split using 70/30 ratio\n",
      "    X_internal, X_external, Y_internal, Y_external = train_test_split(X,\n",
      "                                                        Y, test_size=0.3,\n",
      "                                                        random_state=seed)\n",
      "\n",
      "    # Training set\n",
      "    rf = RandomForestRegressor(n_estimators=400, \n",
      "                               max_features='sqrt', \n",
      "                               min_samples_leaf = 1,\n",
      "                               random_state = 13,\n",
      "                               n_jobs=-1)\n",
      "    \n",
      "    rf.fit(X_internal,Y_internal)\n",
      "    prediction = rf.predict(X_internal)\n",
      "    \n",
      "    # Cross-validation\n",
      "    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)\n",
      "    \n",
      "    # External set  \n",
      "    prediction_external = rf.predict(X_external)\n",
      "    \n",
      "    #print result from each seed    \n",
      "    R2_train.append(r2_score(Y_internal, prediction))\n",
      "    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))\n",
      "    Q2_CV.append(r2_score(Y_internal, cv))\n",
      "    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))\n",
      "    Q2_External.append(r2_score(Y_external, prediction_external))\n",
      "    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))\n",
      "    \n",
      "    #Feature Importance\n",
      "    Feature = hx[:]\n",
      "    feature_importance = rf.feature_importances_\n",
      "    importances = 100.0 * (feature_importance / feature_importance.max()) #index\n",
      "    \n",
      "    for i, fx in enumerate(Feature):\n",
      "        print(type(importances_dict))\n",
      "        importances_dict[fx] = importances[i]\n",
      "       \n",
      "    \n",
      "    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict\n",
      "141/203:\n",
      "import glob # to read multiple files \n",
      "from rdkit import Chem, DataStructs\n",
      "from rdkit.Chem import AllChem\n",
      "import os\n",
      "\n",
      "%config InlineBackend.figure_format = 'retina'\n",
      "\n",
      "! rm Result/Result_LogKp.csv\n",
      "\n",
      "outfile = open('Result/Result_LogKp.csv', 'a')\n",
      "\n",
      "print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\\\n",
      "                    'R2_train,R2_train_std,' + \\\n",
      "                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \\\n",
      "                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')\n",
      "\n",
      "path = r'QSAR/'\n",
      "\n",
      "for f in glob.glob(path + '*.csv'):\n",
      "    df = pd.read_csv(f)\n",
      "    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))\n",
      "    df = df.fillna(method='ffill')\n",
      "    Y = df[\"LogKp\"].values.astype(np.float)\n",
      "    data = df\n",
      "    \n",
      "    print ('\\n\\n************************************************************************************')\n",
      "    print ('')\n",
      "    print (f)\n",
      "    print ('')\n",
      "    \n",
      "    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%\n",
      "    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7\n",
      "\n",
      "    h = data.columns.tolist()\n",
      "    hx = np.array(h)\n",
      "\n",
      "    data = data.values.astype(np.float)\n",
      "    X = np.array(data)\n",
      "    \n",
      "    # Prepare empty lists to plot QSAR model\n",
      "    R2_train = []\n",
      "    RMSE_train = []\n",
      "    Q2_CV = []\n",
      "    RMSE_CV = []\n",
      "    Q2_External = []\n",
      "    RMSE_External = []\n",
      "    importances_dict = defaultdict(list)\n",
      "    \n",
      "    # Prepare empty lists to plot the performance of accuracy.\n",
      "    acclist_realRF                          = []\n",
      "    acclist_realRF_predictTrain             = []\n",
      "    acclist_predictionOnTest_scrambledtrain  = []\n",
      "    acclist_predictionOnTrain_scrambledtrain = []\n",
      "        \n",
      "    for i in range(10):\n",
      "        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)\n",
      "            \n",
      "    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \\\n",
      "                                                                        X_internal, X_external, Y_internal, Y_external)\n",
      "    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \\\n",
      "        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\\\n",
      "        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \n",
      "           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, \n",
      "           RMSE_External_std, X_internal, X_external, X, f, des1, des2)\n",
      "    plot_model(f, X_internal, X_external, Y_internal, Y_external,\n",
      "                R2_train_mean, Q2_External_mean,\n",
      "                importances_mean, importances_std, Feature, prediction,\n",
      "                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)\n",
      "    \n",
      "outfile.close()\n",
      "    #END\n",
      "141/204:\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "\n",
      "def build_model(X, Y, seed, hx, f):\n",
      "    \n",
      "    \n",
      "    #Data split using 70/30 ratio\n",
      "    X_internal, X_external, Y_internal, Y_external = train_test_split(X,\n",
      "                                                        Y, test_size=0.3,\n",
      "                                                        random_state=seed)\n",
      "\n",
      "    # Training set\n",
      "    rf = RandomForestRegressor(n_estimators=400, \n",
      "                               max_features='sqrt', \n",
      "                               min_samples_leaf = 1,\n",
      "                               random_state = 13,\n",
      "                               n_jobs=-1)\n",
      "    \n",
      "    rf.fit(X_internal,Y_internal)\n",
      "    prediction = rf.predict(X_internal)\n",
      "    \n",
      "    # Cross-validation\n",
      "    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)\n",
      "    \n",
      "    # External set  \n",
      "    prediction_external = rf.predict(X_external)\n",
      "    \n",
      "    #print result from each seed    \n",
      "    R2_train.append(r2_score(Y_internal, prediction))\n",
      "    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))\n",
      "    Q2_CV.append(r2_score(Y_internal, cv))\n",
      "    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))\n",
      "    Q2_External.append(r2_score(Y_external, prediction_external))\n",
      "    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))\n",
      "    \n",
      "    #Feature Importance\n",
      "    Feature = hx[:]\n",
      "    feature_importance = rf.feature_importances_\n",
      "    importances = 100.0 * (feature_importance / feature_importance.max()) #index\n",
      "    \n",
      "    for i, fx in enumerate(Feature):\n",
      "        \n",
      "        importances_dict[fx] += importances[i]\n",
      "       \n",
      "    \n",
      "    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict\n",
      "141/205:\n",
      "import glob # to read multiple files \n",
      "from rdkit import Chem, DataStructs\n",
      "from rdkit.Chem import AllChem\n",
      "import os\n",
      "\n",
      "%config InlineBackend.figure_format = 'retina'\n",
      "\n",
      "! rm Result/Result_LogKp.csv\n",
      "\n",
      "outfile = open('Result/Result_LogKp.csv', 'a')\n",
      "\n",
      "print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\\\n",
      "                    'R2_train,R2_train_std,' + \\\n",
      "                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \\\n",
      "                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')\n",
      "\n",
      "path = r'QSAR/'\n",
      "\n",
      "for f in glob.glob(path + '*.csv'):\n",
      "    df = pd.read_csv(f)\n",
      "    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))\n",
      "    df = df.fillna(method='ffill')\n",
      "    Y = df[\"LogKp\"].values.astype(np.float)\n",
      "    data = df\n",
      "    \n",
      "    print ('\\n\\n************************************************************************************')\n",
      "    print ('')\n",
      "    print (f)\n",
      "    print ('')\n",
      "    \n",
      "    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%\n",
      "    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7\n",
      "\n",
      "    h = data.columns.tolist()\n",
      "    hx = np.array(h)\n",
      "\n",
      "    data = data.values.astype(np.float)\n",
      "    X = np.array(data)\n",
      "    \n",
      "    # Prepare empty lists to plot QSAR model\n",
      "    R2_train = []\n",
      "    RMSE_train = []\n",
      "    Q2_CV = []\n",
      "    RMSE_CV = []\n",
      "    Q2_External = []\n",
      "    RMSE_External = []\n",
      "    importances_dict = defaultdict(list)\n",
      "    \n",
      "    # Prepare empty lists to plot the performance of accuracy.\n",
      "    acclist_realRF                          = []\n",
      "    acclist_realRF_predictTrain             = []\n",
      "    acclist_predictionOnTest_scrambledtrain  = []\n",
      "    acclist_predictionOnTrain_scrambledtrain = []\n",
      "        \n",
      "    for i in range(10):\n",
      "        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)\n",
      "            \n",
      "    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \\\n",
      "                                                                        X_internal, X_external, Y_internal, Y_external)\n",
      "    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \\\n",
      "        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\\\n",
      "        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \n",
      "           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, \n",
      "           RMSE_External_std, X_internal, X_external, X, f, des1, des2)\n",
      "    plot_model(f, X_internal, X_external, Y_internal, Y_external,\n",
      "                R2_train_mean, Q2_External_mean,\n",
      "                importances_mean, importances_std, Feature, prediction,\n",
      "                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)\n",
      "    \n",
      "outfile.close()\n",
      "    #END\n",
      "141/206:\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "\n",
      "def build_model(X, Y, seed, hx, f):\n",
      "    \n",
      "    \n",
      "    #Data split using 70/30 ratio\n",
      "    X_internal, X_external, Y_internal, Y_external = train_test_split(X,\n",
      "                                                        Y, test_size=0.3,\n",
      "                                                        random_state=seed)\n",
      "\n",
      "    # Training set\n",
      "    rf = RandomForestRegressor(n_estimators=400, \n",
      "                               max_features='sqrt', \n",
      "                               min_samples_leaf = 1,\n",
      "                               random_state = 13,\n",
      "                               n_jobs=-1)\n",
      "    \n",
      "    rf.fit(X_internal,Y_internal)\n",
      "    prediction = rf.predict(X_internal)\n",
      "    \n",
      "    # Cross-validation\n",
      "    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)\n",
      "    \n",
      "    # External set  \n",
      "    prediction_external = rf.predict(X_external)\n",
      "    \n",
      "    #print result from each seed    \n",
      "    R2_train.append(r2_score(Y_internal, prediction))\n",
      "    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))\n",
      "    Q2_CV.append(r2_score(Y_internal, cv))\n",
      "    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))\n",
      "    Q2_External.append(r2_score(Y_external, prediction_external))\n",
      "    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))\n",
      "    \n",
      "    #Feature Importance\n",
      "    Feature = hx[:]\n",
      "    feature_importance = rf.feature_importances_\n",
      "    importances = 100.0 * (feature_importance / feature_importance.max()) #index\n",
      "    \n",
      "    for i, fx in enumerate(Feature):\n",
      "        \n",
      "        importances_dict[fx] += importances\n",
      "       \n",
      "    \n",
      "    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict\n",
      "141/207:\n",
      "import glob # to read multiple files \n",
      "from rdkit import Chem, DataStructs\n",
      "from rdkit.Chem import AllChem\n",
      "import os\n",
      "\n",
      "%config InlineBackend.figure_format = 'retina'\n",
      "\n",
      "! rm Result/Result_LogKp.csv\n",
      "\n",
      "outfile = open('Result/Result_LogKp.csv', 'a')\n",
      "\n",
      "print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\\\n",
      "                    'R2_train,R2_train_std,' + \\\n",
      "                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \\\n",
      "                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')\n",
      "\n",
      "path = r'QSAR/'\n",
      "\n",
      "for f in glob.glob(path + '*.csv'):\n",
      "    df = pd.read_csv(f)\n",
      "    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))\n",
      "    df = df.fillna(method='ffill')\n",
      "    Y = df[\"LogKp\"].values.astype(np.float)\n",
      "    data = df\n",
      "    \n",
      "    print ('\\n\\n************************************************************************************')\n",
      "    print ('')\n",
      "    print (f)\n",
      "    print ('')\n",
      "    \n",
      "    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%\n",
      "    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7\n",
      "\n",
      "    h = data.columns.tolist()\n",
      "    hx = np.array(h)\n",
      "\n",
      "    data = data.values.astype(np.float)\n",
      "    X = np.array(data)\n",
      "    \n",
      "    # Prepare empty lists to plot QSAR model\n",
      "    R2_train = []\n",
      "    RMSE_train = []\n",
      "    Q2_CV = []\n",
      "    RMSE_CV = []\n",
      "    Q2_External = []\n",
      "    RMSE_External = []\n",
      "    importances_dict = defaultdict(list)\n",
      "    \n",
      "    # Prepare empty lists to plot the performance of accuracy.\n",
      "    acclist_realRF                          = []\n",
      "    acclist_realRF_predictTrain             = []\n",
      "    acclist_predictionOnTest_scrambledtrain  = []\n",
      "    acclist_predictionOnTrain_scrambledtrain = []\n",
      "        \n",
      "    for i in range(10):\n",
      "        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)\n",
      "            \n",
      "    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \\\n",
      "                                                                        X_internal, X_external, Y_internal, Y_external)\n",
      "    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \\\n",
      "        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\\\n",
      "        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \n",
      "           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, \n",
      "           RMSE_External_std, X_internal, X_external, X, f, des1, des2)\n",
      "    plot_model(f, X_internal, X_external, Y_internal, Y_external,\n",
      "                R2_train_mean, Q2_External_mean,\n",
      "                importances_mean, importances_std, Feature, prediction,\n",
      "                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)\n",
      "    \n",
      "outfile.close()\n",
      "    #END\n",
      "141/208:\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "\n",
      "def build_model(X, Y, seed, hx, f):\n",
      "    \n",
      "    \n",
      "    #Data split using 70/30 ratio\n",
      "    X_internal, X_external, Y_internal, Y_external = train_test_split(X,\n",
      "                                                        Y, test_size=0.3,\n",
      "                                                        random_state=seed)\n",
      "\n",
      "    # Training set\n",
      "    rf = RandomForestRegressor(n_estimators=400, \n",
      "                               max_features='sqrt', \n",
      "                               min_samples_leaf = 1,\n",
      "                               random_state = 13,\n",
      "                               n_jobs=-1)\n",
      "    \n",
      "    rf.fit(X_internal,Y_internal)\n",
      "    prediction = rf.predict(X_internal)\n",
      "    \n",
      "    # Cross-validation\n",
      "    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)\n",
      "    \n",
      "    # External set  \n",
      "    prediction_external = rf.predict(X_external)\n",
      "    \n",
      "    #print result from each seed    \n",
      "    R2_train.append(r2_score(Y_internal, prediction))\n",
      "    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))\n",
      "    Q2_CV.append(r2_score(Y_internal, cv))\n",
      "    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))\n",
      "    Q2_External.append(r2_score(Y_external, prediction_external))\n",
      "    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))\n",
      "    \n",
      "    #Feature Importance\n",
      "    Feature = hx[:]\n",
      "    feature_importance = rf.feature_importances_\n",
      "    importances = 100.0 * (feature_importance / feature_importance.max()) #index\n",
      "    \n",
      "    for i, fx in enumerate(Feature):\n",
      "        \n",
      "        importances_dict[fx] = importances(i)\n",
      "       \n",
      "    \n",
      "    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict\n",
      "141/209:\n",
      "import glob # to read multiple files \n",
      "from rdkit import Chem, DataStructs\n",
      "from rdkit.Chem import AllChem\n",
      "import os\n",
      "\n",
      "%config InlineBackend.figure_format = 'retina'\n",
      "\n",
      "! rm Result/Result_LogKp.csv\n",
      "\n",
      "outfile = open('Result/Result_LogKp.csv', 'a')\n",
      "\n",
      "print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\\\n",
      "                    'R2_train,R2_train_std,' + \\\n",
      "                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \\\n",
      "                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')\n",
      "\n",
      "path = r'QSAR/'\n",
      "\n",
      "for f in glob.glob(path + '*.csv'):\n",
      "    df = pd.read_csv(f)\n",
      "    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))\n",
      "    df = df.fillna(method='ffill')\n",
      "    Y = df[\"LogKp\"].values.astype(np.float)\n",
      "    data = df\n",
      "    \n",
      "    print ('\\n\\n************************************************************************************')\n",
      "    print ('')\n",
      "    print (f)\n",
      "    print ('')\n",
      "    \n",
      "    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%\n",
      "    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7\n",
      "\n",
      "    h = data.columns.tolist()\n",
      "    hx = np.array(h)\n",
      "\n",
      "    data = data.values.astype(np.float)\n",
      "    X = np.array(data)\n",
      "    \n",
      "    # Prepare empty lists to plot QSAR model\n",
      "    R2_train = []\n",
      "    RMSE_train = []\n",
      "    Q2_CV = []\n",
      "    RMSE_CV = []\n",
      "    Q2_External = []\n",
      "    RMSE_External = []\n",
      "    importances_dict = defaultdict(list)\n",
      "    \n",
      "    # Prepare empty lists to plot the performance of accuracy.\n",
      "    acclist_realRF                          = []\n",
      "    acclist_realRF_predictTrain             = []\n",
      "    acclist_predictionOnTest_scrambledtrain  = []\n",
      "    acclist_predictionOnTrain_scrambledtrain = []\n",
      "        \n",
      "    for i in range(10):\n",
      "        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)\n",
      "            \n",
      "    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \\\n",
      "                                                                        X_internal, X_external, Y_internal, Y_external)\n",
      "    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \\\n",
      "        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\\\n",
      "        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \n",
      "           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, \n",
      "           RMSE_External_std, X_internal, X_external, X, f, des1, des2)\n",
      "    plot_model(f, X_internal, X_external, Y_internal, Y_external,\n",
      "                R2_train_mean, Q2_External_mean,\n",
      "                importances_mean, importances_std, Feature, prediction,\n",
      "                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)\n",
      "    \n",
      "outfile.close()\n",
      "    #END\n",
      "141/210:\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "\n",
      "def build_model(X, Y, seed, hx, f):\n",
      "    \n",
      "    \n",
      "    #Data split using 70/30 ratio\n",
      "    X_internal, X_external, Y_internal, Y_external = train_test_split(X,\n",
      "                                                        Y, test_size=0.3,\n",
      "                                                        random_state=seed)\n",
      "\n",
      "    # Training set\n",
      "    rf = RandomForestRegressor(n_estimators=400, \n",
      "                               max_features='sqrt', \n",
      "                               min_samples_leaf = 1,\n",
      "                               random_state = 13,\n",
      "                               n_jobs=-1)\n",
      "    \n",
      "    rf.fit(X_internal,Y_internal)\n",
      "    prediction = rf.predict(X_internal)\n",
      "    \n",
      "    # Cross-validation\n",
      "    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)\n",
      "    \n",
      "    # External set  \n",
      "    prediction_external = rf.predict(X_external)\n",
      "    \n",
      "    #print result from each seed    \n",
      "    R2_train.append(r2_score(Y_internal, prediction))\n",
      "    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))\n",
      "    Q2_CV.append(r2_score(Y_internal, cv))\n",
      "    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))\n",
      "    Q2_External.append(r2_score(Y_external, prediction_external))\n",
      "    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))\n",
      "    \n",
      "    #Feature Importance\n",
      "    Feature = hx[:]\n",
      "    feature_importance = rf.feature_importances_\n",
      "    importances = 100.0 * (feature_importance / feature_importance.max()) #index\n",
      "    \n",
      "    for i, fx in enumerate(Feature):\n",
      "        \n",
      "        importances_dict[fx].append(importances[i])\n",
      "       \n",
      "    \n",
      "    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict\n",
      "141/211:\n",
      "import glob # to read multiple files \n",
      "from rdkit import Chem, DataStructs\n",
      "from rdkit.Chem import AllChem\n",
      "import os\n",
      "\n",
      "%config InlineBackend.figure_format = 'retina'\n",
      "\n",
      "! rm Result/Result_LogKp.csv\n",
      "\n",
      "outfile = open('Result/Result_LogKp.csv', 'a')\n",
      "\n",
      "print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\\\n",
      "                    'R2_train,R2_train_std,' + \\\n",
      "                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \\\n",
      "                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')\n",
      "\n",
      "path = r'QSAR/'\n",
      "\n",
      "for f in glob.glob(path + '*.csv'):\n",
      "    df = pd.read_csv(f)\n",
      "    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))\n",
      "    df = df.fillna(method='ffill')\n",
      "    Y = df[\"LogKp\"].values.astype(np.float)\n",
      "    data = df\n",
      "    \n",
      "    print ('\\n\\n************************************************************************************')\n",
      "    print ('')\n",
      "    print (f)\n",
      "    print ('')\n",
      "    \n",
      "    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%\n",
      "    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7\n",
      "\n",
      "    h = data.columns.tolist()\n",
      "    hx = np.array(h)\n",
      "\n",
      "    data = data.values.astype(np.float)\n",
      "    X = np.array(data)\n",
      "    \n",
      "    # Prepare empty lists to plot QSAR model\n",
      "    R2_train = []\n",
      "    RMSE_train = []\n",
      "    Q2_CV = []\n",
      "    RMSE_CV = []\n",
      "    Q2_External = []\n",
      "    RMSE_External = []\n",
      "    importances_dict = defaultdict(list)\n",
      "    \n",
      "    # Prepare empty lists to plot the performance of accuracy.\n",
      "    acclist_realRF                          = []\n",
      "    acclist_realRF_predictTrain             = []\n",
      "    acclist_predictionOnTest_scrambledtrain  = []\n",
      "    acclist_predictionOnTrain_scrambledtrain = []\n",
      "        \n",
      "    for i in range(10):\n",
      "        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)\n",
      "            \n",
      "    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \\\n",
      "                                                                        X_internal, X_external, Y_internal, Y_external)\n",
      "    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \\\n",
      "        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\\\n",
      "        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \n",
      "           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, \n",
      "           RMSE_External_std, X_internal, X_external, X, f, des1, des2)\n",
      "    plot_model(f, X_internal, X_external, Y_internal, Y_external,\n",
      "                R2_train_mean, Q2_External_mean,\n",
      "                importances_mean, importances_std, Feature, prediction,\n",
      "                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)\n",
      "    \n",
      "outfile.close()\n",
      "    #END\n",
      "141/212:\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "\n",
      "def build_model(X, Y, seed, hx, f):\n",
      "    \n",
      "    \n",
      "    #Data split using 70/30 ratio\n",
      "    X_internal, X_external, Y_internal, Y_external = train_test_split(X,\n",
      "                                                        Y, test_size=0.3,\n",
      "                                                        random_state=seed)\n",
      "\n",
      "    # Training set\n",
      "    rf = RandomForestRegressor(n_estimators=400, \n",
      "                               max_features='sqrt', \n",
      "                               min_samples_leaf = 1,\n",
      "                               random_state = 13,\n",
      "                               n_jobs=-1)\n",
      "    \n",
      "    rf.fit(X_internal,Y_internal)\n",
      "    prediction = rf.predict(X_internal)\n",
      "    \n",
      "    # Cross-validation\n",
      "    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)\n",
      "    \n",
      "    # External set  \n",
      "    prediction_external = rf.predict(X_external)\n",
      "    \n",
      "    #print result from each seed    \n",
      "    R2_train.append(r2_score(Y_internal, prediction))\n",
      "    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))\n",
      "    Q2_CV.append(r2_score(Y_internal, cv))\n",
      "    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))\n",
      "    Q2_External.append(r2_score(Y_external, prediction_external))\n",
      "    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))\n",
      "    \n",
      "    #Feature Importance\n",
      "    Feature = hx[:]\n",
      "    feature_importance = rf.feature_importances_\n",
      "    importances = 100.0 * (feature_importance / feature_importance.max()) #index\n",
      "    \n",
      "    for i, fx in enumerate(Feature):\n",
      "        \n",
      "        hash(tuple(importances_dict[fx])).append(importances[i])\n",
      "       \n",
      "    \n",
      "    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict\n",
      "141/213:\n",
      "import glob # to read multiple files \n",
      "from rdkit import Chem, DataStructs\n",
      "from rdkit.Chem import AllChem\n",
      "import os\n",
      "\n",
      "%config InlineBackend.figure_format = 'retina'\n",
      "\n",
      "! rm Result/Result_LogKp.csv\n",
      "\n",
      "outfile = open('Result/Result_LogKp.csv', 'a')\n",
      "\n",
      "print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\\\n",
      "                    'R2_train,R2_train_std,' + \\\n",
      "                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \\\n",
      "                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')\n",
      "\n",
      "path = r'QSAR/'\n",
      "\n",
      "for f in glob.glob(path + '*.csv'):\n",
      "    df = pd.read_csv(f)\n",
      "    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))\n",
      "    df = df.fillna(method='ffill')\n",
      "    Y = df[\"LogKp\"].values.astype(np.float)\n",
      "    data = df\n",
      "    \n",
      "    print ('\\n\\n************************************************************************************')\n",
      "    print ('')\n",
      "    print (f)\n",
      "    print ('')\n",
      "    \n",
      "    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%\n",
      "    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7\n",
      "\n",
      "    h = data.columns.tolist()\n",
      "    hx = np.array(h)\n",
      "\n",
      "    data = data.values.astype(np.float)\n",
      "    X = np.array(data)\n",
      "    \n",
      "    # Prepare empty lists to plot QSAR model\n",
      "    R2_train = []\n",
      "    RMSE_train = []\n",
      "    Q2_CV = []\n",
      "    RMSE_CV = []\n",
      "    Q2_External = []\n",
      "    RMSE_External = []\n",
      "    importances_dict = defaultdict(list)\n",
      "    \n",
      "    # Prepare empty lists to plot the performance of accuracy.\n",
      "    acclist_realRF                          = []\n",
      "    acclist_realRF_predictTrain             = []\n",
      "    acclist_predictionOnTest_scrambledtrain  = []\n",
      "    acclist_predictionOnTrain_scrambledtrain = []\n",
      "        \n",
      "    for i in range(10):\n",
      "        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)\n",
      "            \n",
      "    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \\\n",
      "                                                                        X_internal, X_external, Y_internal, Y_external)\n",
      "    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \\\n",
      "        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\\\n",
      "        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \n",
      "           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, \n",
      "           RMSE_External_std, X_internal, X_external, X, f, des1, des2)\n",
      "    plot_model(f, X_internal, X_external, Y_internal, Y_external,\n",
      "                R2_train_mean, Q2_External_mean,\n",
      "                importances_mean, importances_std, Feature, prediction,\n",
      "                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)\n",
      "    \n",
      "outfile.close()\n",
      "    #END\n",
      "141/214:\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "\n",
      "def build_model(X, Y, seed, hx, f):\n",
      "    \n",
      "    \n",
      "    #Data split using 70/30 ratio\n",
      "    X_internal, X_external, Y_internal, Y_external = train_test_split(X,\n",
      "                                                        Y, test_size=0.3,\n",
      "                                                        random_state=seed)\n",
      "\n",
      "    # Training set\n",
      "    rf = RandomForestRegressor(n_estimators=400, \n",
      "                               max_features='sqrt', \n",
      "                               min_samples_leaf = 1,\n",
      "                               random_state = 13,\n",
      "                               n_jobs=-1)\n",
      "    \n",
      "    rf.fit(X_internal,Y_internal)\n",
      "    prediction = rf.predict(X_internal)\n",
      "    \n",
      "    # Cross-validation\n",
      "    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)\n",
      "    \n",
      "    # External set  \n",
      "    prediction_external = rf.predict(X_external)\n",
      "    \n",
      "    #print result from each seed    \n",
      "    R2_train.append(r2_score(Y_internal, prediction))\n",
      "    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))\n",
      "    Q2_CV.append(r2_score(Y_internal, cv))\n",
      "    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))\n",
      "    Q2_External.append(r2_score(Y_external, prediction_external))\n",
      "    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))\n",
      "    \n",
      "    #Feature Importance\n",
      "    Feature = hx[:]\n",
      "    feature_importance = rf.feature_importances_\n",
      "    importances = 100.0 * (feature_importance / feature_importance.max()) #index\n",
      "    \n",
      "    for i, fx in enumerate(Feature):\n",
      "        \n",
      "        hash(tuple(importances_dict))\n",
      "        importances_dict[fx].append(importances[i])\n",
      "       \n",
      "    \n",
      "    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict\n",
      "141/215:\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "\n",
      "def build_model(X, Y, seed, hx, f):\n",
      "    \n",
      "    \n",
      "    #Data split using 70/30 ratio\n",
      "    X_internal, X_external, Y_internal, Y_external = train_test_split(X,\n",
      "                                                        Y, test_size=0.3,\n",
      "                                                        random_state=seed)\n",
      "\n",
      "    # Training set\n",
      "    rf = RandomForestRegressor(n_estimators=400, \n",
      "                               max_features='sqrt', \n",
      "                               min_samples_leaf = 1,\n",
      "                               random_state = 13,\n",
      "                               n_jobs=-1)\n",
      "    \n",
      "    rf.fit(X_internal,Y_internal)\n",
      "    prediction = rf.predict(X_internal)\n",
      "    \n",
      "    # Cross-validation\n",
      "    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)\n",
      "    \n",
      "    # External set  \n",
      "    prediction_external = rf.predict(X_external)\n",
      "    \n",
      "    #print result from each seed    \n",
      "    R2_train.append(r2_score(Y_internal, prediction))\n",
      "    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))\n",
      "    Q2_CV.append(r2_score(Y_internal, cv))\n",
      "    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))\n",
      "    Q2_External.append(r2_score(Y_external, prediction_external))\n",
      "    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))\n",
      "    \n",
      "    #Feature Importance\n",
      "    Feature = hx[:]\n",
      "    feature_importance = rf.feature_importances_\n",
      "    importances = 100.0 * (feature_importance / feature_importance.max()) #index\n",
      "    \n",
      "    for i, fx in enumerate(Feature):\n",
      "        \n",
      "        importances_dict = hash(tuple(importances_dict))\n",
      "        importances_dict[fx].append(importances[i])\n",
      "       \n",
      "    \n",
      "    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict\n",
      "141/216:\n",
      "import glob # to read multiple files \n",
      "from rdkit import Chem, DataStructs\n",
      "from rdkit.Chem import AllChem\n",
      "import os\n",
      "\n",
      "%config InlineBackend.figure_format = 'retina'\n",
      "\n",
      "! rm Result/Result_LogKp.csv\n",
      "\n",
      "outfile = open('Result/Result_LogKp.csv', 'a')\n",
      "\n",
      "print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\\\n",
      "                    'R2_train,R2_train_std,' + \\\n",
      "                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \\\n",
      "                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')\n",
      "\n",
      "path = r'QSAR/'\n",
      "\n",
      "for f in glob.glob(path + '*.csv'):\n",
      "    df = pd.read_csv(f)\n",
      "    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))\n",
      "    df = df.fillna(method='ffill')\n",
      "    Y = df[\"LogKp\"].values.astype(np.float)\n",
      "    data = df\n",
      "    \n",
      "    print ('\\n\\n************************************************************************************')\n",
      "    print ('')\n",
      "    print (f)\n",
      "    print ('')\n",
      "    \n",
      "    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%\n",
      "    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7\n",
      "\n",
      "    h = data.columns.tolist()\n",
      "    hx = np.array(h)\n",
      "\n",
      "    data = data.values.astype(np.float)\n",
      "    X = np.array(data)\n",
      "    \n",
      "    # Prepare empty lists to plot QSAR model\n",
      "    R2_train = []\n",
      "    RMSE_train = []\n",
      "    Q2_CV = []\n",
      "    RMSE_CV = []\n",
      "    Q2_External = []\n",
      "    RMSE_External = []\n",
      "    importances_dict = defaultdict(list)\n",
      "    \n",
      "    # Prepare empty lists to plot the performance of accuracy.\n",
      "    acclist_realRF                          = []\n",
      "    acclist_realRF_predictTrain             = []\n",
      "    acclist_predictionOnTest_scrambledtrain  = []\n",
      "    acclist_predictionOnTrain_scrambledtrain = []\n",
      "        \n",
      "    for i in range(10):\n",
      "        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)\n",
      "            \n",
      "    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \\\n",
      "                                                                        X_internal, X_external, Y_internal, Y_external)\n",
      "    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \\\n",
      "        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\\\n",
      "        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \n",
      "           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, \n",
      "           RMSE_External_std, X_internal, X_external, X, f, des1, des2)\n",
      "    plot_model(f, X_internal, X_external, Y_internal, Y_external,\n",
      "                R2_train_mean, Q2_External_mean,\n",
      "                importances_mean, importances_std, Feature, prediction,\n",
      "                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)\n",
      "    \n",
      "outfile.close()\n",
      "    #END\n",
      "141/217:\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "\n",
      "def build_model(X, Y, seed, hx, f):\n",
      "    \n",
      "    \n",
      "    #Data split using 70/30 ratio\n",
      "    X_internal, X_external, Y_internal, Y_external = train_test_split(X,\n",
      "                                                        Y, test_size=0.3,\n",
      "                                                        random_state=seed)\n",
      "\n",
      "    # Training set\n",
      "    rf = RandomForestRegressor(n_estimators=400, \n",
      "                               max_features='sqrt', \n",
      "                               min_samples_leaf = 1,\n",
      "                               random_state = 13,\n",
      "                               n_jobs=-1)\n",
      "    \n",
      "    rf.fit(X_internal,Y_internal)\n",
      "    prediction = rf.predict(X_internal)\n",
      "    \n",
      "    # Cross-validation\n",
      "    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)\n",
      "    \n",
      "    # External set  \n",
      "    prediction_external = rf.predict(X_external)\n",
      "    \n",
      "    #print result from each seed    \n",
      "    R2_train.append(r2_score(Y_internal, prediction))\n",
      "    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))\n",
      "    Q2_CV.append(r2_score(Y_internal, cv))\n",
      "    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))\n",
      "    Q2_External.append(r2_score(Y_external, prediction_external))\n",
      "    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))\n",
      "    \n",
      "    #Feature Importance\n",
      "    Feature = hx[:]\n",
      "    feature_importance = rf.feature_importances_\n",
      "    importances = 100.0 * (feature_importance / feature_importance.max()) #index\n",
      "    \n",
      "    for i, fx in enumerate(Feature):\n",
      "        \n",
      "        importances_dict = hash(importances_dict)\n",
      "        importances_dict[fx].append(importances[i])\n",
      "       \n",
      "    \n",
      "    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict\n",
      "141/218:\n",
      "import glob # to read multiple files \n",
      "from rdkit import Chem, DataStructs\n",
      "from rdkit.Chem import AllChem\n",
      "import os\n",
      "\n",
      "%config InlineBackend.figure_format = 'retina'\n",
      "\n",
      "! rm Result/Result_LogKp.csv\n",
      "\n",
      "outfile = open('Result/Result_LogKp.csv', 'a')\n",
      "\n",
      "print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\\\n",
      "                    'R2_train,R2_train_std,' + \\\n",
      "                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \\\n",
      "                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')\n",
      "\n",
      "path = r'QSAR/'\n",
      "\n",
      "for f in glob.glob(path + '*.csv'):\n",
      "    df = pd.read_csv(f)\n",
      "    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))\n",
      "    df = df.fillna(method='ffill')\n",
      "    Y = df[\"LogKp\"].values.astype(np.float)\n",
      "    data = df\n",
      "    \n",
      "    print ('\\n\\n************************************************************************************')\n",
      "    print ('')\n",
      "    print (f)\n",
      "    print ('')\n",
      "    \n",
      "    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%\n",
      "    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7\n",
      "\n",
      "    h = data.columns.tolist()\n",
      "    hx = np.array(h)\n",
      "\n",
      "    data = data.values.astype(np.float)\n",
      "    X = np.array(data)\n",
      "    \n",
      "    # Prepare empty lists to plot QSAR model\n",
      "    R2_train = []\n",
      "    RMSE_train = []\n",
      "    Q2_CV = []\n",
      "    RMSE_CV = []\n",
      "    Q2_External = []\n",
      "    RMSE_External = []\n",
      "    importances_dict = defaultdict(list)\n",
      "    \n",
      "    # Prepare empty lists to plot the performance of accuracy.\n",
      "    acclist_realRF                          = []\n",
      "    acclist_realRF_predictTrain             = []\n",
      "    acclist_predictionOnTest_scrambledtrain  = []\n",
      "    acclist_predictionOnTrain_scrambledtrain = []\n",
      "        \n",
      "    for i in range(10):\n",
      "        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)\n",
      "            \n",
      "    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \\\n",
      "                                                                        X_internal, X_external, Y_internal, Y_external)\n",
      "    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \\\n",
      "        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\\\n",
      "        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \n",
      "           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, \n",
      "           RMSE_External_std, X_internal, X_external, X, f, des1, des2)\n",
      "    plot_model(f, X_internal, X_external, Y_internal, Y_external,\n",
      "                R2_train_mean, Q2_External_mean,\n",
      "                importances_mean, importances_std, Feature, prediction,\n",
      "                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)\n",
      "    \n",
      "outfile.close()\n",
      "    #END\n",
      "141/219:\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "\n",
      "def build_model(X, Y, seed, hx, f):\n",
      "    \n",
      "    \n",
      "    #Data split using 70/30 ratio\n",
      "    X_internal, X_external, Y_internal, Y_external = train_test_split(X,\n",
      "                                                        Y, test_size=0.3,\n",
      "                                                        random_state=seed)\n",
      "\n",
      "    # Training set\n",
      "    rf = RandomForestRegressor(n_estimators=400, \n",
      "                               max_features='sqrt', \n",
      "                               min_samples_leaf = 1,\n",
      "                               random_state = 13,\n",
      "                               n_jobs=-1)\n",
      "    \n",
      "    rf.fit(X_internal,Y_internal)\n",
      "    prediction = rf.predict(X_internal)\n",
      "    \n",
      "    # Cross-validation\n",
      "    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)\n",
      "    \n",
      "    # External set  \n",
      "    prediction_external = rf.predict(X_external)\n",
      "    \n",
      "    #print result from each seed    \n",
      "    R2_train.append(r2_score(Y_internal, prediction))\n",
      "    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))\n",
      "    Q2_CV.append(r2_score(Y_internal, cv))\n",
      "    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))\n",
      "    Q2_External.append(r2_score(Y_external, prediction_external))\n",
      "    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))\n",
      "    \n",
      "    #Feature Importance\n",
      "    Feature = hx[:]\n",
      "    feature_importance = rf.feature_importances_\n",
      "    importances = 100.0 * (feature_importance / feature_importance.max()) #index\n",
      "    \n",
      "    for i, fx in enumerate(Feature):\n",
      "        \n",
      "        hash(importances_dict[fx]).append(importances[i])\n",
      "       \n",
      "    \n",
      "    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict\n",
      "141/220:\n",
      "import glob # to read multiple files \n",
      "from rdkit import Chem, DataStructs\n",
      "from rdkit.Chem import AllChem\n",
      "import os\n",
      "\n",
      "%config InlineBackend.figure_format = 'retina'\n",
      "\n",
      "! rm Result/Result_LogKp.csv\n",
      "\n",
      "outfile = open('Result/Result_LogKp.csv', 'a')\n",
      "\n",
      "print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\\\n",
      "                    'R2_train,R2_train_std,' + \\\n",
      "                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \\\n",
      "                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')\n",
      "\n",
      "path = r'QSAR/'\n",
      "\n",
      "for f in glob.glob(path + '*.csv'):\n",
      "    df = pd.read_csv(f)\n",
      "    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))\n",
      "    df = df.fillna(method='ffill')\n",
      "    Y = df[\"LogKp\"].values.astype(np.float)\n",
      "    data = df\n",
      "    \n",
      "    print ('\\n\\n************************************************************************************')\n",
      "    print ('')\n",
      "    print (f)\n",
      "    print ('')\n",
      "    \n",
      "    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%\n",
      "    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7\n",
      "\n",
      "    h = data.columns.tolist()\n",
      "    hx = np.array(h)\n",
      "\n",
      "    data = data.values.astype(np.float)\n",
      "    X = np.array(data)\n",
      "    \n",
      "    # Prepare empty lists to plot QSAR model\n",
      "    R2_train = []\n",
      "    RMSE_train = []\n",
      "    Q2_CV = []\n",
      "    RMSE_CV = []\n",
      "    Q2_External = []\n",
      "    RMSE_External = []\n",
      "    importances_dict = defaultdict(list)\n",
      "    \n",
      "    # Prepare empty lists to plot the performance of accuracy.\n",
      "    acclist_realRF                          = []\n",
      "    acclist_realRF_predictTrain             = []\n",
      "    acclist_predictionOnTest_scrambledtrain  = []\n",
      "    acclist_predictionOnTrain_scrambledtrain = []\n",
      "        \n",
      "    for i in range(10):\n",
      "        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)\n",
      "            \n",
      "    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \\\n",
      "                                                                        X_internal, X_external, Y_internal, Y_external)\n",
      "    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \\\n",
      "        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\\\n",
      "        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \n",
      "           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, \n",
      "           RMSE_External_std, X_internal, X_external, X, f, des1, des2)\n",
      "    plot_model(f, X_internal, X_external, Y_internal, Y_external,\n",
      "                R2_train_mean, Q2_External_mean,\n",
      "                importances_mean, importances_std, Feature, prediction,\n",
      "                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)\n",
      "    \n",
      "outfile.close()\n",
      "    #END\n",
      "141/221:\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "\n",
      "def build_model(X, Y, seed, hx, f):\n",
      "    \n",
      "    \n",
      "    #Data split using 70/30 ratio\n",
      "    X_internal, X_external, Y_internal, Y_external = train_test_split(X,\n",
      "                                                        Y, test_size=0.3,\n",
      "                                                        random_state=seed)\n",
      "\n",
      "    # Training set\n",
      "    rf = RandomForestRegressor(n_estimators=400, \n",
      "                               max_features='sqrt', \n",
      "                               min_samples_leaf = 1,\n",
      "                               random_state = 13,\n",
      "                               n_jobs=-1)\n",
      "    \n",
      "    rf.fit(X_internal,Y_internal)\n",
      "    prediction = rf.predict(X_internal)\n",
      "    \n",
      "    # Cross-validation\n",
      "    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)\n",
      "    \n",
      "    # External set  \n",
      "    prediction_external = rf.predict(X_external)\n",
      "    \n",
      "    #print result from each seed    \n",
      "    R2_train.append(r2_score(Y_internal, prediction))\n",
      "    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))\n",
      "    Q2_CV.append(r2_score(Y_internal, cv))\n",
      "    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))\n",
      "    Q2_External.append(r2_score(Y_external, prediction_external))\n",
      "    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))\n",
      "    \n",
      "    #Feature Importance\n",
      "    Feature = hx[:]\n",
      "    feature_importance = rf.feature_importances_\n",
      "    importances = 100.0 * (feature_importance / feature_importance.max()) #index\n",
      "    \n",
      "       \n",
      "    \n",
      "    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict\n",
      "141/222:\n",
      "import glob # to read multiple files \n",
      "from rdkit import Chem, DataStructs\n",
      "from rdkit.Chem import AllChem\n",
      "import os\n",
      "\n",
      "%config InlineBackend.figure_format = 'retina'\n",
      "\n",
      "! rm Result/Result_LogKp.csv\n",
      "\n",
      "outfile = open('Result/Result_LogKp.csv', 'a')\n",
      "\n",
      "print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\\\n",
      "                    'R2_train,R2_train_std,' + \\\n",
      "                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \\\n",
      "                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')\n",
      "\n",
      "path = r'QSAR/'\n",
      "\n",
      "for f in glob.glob(path + '*.csv'):\n",
      "    df = pd.read_csv(f)\n",
      "    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))\n",
      "    df = df.fillna(method='ffill')\n",
      "    Y = df[\"LogKp\"].values.astype(np.float)\n",
      "    data = df\n",
      "    \n",
      "    print ('\\n\\n************************************************************************************')\n",
      "    print ('')\n",
      "    print (f)\n",
      "    print ('')\n",
      "    \n",
      "    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%\n",
      "    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7\n",
      "\n",
      "    h = data.columns.tolist()\n",
      "    hx = np.array(h)\n",
      "\n",
      "    data = data.values.astype(np.float)\n",
      "    X = np.array(data)\n",
      "    \n",
      "    # Prepare empty lists to plot QSAR model\n",
      "    R2_train = []\n",
      "    RMSE_train = []\n",
      "    Q2_CV = []\n",
      "    RMSE_CV = []\n",
      "    Q2_External = []\n",
      "    RMSE_External = []\n",
      "    importances_dict = defaultdict(list)\n",
      "    \n",
      "    # Prepare empty lists to plot the performance of accuracy.\n",
      "    acclist_realRF                          = []\n",
      "    acclist_realRF_predictTrain             = []\n",
      "    acclist_predictionOnTest_scrambledtrain  = []\n",
      "    acclist_predictionOnTrain_scrambledtrain = []\n",
      "        \n",
      "    for i in range(10):\n",
      "        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)\n",
      "            \n",
      "    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \\\n",
      "                                                                        X_internal, X_external, Y_internal, Y_external)\n",
      "    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \\\n",
      "        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\\\n",
      "        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \n",
      "           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, \n",
      "           RMSE_External_std, X_internal, X_external, X, f, des1, des2)\n",
      "    plot_model(f, X_internal, X_external, Y_internal, Y_external,\n",
      "                R2_train_mean, Q2_External_mean,\n",
      "                importances_mean, importances_std, Feature, prediction,\n",
      "                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)\n",
      "    \n",
      "outfile.close()\n",
      "    #END\n",
      "141/223:\n",
      "def mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, importances_dict):\n",
      "    R2_train_mean = np.mean(R2_train)\n",
      "    RMSE_train_mean = np.mean(RMSE_train)\n",
      "    Q2_CV_mean = np.mean(Q2_CV)\n",
      "    RMSE_CV_mean = np.mean(RMSE_CV)\n",
      "    Q2_External_mean = np.mean(Q2_External)\n",
      "    RMSE_External_mean = np.mean(RMSE_External)\n",
      "    importances_mean0 = {}\n",
      "    for fx in importances_dict:\n",
      "        importances_mean0[fx] = np.mean(importances_dict[fx])\n",
      "    importances_mean = sorted([(k,v) for k,v in importances_mean0.items()],\n",
      "                                    key=lambda x: x[1], reverse=True)\n",
      "    \n",
      "    #predictionOnTest_mean = np.mean(acclist_predictionOnTest_scrambledtrain)\n",
      "    #predictionOnTrain_mean = np.mean(acclist_predictionOnTrain_scrambledtrain)\n",
      "    \n",
      "    return R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \\\n",
      "            RMSE_External_mean, importances_mean\n",
      "141/224:\n",
      "import glob # to read multiple files \n",
      "from rdkit import Chem, DataStructs\n",
      "from rdkit.Chem import AllChem\n",
      "import os\n",
      "\n",
      "%config InlineBackend.figure_format = 'retina'\n",
      "\n",
      "! rm Result/Result_LogKp.csv\n",
      "\n",
      "outfile = open('Result/Result_LogKp.csv', 'a')\n",
      "\n",
      "print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\\\n",
      "                    'R2_train,R2_train_std,' + \\\n",
      "                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \\\n",
      "                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')\n",
      "\n",
      "path = r'QSAR/'\n",
      "\n",
      "for f in glob.glob(path + '*.csv'):\n",
      "    df = pd.read_csv(f)\n",
      "    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))\n",
      "    df = df.fillna(method='ffill')\n",
      "    Y = df[\"LogKp\"].values.astype(np.float)\n",
      "    data = df\n",
      "    \n",
      "    print ('\\n\\n************************************************************************************')\n",
      "    print ('')\n",
      "    print (f)\n",
      "    print ('')\n",
      "    \n",
      "    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%\n",
      "    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7\n",
      "\n",
      "    h = data.columns.tolist()\n",
      "    hx = np.array(h)\n",
      "\n",
      "    data = data.values.astype(np.float)\n",
      "    X = np.array(data)\n",
      "    \n",
      "    # Prepare empty lists to plot QSAR model\n",
      "    R2_train = []\n",
      "    RMSE_train = []\n",
      "    Q2_CV = []\n",
      "    RMSE_CV = []\n",
      "    Q2_External = []\n",
      "    RMSE_External = []\n",
      "    importances_dict = defaultdict(list)\n",
      "    \n",
      "    # Prepare empty lists to plot the performance of accuracy.\n",
      "    acclist_realRF                          = []\n",
      "    acclist_realRF_predictTrain             = []\n",
      "    acclist_predictionOnTest_scrambledtrain  = []\n",
      "    acclist_predictionOnTrain_scrambledtrain = []\n",
      "        \n",
      "    for i in range(10):\n",
      "        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)\n",
      "            \n",
      "    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \\\n",
      "                                                                        X_internal, X_external, Y_internal, Y_external)\n",
      "    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \\\n",
      "        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\\\n",
      "        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \n",
      "           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, \n",
      "           RMSE_External_std, X_internal, X_external, X, f, des1, des2)\n",
      "    plot_model(f, X_internal, X_external, Y_internal, Y_external,\n",
      "                R2_train_mean, Q2_External_mean,\n",
      "                importances_mean, importances_std, Feature, prediction,\n",
      "                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)\n",
      "    \n",
      "outfile.close()\n",
      "    #END\n",
      "141/225:\n",
      "import numpy as np\n",
      "\n",
      "def Remove_useless_descriptor(df, threshold):\n",
      "    \n",
      "    des1 = len(df.columns) \n",
      "    \n",
      "    h = df.columns.tolist()\n",
      "    df = df.values.astype(np.float)\n",
      "    df = np.array(df)\n",
      "\n",
      "    STDEV = np.std(df, axis=0)\n",
      "    idx = [idx for idx, val in enumerate(STDEV) if val > threshold]\n",
      "    df2 = df[:,idx]\n",
      "    hx = np.array(h)[idx]\n",
      "    \n",
      "    df = pd.DataFrame(df2, columns=[hx])\n",
      "    \n",
      "    des2 = len(df.columns)\n",
      "    \n",
      "    print('from Remove useless descriptor')\n",
      "    print(\"The initial set of \" + str(des1) + \\\n",
      "          \" descriptors has been reduced to \" + str(des2) + \" descriptors.\")\n",
      "    \n",
      "    return df, des1, des2\n",
      "141/226:\n",
      "from scipy import stats\n",
      "\n",
      "def correlation(df, threshold):\n",
      "\n",
      "    des3 = len(df.columns) \n",
      "    corr = stats.pearsonr\n",
      "    col_corr = set() # Set of all the names of deleted columns\n",
      "    corr_matrix = df.corr()\n",
      "    for i in range(len(corr_matrix.columns)):\n",
      "        for j in range(i):\n",
      "            if corr_matrix.iloc[i, j] >= threshold:\n",
      "                colname = corr_matrix.columns[i] # getting the name of column\n",
      "                col_corr.add(colname)\n",
      "                if colname in df.columns:\n",
      "                    del df[colname] # deleting the column from the dataset\n",
      "    des4 = len(df.columns) \n",
      "\n",
      "    print ('from Remove correlation')\n",
      "    print (\"The initial set of \" + str(des3) + ' descriptors'+ \n",
      "           \" has been reduced to \" + str(des4) + \" descriptors.\")\n",
      "\n",
      "    return df, des3, des4\n",
      "141/227:\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "\n",
      "def build_model(X, Y, seed, hx, f):\n",
      "    \n",
      "    \n",
      "    #Data split using 70/30 ratio\n",
      "    X_internal, X_external, Y_internal, Y_external = train_test_split(X,\n",
      "                                                        Y, test_size=0.3,\n",
      "                                                        random_state=seed)\n",
      "\n",
      "    # Training set\n",
      "    rf = RandomForestRegressor(n_estimators=400, \n",
      "                               max_features='sqrt', \n",
      "                               min_samples_leaf = 1,\n",
      "                               random_state = 13,\n",
      "                               n_jobs=-1)\n",
      "    \n",
      "    rf.fit(X_internal,Y_internal)\n",
      "    prediction = rf.predict(X_internal)\n",
      "    \n",
      "    # Cross-validation\n",
      "    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)\n",
      "    \n",
      "    # External set  \n",
      "    prediction_external = rf.predict(X_external)\n",
      "    \n",
      "    #print result from each seed    \n",
      "    R2_train.append(r2_score(Y_internal, prediction))\n",
      "    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))\n",
      "    Q2_CV.append(r2_score(Y_internal, cv))\n",
      "    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))\n",
      "    Q2_External.append(r2_score(Y_external, prediction_external))\n",
      "    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))\n",
      "    \n",
      "    #Feature Importance\n",
      "    Feature = hx[:]\n",
      "    feature_importance = rf.feature_importances_\n",
      "    importances = 100.0 * (feature_importance / feature_importance.max()) #index\n",
      "    \n",
      "    for i, fx in enumerate(Feature):\n",
      "        \n",
      "        hash(importances_dict[fx]).append(importances[i])\n",
      "       \n",
      "    \n",
      "    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict\n",
      "141/228:\n",
      "from copy import deepcopy\n",
      "\n",
      "def Y_scrambling(X_internal, X_external, Y_internal, Y_external):\n",
      "    # Do the Y-scrambling. Loop over the actual learning for 100 times.\n",
      "    for randomseedcounter in range(1,101):\n",
      "        y_train_scrambled = deepcopy(Y_internal)\n",
      "        X_train_scrambled = deepcopy(X_internal)\n",
      "        np.random.shuffle(y_train_scrambled)\n",
      "        np.random.shuffle(X_train_scrambled)\n",
      "\n",
      "        # training was done on \"scrambled\" data - prediction on test set\n",
      "        RF_scrambled         = RandomForestRegressor()\n",
      "        RF_scrambled         = RF_scrambled.fit(X_internal,y_train_scrambled)\n",
      "        y_predict_scrambled  = RF_scrambled.predict(X_external)\n",
      "    \n",
      "        acclist_predictionOnTest_scrambledtrain.append((RF_scrambled.score(X_external,Y_external))**2)\n",
      "    \n",
      "        # training was done on \"scrambled\" data - prediction on train set\n",
      "        y_predict_scrambled_predictTrain  = RF_scrambled.predict(X_internal)\n",
      "    \n",
      "        acclist_predictionOnTrain_scrambledtrain.append((RF_scrambled.score(X_internal,Y_internal))**2)\n",
      "        \n",
      "        r2 = pd.DataFrame(acclist_predictionOnTrain_scrambledtrain, columns=['R2'])\n",
      "        q2 = pd.DataFrame(acclist_predictionOnTest_scrambledtrain, columns=['Q2'])\n",
      "\n",
      "        #result = pd.concat([r2, q2], axis=1, join='inner').to_csv(f+\"_Y_scrambling.csv\", header=False, index=False)\n",
      "        \n",
      "        \n",
      "    return acclist_predictionOnTest_scrambledtrain,acclist_predictionOnTrain_scrambledtrain\n",
      "141/229:\n",
      "def mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, importances_dict):\n",
      "    R2_train_mean = np.mean(R2_train)\n",
      "    RMSE_train_mean = np.mean(RMSE_train)\n",
      "    Q2_CV_mean = np.mean(Q2_CV)\n",
      "    RMSE_CV_mean = np.mean(RMSE_CV)\n",
      "    Q2_External_mean = np.mean(Q2_External)\n",
      "    RMSE_External_mean = np.mean(RMSE_External)\n",
      "    importances_mean0 = {}\n",
      "    for fx in importances_dict:\n",
      "        importances_mean0[fx] = np.mean(importances_dict[fx])\n",
      "    importances_mean = sorted([(k,v) for k,v in importances_mean0.items()],\n",
      "                                    key=lambda x: x[1], reverse=True)\n",
      "    \n",
      "    #predictionOnTest_mean = np.mean(acclist_predictionOnTest_scrambledtrain)\n",
      "    #predictionOnTrain_mean = np.mean(acclist_predictionOnTrain_scrambledtrain)\n",
      "    \n",
      "    return R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \\\n",
      "            RMSE_External_mean, importances_mean\n",
      "141/230:\n",
      "def std(R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, importances_dict):\n",
      "    R2_train_std = np.std(R2_train)\n",
      "    RMSE_train_std = np.std(RMSE_train)\n",
      "    Q2_CV_std = np.std(Q2_CV)\n",
      "    RMSE_CV_std = np.std(RMSE_CV)\n",
      "    Q2_External_std = np.std(Q2_External)\n",
      "    RMSE_External_std = np.std(RMSE_External)\n",
      "    importances_std0 = {}\n",
      "    for fx in importances_dict:\n",
      "        importances_std0[fx] = np.std(importances_dict[fx])\n",
      "    importances_std = sorted([(k,v) for k,v in importances_std0.items()],\n",
      "                                    key=lambda x: x[1], reverse=True)\n",
      "    #predictionOnTest_std = np.std(acclist_predictionOnTest_scrambledtrain)\n",
      "    #predictionOnTrain_std = np.std(acclist_predictionOnTrain_scrambledtrain)\n",
      "    \n",
      "    return R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\\\n",
      "            RMSE_External_std, importances_std\n",
      "141/231:\n",
      "def print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \n",
      "           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, \n",
      "           RMSE_External_std, X_internal, X_external, X, f, des1, des2 ):\n",
      "    #outfile = open('all_output.csv', 'a')\n",
      "    name = [os.path.basename(f)]\n",
      "    print_out = name\n",
      "    print_out = [name.replace('.smi','') for name in print_out]\n",
      "    \n",
      "    print (outfile, '%s,%d,%d,%d,%d,%0.4f,%0.4f,%0.4f,%0.4f,%d,%0.4f,%0.4f,%0.4f,%0.4f,%d,%0.4f,%0.4f,%0.4f,%0.4f' \\\n",
      "                                     % (print_out, len(X_internal),\n",
      "                                        des1, des2, des4,\n",
      "                                        R2_train_mean, R2_train_std,\n",
      "                                        RMSE_train_mean, RMSE_train_std,\n",
      "                                        len(X_internal),\n",
      "                                        Q2_CV_mean, Q2_CV_std,\n",
      "                                        RMSE_CV_mean, RMSE_CV_std,\n",
      "                                        len(X_external),\n",
      "                                        Q2_External_mean, Q2_External_std,\n",
      "                                        RMSE_External_mean, RMSE_External_std,))\n",
      " \n",
      "    print ('\\nTraining set\\n------------')\n",
      "    print ('N: ' + (str(len(X_internal))))\n",
      "    print ('R2: %0.4f'%(R2_train_mean))\n",
      "    print ('std_R2: %0.4f'%(R2_train_std))\n",
      "    print ('RMSE: %0.4f'%(RMSE_train_mean))\n",
      "    print ('std_RMSE: %0.4f'%(RMSE_train_std))\n",
      "\n",
      "    print ('\\nCross-validation set\\n------------')\n",
      "    print ('N: ' + (str(len(X_internal))))\n",
      "    print ('Q2: %0.4f'%(Q2_CV_mean))\n",
      "    print ('std_Q2: %0.4f'%(Q2_CV_std))\n",
      "    print ('RMSE: %0.4f'%(RMSE_CV_mean))\n",
      "    print ('std_RMSE: %0.4f'%(RMSE_CV_std))\n",
      "\n",
      "    print ('\\nExternal set\\n------------')\n",
      "    print ('N: ' + (str(len(X_external))))\n",
      "    print ('Q2_EXt: %0.4f'%(Q2_External_mean))\n",
      "    print ('std_Q2_EXt: %0.4f'%(Q2_External_std))\n",
      "    print ('RMSE: %0.4f'%(RMSE_External_mean))\n",
      "    print ('std_RMSE: %0.4f'%(RMSE_External_std))\n",
      "141/232:\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.ticker import MultipleLocator\n",
      "import pylab as py \n",
      "import statsmodels.api as sm\n",
      "from statsmodels.stats.outliers_influence import summary_table\n",
      "\n",
      "def plot_model (f, X_internal, X_external, Y_internal, Y_external,\n",
      "                R2_train_mean, Q2_External_mean,\n",
      "                importances_mean, importances_std, Feature, prediction, \n",
      "                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain):\n",
      "    \n",
      "    # Prepare plot\n",
      "    m  = rf.fit(X_internal,Y_internal)\n",
      "    cm = plt.cm.RdBu\n",
      "    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)\n",
      "\n",
      "    fig_size = plt.rcParams[\"figure.figsize\"]\n",
      "    fig_size[0]=5\n",
      "    fig_size[1]=5\n",
      "\n",
      "    # Train Set\n",
      "    x_train = np.array(Y_internal)\n",
      "    y_train = m.predict(X_internal).flatten()\n",
      "    py.scatter(x_train, y_train, s=50, marker='.', alpha=0.3,\n",
      "            c='g', cmap=cm ,edgecolors='g')\n",
      "                 #label=r\"$R^{2}_{Tr}$ = %.4f\" % R2_train_mean)\n",
      "    \n",
      "    \n",
      "    # CV Set\n",
      "    np.array(cv)\n",
      "    x_test = np.array(Y_internal)\n",
      "    y_test = cv\n",
      "    py.scatter(x_test, y_test, s=50, marker='.', alpha=0.3,\n",
      "            c='b', cmap=cm ,edgecolors='b') \n",
      "                 #label=r\"$Q^{2}_{Ext}$ = %.4f\" % Q2_External_mean)\n",
      "        #2SD line\n",
      "    X = sm.add_constant(x_test)\n",
      "    res = sm.OLS(y_test, X).fit()\n",
      "\n",
      "    st, data, ss2 = summary_table(res, alpha=0.05)\n",
      "    fittedvalues = data[:,2]\n",
      "        #predict_mean_se  = data[:,3]\n",
      "    predict_mean_ci_low, predict_mean_ci_upp = data[:,4:6].T\n",
      "    predict_ci_low, predict_ci_upp = data[:,6:8].T\n",
      "        #2SD line\n",
      "    plt.plot(X, predict_ci_low, '--b', linewidth=0.5, alpha=0.5)\n",
      "    plt.plot(X, predict_ci_upp, '--b', linewidth=0.5, alpha=0.5)\n",
      "        \n",
      "    # External Set\n",
      "    x_test = np.array(Y_external)\n",
      "    y_test = m.predict(X_external).flatten()\n",
      "    py.scatter(x_test, y_test, s=50, marker='.', alpha=0.3,\n",
      "            c='r', cmap=cm ,edgecolors='r') \n",
      "                 #label=r\"$Q^{2}_{Ext}$ = %.4f\" % Q2_External_mean)\n",
      "    \n",
      "    \n",
      "    #py.plot(x_train, np.polyval(np.polyfit(x_train,y_train,1), x_train), '--r') #mean line\n",
      "    plt.legend(loc=2,prop={'size':6})\n",
      "    plt.xlabel(\"Experimental $pIC_{50}$ values\", fontsize=10)\n",
      "    plt.ylabel(\"Predicted $pIC_{50}$ values\", fontsize=10)\n",
      "        \n",
      "    min_axis = np.min(np.concatenate([Y_internal, prediction], axis=0))\n",
      "    max_axis = np.max(np.concatenate([Y_internal, prediction], axis=0))\n",
      "    plt.xlim([(min_axis*0.9),(max_axis*1.05)])\n",
      "    plt.ylim([(min_axis*0.9),(max_axis*1.05)])\n",
      "    plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "    \n",
      "    # Save plot to file\n",
      "    plt.savefig(f+'_compair_Fp.pdf', dpi=300)\n",
      "    plt.show()\n",
      "    \n",
      "    # Y-scrambling plot\n",
      "    py.scatter(Q2_CV_mean, R2_train_mean, s=100, marker='.', alpha=0.3,\n",
      "            c='b', cmap=cm ,edgecolors='b') \n",
      "            \n",
      "    py.scatter(acclist_predictionOnTest_scrambledtrain,acclist_predictionOnTrain_scrambledtrain, \\\n",
      "               s=100, marker='.', alpha=0.3, c='r', cmap=cm ,edgecolors='r')\n",
      "\n",
      "    plt.legend(loc=2,prop={'size':6})\n",
      "    plt.xlabel(\"$Q^{2}$\", fontsize=10)\n",
      "    plt.ylabel(\"$R^{2}$\", fontsize=10)\n",
      "    plt.tick_params(axis='both', which='major', labelsize=14)\n",
      "    \n",
      "    plt.ylim([0, 1])\n",
      "    plt.xlim([0, 1])\n",
      "    plt.axhline(0.5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "    plt.axvline(0.5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "    \n",
      "    plt.savefig(f+'_Y_scrambling.pdf', dpi=300)\n",
      "\n",
      "    plt.show()\n",
      "\n",
      "    #Feature Importance \n",
      "    fig_size = plt.rcParams[\"figure.figsize\"]\n",
      "    fig_size[0]=5\n",
      "    fig_size[1]=10\n",
      "\n",
      "    barlist = plt.barh(range(20), [x[1] for x in importances_mean[:20]], \n",
      "          color=\"g\", xerr=[x[1] for x in importances_std[:20]], align=\"center\", \\\n",
      "                       error_kw=dict(ecolor='gray', lw=2, capsize=5, capthick=2))\n",
      "\n",
      "    print (str(\"top10\"), [x[0] for x in importances_mean[:10]])\n",
      "    print ('')\n",
      "    print (str(\"top20\"), [x[0] for x in importances_mean[:20]])\n",
      "    print ('')\n",
      "    print (str(\"top30\"), [x[0] for x in importances_mean[:30]])\n",
      "    print ('')\n",
      "    print (str(\"top40\"), [x[0] for x in importances_mean[:40]])\n",
      "    print ('')\n",
      "    print (str(\"top50\"), [x[0] for x in importances_mean[:50]])\n",
      "    \n",
      "    plt.yticks(range(20), [x[0] for x in importances_mean[:20]])\n",
      "    plt.ylim([-1, 20])\n",
      "    \n",
      "    plt.xlabel(r\"$\\bf{Gini}$\" + \" \"+ r\"$\\bf{index}$\", fontsize=12)\n",
      "    ax = plt.gca()\n",
      "    ax.invert_yaxis()\n",
      "    plt.tight_layout(pad=2.0, w_pad=0.7, h_pad=2.0)\n",
      "    plt.savefig(f+'_Feature_importances.pdf', dpi=300)\n",
      "    plt.show()\n",
      "141/233:\n",
      "import glob # to read multiple files \n",
      "from rdkit import Chem, DataStructs\n",
      "from rdkit.Chem import AllChem\n",
      "import os\n",
      "\n",
      "%config InlineBackend.figure_format = 'retina'\n",
      "\n",
      "! rm Result/Result_LogKp.csv\n",
      "\n",
      "outfile = open('Result/Result_LogKp.csv', 'a')\n",
      "\n",
      "print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\\\n",
      "                    'R2_train,R2_train_std,' + \\\n",
      "                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \\\n",
      "                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')\n",
      "\n",
      "path = r'QSAR/'\n",
      "\n",
      "for f in glob.glob(path + '*.csv'):\n",
      "    df = pd.read_csv(f)\n",
      "    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))\n",
      "    df = df.fillna(method='ffill')\n",
      "    Y = df[\"LogKp\"].values.astype(np.float)\n",
      "    data = df\n",
      "    \n",
      "    print ('\\n\\n************************************************************************************')\n",
      "    print ('')\n",
      "    print (f)\n",
      "    print ('')\n",
      "    \n",
      "    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%\n",
      "    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7\n",
      "\n",
      "    h = data.columns.tolist()\n",
      "    hx = np.array(h)\n",
      "\n",
      "    data = data.values.astype(np.float)\n",
      "    X = np.array(data)\n",
      "    \n",
      "    # Prepare empty lists to plot QSAR model\n",
      "    R2_train = []\n",
      "    RMSE_train = []\n",
      "    Q2_CV = []\n",
      "    RMSE_CV = []\n",
      "    Q2_External = []\n",
      "    RMSE_External = []\n",
      "    importances_dict = defaultdict(list)\n",
      "    \n",
      "    # Prepare empty lists to plot the performance of accuracy.\n",
      "    acclist_realRF                          = []\n",
      "    acclist_realRF_predictTrain             = []\n",
      "    acclist_predictionOnTest_scrambledtrain  = []\n",
      "    acclist_predictionOnTrain_scrambledtrain = []\n",
      "        \n",
      "    for i in range(10):\n",
      "        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)\n",
      "            \n",
      "    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \\\n",
      "                                                                        X_internal, X_external, Y_internal, Y_external)\n",
      "    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \\\n",
      "        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\\\n",
      "        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \\\n",
      "                                 Q2_External, RMSE_External, importances_dict)\n",
      "    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \n",
      "           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, \n",
      "           RMSE_External_std, X_internal, X_external, X, f, des1, des2)\n",
      "    plot_model(f, X_internal, X_external, Y_internal, Y_external,\n",
      "                R2_train_mean, Q2_External_mean,\n",
      "                importances_mean, importances_std, Feature, prediction,\n",
      "                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)\n",
      "    \n",
      "outfile.close()\n",
      "    #END\n",
      "143/1:\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "\n",
      "def build_model(X, Y, seed, hx, f):\n",
      "    \n",
      "    \n",
      "    #Data split using 70/30 ratio\n",
      "    X_internal, X_external, Y_internal, Y_external = train_test_split(X,\n",
      "                                                        Y, test_size=0.3,\n",
      "                                                        random_state=seed)\n",
      "\n",
      "    # Training set\n",
      "    rf = RandomForestRegressor(n_estimators=400, \n",
      "                               max_features='sqrt', \n",
      "                               min_samples_leaf = 1,\n",
      "                               random_state = 13,\n",
      "                               n_jobs=-1)\n",
      "    \n",
      "    rf.fit(X_internal,Y_internal)\n",
      "    prediction = rf.predict(X_internal)\n",
      "    \n",
      "    # Cross-validation\n",
      "    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)\n",
      "    \n",
      "    # External set  \n",
      "    prediction_external = rf.predict(X_external)\n",
      "    \n",
      "    #print result from each seed    \n",
      "    R2_train.append(r2_score(Y_internal, prediction))\n",
      "    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))\n",
      "    Q2_CV.append(r2_score(Y_internal, cv))\n",
      "    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))\n",
      "    Q2_External.append(r2_score(Y_external, prediction_external))\n",
      "    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))\n",
      "    \n",
      "    #Feature Importance\n",
      "    Feature = hx[:]\n",
      "    feature_importance = rf.feature_importances_\n",
      "    importances = 100.0 * (feature_importance / feature_importance.max()) #index\n",
      "    \n",
      "    for i, fx in enumerate(Feature):\n",
      "        \n",
      "        hash(importances_dict[fx]).append(importances[i])\n",
      "       \n",
      "    \n",
      "    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \\\n",
      "           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict\n",
      "143/2:\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "143/3:\n",
      "from sklearn.linear_model import LinearRegression\n",
      "lr = LinearRegression()\n",
      "scores = cross_val_score(lr, X, y, cv=kf)\n",
      "print(scores)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "143/4:\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate, cross_val_score\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "143/5: df = pd.read_csv('QSAR/Kp_Fingerprinter.csv')\n",
      "143/6:\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate, cross_val_score\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn import svm\n",
      "143/7: df = pd.read_csv('QSAR/Kp_Fingerprinter.csv')\n",
      "143/8: df = pd.read_csv(r'QSAR/Kp_Fingerprinter.csv')\n",
      "143/9:\n",
      "df = pd.read_csv(r'QSAR/Kp_Fingerprinter.csv')\n",
      "df\n",
      "143/10:\n",
      "df = pd.read_csv(r'QSAR/Kp_Fingerprinter.csv')\n",
      "df\n",
      "143/11:\n",
      "df = pd.read_csv(r'QSAR\\Kp_Fingerprinter.csv')\n",
      "df\n",
      "143/12: pwd\n",
      "146/1:\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate, cross_val_score\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn import svm\n",
      "146/2: pwd\n",
      "146/3:\n",
      "df = pd.read_csv(r'QSAR/Kp_Fingerprinter.csv')\n",
      "df\n",
      "146/4: X = df.drop('LogKp')\n",
      "146/5: X = df.drop('LogKp', axis=1)\n",
      "146/6: X\n",
      "146/7: y = df('LogKp', axis=1)\n",
      "146/8: y = df('LogKp')\n",
      "146/9: y = df[]'LogKp']\n",
      "146/10: y = df['LogKp']\n",
      "146/11: y\n",
      "146/12: y.to_frame\n",
      "146/13: y.to_frame()\n",
      "146/14: y\n",
      "146/15: y = y.to_frame()\n",
      "146/16: y\n",
      "146/17:\n",
      "from sklearn.linear_model import LinearRegression\n",
      "lr = LinearRegression()\n",
      "scores = cross_val_score(lr, X, y, cv=kf)\n",
      "print(scores)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "146/18:\n",
      "from sklearn.linear_model import LinearRegression\n",
      "lr = LinearRegression()\n",
      "scores = cross_val_score(lr, X, y, cv=10)\n",
      "print(scores)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "146/19:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "pls2 = PLSRegression(n_components=2)\n",
      "scores = cross_val_score(pls2, X, y, cv=kf)\n",
      "scores\n",
      "146/20:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "pls2 = PLSRegression(n_components=2)\n",
      "scores = cross_val_score(pls2, X, y, cv=10)\n",
      "scores\n",
      "146/21:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "model = RandomForestRegressor(n_estimators=200)\n",
      "scores = cross_val_score(model, X, y, cv=10)\n",
      "scores\n",
      "146/22:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "model = RandomForestRegressor(n_estimators=150)\n",
      "scores = cross_val_score(model, X, y, cv=10)\n",
      "scores\n",
      "152/1:\n",
      "import numpy as np\n",
      "\n",
      "def Remove_useless_descriptor(df, threshold):\n",
      "    \n",
      "    des1 = len(df.columns) \n",
      "    \n",
      "    h = df.columns.tolist()\n",
      "    df = df.values.astype(np.float)\n",
      "    df = np.array(df)\n",
      "\n",
      "    STDEV = np.std(df, axis=0)\n",
      "    idx = [idx for idx, val in enumerate(STDEV) if val > threshold]\n",
      "    df2 = df[:,idx]\n",
      "    hx = np.array(h)[idx]\n",
      "    \n",
      "    df = pd.DataFrame(df2, columns=[hx])\n",
      "    \n",
      "    des2 = len(df.columns)\n",
      "    \n",
      "    print('from Remove useless descriptor')\n",
      "    print(\"The initial set of \" + str(des1) + \\\n",
      "          \" descriptors has been reduced to \" + str(des2) + \" descriptors.\")\n",
      "    \n",
      "    return df, des1, des2\n",
      "155/1:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split\n",
      "155/2:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "155/3: pwd\n",
      "155/4:\n",
      "df = pd.read_csv(\"df6_lipinski.csv\")\n",
      "df\n",
      "155/5: df1 = df.drop(['Unnamed: 0', 'smiles'], axis=True)\n",
      "155/6: df1\n",
      "155/7: df1 = df.drop(['Unnamed: 0', 'smiles', 'Cells'], axis=True)\n",
      "155/8:\n",
      "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict\n",
      "kf =KFold(n_splits=10, shuffle=False)\n",
      "155/9:\n",
      "cnt = 1\n",
      "# split()  method generate indices to split data into training and test set.\n",
      "for train_index, test_index in kf.split(X, y):\n",
      "    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n",
      "    cnt += 1\n",
      "155/10: df1\n",
      "155/11: df1 = df.drop(['Unnamed: 0', 'smiles', 'Cells', 'compound'], axis=True)\n",
      "155/12: df1\n",
      "155/13: X = df1.drop['LogKp']\n",
      "155/14: X = df1.drop[LogKp]\n",
      "155/15: X = df1.drop(['LogKp'])\n",
      "155/16: X = df1.drop(['LogKp'], axis=1)\n",
      "155/17:\n",
      "X = df1.drop(['LogKp'], axis=1)\n",
      "X\n",
      "155/18: y = df1['LogKp']\n",
      "155/19:\n",
      "y = df1['LogKp']\n",
      "y\n",
      "155/20:\n",
      "y = df1['LogKp']\n",
      "y.to_frame()\n",
      "155/21:\n",
      "y = df1['LogKp']\n",
      "y.to_frame()\n",
      "t\n",
      "155/22:\n",
      "y = df1['LogKp']\n",
      "y.to_frame()\n",
      "y\n",
      "155/23:\n",
      "y = df1['LogKp']\n",
      "y = y.to_frame()\n",
      "y\n",
      "155/24:\n",
      "from sklearn.model_to_frametion import KFold, cross_val_score, cross_val_predict\n",
      "kf =KFold(n_splits=10, shuffle=False)\n",
      "155/25:\n",
      "from sklearn.model_to_selection import KFold, cross_val_score, cross_val_predict\n",
      "kf =KFold(n_splits=10, shuffle=False)\n",
      "155/26:\n",
      "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict\n",
      "kf =KFold(n_splits=10, shuffle=False)\n",
      "155/27:\n",
      "cnt = 1\n",
      "# split()  method generate indices to split data into training and test set.\n",
      "for train_index, test_index in kf.split(X, y):\n",
      "    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n",
      "    cnt += 1\n",
      "155/28:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "RF = RandomForestRegressor()\n",
      "155/29:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "RF = RandomForestRegressor(n_estimators=150)\n",
      "155/30:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "RF = RandomForestRegressor(n_estimators=150)\n",
      "y_pred = cross_val_predict(RF, X, y, cv=kf)\n",
      "155/31: y.shape\n",
      "155/32:\n",
      "y = df1['LogKp']\n",
      "y\n",
      "155/33: y.shape\n",
      "155/34:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "RF = RandomForestRegressor(n_estimators=150)\n",
      "y_pred = cross_val_predict(RF, X, y, cv=kf)\n",
      "155/35:\n",
      "from sklearn.metrics import r2_score\n",
      "r2_score(y, y_pred)\n",
      "155/36: plt.scatter(y, y_pred)\n",
      "155/37:\n",
      "X = df1.drop(['LogKp'], axis=1)\n",
      "X.shape\n",
      "155/38:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "RF = RandomForestRegressor(n_estimators=400)\n",
      "y_pred = cross_val_predict(RF, X, y, cv=kf)\n",
      "155/39:\n",
      "from sklearn.metrics import r2_score\n",
      "#Rsqure\n",
      "r2_score(y, y_pred)\n",
      "155/40: plt.scatter(y, y_pred)\n",
      "155/41:\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE=np.sqrt(mean_absolute_error(y, y_pred))\n",
      "155/42: RMSE\n",
      "155/43:\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE=np.sqrt(mean_squared_error(y, y_pred))\n",
      "155/44: RMSE\n",
      "155/45:\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE=mean_squared_error(y, y_pred)\n",
      "155/46: RMSE\n",
      "155/47:\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE = mean_squared_error(y, y_pred)\n",
      "155/48: RMSE\n",
      "155/49:\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(y, y_pred)\n",
      "155/50: RMSE_pred\n",
      "157/1:\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_RF = mean_squared_error(y, y_pred)\n",
      "157/2:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "157/3: pwd\n",
      "157/4:\n",
      "df = pd.read_csv(\"df6_lipinski.csv\")\n",
      "df\n",
      "157/5: df1 = df.drop(['Unnamed: 0', 'smiles', 'Cells', 'compound'], axis=True)\n",
      "157/6: df1\n",
      "157/7:\n",
      "X = df1.drop(['LogKp'], axis=1)\n",
      "X.shape\n",
      "157/8:\n",
      "y = df1['LogKp']\n",
      "y\n",
      "157/9: y.shape\n",
      "157/10:\n",
      "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict\n",
      "kf =KFold(n_splits=10, shuffle=False)\n",
      "157/11:\n",
      "cnt = 1\n",
      "# split()  method generate indices to split data into training and test set.\n",
      "for train_index, test_index in kf.split(X, y):\n",
      "    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n",
      "    cnt += 1\n",
      "157/12:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "RF = RandomForestRegressor(n_estimators=400)\n",
      "y_pred = cross_val_predict(RF, X, y, cv=kf)\n",
      "157/13:\n",
      "from sklearn.metrics import r2_score\n",
      "#Rsqure\n",
      "r2_score(y, y_pred)\n",
      "157/14: plt.scatter(y, y_pred)\n",
      "157/15:\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_RF = mean_squared_error(y, y_pred)\n",
      "157/16: RMSE_RF\n",
      "157/17: from sklearn.svm import svm\n",
      "157/18:     from sklearn.svm import SVM\n",
      "157/19: from sklearn.svm import SVM\n",
      "157/20:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)\n",
      "    Fp_normalized\n",
      "    \n",
      "    return Fp_normalized\n",
      "157/21: X = normalized(X)\n",
      "157/22:\n",
      "X = normalized(X)\n",
      "X\n",
      "157/23:\n",
      "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict\n",
      "kf =KFold(n_splits=10, shuffle=False)\n",
      "157/24:\n",
      "cnt = 1\n",
      "# split()  method generate indices to split data into training and test set.\n",
      "for train_index, test_index in kf.split(X, y):\n",
      "    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n",
      "    cnt += 1\n",
      "157/25:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "RF = RandomForestRegressor(n_estimators=400)\n",
      "y_pred = cross_val_predict(RF, X, y, cv=kf)\n",
      "157/26:\n",
      "from sklearn.metrics import r2_score\n",
      "#Rsqure\n",
      "r2_score(y, y_pred)\n",
      "157/27: plt.scatter(y, y_pred)\n",
      "157/28:\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_RF = mean_squared_error(y, y_pred)\n",
      "157/29: RMSE_RF\n",
      "157/30: from sklearn.svm import SVC\n",
      "157/31: from sklearn.svm import SVR\n",
      "157/32:\n",
      "from sklearn.svm import SVR\n",
      "SVR = sklearn.svm.SVR()\n",
      "157/33:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "157/34: y_pred = cross_val_predict(SVR, X, y, cv=kf)\n",
      "157/35: r2_score(y, y_pred)\n",
      "157/36: plt.scatter(y, y_pred)\n",
      "157/37:\n",
      "X = df1.drop(['LogKp'], axis=1)\n",
      "X.shape\n",
      "157/38:\n",
      "y = df1['LogKp']\n",
      "y\n",
      "157/39: y.shape\n",
      "157/40:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)\n",
      "    Fp_normalized\n",
      "    \n",
      "    return Fp_normalized\n",
      "157/41: X\n",
      "157/42:\n",
      "Xn = normalized(X)\n",
      "Xn\n",
      "157/43: y_pred = cross_val_predict(SVR, X, y, cv=kf)\n",
      "157/44: r2_score(y, y_pred)\n",
      "157/45: plt.scatter(y, y_pred)\n",
      "157/46: X\n",
      "157/47:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "pls2 = PLSRegression(n_components=10)\n",
      "157/48:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "pls = PLSRegression(n_components=10)\n",
      "157/49: y_pred = cross_val_predict(pls, X, y, cv=kf)\n",
      "157/50: r2_score(y, y_pred)\n",
      "157/51: plt.scatter(y, y_pred)\n",
      "157/52:\n",
      "#only LogP\n",
      "X = df1(['LogP'], axis=1)\n",
      "157/53:\n",
      "#only LogP\n",
      "X = df1['LogP']\n",
      "157/54:\n",
      "#only LogP\n",
      "XlogP = df1['LogP']\n",
      "157/55:\n",
      "X = df1.drop(['LogKp'], axis=1)\n",
      "X.shape\n",
      "157/56:\n",
      "RF = RandomForestRegressor(n_estimators=400)\n",
      "y_pred = cross_val_predict(RF, XlogP, y, cv=kf)\n",
      "r2_score = score(y, y_pred)\n",
      "157/57:\n",
      "#only LogP\n",
      "XlogP = df1['LogP']\n",
      "XlogP\n",
      "157/58:\n",
      "#only LogP\n",
      "XlogP = df1['LogP']\n",
      "XlogP = XlogP.to_frame\n",
      "157/59:\n",
      "RF = RandomForestRegressor(n_estimators=400)\n",
      "y_pred = cross_val_predict(RF, XlogP, y, cv=kf)\n",
      "r2_score = score(y, y_pred)\n",
      "157/60:\n",
      "#only LogP\n",
      "XlogP = df1['LogP']\n",
      "XlogP = XlogP.to_frame\n",
      "XlogP\n",
      "157/61:\n",
      "#only LogP\n",
      "XlogP = df1['LogP']\n",
      "XlogP = XlogP.to_frame()\n",
      "XlogP\n",
      "157/62:\n",
      "RF = RandomForestRegressor(n_estimators=400)\n",
      "y_pred = cross_val_predict(RF, XlogP, y, cv=kf)\n",
      "r2_score = score(y, y_pred)\n",
      "157/63:\n",
      "RF = RandomForestRegressor(n_estimators=400)\n",
      "y_pred = cross_val_predict(RF, XlogP, y, cv=kf)\n",
      "r2_score = score(y, y_pred)\n",
      "r2_score\n",
      "157/64:\n",
      "RF = RandomForestRegressor(n_estimators=400)\n",
      "y_pred = cross_val_predict(RF, XlogP, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "r2_score\n",
      "157/65:\n",
      "RF = RandomForestRegressor(n_estimators=400)\n",
      "y_pred = cross_val_predict(RF, XlogP, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "157/66: plt.scatter(y, y_pred)\n",
      "155/51:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "155/52:\n",
      "df = pd.read_csv('QSAR/Kp_AtomPairs2DFingerprinter.csv')\n",
      "df\n",
      "155/53:\n",
      "X = df1.drop(['LogKp'], axis=1)\n",
      "X.shape\n",
      "155/54:\n",
      "y = df1['LogKp']\n",
      "y\n",
      "155/55: y.shape\n",
      "155/56:\n",
      "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict\n",
      "kf =KFold(n_splits=10, shuffle=False)\n",
      "155/57:\n",
      "cnt = 1\n",
      "# split()  method generate indices to split data into training and test set.\n",
      "for train_index, test_index in kf.split(X, y):\n",
      "    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n",
      "    cnt += 1\n",
      "155/58:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "RF = RandomForestRegressor(n_estimators=400)\n",
      "y_pred = cross_val_predict(RF, X, y, cv=kf)\n",
      "155/59:\n",
      "from sklearn.metrics import r2_score\n",
      "#Rsqure\n",
      "r2_score(y, y_pred)\n",
      "155/60: plt.scatter(y, y_pred)\n",
      "155/61:\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(y, y_pred)\n",
      "155/62: RMSE_pred\n",
      "155/63: df1 = df.drop(['Unnamed: 0', 'smiles', 'Cells', 'compound'], axis=True)\n",
      "155/64:\n",
      "df = pd.read_csv('QSAR/Kp_AtomPairs2DFingerprinter.csv')\n",
      "df\n",
      "155/65:\n",
      "X = df.drop(['LogKp'], axis=1)\n",
      "X.shape\n",
      "155/66:\n",
      "y = df['LogKp']\n",
      "y\n",
      "155/67: y.shape\n",
      "155/68:\n",
      "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict\n",
      "kf =KFold(n_splits=10, shuffle=False)\n",
      "155/69:\n",
      "cnt = 1\n",
      "# split()  method generate indices to split data into training and test set.\n",
      "for train_index, test_index in kf.split(X, y):\n",
      "    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n",
      "    cnt += 1\n",
      "155/70:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "RF = RandomForestRegressor(n_estimators=400)\n",
      "y_pred = cross_val_predict(RF, X, y, cv=kf)\n",
      "155/71:\n",
      "from sklearn.metrics import r2_score\n",
      "#Rsqure\n",
      "r2_score(y, y_pred)\n",
      "155/72: plt.scatter(y, y_pred)\n",
      "155/73:\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(y, y_pred)\n",
      "155/74: RMSE_pred\n",
      "157/67:\n",
      "RF = RandomForestRegressor(n_estimators=400)\n",
      "y_pred = cross_val_predict(RF, XlogP, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "print (plt.scatter(y, y_pred))\n",
      "157/68:\n",
      "RF = RandomForestRegressor(n_estimators=400)\n",
      "y_pred = cross_val_predict(RF, XlogP, y, cv=kf)\n",
      "RMSE_RF = mean_squared_error(y, y_pred)\n",
      "print (RMSE_RF)\n",
      "print (r2_score(y, y_pred)\n",
      "print (plt.scatter(y, y_pred))\n",
      "157/69:\n",
      "RF = RandomForestRegressor(n_estimators=400)\n",
      "y_pred = cross_val_predict(RF, XlogP, y, cv=kf)\n",
      "RMSE_RF = mean_squared_error(y, y_pred)\n",
      "print (RMSE_RF)\n",
      "print (r2_score(y, y_pred)\n",
      "print (plt.scatter(y, y_pred))\n",
      "157/70:\n",
      "RF = RandomForestRegressor(n_estimators=400)\n",
      "y_pred = cross_val_predict(RF, XlogP, y, cv=kf)\n",
      "RMSE_RF = mean_squared_error(y, y_pred)\n",
      "print (RMSE_RF)\n",
      "print (r2_score(y, y_pred)\n",
      "    print (plt.scatter(y, y_pred))\n",
      "157/71:\n",
      "RF = RandomForestRegressor(n_estimators=400)\n",
      "y_pred = cross_val_predict(RF, XlogP, y, cv=kf)\n",
      "RMSE_RF = mean_squared_error(y, y_pred)\n",
      "    print (RMSE_RF)\n",
      "    print (r2_score(y, y_pred)\n",
      "    print (plt.scatter(y, y_pred))\n",
      "157/72:\n",
      "RF = RandomForestRegressor(n_estimators=400)\n",
      "y_pred = cross_val_predict(RF, XlogP, y, cv=kf)\n",
      "RMSE_RF = mean_squared_error(y, y_pred)\n",
      "\n",
      "print (RMSE_RF)\n",
      "print (r2_score(y, y_pred)\n",
      "print (plt.scatter(y, y_pred))\n",
      "157/73:\n",
      "RF = RandomForestRegressor(n_estimators=400)\n",
      "y_pred = cross_val_predict(RF, XlogP, y, cv=kf)\n",
      "RMSE_RF = mean_squared_error(y, y_pred)\n",
      "\n",
      "print (RMSE_RF)\n",
      "print (r2_score(y, y_pred)\n",
      "       \n",
      "print (plt.scatter(y, y_pred))\n",
      "157/74:\n",
      "RF = RandomForestRegressor(n_estimators=400)\n",
      "y_pred = cross_val_predict(RF, XlogP, y, cv=kf)\n",
      "RMSE_RF = mean_squared_error(y, y_pred)\n",
      "\n",
      "print (RMSE_RF)\n",
      "print (r2_score(y, y_pred)\n",
      "157/75:\n",
      "RF = RandomForestRegressor(n_estimators=400)\n",
      "y_pred = cross_val_predict(RF, XlogP, y, cv=kf)\n",
      "RMSE_RF = mean_squared_error(y, y_pred)\n",
      "\n",
      "    print (plt.scatter(y, y_pred))\n",
      "157/76:\n",
      "RF = RandomForestRegressor(n_estimators=400)\n",
      "y_pred = cross_val_predict(RF, XlogP, y, cv=kf)\n",
      "RMSE_RF = mean_squared_error(y, y_pred)\n",
      "\n",
      "print (plt.scatter(y, y_pred))\n",
      "157/77:\n",
      "RF = RandomForestRegressor(n_estimators=400)\n",
      "y_pred = cross_val_predict(RF, XlogP, y, cv=kf)\n",
      "RMSE_RF = mean_squared_error(y, y_pred)\n",
      "r2_score = r2_score(y, y_pred)\n",
      "print (plt.scatter(y, y_pred), RMSE_RF, r2_score)\n",
      "157/78:\n",
      "RF = RandomForestRegressor(n_estimators=400)\n",
      "y_pred = cross_val_predict(RF, XlogP, y, cv=kf)\n",
      "RMSE_RF = mean_squared_error(y, y_pred)\n",
      "r2_score = r2_score(y, y_pred)\n",
      "print (plt.scatter(y, y_pred) \\\n",
      "       , RMSE_RF \\\n",
      "       , r2_score)\n",
      "157/79:\n",
      "RF = RandomForestRegressor(n_estimators=400)\n",
      "y_pred = cross_val_predict(RF, XlogP, y, cv=kf)\n",
      "RMSE_RF = mean_squared_error(y, y_pred)\n",
      "plt.scatter(y, y_pred)\n",
      "RMSE_RF\n",
      "157/80:\n",
      "RF = RandomForestRegressor(n_estimators=400)\n",
      "y_pred = cross_val_predict(RF, XlogP, y, cv=kf)\n",
      "RMSE_RF = mean_squared_error(y, y_pred)\n",
      "r2_score = r2_score(y, y_pred)\n",
      "plt.scatter(y, y_pred)\n",
      "print('%4f'.RMSE_RF, 'RMSE', '%4f'.r2_score)\n",
      "157/81:\n",
      "RF = RandomForestRegressor(n_estimators=400)\n",
      "y_pred = cross_val_predict(RF, XlogP, y, cv=kf)\n",
      "RMSE_RF = mean_squared_error(y, y_pred)\n",
      "r2_score(y, y_pred)\n",
      "plt.scatter(y, y_pred)\n",
      "print('%4f'.RMSE_RF, 'RMSE', '%4f'.r2_score)\n",
      "157/82:\n",
      "RF = RandomForestRegressor(n_estimators=400)\n",
      "y_pred = cross_val_predict(RF, XlogP, y, cv=kf)\n",
      "RMSE_RF = mean_squared_error(y, y_pred)\n",
      "plt.scatter(y, y_pred)\n",
      "print('%4f'.RMSE_RF, 'RMSE', '%4f'.r2_score)\n",
      "157/83:\n",
      "RF = RandomForestRegressor(n_estimators=400)\n",
      "y_pred = cross_val_predict(RF, XlogP, y, cv=kf)\n",
      "RMSE_RF = mean_squared_error(y, y_pred)\n",
      "plt.scatter(y, y_pred)\n",
      "print('{4f}'.RMSE_RF, 'RMSE', '{%4f}'.r2_score)\n",
      "157/84: r2_score(y, y_pred)\n",
      "155/75:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "pls = PLSRegression(n_components=10)\n",
      "y_pred = cross_val_predict(pls, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "155/76: plt.scatter(y, y_pred)\n",
      "155/77:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "y_pred = cross_val_predict(SVR, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "155/78: plt.scatter(y, y_pred)\n",
      "155/79:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='poly')\n",
      "y_pred = cross_val_predict(SVR, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "155/80:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='linear')\n",
      "y_pred = cross_val_predict(SVR, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "155/81: plt.scatter(y, y_pred)\n",
      "155/82:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='sigmoid')\n",
      "y_pred = cross_val_predict(SVR, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "155/83: plt.scatter(y, y_pred)\n",
      "155/84:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "RF = RandomForestRegressor(n_estimators=600)\n",
      "y_pred = cross_val_predict(RF, X, y, cv=kf)\n",
      "155/85:\n",
      "from sklearn.metrics import r2_score\n",
      "#Rsqure\n",
      "r2_score(y, y_pred)\n",
      "155/86: plt.scatter(y, y_pred)\n",
      "157/85:\n",
      "RF = RandomForestRegressor(n_estimators=400)\n",
      "y_pred = cross_val_predict(RF, XlogP, y, cv=kf)\n",
      "RMSE_RF = mean_squared_error(y, y_pred)\n",
      "r2_score(y, y_pred)\n",
      "158/1:\n",
      "df = pd.read_csv('QSAR/Kp_AtomPairs2DFingerprintCount.csv')\n",
      "df\n",
      "158/2:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "158/3: pwd\n",
      "158/4:\n",
      "df = pd.read_csv('QSAR/Kp_AtomPairs2DFingerprintCount.csv')\n",
      "df\n",
      "158/5:\n",
      "X = df.drop(['LogKp'], axis=1)\n",
      "X.shape\n",
      "158/6:\n",
      "y = df['LogKp']\n",
      "y\n",
      "158/7: y.shape\n",
      "158/8:\n",
      "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict\n",
      "kf =KFold(n_splits=10, shuffle=False)\n",
      "158/9:\n",
      "cnt = 1\n",
      "# split()  method generate indices to split data into training and test set.\n",
      "for train_index, test_index in kf.split(X, y):\n",
      "    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n",
      "    cnt += 1\n",
      "158/10:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "RF = RandomForestRegressor(n_estimators=600)\n",
      "y_pred = cross_val_predict(RF, X, y, cv=kf)\n",
      "158/11:\n",
      "from sklearn.metrics import r2_score\n",
      "#Rsqure\n",
      "r2_score(y, y_pred)\n",
      "158/12: plt.scatter(y, y_pred)\n",
      "158/13:\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(y, y_pred)\n",
      "158/14: RMSE_pred\n",
      "158/15:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "pls = PLSRegression(n_components=10)\n",
      "y_pred = cross_val_predict(pls, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "158/16: plt.scatter(y, y_pred)\n",
      "158/17:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "y_pred = cross_val_predict(SVR, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "158/18: plt.scatter(y, y_pred)\n",
      "158/19:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='sigmoid')\n",
      "y_pred = cross_val_predict(SVR, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "158/20: plt.scatter(y, y_pred)\n",
      "159/1:\n",
      "df = pd.read_csv('QSAR/Kp_MACCSFingerprinter.csv')\n",
      "df\n",
      "159/2:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "159/3: pwd\n",
      "159/4:\n",
      "df = pd.read_csv('QSAR/Kp_MACCSFingerprinter.csv')\n",
      "df\n",
      "159/5:\n",
      "X = df.drop(['LogKp'], axis=1)\n",
      "X.shape\n",
      "159/6:\n",
      "y = df['LogKp']\n",
      "y\n",
      "159/7: y.shape\n",
      "159/8:\n",
      "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict\n",
      "kf =KFold(n_splits=10, shuffle=False)\n",
      "159/9:\n",
      "cnt = 1\n",
      "# split()  method generate indices to split data into training and test set.\n",
      "for train_index, test_index in kf.split(X, y):\n",
      "    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n",
      "    cnt += 1\n",
      "159/10:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "RF = RandomForestRegressor(n_estimators=600)\n",
      "y_pred = cross_val_predict(RF, X, y, cv=kf)\n",
      "159/11:\n",
      "from sklearn.metrics import r2_score\n",
      "#Rsqure\n",
      "r2_score(y, y_pred)\n",
      "159/12: plt.scatter(y, y_pred)\n",
      "159/13:\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(y, y_pred)\n",
      "159/14: RMSE_pred\n",
      "159/15:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "pls = PLSRegression(n_components=10)\n",
      "y_pred = cross_val_predict(pls, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "159/16: plt.scatter(y, y_pred)\n",
      "159/17:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "y_pred = cross_val_predict(SVR, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "159/18: plt.scatter(y, y_pred)\n",
      "159/19:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='sigmoid')\n",
      "y_pred = cross_val_predict(SVR, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "159/20: plt.scatter(y, y_pred)\n",
      "159/21:\n",
      "from sklearn.feature_selection import SelectKBest\n",
      "from sklearn.feature_selection import chi2\n",
      "X_new = SelectKBest(chi2, k=2).fit_transform(X, y)\n",
      "159/22:\n",
      "from sklearn.feature_selection import SelectKBest\n",
      "from sklearn.feature_selection import chi2\n",
      "X_new = SelectKBest(chi2, k=).fit_transform(X, y)\n",
      "159/23:\n",
      "from sklearn.feature_selection import SelectKBest\n",
      "from sklearn.feature_selection import chi2\n",
      "X_new = SelectKBest(chi2, k=10).fit_transform(X, y)\n",
      "159/24:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "159/25: pwd\n",
      "159/26:\n",
      "df = pd.read_csv('QSAR/Kp_MACCSFingerprinter.csv')\n",
      "df\n",
      "159/27:\n",
      "X = df.drop(['LogKp'], axis=1)\n",
      "X.shape\n",
      "159/28:\n",
      "y = df['LogKp']\n",
      "y\n",
      "159/29: y.shape\n",
      "159/30:\n",
      "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict\n",
      "kf =KFold(n_splits=10, shuffle=False)\n",
      "159/31:\n",
      "cnt = 1\n",
      "# split()  method generate indices to split data into training and test set.\n",
      "for train_index, test_index in kf.split(X, y):\n",
      "    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n",
      "    cnt += 1\n",
      "159/32:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "RF = RandomForestRegressor(n_estimators=600)\n",
      "y_pred = cross_val_predict(RF, X, y, cv=kf)\n",
      "159/33:\n",
      "from sklearn.metrics import r2_score\n",
      "#Rsqure\n",
      "r2_score(y, y_pred)\n",
      "159/34: plt.scatter(y, y_pred)\n",
      "159/35:\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(y, y_pred)\n",
      "159/36: RMSE_pred\n",
      "159/37:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "pls = PLSRegression(n_components=10)\n",
      "y_pred = cross_val_predict(pls, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "159/38: plt.scatter(y, y_pred)\n",
      "159/39:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "y_pred = cross_val_predict(SVR, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "159/40: plt.scatter(y, y_pred)\n",
      "159/41:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='sigmoid')\n",
      "y_pred = cross_val_predict(SVR, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "159/42: plt.scatter(y, y_pred)\n",
      "159/43:\n",
      "from sklearn.feature_selection import SelectKBest\n",
      "from sklearn.feature_selection import chi2\n",
      "X_new = SelectKBest(chi2, k=10).fit_transform(X, y)\n",
      "159/44:\n",
      "from sklearn.feature_selection import SelectKBest\n",
      "from sklearn.feature_selection import chi2\n",
      "X = X.astype(float)\n",
      "y = y.astype(float)\n",
      "X_new = SelectKBest(chi2, k=10).fit_transform(X, y)\n",
      "159/45:\n",
      "from sklearn.feature_selection import SelectKBest\n",
      "from sklearn.feature_selection import chi2\n",
      "X = X.astype(float)\n",
      "y = y.astype(float)\n",
      "SelectKBest(chi2, k=10).fit_transform(X, y)\n",
      "159/46:\n",
      "from sklearn.feature_selection import SelectKBest\n",
      "from sklearn.feature_selection import chi2\n",
      "\n",
      "SelectKBest(chi2, k=10).fit_transform(X, y)\n",
      "159/47:\n",
      "from sklearn.feature_selection import SelectKBest\n",
      "from sklearn.feature_selection import chi2\n",
      "159/48: SelectKBest(chi2)\n",
      "159/49: SelectKBest(chi2, X, y)\n",
      "159/50: SelectKBest(chi2).fit_transform(X, y)\n",
      "159/51: SelectKBest(chi2).fit_transform(X)\n",
      "159/52: SelectKBest(chi2).fit_transform(X, y)\n",
      "159/53: X.sh\n",
      "159/54: X.shape\n",
      "159/55: y.shape\n",
      "159/56: SelectKBest(ch2).fit_transform(X, y)\n",
      "159/57:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
      "159/58:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
      "sel.fit_transform(X)\n",
      "159/59:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
      "des2 = sel.fit_transform(X)\n",
      "159/60: des2\n",
      "159/61: des2.to_frame()\n",
      "159/62:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
      "X = sel.fit_transform(X)\n",
      "159/63: X\n",
      "159/64:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='sigmoid')\n",
      "y_pred = cross_val_predict(SVR, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "159/65: plt.scatter(y, y_pred)\n",
      "159/66: X.shape\n",
      "159/67: plt.scatter(y, y_pred)\n",
      "159/68:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='sigmoid')\n",
      "y_pred = cross_val_predict(RF, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "159/69: plt.scatter(y, y_pred)\n",
      "159/70:\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3\n",
      "                                                    random_state=29)\n",
      "159/71:\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
      "                                                    random_state=29)\n",
      "159/72:\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
      "                                                    random_state=10)\n",
      "159/73: pls.fit_transform(X_train, y_train)\n",
      "159/74:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "coefs = pd.DataFrame(\n",
      "   RF.coef_,\n",
      "   columns=['Coefficients'], index=X_train.columns\n",
      ")\n",
      "\n",
      "coefs.plot(kind='barh', figsize=(9, 7))\n",
      "plt.title('Ridge model')\n",
      "plt.axvline(x=0, color='.5')\n",
      "plt.subplots_adjust(left=.3)\n",
      "159/75:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "coefs = pd.DataFrame(\n",
      "   RF.feature_importances_,\n",
      "   columns=['Coefficients'], index=X_train.columns\n",
      ")\n",
      "\n",
      "coefs.plot(kind='barh', figsize=(9, 7))\n",
      "plt.title('Ridge model')\n",
      "plt.axvline(x=0, color='.5')\n",
      "plt.subplots_adjust(left=.3)\n",
      "159/76:\n",
      "import matplotlib.pyplot as plt\n",
      "X_internal, X_external, Y_internal, Y_external = train_test_split(X,\n",
      "                                                        Y, test_size=0.3,\n",
      "                                                        random_state=seed)\n",
      "\n",
      "coefs = pd.DataFrame(\n",
      "   RF.feature_importances_,\n",
      "   columns=['Coefficients'], index=X_train.columns\n",
      ")\n",
      "\n",
      "coefs.plot(kind='barh', figsize=(9, 7))\n",
      "plt.title('Ridge model')\n",
      "plt.axvline(x=0, color='.5')\n",
      "plt.subplots_adjust(left=.3)\n",
      "159/77:\n",
      "import matplotlib.pyplot as plt\n",
      "X_internal, X_external, Y_internal, Y_external = train_test_split(X,\n",
      "                                                        y, test_size=0.3,\n",
      "                                                        random_state=seed)\n",
      "\n",
      "coefs = pd.DataFrame(\n",
      "   RF.feature_importances_,\n",
      "   columns=['Coefficients'], index=X_train.columns\n",
      ")\n",
      "\n",
      "coefs.plot(kind='barh', figsize=(9, 7))\n",
      "plt.title('Ridge model')\n",
      "plt.axvline(x=0, color='.5')\n",
      "plt.subplots_adjust(left=.3)\n",
      "159/78:\n",
      "import matplotlib.pyplot as plt\n",
      "X_internal, X_external, Y_internal, Y_external = train_test_split(X,\n",
      "                                                        y, test_size=0.3)\n",
      "\n",
      "coefs = pd.DataFrame(\n",
      "   RF.feature_importances_,\n",
      "   columns=['Coefficients'], index=X_train.columns\n",
      ")\n",
      "\n",
      "coefs.plot(kind='barh', figsize=(9, 7))\n",
      "plt.title('Ridge model')\n",
      "plt.axvline(x=0, color='.5')\n",
      "plt.subplots_adjust(left=.3)\n",
      "159/79:\n",
      "import matplotlib.pyplot as plt\n",
      "X_internal, X_external, Y_internal, Y_external = train_test_split(X,\n",
      "                                                        y, test_size=0.3)\n",
      "RF.fit(X_internal, Y_internal)\n",
      "coefs = pd.DataFrame(\n",
      "   RF.feature_importances_,\n",
      "   columns=['Coefficients'], index=X_train.columns\n",
      ")\n",
      "\n",
      "coefs.plot(kind='barh', figsize=(9, 7))\n",
      "plt.title('Ridge model')\n",
      "plt.axvline(x=0, color='.5')\n",
      "plt.subplots_adjust(left=.3)\n",
      "159/80: RF.feature_importances_\n",
      "159/81:\n",
      "def Remove_useless_descriptor(df, threshold):\n",
      "    \n",
      "    des1 = len(df.columns) \n",
      "    \n",
      "    h = df.columns.tolist()\n",
      "    df = df.values.astype(np.float)\n",
      "    df = np.array(df)\n",
      "\n",
      "    STDEV = np.std(df, axis=0)\n",
      "    idx = [idx for idx, val in enumerate(STDEV) if val > threshold]\n",
      "    df2 = df[:,idx]\n",
      "    hx = np.array(h)[idx]\n",
      "    \n",
      "    df = pd.DataFrame(df2, columns=[hx])\n",
      "    \n",
      "    des2 = len(df.columns)\n",
      "    \n",
      "    print('from Remove useless descriptor')\n",
      "    print(\"The initial set of \" + str(des1) + \\\n",
      "          \" descriptors has been reduced to \" + str(des2) + \" descriptors.\")\n",
      "    \n",
      "    return df, des1, des2\n",
      "159/82: Remove_useless_descriptor(df, 0.05)\n",
      "159/83: df 2 =Remove_useless_descriptor(df, 0.05)\n",
      "159/84: df2 = Remove_useless_descriptor(df, 0.05)\n",
      "159/85: df2\n",
      "159/86: df2.to_frame\n",
      "159/87: df2\n",
      "159/88: df3 = pd.DataFrame(df2)\n",
      "159/89: df3\n",
      "159/90:\n",
      "def Remove_useless_descriptor(df, threshold):\n",
      "    \n",
      "    des1 = len(df.columns) \n",
      "    \n",
      "    h = df.columns.tolist()\n",
      "    df = df.values.astype(np.float)\n",
      "    df = np.array(df)\n",
      "\n",
      "    STDEV = np.std(df, axis=0)\n",
      "    idx = [idx for idx, val in enumerate(STDEV) if val > threshold]\n",
      "    df2 = df[:,idx]\n",
      "    hx = np.array(h)[idx]\n",
      "    \n",
      "    df = pd.DataFrame(df, columns=[hx])\n",
      "    \n",
      "    des2 = len(df.columns)\n",
      "    \n",
      "    print('from Remove useless descriptor')\n",
      "    print(\"The initial set of \" + str(des1) + \\\n",
      "          \" descriptors has been reduced to \" + str(des2) + \" descriptors.\")\n",
      "    \n",
      "    return df, des1, des2\n",
      "159/91: RF.feature_importances_\n",
      "159/92: df2 = Remove_useless_descriptor(df, 0.05)\n",
      "159/93:\n",
      "def Remove_useless_descriptor(df, threshold):\n",
      "    \n",
      "    des1 = len(df.columns) \n",
      "    \n",
      "    h = df.columns.tolist()\n",
      "    df = df.values.astype(np.float)\n",
      "    df = np.array(df)\n",
      "\n",
      "    STDEV = np.std(df, axis=0)\n",
      "    idx = [idx for idx, val in enumerate(STDEV) if val > threshold]\n",
      "    df2 = df[:,idx]\n",
      "    hx = np.array(h)[idx]\n",
      "    \n",
      "    df = pd.DataFrame(df2, columns=[hx])\n",
      "    \n",
      "    des2 = len(df.columns)\n",
      "    \n",
      "    print('from Remove useless descriptor')\n",
      "    print(\"The initial set of \" + str(des1) + \\\n",
      "          \" descriptors has been reduced to \" + str(des2) + \" descriptors.\")\n",
      "    \n",
      "    return df, des1, des2\n",
      "159/94: df2 = Remove_useless_descriptor(df, 0.05)\n",
      "159/95: df2\n",
      "159/96:\n",
      "def Remove_useless_descriptor(df, threshold):\n",
      "    \n",
      "    des1 = len(df.columns) \n",
      "    \n",
      "    h = df.columns.tolist()\n",
      "    df = df.values.astype(np.float)\n",
      "    df = np.array(df)\n",
      "\n",
      "    STDEV = np.std(df, axis=0)\n",
      "    idx = [idx for idx, val in enumerate(STDEV) if val > threshold]\n",
      "    df2 = df[:,idx]\n",
      "    hx = np.array(h)[idx]\n",
      "    \n",
      "    df = pd.DataFrame([df2], columns=[hx])\n",
      "    \n",
      "    des2 = len(df.columns)\n",
      "    \n",
      "    print('from Remove useless descriptor')\n",
      "    print(\"The initial set of \" + str(des1) + \\\n",
      "          \" descriptors has been reduced to \" + str(des2) + \" descriptors.\")\n",
      "    \n",
      "    return df, des1, des2\n",
      "159/97: RF.feature_importances_\n",
      "159/98: df2 = Remove_useless_descriptor(df, 0.05)\n",
      "159/99:\n",
      "def Remove_useless_descriptor(df, threshold):\n",
      "    \n",
      "    des1 = len(df.columns) \n",
      "    \n",
      "    h = df.columns.tolist()\n",
      "    df = df.values.astype(np.float)\n",
      "    df = np.array(df)\n",
      "\n",
      "    STDEV = np.std(df, axis=0)\n",
      "    idx = [idx for idx, val in enumerate(STDEV) if val > threshold]\n",
      "    df2 = df[:,idx]\n",
      "    hx = np.array(h)[idx]\n",
      "    \n",
      "    df = pd.DataFrame(df2, columns=[hx])\n",
      "    \n",
      "    des2 = len(df.columns)\n",
      "    \n",
      "    print('from Remove useless descriptor')\n",
      "    print(\"The initial set of \" + str(des1) + \\\n",
      "          \" descriptors has been reduced to \" + str(des2) + \" descriptors.\")\n",
      "    \n",
      "    return df, des1, des2\n",
      "159/100: RF.feature_importances_\n",
      "159/101: df2 = Remove_useless_descriptor(df, 0.05)\n",
      "159/102: df2.\n",
      "159/103: df2\n",
      "159/104: df2 = pd.DataFrame(df2, columns=[hx]\n",
      "159/105: df2 = pd.DataFrame(df2, columns=df[h]\n",
      "159/106: df2 = pd.DataFrame(df2, columns=df[h])\n",
      "159/107: df2 = pd.DataFrame(df2, columns=df.head())\n",
      "159/108:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05)\n",
      "X = sel.fit_transform(X)\n",
      "159/109: X.shape\n",
      "159/110: y.shape\n",
      "159/111:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05)\n",
      "X = sel.fit_transform(X)\n",
      "159/112:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "159/113: pwd\n",
      "159/114:\n",
      "df = pd.read_csv('QSAR/Kp_MACCSFingerprinter.csv')\n",
      "df\n",
      "159/115:\n",
      "X = df.drop(['LogKp'], axis=1)\n",
      "X.shape\n",
      "159/116:\n",
      "y = df['LogKp']\n",
      "y\n",
      "159/117: y.shape\n",
      "159/118:\n",
      "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict\n",
      "kf =KFold(n_splits=10, shuffle=False)\n",
      "159/119:\n",
      "cnt = 1\n",
      "# split()  method generate indices to split data into training and test set.\n",
      "for train_index, test_index in kf.split(X, y):\n",
      "    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n",
      "    cnt += 1\n",
      "159/120:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "RF = RandomForestRegressor(n_estimators=600)\n",
      "y_pred = cross_val_predict(RF, X, y, cv=kf)\n",
      "159/121:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05)\n",
      "X = sel.fit_transform(X)\n",
      "159/122:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05)\n",
      "X =sel.fit_transform(X)\n",
      "159/123:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05)\n",
      "                        sel.fit_transform(X)\n",
      "159/124:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05)\n",
      "sel.fit_transform(X)\n",
      "159/125:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05)\n",
      "159/126:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "X = sel.fit_transform(X)\n",
      "159/127:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "X = sel.fit_transform(X)\n",
      "X.shape\n",
      "159/128:\n",
      "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict\n",
      "kf =KFold(n_splits=10, shuffle=False)\n",
      "159/129:\n",
      "cnt = 1\n",
      "# split()  method generate indices to split data into training and test set.\n",
      "for train_index, test_index in kf.split(X, y):\n",
      "    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n",
      "    cnt += 1\n",
      "159/130:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "RF = RandomForestRegressor(n_estimators=600)\n",
      "y_pred = cross_val_predict(RF, X, y, cv=kf)\n",
      "159/131:\n",
      "from sklearn.metrics import r2_score\n",
      "#Rsqure\n",
      "r2_score(y, y_pred)\n",
      "159/132: plt.scatter(y, y_pred)\n",
      "159/133:\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(y, y_pred)\n",
      "159/134: RMSE_pred\n",
      "159/135:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "pls = PLSRegression(n_components=10)\n",
      "y_pred = cross_val_predict(pls, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "159/136: plt.scatter(y, y_pred)\n",
      "159/137:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "y_pred = cross_val_predict(SVR, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "159/138: plt.scatter(y, y_pred)\n",
      "159/139:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='sigmoid')\n",
      "y_pred = cross_val_predict(SVR, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "159/140: plt.scatter(y, y_pred)\n",
      "159/141:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05)\n",
      "X = sel.fit_transform(X)\n",
      "159/142: X.shape\n",
      "159/143:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='sigmoid')\n",
      "y_pred = cross_val_predict(RF, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "159/144: plt.scatter(y, y_pred)\n",
      "160/1:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "160/2: pwd\n",
      "160/3:\n",
      "df = pd.read_csv('QSAR/Kp_AtomPairs2DFingerprinter.csv')\n",
      "df\n",
      "160/4:\n",
      "X = df.drop(['LogKp'], axis=1)\n",
      "X.shape\n",
      "160/5:\n",
      "y = df['LogKp']\n",
      "y\n",
      "160/6: y.shape\n",
      "160/7:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "X = sel.fit_transform(X)\n",
      "X.shape\n",
      "160/8:\n",
      "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict\n",
      "kf =KFold(n_splits=10, shuffle=False)\n",
      "160/9:\n",
      "cnt = 1\n",
      "# split()  method generate indices to split data into training and test set.\n",
      "for train_index, test_index in kf.split(X, y):\n",
      "    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n",
      "    cnt += 1\n",
      "160/10:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "RF = RandomForestRegressor(n_estimators=600)\n",
      "y_pred = cross_val_predict(RF, X, y, cv=kf)\n",
      "160/11:\n",
      "from sklearn.metrics import r2_score\n",
      "#Rsqure\n",
      "r2_score(y, y_pred)\n",
      "160/12: plt.scatter(y, y_pred)\n",
      "160/13:\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(y, y_pred)\n",
      "160/14: RMSE_pred\n",
      "160/15:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "pls = PLSRegression(n_components=10)\n",
      "y_pred = cross_val_predict(pls, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "160/16: plt.scatter(y, y_pred)\n",
      "160/17:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "y_pred = cross_val_predict(SVR, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "160/18: plt.scatter(y, y_pred)\n",
      "160/19:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='sigmoid')\n",
      "y_pred = cross_val_predict(SVR, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "160/20: plt.scatter(y, y_pred)\n",
      "160/21: from sklearn.feature_selection import VarianceThreshold\n",
      "161/1:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "161/2: pwd\n",
      "161/3:\n",
      "df = pd.read_csv('QSAR/Kp_AtomPairs2DFingerprintCount.csv')\n",
      "df\n",
      "161/4:\n",
      "X = df.drop(['LogKp'], axis=1)\n",
      "X.shape\n",
      "161/5:\n",
      "y = df['LogKp']\n",
      "y\n",
      "161/6: y.shape\n",
      "161/7:\n",
      "X = df.drop(['LogKp'], axis=1)\n",
      "X.shape\n",
      "161/8:\n",
      "y = df['LogKp']\n",
      "y\n",
      "161/9: y.shape\n",
      "161/10:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "X = sel.fit_transform(X)\n",
      "X.shape\n",
      "161/11:\n",
      "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict\n",
      "kf =KFold(n_splits=10, shuffle=False)\n",
      "161/12:\n",
      "cnt = 1\n",
      "# split()  method generate indices to split data into training and test set.\n",
      "for train_index, test_index in kf.split(X, y):\n",
      "    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n",
      "    cnt += 1\n",
      "161/13:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "RF = RandomForestRegressor(n_estimators=600)\n",
      "y_pred = cross_val_predict(RF, X, y, cv=kf)\n",
      "161/14:\n",
      "from sklearn.metrics import r2_score\n",
      "#Rsqure\n",
      "r2_score(y, y_pred)\n",
      "161/15: plt.scatter(y, y_pred)\n",
      "161/16:\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(y, y_pred)\n",
      "161/17: RMSE_pred\n",
      "161/18:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "pls = PLSRegression(n_components=10)\n",
      "y_pred = cross_val_predict(pls, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "161/19: plt.scatter(y, y_pred)\n",
      "161/20:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "y_pred = cross_val_predict(SVR, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "161/21: plt.scatter(y, y_pred)\n",
      "161/22:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='sigmoid')\n",
      "y_pred = cross_val_predict(SVR, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "161/23: plt.scatter(y, y_pred)\n",
      "161/24:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "RF = RandomForestRegressor(n_estimators=10)\n",
      "y_pred = cross_val_predict(RF, X, y, cv=kf)\n",
      "161/25:\n",
      "from sklearn.metrics import r2_score\n",
      "#Rsqure\n",
      "r2_score(y, y_pred)\n",
      "161/26: plt.scatter(y, y_pred)\n",
      "161/27:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "RF = RandomForestRegressor(n_estimators=100)\n",
      "y_pred = cross_val_predict(RF, X, y, cv=kf)\n",
      "161/28:\n",
      "from sklearn.metrics import r2_score\n",
      "#Rsqure\n",
      "r2_score(y, y_pred)\n",
      "161/29: plt.scatter(y, y_pred)\n",
      "162/1:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "162/2: pwd\n",
      "162/3:\n",
      "df = pd.read_csv('QSAR/Kp_Fingerprinter.csv')\n",
      "df\n",
      "162/4:\n",
      "X = df.drop(['LogKp'], axis=1)\n",
      "X.shape\n",
      "162/5:\n",
      "y = df['LogKp']\n",
      "y\n",
      "162/6: y.shape\n",
      "162/7:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "X = sel.fit_transform(X)\n",
      "X.shape\n",
      "162/8:\n",
      "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict\n",
      "kf =KFold(n_splits=10, shuffle=False)\n",
      "162/9:\n",
      "cnt = 1\n",
      "# split()  method generate indices to split data into training and test set.\n",
      "for train_index, test_index in kf.split(X, y):\n",
      "    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n",
      "    cnt += 1\n",
      "162/10:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "RF = RandomForestRegressor(n_estimators=600)\n",
      "y_pred = cross_val_predict(RF, X, y, cv=kf)\n",
      "162/11:\n",
      "from sklearn.metrics import r2_score\n",
      "#Rsqure\n",
      "r2_score(y, y_pred)\n",
      "162/12: plt.scatter(y, y_pred)\n",
      "162/13:\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(y, y_pred)\n",
      "162/14: RMSE_pred\n",
      "162/15:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "pls = PLSRegression(n_components=10)\n",
      "y_pred = cross_val_predict(pls, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "162/16: plt.scatter(y, y_pred)\n",
      "162/17:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "y_pred = cross_val_predict(SVR, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "162/18: plt.scatter(y, y_pred)\n",
      "162/19:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='sigmoid')\n",
      "y_pred = cross_val_predict(SVR, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "162/20: plt.scatter(y, y_pred)\n",
      "162/21:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='sigmoid')\n",
      "y_pred = cross_val_predict(RF, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "159/145:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='linear')\n",
      "y_pred = cross_val_predict(RF, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "159/146: plt.scatter(y, y_pred)\n",
      "162/22: plt.scatter(y, y_pred)\n",
      "162/23:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='poly')\n",
      "y_pred = cross_val_predict(RF, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "163/1:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "163/2: pwd\n",
      "163/3:\n",
      "df = pd.read_csv('QSAR/Kp_ExtendedFingerprinter.csv')\n",
      "df\n",
      "163/4:\n",
      "X = df.drop(['LogKp'], axis=1)\n",
      "X.shape\n",
      "163/5:\n",
      "y = df['LogKp']\n",
      "y\n",
      "163/6: y.shape\n",
      "163/7:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "X = sel.fit_transform(X)\n",
      "X.shape\n",
      "163/8:\n",
      "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict\n",
      "kf =KFold(n_splits=10, shuffle=False)\n",
      "163/9:\n",
      "cnt = 1\n",
      "# split()  method generate indices to split data into training and test set.\n",
      "for train_index, test_index in kf.split(X, y):\n",
      "    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n",
      "    cnt += 1\n",
      "163/10:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "RF = RandomForestRegressor(n_estimators=600)\n",
      "y_pred = cross_val_predict(RF, X, y, cv=kf)\n",
      "162/24: plt.scatter(y, y_pred)\n",
      "163/11:\n",
      "from sklearn.metrics import r2_score\n",
      "#Rsqure\n",
      "r2_score(y, y_pred)\n",
      "163/12: plt.scatter(y, y_pred)\n",
      "163/13:\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(y, y_pred)\n",
      "163/14: RMSE_pred\n",
      "163/15:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "pls = PLSRegression(n_components=10)\n",
      "y_pred = cross_val_predict(pls, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "163/16: plt.scatter(y, y_pred)\n",
      "163/17:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "y_pred = cross_val_predict(SVR, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "163/18: plt.scatter(y, y_pred)\n",
      "163/19:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='sigmoid')\n",
      "y_pred = cross_val_predict(SVR, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "163/20: plt.scatter(y, y_pred)\n",
      "163/21:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='poly')\n",
      "y_pred = cross_val_predict(RF, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "163/22: plt.scatter(y, y_pred)\n",
      "164/1:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "164/2: pwd\n",
      "164/3:\n",
      "df = pd.read_csv('QSAR/Kp_EStateFingerprinter.csv')\n",
      "df\n",
      "164/4:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "164/5: pwd\n",
      "164/6:\n",
      "df = pd.read_csv('QSAR/Kp_EStateFingerprinter.csv')\n",
      "df\n",
      "164/7:\n",
      "X = df.drop(['LogKp'], axis=1)\n",
      "X.shape\n",
      "164/8:\n",
      "y = df['LogKp']\n",
      "y\n",
      "164/9: y.shape\n",
      "164/10:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "X = sel.fit_transform(X)\n",
      "X.shape\n",
      "164/11:\n",
      "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict\n",
      "kf =KFold(n_splits=10, shuffle=False)\n",
      "164/12:\n",
      "cnt = 1\n",
      "# split()  method generate indices to split data into training and test set.\n",
      "for train_index, test_index in kf.split(X, y):\n",
      "    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n",
      "    cnt += 1\n",
      "164/13:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "RF = RandomForestRegressor(n_estimators=600)\n",
      "y_pred = cross_val_predict(RF, X, y, cv=kf)\n",
      "164/14:\n",
      "from sklearn.metrics import r2_score\n",
      "#Rsqure\n",
      "r2_score(y, y_pred)\n",
      "164/15: plt.scatter(y, y_pred)\n",
      "164/16:\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(y, y_pred)\n",
      "164/17: RMSE_pred\n",
      "164/18:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "pls = PLSRegression(n_components=10)\n",
      "y_pred = cross_val_predict(pls, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "164/19: plt.scatter(y, y_pred)\n",
      "164/20:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "y_pred = cross_val_predict(SVR, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "164/21: plt.scatter(y, y_pred)\n",
      "164/22:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='sigmoid')\n",
      "y_pred = cross_val_predict(SVR, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "164/23: plt.scatter(y, y_pred)\n",
      "164/24:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='poly')\n",
      "y_pred = cross_val_predict(RF, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "164/25: plt.scatter(y, y_pred)\n",
      "165/1:\n",
      "df = pd.read_csv('QSAR/Kp_GraphOnlyFingerprinter.csv')\n",
      "df\n",
      "165/2:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "165/3: pwd\n",
      "165/4:\n",
      "df = pd.read_csv('QSAR/Kp_GraphOnlyFingerprinter.csv')\n",
      "df\n",
      "165/5:\n",
      "X = df.drop(['LogKp'], axis=1)\n",
      "X.shape\n",
      "165/6:\n",
      "y = df['LogKp']\n",
      "y\n",
      "165/7: y.shape\n",
      "165/8:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "X = sel.fit_transform(X)\n",
      "X.shape\n",
      "165/9:\n",
      "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict\n",
      "kf =KFold(n_splits=10, shuffle=False)\n",
      "165/10:\n",
      "cnt = 1\n",
      "# split()  method generate indices to split data into training and test set.\n",
      "for train_index, test_index in kf.split(X, y):\n",
      "    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n",
      "    cnt += 1\n",
      "165/11:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "RF = RandomForestRegressor(n_estimators=600)\n",
      "y_pred = cross_val_predict(RF, X, y, cv=kf)\n",
      "165/12:\n",
      "from sklearn.metrics import r2_score\n",
      "#Rsqure\n",
      "r2_score(y, y_pred)\n",
      "165/13: plt.scatter(y, y_pred)\n",
      "165/14:\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(y, y_pred)\n",
      "165/15: RMSE_pred\n",
      "165/16:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "pls = PLSRegression(n_components=10)\n",
      "y_pred = cross_val_predict(pls, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "165/17: plt.scatter(y, y_pred)\n",
      "165/18:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "y_pred = cross_val_predict(SVR, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "165/19: plt.scatter(y, y_pred)\n",
      "165/20:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='sigmoid')\n",
      "y_pred = cross_val_predict(SVR, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "165/21: plt.scatter(y, y_pred)\n",
      "165/22:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='poly')\n",
      "y_pred = cross_val_predict(RF, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "165/23: plt.scatter(y, y_pred)\n",
      "166/1:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "166/2: pwd\n",
      "166/3:\n",
      "df = pd.read_csv('QSAR/Kp_PubchemFingerprinter.csv')\n",
      "df\n",
      "166/4:\n",
      "X = df.drop(['LogKp'], axis=1)\n",
      "X.shape\n",
      "166/5:\n",
      "y = df['LogKp']\n",
      "y\n",
      "166/6: y.shape\n",
      "166/7:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "X = sel.fit_transform(X)\n",
      "X.shape\n",
      "166/8:\n",
      "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict\n",
      "kf =KFold(n_splits=10, shuffle=False)\n",
      "166/9:\n",
      "cnt = 1\n",
      "# split()  method generate indices to split data into training and test set.\n",
      "for train_index, test_index in kf.split(X, y):\n",
      "    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n",
      "    cnt += 1\n",
      "166/10:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "RF = RandomForestRegressor(n_estimators=600)\n",
      "y_pred = cross_val_predict(RF, X, y, cv=kf)\n",
      "166/11:\n",
      "from sklearn.metrics import r2_score\n",
      "#Rsqure\n",
      "r2_score(y, y_pred)\n",
      "166/12: plt.scatter(y, y_pred)\n",
      "166/13:\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(y, y_pred)\n",
      "166/14: RMSE_pred\n",
      "166/15:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "pls = PLSRegression(n_components=10)\n",
      "y_pred = cross_val_predict(pls, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "166/16: plt.scatter(y, y_pred)\n",
      "166/17:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "y_pred = cross_val_predict(SVR, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "166/18: plt.scatter(y, y_pred)\n",
      "166/19:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='sigmoid')\n",
      "y_pred = cross_val_predict(SVR, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "166/20: plt.scatter(y, y_pred)\n",
      "166/21:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='poly')\n",
      "y_pred = cross_val_predict(RF, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "165/24:\n",
      "from sklearn.linear_model import LinearRegression\n",
      "Lr = LinearRegression()\n",
      "y_pred = cross_val_predict(Lr, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "165/25: plt.scatter(y, y_pred)\n",
      "166/22: plt.scatter(y, y_pred)\n",
      "166/23: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
      "166/24: Fit = RF.fit(X_train, y_train)\n",
      "166/25:\n",
      "# Get numerical feature importances\n",
      "importances = list(RF.feature_importances_)# List of tuples with variable and importance\n",
      "feature_importances = [(feature, round(importance, 2)) \n",
      "for feature, importance in zip(feature_list, importances)]\n",
      "\n",
      "# Sort the feature importances by most important first\n",
      "\n",
      "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
      "\n",
      "# Print out the feature and importances \n",
      "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];\n",
      "166/26: RF.fit(X_train, y_train)\n",
      "166/27: Fit = RF.fit(X_train, y_train)\n",
      "166/28:\n",
      "Fit = RF.fit(X_train, y_train)\n",
      "Fit\n",
      "166/29:\n",
      "RF.fit(X_train, y_train)\n",
      "r2 = RF.score(X_train,y_train)\n",
      "r2\n",
      "166/30:\n",
      "result = permutation_importance(RF, X_test, y_test, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances[sorted_idx].T,\n",
      "           vert=False, labels=X_test.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (test set)\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "166/31:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn.datasets import fetch_openml\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.impute import SimpleImputer\n",
      "from sklearn.inspection import permutation_importance\n",
      "from sklearn.compose import ColumnTransformer\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "result = permutation_importance(RF, X_test, y_test, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances[sorted_idx].T,\n",
      "           vert=False, labels=X_test.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (test set)\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "166/32:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from sklearn.datasets import fetch_openml\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.impute import SimpleImputer\n",
      "from sklearn.inspection import permutation_importance\n",
      "from sklearn.compose import ColumnTransformer\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "result = permutation_importance(RF, X_test, y_test, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances[sorted_idx].T,\n",
      "           vert=False, labels=X.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (test set)\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "166/33: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
      "166/34:\n",
      "RF.fit(X_train, y_train)\n",
      "r2 = RF.score(X_train,y_train)\n",
      "r2\n",
      "166/35: y_pred = RF.predict(X_test)\n",
      "166/36:\n",
      "y_pred = RF.predict(X_test)\n",
      "r2_score(y,y_pred)\n",
      "166/37:\n",
      "y_pred = RF.predict(X_test)\n",
      "r2_score(y, _pred)\n",
      "166/38:\n",
      "y_pred = RF.predict(X_test)\n",
      "r2_score(y, y_pred)\n",
      "166/39: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
      "166/40:\n",
      "RF.fit(X_train, y_train)\n",
      "r2 = RF.score(X_train,y_train)\n",
      "r2\n",
      "166/41:\n",
      "y_pred = RF.predict(X_test)\n",
      "r2_score(y, y_pred)\n",
      "166/42:\n",
      "y_pred = RF.predict(X_test)\n",
      "y_pred\n",
      "166/43:\n",
      "y_pred = RF.predict(X_test)\n",
      "y_pred.sha\n",
      "166/44:\n",
      "y_pred = RF.predict(X_test)\n",
      "y_pred.shape\n",
      "166/45:\n",
      "y_pred = RF.predict(X_train)\n",
      "y_pred.shape\n",
      "166/46:\n",
      "y_pred = RF.predict(X_test)\n",
      "y_pred.shape\n",
      "166/47:\n",
      "y_pred = RF.predict(y_test)\n",
      "y_pred.shape\n",
      "166/48:\n",
      "y_pred = RF.predict(X_test)\n",
      "y_pred.shape\n",
      "166/49: r2_score(y_test, y_pred)\n",
      "166/50:\n",
      "import matplotlib.pyplot as plt\n",
      "impshape numpy as np\n",
      "from sklearn.datasets import fetch_openml\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.impute import SimpleImputer\n",
      "from sklearn.inspection import permutation_importance\n",
      "from sklearn.compose import ColumnTransformer\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "\n",
      "result = permutation_importance(RF, X_test, y_test, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances[sorted_idx].T,\n",
      "           vert=False, labels=X.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (test set)\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "166/51:\n",
      "from sklearn.inspection import permutation_importance\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "result = permutation_importance(RF, X_test, y_test, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances[sorted_idx].T,\n",
      "           vert=False, labels=X.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (test set)\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "168/1:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "168/2: pwd\n",
      "168/3:\n",
      "df = pd.read_csv('QSAR/Kp_SubstructureFingerprinter.csv')\n",
      "df\n",
      "168/4:\n",
      "X = df.drop(['LogKp'], axis=1)\n",
      "X.shape\n",
      "168/5:\n",
      "y = df['LogKp']\n",
      "y\n",
      "168/6: y.shape\n",
      "168/7:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "X = sel.fit_transform(X)\n",
      "X.shape\n",
      "168/8:\n",
      "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict\n",
      "kf =KFold(n_splits=10, shuffle=False)\n",
      "168/9:\n",
      "cnt = 1\n",
      "# split()  method generate indices to split data into training and test set.\n",
      "for train_index, test_index in kf.split(X, y):\n",
      "    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n",
      "    cnt += 1\n",
      "168/10:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "RF = RandomForestRegressor(n_estimators=600)\n",
      "y_pred = cross_val_predict(RF, X, y, cv=kf)\n",
      "168/11:\n",
      "from sklearn.metrics import r2_score\n",
      "#Rsqure\n",
      "r2_score(y, y_pred)\n",
      "168/12: plt.scatter(y, y_pred)\n",
      "168/13:\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(y, y_pred)\n",
      "168/14: RMSE_pred\n",
      "168/15:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "pls = PLSRegression(n_components=10)\n",
      "y_pred = cross_val_predict(pls, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "168/16: plt.scatter(y, y_pred)\n",
      "168/17:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "y_pred = cross_val_predict(SVR, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "168/18: plt.scatter(y, y_pred)\n",
      "168/19:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='sigmoid')\n",
      "y_pred = cross_val_predict(SVR, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "168/20: plt.scatter(y, y_pred)\n",
      "168/21:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='poly')\n",
      "y_pred = cross_val_predict(RF, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "168/22: plt.scatter(y, y_pred)\n",
      "168/23: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
      "168/24:\n",
      "RF.fit(X_train, y_train)\n",
      "r2 = RF.score(X_train,y_train)\n",
      "r2\n",
      "168/25:\n",
      "y_pred = RF.predict(X_test)\n",
      "y_pred.shape\n",
      "168/26: r2_score(y_test, y_pred)\n",
      "168/27:\n",
      "from sklearn.inspection import permutation_importance\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "result = permutation_importance(RF, X_test, y_test, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances[sorted_idx].T,\n",
      "           vert=False, labels=X.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (test set)\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "168/28:\n",
      "from sklearn.inspection import permutation_importance\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "result = permutation_importance(RF, X_test, y_test, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances[sorted_idx].T,\n",
      "           vert=False, labels=X_test.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (test set)\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "168/29:\n",
      "from sklearn.inspection import permutation_importance\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "result = permutation_importance(RF, X_test, y_test, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances[sorted_idx].T,\n",
      "           vert=False)\n",
      "ax.set_title(\"Permutation Importances (test set)\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "168/30:\n",
      "from sklearn.inspection import permutation_importance\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "result = permutation_importance(RF, X_test, y_test, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances[sorted_idx].T,\n",
      "           vert=False, labels=X_test.head())\n",
      "ax.set_title(\"Permutation Importances (test set)\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "168/31:\n",
      "from sklearn.inspection import permutation_importance\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "result = permutation_importance(RF, X_test, y_test, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances[sorted_idx].T,\n",
      "           vert=False, labels=X_test.[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (test set)\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "168/32:\n",
      "from sklearn.inspection import permutation_importance\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "result = permutation_importance(RF, X_test, y_test, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "168/33:\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances[sorted_idx].T,\n",
      "           vert=False, labels=X.[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (test set)\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "168/34:\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances[sorted_idx].T,\n",
      "           vert=False, labels=X.c[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (test set)\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "168/35:\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances[sorted_idx].T,\n",
      "           vert=False, labels=X.column[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (test set)\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "168/36:\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances[sorted_idx].T,\n",
      "           vert=False, labels=X.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (test set)\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "168/37:\n",
      "X = df.drop(['LogKp'], axis=1)\n",
      "X.shape\n",
      "168/38:\n",
      "X = df.drop(['LogKp'], axis=1)\n",
      "X.shape\n",
      "X\n",
      "168/39:\n",
      "X = df.drop(['LogKp'], axis=1)\n",
      "X.shape\n",
      "168/40:\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances[sorted_idx].T,\n",
      "           vert=False, labels=X.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (test set)\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "168/41:\n",
      "from sklearn.inspection import permutation_importance\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "result = permutation_importance(RF, X_test, y_test, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances[sorted_idx].T,\n",
      "           vert=False, labels=X.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (test set)\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "169/1:\n",
      "df = pd.read_csv('QSAR/Kp_SubstructureFingerprintCount.csv')\n",
      "df\n",
      "169/2:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "169/3: pwd\n",
      "169/4:\n",
      "df = pd.read_csv('QSAR/Kp_SubstructureFingerprintCount.csv')\n",
      "df\n",
      "169/5:\n",
      "X = df.drop(['LogKp'], axis=1)\n",
      "X.shape\n",
      "169/6:\n",
      "y = df['LogKp']\n",
      "y\n",
      "169/7: y.shape\n",
      "169/8:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "X = sel.fit_transform(X)\n",
      "X.shape\n",
      "169/9:\n",
      "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict\n",
      "kf =KFold(n_splits=10, shuffle=False)\n",
      "169/10:\n",
      "cnt = 1\n",
      "# split()  method generate indices to split data into training and test set.\n",
      "for train_index, test_index in kf.split(X, y):\n",
      "    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n",
      "    cnt += 1\n",
      "169/11:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "RF = RandomForestRegressor(n_estimators=600)\n",
      "y_pred = cross_val_predict(RF, X, y, cv=kf)\n",
      "169/12:\n",
      "from sklearn.metrics import r2_score\n",
      "#Rsqure\n",
      "r2_score(y, y_pred)\n",
      "169/13: plt.scatter(y, y_pred)\n",
      "169/14:\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(y, y_pred)\n",
      "169/15: RMSE_pred\n",
      "169/16:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "pls = PLSRegression(n_components=10)\n",
      "y_pred = cross_val_predict(pls, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "169/17: plt.scatter(y, y_pred)\n",
      "169/18:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "y_pred = cross_val_predict(SVR, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "169/19: plt.scatter(y, y_pred)\n",
      "169/20:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='sigmoid')\n",
      "y_pred = cross_val_predict(SVR, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "169/21: plt.scatter(y, y_pred)\n",
      "169/22:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='poly')\n",
      "y_pred = cross_val_predict(RF, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "169/23: plt.scatter(y, y_pred)\n",
      "169/24: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
      "169/25:\n",
      "RF.fit(X_train, y_train)\n",
      "r2 = RF.score(X_train,y_train)\n",
      "r2\n",
      "169/26:\n",
      "y_pred = RF.predict(X_test)\n",
      "y_pred.shape\n",
      "169/27: r2_score(y_test, y_pred)\n",
      "169/28:\n",
      "from sklearn.inspection import permutation_importance\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "result = permutation_importance(RF, X_test, y_test, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "170/1:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "170/2: pwd\n",
      "170/3:\n",
      "df = pd.read_csv('QSAR/Kp_KlekotaRothFingerprinter.csv')\n",
      "df\n",
      "170/4:\n",
      "X = df.drop(['LogKp'], axis=1)\n",
      "X.shape\n",
      "170/5:\n",
      "y = df['LogKp']\n",
      "y\n",
      "170/6: y.shape\n",
      "170/7:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "X = sel.fit_transform(X)\n",
      "X.shape\n",
      "170/8:\n",
      "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict\n",
      "kf =KFold(n_splits=10, shuffle=False)\n",
      "170/9:\n",
      "cnt = 1\n",
      "# split()  method generate indices to split data into training and test set.\n",
      "for train_index, test_index in kf.split(X, y):\n",
      "    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n",
      "    cnt += 1\n",
      "170/10:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "RF = RandomForestRegressor(n_estimators=600)\n",
      "y_pred = cross_val_predict(RF, X, y, cv=kf)\n",
      "169/29:\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances[sorted_idx].T,\n",
      "           vert=False, labels=df.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (test set)\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "171/1:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "170/11:\n",
      "from sklearn.metrics import r2_score\n",
      "#Rsqure\n",
      "r2_score(y, y_pred)\n",
      "170/12: plt.scatter(y, y_pred)\n",
      "170/13:\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(y, y_pred)\n",
      "170/14: RMSE_pred\n",
      "170/15:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "pls = PLSRegression(n_components=10)\n",
      "y_pred = cross_val_predict(pls, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "170/16: plt.scatter(y, y_pred)\n",
      "170/17:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "y_pred = cross_val_predict(SVR, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "170/18: plt.scatter(y, y_pred)\n",
      "171/2: pwd\n",
      "171/3:\n",
      "df = pd.read_csv('QSAR/Kp_KlekotaRothFingerprintCount.csv')\n",
      "df\n",
      "170/19:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='sigmoid')\n",
      "y_pred = cross_val_predict(SVR, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "170/20: plt.scatter(y, y_pred)\n",
      "170/21:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='poly')\n",
      "y_pred = cross_val_predict(RF, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "171/4:\n",
      "X = df.drop(['LogKp'], axis=1)\n",
      "X.shape\n",
      "171/5:\n",
      "y = df['LogKp']\n",
      "y\n",
      "171/6: y.shape\n",
      "171/7:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "X = sel.fit_transform(X)\n",
      "X.shape\n",
      "171/8:\n",
      "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict\n",
      "kf =KFold(n_splits=10, shuffle=False)\n",
      "171/9:\n",
      "cnt = 1\n",
      "# split()  method generate indices to split data into training and test set.\n",
      "for train_index, test_index in kf.split(X, y):\n",
      "    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n",
      "    cnt += 1\n",
      "171/10:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "RF = RandomForestRegressor(n_estimators=600)\n",
      "y_pred = cross_val_predict(RF, X, y, cv=kf)\n",
      "171/11:\n",
      "from sklearn.metrics import r2_score\n",
      "#Rsqure\n",
      "r2_score(y, y_pred)\n",
      "171/12: plt.scatter(y, y_pred)\n",
      "171/13:\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(y, y_pred)\n",
      "171/14: RMSE_pred\n",
      "171/15:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "pls = PLSRegression(n_components=10)\n",
      "y_pred = cross_val_predict(pls, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "171/16: plt.scatter(y, y_pred)\n",
      "171/17:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "y_pred = cross_val_predict(SVR, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "171/18: plt.scatter(y, y_pred)\n",
      "171/19:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='sigmoid')\n",
      "y_pred = cross_val_predict(SVR, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "171/20: plt.scatter(y, y_pred)\n",
      "171/21:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='poly')\n",
      "y_pred = cross_val_predict(RF, X, y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "170/22: plt.scatter(y, y_pred)\n",
      "170/23: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
      "170/24:\n",
      "RF.fit(X_train, y_train)\n",
      "r2 = RF.score(X_train,y_train)\n",
      "r2\n",
      "170/25:\n",
      "y_pred = RF.predict(X_test)\n",
      "y_pred.shape\n",
      "170/26: r2_score(y_test, y_pred)\n",
      "170/27:\n",
      "from sklearn.inspection import permutation_importance\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "result = permutation_importance(RF, X_test, y_test, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances[sorted_idx].T,\n",
      "           vert=False, labels=X.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (test set)\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "171/22: plt.scatter(y, y_pred)\n",
      "171/23: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
      "171/24:\n",
      "RF.fit(X_train, y_train)\n",
      "r2 = RF.score(X_train,y_train)\n",
      "r2\n",
      "171/25:\n",
      "y_pred = RF.predict(X_test)\n",
      "y_pred.shape\n",
      "171/26: r2_score(y_test, y_pred)\n",
      "171/27:\n",
      "from sklearn.inspection import permutation_importance\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "result = permutation_importance(RF, X_test, y_test, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances[sorted_idx].T,\n",
      "           vert=False, labels=X.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (test set)\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "170/28:\n",
      "X = df.drop(['LogKp'], axis=1)\n",
      "X.shape\n",
      "170/29:\n",
      "from sklearn.inspection import permutation_importance\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "result = permutation_importance(RF, X_test, y_test, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances[sorted_idx].T,\n",
      "           vert=False, labels=X.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (test set)\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "171/28:\n",
      "from sklearn.inspection import permutation_importance\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "result = permutation_importance(RF, X_test, y_test, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances[sorted_idx].T,\n",
      "           vert=False, labels=X.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (test set)\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "168/42: result.importances[sorted_idx]\n",
      "168/43: result.importances[sorted_idx][:20]\n",
      "168/44: result.importances[sorted_idx][:19]\n",
      "168/45: result.importances[sorted_idx].[:19]\n",
      "168/46: result.importances[sorted_idx]\n",
      "170/30:\n",
      "from sklearn.inspection import permutation_importance\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "result = permutation_importance(RF, X_test, y_test, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances[sorted_idx].T,\n",
      "           vert=False, labels=df.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (test set)\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "171/29:\n",
      "from sklearn.inspection import permutation_importance\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "result = permutation_importance(RF, X_test, y_test, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances[sorted_idx].T,\n",
      "           vert=False, labels=X.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (test set)\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "166/52:\n",
      "from sklearn.inspection import permutation_importance\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "result = permutation_importance(RF, X_test, y_test, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances[sorted_idx].T,\n",
      "           vert=False, labels=df.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (test set)\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "168/47: X.columns[sorted_idx]\n",
      "168/48: List_importance = X.columns[sorted_idx]\n",
      "168/49: List_importance[:20]\n",
      "168/50:\n",
      "List_importance = X.columns[sorted_idx]\n",
      "List_importance[:20]\n",
      "169/30:\n",
      "List_importance = df.columns[sorted_idx]\n",
      "List_importance[:20]\n",
      "169/31:\n",
      "List_importance = X.columns[sorted_idx]\n",
      "List_importance[:20]\n",
      "168/51: List_importance = X.columns[sorted_idx]\n",
      "168/52: List_importance\n",
      "168/53: result\n",
      "168/54: result.shape\n",
      "168/55: result.importances.shape\n",
      "168/56: result.importances\n",
      "168/57: pd.DataFrame(result.importances)\n",
      "168/58: list(result.importances)\n",
      "168/59: pd.DataFrame(result.importances, [:0])\n",
      "168/60: result.importances.shape\n",
      "168/61: result.importances.to_csv('test')\n",
      "168/62: result.importances.reshape[:-1]\n",
      "168/63: result.importances.reshape(:1)\n",
      "168/64: result.importances.reshape[:1]\n",
      "168/65: result.importances.reshape(0,1)\n",
      "168/66: result.importances.reshape(1,0)\n",
      "168/67: result.importances.reshape(0,-1)\n",
      "168/68: result.importances\n",
      "168/69: X.columns[sorted_idx]\n",
      "168/70: result.importances[sorted_idx]\n",
      "168/71: result.importances\n",
      "168/72: X.columns\n",
      "168/73: result.importances.shape\n",
      "168/74: X.columns.shape\n",
      "168/75: X.columns[sorted_idx].shape\n",
      "168/76: result.importances.shape\n",
      "168/77: result.importances[sorted_idx]\n",
      "168/78: result.importances[sorted_idx].shape\n",
      "168/79: result.importances[sorted_idx]\n",
      "168/80: X.columns[sorted_idx][:19]\n",
      "168/81: X.columns[sorted_idx][:20]\n",
      "168/82:\n",
      "from sklearn.inspection import permutation_importance\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "result = permutation_importance(RF, X_test, y_test, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances[sorted_idx][:20],\n",
      "           vert=False, labels=X.columns[sorted_idx][:20])\n",
      "ax.set_title(\"Permutation Importances (test set)\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "168/83:\n",
      "from sklearn.inspection import permutation_importance\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "result = permutation_importance(RF, X_test, y_test, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances[sorted_idx].T[:20],\n",
      "           vert=False, labels=X.columns[sorted_idx][:20])\n",
      "ax.set_title(\"Permutation Importances (test set)\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "168/84:\n",
      "from sklearn.inspection import permutation_importance\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "result = permutation_importance(RF, X_test, y_test, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances[sorted_idx].T,\n",
      "           vert=False, labels=X.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (test set)\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "168/85:\n",
      "for i in r.importances_mean.argsort()[::-1]:\n",
      "...     if r.importances_mean[i] - 2 * r.importances_std[i] > 0:\n",
      "...         print(f\"{diabetes.feature_names[i]:<8}\"\n",
      "...               f\"{r.importances_mean[i]:.3f}\"\n",
      "...               f\" +/- {r.importances_std[i]:.3f}\")\n",
      "168/86:\n",
      "for i in r.importances_mean.argsort()[::-1]:\n",
      "    if r.importances_mean[i] - 2 * r.importances_std[i] > 0:\n",
      "        print(f\"{diabetes.feature_names[i]:<8}\"\n",
      "               f\"{r.importances_mean[i]:.3f}\"\n",
      "               f\" +/- {r.importances_std[i]:.3f}\")\n",
      "168/87:\n",
      "for i in result .importances_mean.argsort()[::-1]:\n",
      "    if result .importances_mean[i] - 2 * r.importances_std[i] > 0:\n",
      "        print(f\"{diabetes.feature_names[i]:<8}\"\n",
      "               f\"{r.importances_mean[i]:.3f}\"\n",
      "               f\" +/- {r.importances_std[i]:.3f}\")\n",
      "168/88:\n",
      "for i in result .importances_mean.argsort()[::-1]:\n",
      "    if result .importances_mean[i] - 2 * result .importances_std[i] > 0:\n",
      "        print(f\"{diabetes.feature_names[i]:<8}\"\n",
      "               f\"{r.importances_mean[i]:.3f}\"\n",
      "               f\" +/- {r.importances_std[i]:.3f}\")\n",
      "168/89:\n",
      "for i in result .importances_mean.argsort()[::-1]:\n",
      "    if result .importances_mean[i] - 2 * result .importances_std[i] > 0:\n",
      "        print(f\"{X.feature_names[i]:<8}\"\n",
      "               f\"{r.importances_mean[i]:.3f}\"\n",
      "               f\" +/- {r.importances_std[i]:.3f}\")\n",
      "168/90:\n",
      "for i in result .importances_mean.argsort()[::-1]:\n",
      "    if result .importances_mean[i] - 2 * result .importances_std[i] > 0:\n",
      "        print(f\"{X.columns[i]:<8}\"\n",
      "               f\"{r.importances_mean[i]:.3f}\"\n",
      "               f\" +/- {r.importances_std[i]:.3f}\")\n",
      "168/91:\n",
      "for i in result .importances_mean.argsort()[::-1]:\n",
      "    if result .importances_mean[i] - 2 * result .importances_std[i] > 0:\n",
      "        print(f\"{X.columns[i]:<8}\"\n",
      "               f\"{result.importances_mean[i]:.3f}\"\n",
      "               f\" +/- {result.importances_std[i]:.3f}\")\n",
      "168/92:\n",
      "for i in result .importances_mean.argsort()[::-1]:\n",
      "    if result .importances_mean[i] - 2 * result .importances_std[i] > 0:\n",
      "        print(f\"{X.columns[i]:<20}\"\n",
      "               f\"{result.importances_mean[i]:.3f}\"\n",
      "               f\" +/- {result.importances_std[i]:.3f}\")\n",
      "168/93:\n",
      "for i in result .importances_mean.argsort()[::-1]:\n",
      "    if result .importances_mean[i] - 2 * result .importances_std[i] > 0:\n",
      "        print(f\"{X.columns[i]:<8}\"\n",
      "               f\"{result.importances_mean[i]:.3f}\"\n",
      "               f\" +/- {result.importances_std[i]:.3f}\")\n",
      "168/94:\n",
      "for i in result .importances_mean.argsort()[::-1]:\n",
      "    if result .importances_mean[i] - 2 * result .importances_std[i] > 0.05:\n",
      "        print(f\"{X.columns[i]:<8}\"\n",
      "               f\"{result.importances_mean[i]:.3f}\"\n",
      "               f\" +/- {result.importances_std[i]:.3f}\")\n",
      "168/95:\n",
      "for i in result .importances_mean.argsort()[::-1]:\n",
      "    if result .importances_mean[i] - 2 * result .importances_std[i] > 0:\n",
      "        print(f\"{X.columns[i]:<8}\"\n",
      "               f\"{result.importances_mean[i]:.3f}\"\n",
      "               f\" +/- {result.importances_std[i]:.3f}\")\n",
      "169/32:\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances[sorted_idx].T,\n",
      "           vert=False, labels=df.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (test set)\")\n",
      "ax.set_ylim(:20)\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "169/33:\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances[sorted_idx].T,\n",
      "           vert=False, labels=df.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (test set)\")\n",
      "ax.set_ylim([:20])\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "169/34:\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances[sorted_idx].T,\n",
      "           vert=False, labels=df.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (test set)\")\n",
      "ax.set_ylim(0,20)\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "169/35:\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances[sorted_idx].T,\n",
      "           vert=False, labels=df.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (test set)\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "169/36:\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances[sorted_idx].T,\n",
      "           vert=False, labels=df.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (test set)\")\n",
      "ax.set_ylim(1,20)\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "169/37:\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances[sorted_idx].T,\n",
      "           vert=False, labels=df.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (test set)\")\n",
      "ax.set_ylim(0,20)\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "169/38:\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances[sorted_idx].T,\n",
      "           vert=False, labels=df.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (test set)\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "171/30:\n",
      "from sklearn.inspection import permutation_importance\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "result = permutation_importance(RF, X_test, y_test, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances[sorted_idx].T,\n",
      "           vert=False, labels=df.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (test set)\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "172/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "172/2:\n",
      "path = r'Fingerprint/'\n",
      "\n",
      "Fp1  = pd.read_csv(path + 'Fingerprinter.csv'               , header = 0)\n",
      "Fp2  = pd.read_csv(path + 'ExtendedFingerprinter.csv'       , header = 0)\n",
      "Fp3  = pd.read_csv(path + 'EStateFingerprinter.csv'         , header = 0)\n",
      "Fp4  = pd.read_csv(path + 'GraphOnlyFingerprinter.csv'      , header = 0)\n",
      "Fp5  = pd.read_csv(path + 'MACCSFingerprinter.csv'          , header = 0)\n",
      "Fp6  = pd.read_csv(path + 'PubchemFingerprinter.csv'        , header = 0)\n",
      "Fp7  = pd.read_csv(path + 'SubstructureFingerprinter.csv'   , header = 0)\n",
      "Fp8  = pd.read_csv(path + 'SubstructureFingerprintCount.csv', header = 0)\n",
      "Fp9  = pd.read_csv(path + 'KlekotaRothFingerprinter.csv'    , header = 0)\n",
      "Fp10 = pd.read_csv(path + 'KlekotaRothFingerprintCount.csv' , header = 0)\n",
      "Fp11 = pd.read_csv(path + 'AtomPairs2DFingerprinter.csv'    , header = 0)\n",
      "Fp12 = pd.read_csv(path + 'AtomPairs2DFingerprintCount.csv' , header = 0)\n",
      "172/3: Fp1.shape\n",
      "172/4:\n",
      "df = pd.read_csv('df6_lipinski.csv')\n",
      "df\n",
      "172/5:\n",
      "df = pd.read_csv('df6_lipinski.csv')\n",
      "df\n",
      "172/6: name = df['compound']\n",
      "172/7: name.shape\n",
      "172/8:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df2 = df2.drop('Unnamed: 0', axis=1)\n",
      "df3 = df2.set_index('Name')\n",
      "df3\n",
      "174/1:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "174/2: pwd\n",
      "174/3: df = pd.read_csv('rawdataqsar_sm.csv')\n",
      "174/4: df\n",
      "175/1:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "175/2: pwd\n",
      "175/3: df = pd.read_csv('rawdataqsar_sm.csv')\n",
      "175/4: df\n",
      "175/5: df2 =df[df.kp.notna()]\n",
      "175/6: df2\n",
      "175/7: df3 = df2.drop(['logkp', 'kpu'], axis=1)\n",
      "175/8: df3 = df2.drop(['logkp', 'kp'], axis=1)\n",
      "175/9: df3\n",
      "175/10:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "175/11: pwd\n",
      "175/12: df = pd.read_csv('rawdataqsar_sm.csv')\n",
      "175/13: df\n",
      "175/14: df2 =df[df.kp.notna()]\n",
      "175/15: df2\n",
      "175/16: df3 = df2.drop(['logkp', 'kp'], axis=1)\n",
      "175/17: df3\n",
      "175/18:\n",
      "def LogKp(input):\n",
      "    LogKp = []\n",
      "\n",
      "    for i in input['kp']:\n",
      "        LogKp.append(np.log10(i))\n",
      "\n",
      "    input['LogKp'] = LogKp\n",
      "    x = input.drop('kp', 1)   \n",
      "    return x\n",
      "175/19: df4 = LogKp(df3)\n",
      "175/20: df4\n",
      "175/21: df3\n",
      "175/22:\n",
      "def LogKp(input):\n",
      "    LogKp = []\n",
      "\n",
      "    for i in input['kp']:\n",
      "        LogKp.append(np.log10(i))\n",
      "\n",
      "    input['LogKp'] = LogKp\n",
      "    x = input.drop('kp', 1)   \n",
      "    return x\n",
      "175/23: df4 = LogKp(df3)\n",
      "175/24:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "175/25: pwd\n",
      "175/26: df = pd.read_csv('rawdataqsar_sm.csv')\n",
      "175/27: df\n",
      "175/28: df2 =df[df.kp.notna()]\n",
      "175/29: df2\n",
      "175/30: df3 = df2.drop(['logkp', 'kp'], axis=1)\n",
      "175/31: df3\n",
      "175/32:\n",
      "def LogKp(input):\n",
      "    LogKp = []\n",
      "\n",
      "    for i in input['kp']:\n",
      "        LogKp.append(np.log10(i))\n",
      "\n",
      "    input['LogKp'] = LogKp\n",
      "    x = input.drop('kp', 1)   \n",
      "    return x\n",
      "175/33: df4 = LogKp(df3)\n",
      "175/34:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "175/35: pwd\n",
      "175/36: df = pd.read_csv('rawdataqsar_sm.csv')\n",
      "175/37: df\n",
      "175/38: df2 =df[df.kp.notna()]\n",
      "175/39: df2\n",
      "175/40: df3 = df2.drop(['logkp', 'kp'], axis=1)\n",
      "175/41: df3\n",
      "175/42:\n",
      "def LogKpu(input):\n",
      "    LogKpu = []\n",
      "\n",
      "    for i in input['kpu']:\n",
      "        LogKpu.append(np.log10(i))\n",
      "\n",
      "    input['LogKpu'] = LogKp\n",
      "    x = input.drop('kpu', 1)   \n",
      "    return x\n",
      "175/43: df4 = LogKpu(df3)\n",
      "175/44: df4\n",
      "175/45:\n",
      "df5 = df4.drop_duplicates(subset=['smiles'])\n",
      "df5\n",
      "175/46:\n",
      "df6 = df5.reset_index(drop=True)\n",
      "df6\n",
      "175/47:\n",
      "# Calculate Molecular Descriptors\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Descriptors, Lipinski\n",
      "\n",
      "def lipinski(smiles, verbose=False):\n",
      "\n",
      "    moldata= []\n",
      "    for elem in smiles:\n",
      "        mol=Chem.MolFromSmiles(elem) \n",
      "        moldata.append(mol)\n",
      "       \n",
      "    baseData= np.arange(1,1)\n",
      "    i=0  \n",
      "    for mol in moldata:        \n",
      "       \n",
      "        desc_MolWt = Descriptors.MolWt(mol)\n",
      "        desc_TPSA = Descriptors.TPSA(mol)\n",
      "        desc_MolLogP = Descriptors.MolLogP(mol)\n",
      "        desc_NumHDonors = Lipinski.NumHDonors(mol)\n",
      "        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)\n",
      "        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)\n",
      "        desc_FpDensityMorgan1 = Descriptors.FpDensityMorgan1(mol)\n",
      "        desc_FpDensityMorgan2 = Descriptors.FpDensityMorgan2(mol)\n",
      "        desc_FpDensityMorgan3 = Descriptors.FpDensityMorgan3(mol)\n",
      "        desc_NumRadicalElectrons = Descriptors.NumRadicalElectrons(mol)\n",
      "        desc_NumValenceElectrons = Descriptors.NumValenceElectrons(mol)\n",
      "           \n",
      "        row = np.array([desc_MolWt,\n",
      "                        desc_TPSA,\n",
      "                        desc_MolLogP,\n",
      "                        desc_NumHDonors,\n",
      "                        desc_NumHAcceptors,\n",
      "                        desc_NumRotatableBonds,\n",
      "                        desc_FpDensityMorgan1,\n",
      "                        desc_FpDensityMorgan2,\n",
      "                        desc_FpDensityMorgan3,\n",
      "                        desc_NumRadicalElectrons,\n",
      "                        desc_NumValenceElectrons])   \n",
      "    \n",
      "        if(i==0):\n",
      "            baseData=row\n",
      "        else:\n",
      "            baseData=np.vstack([baseData, row])\n",
      "        i=i+1      \n",
      "    \n",
      "    columnNames=[\"MW\",\"TPSA\", \"LogP\",\"NumHDonors\",\"NumHAcceptors\", \"NumRotatableBonds\", 'FpDensityMorgan1', 'FpDensityMorgan2', 'FpDensityMorgan3', 'NumRadicalElectrons', 'NumValenceElectrons']   \n",
      "    descriptors = pd.DataFrame(data=baseData,columns=columnNames)\n",
      "    \n",
      "    return descriptors\n",
      "175/48:\n",
      "df6_lipinski = lipinski(df6.smiles)\n",
      "df6_lipinski\n",
      "175/49:\n",
      "data = pd.concat([df6, df6_lipinski], axis=1)\n",
      "data.to_csv('df6_lipinski_Kpu.csv')\n",
      "data\n",
      "175/50:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt1.set_ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)\n",
      "plt2.set_xlabel(\"LogP\", fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt2.set_ylim(0, 50)\n",
      "plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "\n",
      "#Histogram for NumHDonors\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 30, color='#FF9A00', edgecolor='black', linewidth=0.5)\n",
      "plt3.set_xlabel(\"Number of Hydrogen Bond Donors\", fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt3.set_ylim(0, 50)\n",
      "plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for NumHAcceptors\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 30, color='#FF165D', edgecolor='black', linewidth=0.5)\n",
      "plt4.set_xlabel(\"Number of Hydrogen Bond Acceptors\", fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt4.set_ylim(0, 50)\n",
      "plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for NumRotatableBonds\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 30, color='#52006A', edgecolor='black', linewidth=0.5)\n",
      "plt5.set_xlabel(\"Number of RotatableBonds\", fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt5.set_ylim(0, 50)\n",
      "plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for TPSA\n",
      "plt6.hist(data['TPSA'], density=False, bins= 30, color='#01937C', edgecolor='black', linewidth=0.5)\n",
      "plt6.set_xlabel(\"Topolocial Plar Surface Area\", fontsize=16, fontweight='bold')\n",
      "plt6.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt6.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt6.set_ylim(0, 50)\n",
      "plt6.axvline(90, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Lipinski_basic descriptors.pdf', dpi=300)\n",
      "175/51:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "x1 = data['MW']\n",
      "x2 = data['LogP']\n",
      "x3 = data['NumHDonors']\n",
      "x4 = data['NumHAcceptors']\n",
      "x5 = data['NumRotatableBonds']\n",
      "x6 = data['TPSA']\n",
      "y = data['LogKpu']\n",
      "colors = {'HEK293':'red', 'SK-MEL-2':'blue', 'HL60':'green', 'SH-5Y':'lightblue', 'PBMC':'yellow'}\n",
      "\n",
      "#MW\n",
      "plt1.scatter(x1, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"Kpu\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#LogP\n",
      "plt2.scatter(x2, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"Kpu\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondDonors\n",
      "plt3.scatter(x3, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"Kpu\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt4.scatter(x4, y,  alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"log Kp\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt5.scatter(x5, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"Kpu\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#TPSA\n",
      "plt6.scatter(x6, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt6.set_xlabel('Topological Polar Surface Area', fontsize=16, fontweight='bold')\n",
      "plt6.set_ylabel(\"Kpu\", fontsize=16, fontweight='bold')\n",
      "plt6.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('RO5_and_Kpu.pdf', dpi=300)\n",
      "175/52:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "175/53: pwd\n",
      "175/54: df = pd.read_csv('rawdataqsar_sm.csv')\n",
      "175/55: df\n",
      "175/56: df2 =df[df.kp.notna()]\n",
      "175/57: df2\n",
      "175/58: df3 = df2.drop(['logkp', 'kp'], axis=1)\n",
      "175/59: df3\n",
      "175/60:\n",
      "def LogKpu(input):\n",
      "    LogKpu = []\n",
      "\n",
      "    for i in input['kpu']:\n",
      "        LogKpu.append(np.log10(i))\n",
      "\n",
      "    input['LogKpu'] = LogKpu\n",
      "    x = input.drop('kpu', 1)   \n",
      "    return x\n",
      "175/61: df4 = LogKpu(df3)\n",
      "175/62: df4\n",
      "175/63:\n",
      "df5 = df4.drop_duplicates(subset=['smiles'])\n",
      "df5\n",
      "175/64:\n",
      "df6 = df5.reset_index(drop=True)\n",
      "df6\n",
      "175/65:\n",
      "# Calculate Molecular Descriptors\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Descriptors, Lipinski\n",
      "\n",
      "def lipinski(smiles, verbose=False):\n",
      "\n",
      "    moldata= []\n",
      "    for elem in smiles:\n",
      "        mol=Chem.MolFromSmiles(elem) \n",
      "        moldata.append(mol)\n",
      "       \n",
      "    baseData= np.arange(1,1)\n",
      "    i=0  \n",
      "    for mol in moldata:        \n",
      "       \n",
      "        desc_MolWt = Descriptors.MolWt(mol)\n",
      "        desc_TPSA = Descriptors.TPSA(mol)\n",
      "        desc_MolLogP = Descriptors.MolLogP(mol)\n",
      "        desc_NumHDonors = Lipinski.NumHDonors(mol)\n",
      "        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)\n",
      "        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)\n",
      "        desc_FpDensityMorgan1 = Descriptors.FpDensityMorgan1(mol)\n",
      "        desc_FpDensityMorgan2 = Descriptors.FpDensityMorgan2(mol)\n",
      "        desc_FpDensityMorgan3 = Descriptors.FpDensityMorgan3(mol)\n",
      "        desc_NumRadicalElectrons = Descriptors.NumRadicalElectrons(mol)\n",
      "        desc_NumValenceElectrons = Descriptors.NumValenceElectrons(mol)\n",
      "           \n",
      "        row = np.array([desc_MolWt,\n",
      "                        desc_TPSA,\n",
      "                        desc_MolLogP,\n",
      "                        desc_NumHDonors,\n",
      "                        desc_NumHAcceptors,\n",
      "                        desc_NumRotatableBonds,\n",
      "                        desc_FpDensityMorgan1,\n",
      "                        desc_FpDensityMorgan2,\n",
      "                        desc_FpDensityMorgan3,\n",
      "                        desc_NumRadicalElectrons,\n",
      "                        desc_NumValenceElectrons])   \n",
      "    \n",
      "        if(i==0):\n",
      "            baseData=row\n",
      "        else:\n",
      "            baseData=np.vstack([baseData, row])\n",
      "        i=i+1      \n",
      "    \n",
      "    columnNames=[\"MW\",\"TPSA\", \"LogP\",\"NumHDonors\",\"NumHAcceptors\", \"NumRotatableBonds\", 'FpDensityMorgan1', 'FpDensityMorgan2', 'FpDensityMorgan3', 'NumRadicalElectrons', 'NumValenceElectrons']   \n",
      "    descriptors = pd.DataFrame(data=baseData,columns=columnNames)\n",
      "    \n",
      "    return descriptors\n",
      "175/66:\n",
      "df6_lipinski = lipinski(df6.smiles)\n",
      "df6_lipinski\n",
      "175/67:\n",
      "data = pd.concat([df6, df6_lipinski], axis=1)\n",
      "data.to_csv('df6_lipinski_Kpu.csv')\n",
      "data\n",
      "175/68:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "# Histogram for MW\n",
      "plt1.hist(data['MW'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt1.set_ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Log P\n",
      "plt2.hist(data['LogP'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)\n",
      "plt2.set_xlabel(\"LogP\", fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt2.set_ylim(0, 50)\n",
      "plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "\n",
      "#Histogram for NumHDonors\n",
      "plt3.hist(data['NumHDonors'], density=False, bins= 30, color='#FF9A00', edgecolor='black', linewidth=0.5)\n",
      "plt3.set_xlabel(\"Number of Hydrogen Bond Donors\", fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt3.set_ylim(0, 50)\n",
      "plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for NumHAcceptors\n",
      "plt4.hist(data['NumHAcceptors'], density=False, bins= 30, color='#FF165D', edgecolor='black', linewidth=0.5)\n",
      "plt4.set_xlabel(\"Number of Hydrogen Bond Acceptors\", fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt4.set_ylim(0, 50)\n",
      "plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for NumRotatableBonds\n",
      "plt5.hist(data['NumRotatableBonds'], density=False, bins= 30, color='#52006A', edgecolor='black', linewidth=0.5)\n",
      "plt5.set_xlabel(\"Number of RotatableBonds\", fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt5.set_ylim(0, 50)\n",
      "plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for TPSA\n",
      "plt6.hist(data['TPSA'], density=False, bins= 30, color='#01937C', edgecolor='black', linewidth=0.5)\n",
      "plt6.set_xlabel(\"Topolocial Plar Surface Area\", fontsize=16, fontweight='bold')\n",
      "plt6.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt6.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt6.set_ylim(0, 50)\n",
      "plt6.axvline(90, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Lipinski_basic descriptors.pdf', dpi=300)\n",
      "175/69:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "x1 = data['MW']\n",
      "x2 = data['LogP']\n",
      "x3 = data['NumHDonors']\n",
      "x4 = data['NumHAcceptors']\n",
      "x5 = data['NumRotatableBonds']\n",
      "x6 = data['TPSA']\n",
      "y = data['LogKpu']\n",
      "colors = {'HEK293':'red', 'SK-MEL-2':'blue', 'HL60':'green', 'SH-5Y':'lightblue', 'PBMC':'yellow'}\n",
      "\n",
      "#MW\n",
      "plt1.scatter(x1, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"Kpu\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#LogP\n",
      "plt2.scatter(x2, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"Kpu\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondDonors\n",
      "plt3.scatter(x3, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"Kpu\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt4.scatter(x4, y,  alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"log Kp\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt5.scatter(x5, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"Kpu\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#TPSA\n",
      "plt6.scatter(x6, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt6.set_xlabel('Topological Polar Surface Area', fontsize=16, fontweight='bold')\n",
      "plt6.set_ylabel(\"Kpu\", fontsize=16, fontweight='bold')\n",
      "plt6.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('RO5_and_Kpu.pdf', dpi=300)\n",
      "175/70:\n",
      "selection = ['smiles']\n",
      "smiles = data[selection]\n",
      "smiles.to_csv(r'smiles/new_smiles.smi', sep='\\t', index=False, header=False)\n",
      "175/71:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "x1 = data['MW']\n",
      "x2 = data['LogP']\n",
      "x3 = data['NumHDonors']\n",
      "x4 = data['NumHAcceptors']\n",
      "x5 = data['NumRotatableBonds']\n",
      "x6 = data['TPSA']\n",
      "y = data['LogKpu']\n",
      "colors = {'HEK293':'red', 'SK-MEL-2':'blue', 'HL60':'green', 'SH-5Y':'lightblue', 'PBMC':'yellow'}\n",
      "\n",
      "#MW\n",
      "plt1.scatter(x1, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"Log Kpu\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#LogP\n",
      "plt2.scatter(x2, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"Log Kpu\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondDonors\n",
      "plt3.scatter(x3, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"Kpu\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt4.scatter(x4, y,  alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"Log Kpu\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt5.scatter(x5, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"Log Kpu\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#TPSA\n",
      "plt6.scatter(x6, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt6.set_xlabel('Topological Polar Surface Area', fontsize=16, fontweight='bold')\n",
      "plt6.set_ylabel(\"Log Kpu\", fontsize=16, fontweight='bold')\n",
      "plt6.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('RO5_and_Kpu.pdf', dpi=300)\n",
      "175/72:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "x1 = data['MW']\n",
      "x2 = data['LogP']\n",
      "x3 = data['NumHDonors']\n",
      "x4 = data['NumHAcceptors']\n",
      "x5 = data['NumRotatableBonds']\n",
      "x6 = data['TPSA']\n",
      "y = data['LogKpu']\n",
      "colors = {'HEK293':'red', 'SK-MEL-2':'blue', 'HL60':'green', 'SH-5Y':'lightblue', 'PBMC':'yellow'}\n",
      "\n",
      "#MW\n",
      "plt1.scatter(x1, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"Log Kpu\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#LogP\n",
      "plt2.scatter(x2, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"Log Kpu\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondDonors\n",
      "plt3.scatter(x3, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')\n",
      "plt3.set_ylabel(\"Log Kpu\", fontsize=16, fontweight='bold')\n",
      "plt3.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt4.scatter(x4, y,  alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')\n",
      "plt4.set_ylabel(\"Log Kpu\", fontsize=16, fontweight='bold')\n",
      "plt4.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#NumHbondAcceptors\n",
      "plt5.scatter(x5, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')\n",
      "plt5.set_ylabel(\"Log Kpu\", fontsize=16, fontweight='bold')\n",
      "plt5.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#TPSA\n",
      "plt6.scatter(x6, y, alpha=0.5, c=data['Cells'].map(colors))\n",
      "plt6.set_xlabel('Topological Polar Surface Area', fontsize=16, fontweight='bold')\n",
      "plt6.set_ylabel(\"Log Kpu\", fontsize=16, fontweight='bold')\n",
      "plt6.tick_params(axis='both', which='major', labelsize=14)\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('RO5_and_Kpu.pdf', dpi=300)\n",
      "172/9:\n",
      "df = pd.read_csv('df6_lipinski_kpu.csv')\n",
      "df\n",
      "172/10: name = df['compound']\n",
      "172/11: name.shape\n",
      "172/12:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df2 = df2.drop('Unnamed: 0', axis=1)\n",
      "df3 = df2.set_index('Name')\n",
      "df3\n",
      "172/13:\n",
      "LogKpu = df3['LogKpu']\n",
      "LogKpu = LogKpu.to_frame()\n",
      "LogKpu\n",
      "172/14:\n",
      "Fp1.Name = name\n",
      "Fp2.Name = name\n",
      "Fp3.Name = name\n",
      "Fp4.Name = name\n",
      "Fp5.Name = name\n",
      "Fp6.Name = name\n",
      "Fp7.Name = name\n",
      "Fp8.Name = name\n",
      "Fp9.Name = name\n",
      "Fp10.Name = name\n",
      "Fp11.Name = name\n",
      "Fp12.Name = name\n",
      "172/15:\n",
      "Fpp1 = Fp1.drop('Name', axis=1)\n",
      "Fpp2 = Fp2.drop('Name', axis=1)\n",
      "Fpp3 = Fp3.drop('Name', axis=1)\n",
      "Fpp4 = Fp4.drop('Name', axis=1)\n",
      "Fpp5 = Fp5.drop('Name', axis=1)\n",
      "Fpp6 = Fp6.drop('Name', axis=1)\n",
      "Fpp7 = Fp7.drop('Name', axis=1)\n",
      "Fpp8 = Fp8.drop('Name', axis=1)\n",
      "Fpp9 = Fp9.drop('Name', axis=1)\n",
      "Fpp10 = Fp10.drop('Name', axis=1)\n",
      "Fpp11 = Fp11.drop('Name', axis=1)\n",
      "Fpp12 = Fp12.drop('Name', axis=1)\n",
      "172/16:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)\n",
      "    Fp_normalized\n",
      "    \n",
      "    return Fp_normalized\n",
      "172/17:\n",
      "Fp1_n  = normalized (Fpp1 )\n",
      "Fp2_n  = normalized (Fpp2 )\n",
      "Fp3_n  = normalized (Fpp3 )\n",
      "Fp4_n  = normalized (Fpp4 )\n",
      "Fp5_n  = normalized (Fpp5 )\n",
      "Fp6_n  = normalized (Fpp6 )\n",
      "Fp7_n  = normalized (Fpp7 )\n",
      "Fp8_n  = normalized (Fpp8 )\n",
      "Fp9_n  = normalized (Fpp9 )\n",
      "Fp10_n = normalized (Fpp10)\n",
      "Fp11_n = normalized (Fpp11)\n",
      "Fp12_n = normalized (Fpp12)\n",
      "172/18: Fp1_n\n",
      "172/19:\n",
      "Fp1_n[\"Name\"] = Fp1.Name\n",
      "Fp2_n[\"Name\"] = Fp2.Name\n",
      "Fp3_n[\"Name\"] = Fp3.Name\n",
      "Fp4_n[\"Name\"] = Fp4.Name\n",
      "Fp5_n[\"Name\"] = Fp5.Name\n",
      "Fp6_n[\"Name\"] = Fp6.Name\n",
      "Fp7_n[\"Name\"] = Fp7.Name\n",
      "Fp8_n[\"Name\"] = Fp8.Name\n",
      "Fp9_n[\"Name\"] = Fp9.Name\n",
      "Fp10_n[\"Name\"] = Fp10.Name\n",
      "Fp11_n[\"Name\"] = Fp11.Name\n",
      "Fp12_n[\"Name\"] = Fp12.Name\n",
      "172/20: Fp1_n\n",
      "172/21:\n",
      "Fp1_n = Fp1_n.set_index('Name')\n",
      "Fp2_n = Fp2_n.set_index('Name')\n",
      "Fp3_n = Fp3_n.set_index('Name')\n",
      "Fp4_n = Fp4_n.set_index('Name')\n",
      "Fp5_n = Fp5_n.set_index('Name')\n",
      "Fp6_n = Fp6_n.set_index('Name')\n",
      "Fp7_n = Fp7_n.set_index('Name')\n",
      "Fp8_n = Fp8_n.set_index('Name')\n",
      "Fp9_n = Fp9_n.set_index('Name')\n",
      "Fp10_n = Fp10_n.set_index('Name')\n",
      "Fp11_n = Fp11_n.set_index('Name')\n",
      "Fp12_n = Fp12_n.set_index('Name')\n",
      "172/22: Fp1_n\n",
      "172/23:\n",
      "raw1  = LogKpu.merge(Fp1_n, on='Name', how='outer')\n",
      "raw2  = LogKpu.merge(Fp2_n, on='Name', how='outer')\n",
      "raw3  = LogKpu.merge(Fp3_n, on='Name', how='outer')\n",
      "raw4  = LogKpu.merge(Fp4_n, on='Name', how='outer')\n",
      "raw5  = LogKpu.merge(Fp5_n, on='Name', how='outer')\n",
      "raw6  = LogKpu.merge(Fp6_n, on='Name', how='outer')\n",
      "raw7  = LogKpu.merge(Fp7_n, on='Name', how='outer')\n",
      "raw8  = LogKpu.merge(Fp8_n, on='Name', how='outer')\n",
      "raw9  = LogKpu.merge(Fp9_n, on='Name', how='outer')\n",
      "raw10  = LogKpu.merge(Fp10_n, on='Name', how='outer')\n",
      "raw11  = LogKpu.merge(Fp11_n, on='Name', how='outer')\n",
      "raw12  = LogKpu.merge(Fp12_n, on='Name', how='outer')\n",
      "172/24: raw1\n",
      "172/25:\n",
      "raw1 .to_csv('QSAR_Kpu/Kp_Fingerprinter.csv'               , sep=',' ,index=False)\n",
      "raw2 .to_csv('QSAR_Kpu/Kp_ExtendedFingerprinter.csv'       , sep=',' ,index=False)\n",
      "raw3 .to_csv('QSAR_Kpu/Kp_EStateFingerprinter.csv'         , sep=',' ,index=False)\n",
      "raw4 .to_csv('QSAR_Kpu/Kp_GraphOnlyFingerprinter.csv'      , sep=',' ,index=False)\n",
      "raw5 .to_csv('QSAR_Kpu/Kp_MACCSFingerprinter.csv'          , sep=',' ,index=False)\n",
      "raw6 .to_csv('QSAR_Kpu/Kp_PubchemFingerprinter.csv'        , sep=',' ,index=False)\n",
      "raw7 .to_csv('QSAR_Kpu/Kp_SubstructureFingerprinter.csv'   , sep=',' ,index=False)\n",
      "raw8 .to_csv('QSAR_Kpu/Kp_SubstructureFingerprintCount.csv', sep=',' ,index=False)\n",
      "raw9 .to_csv('QSAR_Kpu/Kp_KlekotaRothFingerprinter.csv'    , sep=',' ,index=False)\n",
      "raw10.to_csv('QSAR_Kpu/Kp_KlekotaRothFingerprintCount.csv' , sep=',' ,index=False)\n",
      "raw11.to_csv('QSAR_Kpu/Kp_AtomPairs2DFingerprinter.csv'    , sep=',' ,index=False)\n",
      "raw12.to_csv('QSAR_Kpu/Kp_AtomPairs2DFingerprintCount.csv' , sep=',' ,index=False)\n",
      "172/26:\n",
      "print (len(raw1 ),len(raw2 ),len(raw3 ),len(raw4 ),len(raw5 )\n",
      "      ,len(raw6 ),len(raw7 ),len(raw8 ),len(raw9 ),len(raw10)\n",
      "      ,len(raw11),len(raw12))\n",
      "172/27:\n",
      "print (len(raw1 .columns),len(raw2 .columns),len(raw3 .columns),len(raw4 .columns),len(raw5 )\n",
      "      ,len(raw6 .columns),len(raw7 .columns),len(raw8 .columns),len(raw9 .columns),len(raw10)\n",
      "      ,len(raw11.columns),len(raw12.columns))\n",
      "176/1: pwd\n",
      "176/2:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "176/3:\n",
      "df = pd.read_csv(\"df6_lipinski_kpu.csv\")\n",
      "df\n",
      "176/4: df1 = df.drop(['Unnamed: 0', 'smiles', 'Cells', 'compound'], axis=True)\n",
      "176/5:\n",
      "df = pd.read_csv(\"/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/df6_lipinski_kpu.csv\")\n",
      "df\n",
      "176/6: df1 = df.drop(['Unnamed: 0', 'smiles', 'Cells', 'compound'], axis=True)\n",
      "178/1: !python -m mordred --help\n",
      "178/2:\n",
      "!python -m mordred -t smiles/new_smiles.smi -o smiles/mordard.csv -d ABCIndex AcidBase AdjacencyMatrix Aromatic AtomCount\n",
      "Autocorrelation BCUT BalabanJ BaryszMatrix BertzCT BondCount CarbonTypes\n",
      "Chi Constitutional DetourMatrix DistanceMatrix EState\n",
      "EccentricConnectivityIndex ExtendedTopochemicalAtom FragmentComplexity\n",
      "Framework GeometricalIndex GravitationalIndex HydrogenBond InformationContent\n",
      "KappaShapeIndex Lipinski LogS McGowanVolume MoeType\n",
      "MolecularDistanceEdge MolecularId PathCount Polarizability\n",
      "RingCount RotatableBond SLogP TopoPSA TopologicalCharge TopologicalIndex\n",
      "VdwVolumeABC VertexAdjacencyInformation WalkCount Weight WienerIndex\n",
      "ZagrebIndex\n",
      "178/3:\n",
      "!python -m mordred -t smiles/new_smiles.smi -o smiles/mordard.csv -d ABCIndex AcidBase AdjacencyMatrix Aromatic AtomCount\n",
      "Autocorrelation BalabanJ BaryszMatrix BertzCT BondCount CarbonTypes\n",
      "Chi Constitutional DetourMatrix DistanceMatrix EState\n",
      "EccentricConnectivityIndex ExtendedTopochemicalAtom FragmentComplexity\n",
      "Framework GeometricalIndex GravitationalIndex HydrogenBond InformationContent\n",
      "KappaShapeIndex Lipinski LogS McGowanVolume MoeType\n",
      "MolecularDistanceEdge MolecularId PathCount Polarizability\n",
      "RingCount RotatableBond SLogP TopoPSA TopologicalCharge TopologicalIndex\n",
      "VdwVolumeABC VertexAdjacencyInformation WalkCount Weight WienerIndex\n",
      "ZagrebIndex\n",
      "178/4: !python -m mordred -t smiles/new_smiles.smi -o smiles/mordard.csv -d ABCIndex\n",
      "178/5: !python -m mordred -t smi smiles/new_smiles.smi -o smiles/mordard.csv -d ABCIndex\n",
      "178/6:\n",
      "!python -m mordred -t smi smiles/new_smiles.smi -o smiles/mordard.csv -d ABCIndex AcidBase AdjacencyMatrix Aromatic AtomCount\n",
      "Autocorrelation BCUT BalabanJ BaryszMatrix BertzCT BondCount CarbonTypes\n",
      "Chi Constitutional DetourMatrix DistanceMatrix EState\n",
      "EccentricConnectivityIndex ExtendedTopochemicalAtom FragmentComplexity\n",
      "Framework GeometricalIndex GravitationalIndex HydrogenBond InformationContent\n",
      "KappaShapeIndex Lipinski LogS McGowanVolume MoeType\n",
      "MolecularDistanceEdge MolecularId PathCount Polarizability\n",
      "RingCount RotatableBond SLogP TopoPSA TopologicalCharge TopologicalIndex\n",
      "VdwVolumeABC VertexAdjacencyInformation WalkCount Weight WienerIndex\n",
      "ZagrebIndex\n",
      "178/7: !python -m mordred -t smi smiles/new_smiles.smi -o smiles/mordard.csv -d ABCIndex, AcidBase\n",
      "178/8:\n",
      "!python -m mordred -t smi smiles/new_smiles.smi -o smiles/mordard.csv -d \n",
      "'ABCIndex,' (choose from 'ABCIndex', 'AcidBase', 'AdjacencyMatrix', 'Aromatic', 'AtomCount', 'Autocorrelation', 'BCUT', 'BalabanJ', 'BaryszMatrix', 'BertzCT', 'BondCount', 'CPSA', 'CarbonTypes', 'Chi', 'Constitutional', 'DetourMatrix', 'DistanceMatrix', 'EState', 'EccentricConnectivityIndex', 'ExtendedTopochemicalAtom', 'FragmentComplexity', 'Framework', 'GeometricalIndex', 'GravitationalIndex', 'HydrogenBond', 'InformationContent', 'KappaShapeIndex', 'Lipinski', 'LogS', 'McGowanVolume', 'MoRSE', 'MoeType', 'MolecularDistanceEdge', 'MolecularId', 'MomentOfInertia', 'PBF', 'PathCount', 'Polarizability', 'RingCount', 'RotatableBond', 'SLogP', 'TopoPSA', 'TopologicalCharge', 'TopologicalIndex', 'VdwVolumeABC', 'VertexAdjacencyInformation', 'WalkCount', 'Weight', 'WienerIndex', 'ZagrebIndex'\n",
      "178/9: !python -m mordred -t smi smiles/new_smiles.smi -o smiles/mordard.csv -d 'ABCIndex', 'AcidBase'\n",
      "178/10: !python -m mordred -t smi smiles/new_smiles.smi -o smiles/mordard.csv -d ('ABCIndex', 'AcidBase')\n",
      "178/11: !python -m mordred -t smi smiles/new_smiles.smi -o smiles/mordard.csv -d ABCIndex, AcidBase\n",
      "178/12: !python -m mordred -t smi smiles/new_smiles.smi -o smiles/mordard.csv -d ABCIndex AcidBase\n",
      "178/13: !python -m mordred -t smi smiles/new_smiles.smi -o smiles/mordard.csv -d ABCIndex\n",
      "178/14: !python -m mordred -t smi smiles/new_smiles.smi -o smiles/mordard.csv -d AcidBase\n",
      "178/15:\n",
      "!python -m mordred -t smi smiles/new_smiles.smi -o smiles/mordard.csv -d ABCIndex, -d AcidBase -d AdjacencyMatrix -d Aromatic -d AtomCount\n",
      "-d Autocorrelation -d BCUT -d BalabanJ -d BaryszMatrix -d BertzCT -d BondCount -d CarbonTypes\n",
      "-d Chi -d Constitutional -d DetourMatrix -d DistanceMatrix -d EState\n",
      "-d EccentricConnectivityIndex -d ExtendedTopochemicalAtom -d FragmentComplexity\n",
      "-d Framework -d GeometricalIndex -d GravitationalIndex -d HydrogenBond -d InformationContent\n",
      "-d KappaShapeIndex -d Lipinski -d LogS -d McGowanVolume -d MoeType\n",
      "-d MolecularDistanceEdge -d MolecularId -d PathCount -d Polarizability\n",
      "-d RingCount -d RotatableBond -d SLogP -d TopoPSA -d TopologicalCharge -d TopologicalIndex\n",
      "-d VdwVolumeABC -d VertexAdjacencyInformation -d WalkCount -d Weight -d WienerIndex\n",
      "-d ZagrebIndex\n",
      "178/16:\n",
      "!python -m mordred -t smi smiles/new_smiles.smi -o smiles/mordard.csv -d ABCIndex, -d AcidBase -d AdjacencyMatrix -d Aromatic -d AtomCount -d Autocorrelation -d BCUT -d BalabanJ -d BaryszMatrix -d BertzCT -d BondCount -d CarbonTypes\n",
      "-d Chi -d Constitutional -d DetourMatrix -d DistanceMatrix -d EState\n",
      "-d EccentricConnectivityIndex -d ExtendedTopochemicalAtom -d FragmentComplexity\n",
      "-d Framework -d GeometricalIndex -d GravitationalIndex -d HydrogenBond -d InformationContent\n",
      "-d KappaShapeIndex -d Lipinski -d LogS -d McGowanVolume -d MoeType\n",
      "-d MolecularDistanceEdge -d MolecularId -d PathCount -d Polarizability\n",
      "-d RingCount -d RotatableBond -d SLogP -d TopoPSA -d TopologicalCharge -d TopologicalIndex\n",
      "-d VdwVolumeABC -d VertexAdjacencyInformation -d WalkCount -d Weight -d WienerIndex\n",
      "-d ZagrebIndex\n",
      "178/17: !python -m mordred -t smi smiles/new_smiles.smi -o smiles/mordard.csv -d ABCIndex, -d AcidBase -d AdjacencyMatrix -d Aromatic -d AtomCount -d Autocorrelation -d BCUT -d BalabanJ -d BaryszMatrix -d BertzCT -d BondCount -d CarbonTypes\n",
      "178/18:\n",
      "!python -m mordred -t smi smiles/new_smiles.smi -o smiles/mordard.csv -d ABCIndex -d AcidBase -d AdjacencyMatrix -d Aromatic -d AtomCount -d Autocorrelation -d BCUT -d BalabanJ -d BaryszMatrix -d BertzCT -d BondCount -d CarbonTypes\n",
      "-d Chi -d Constitutional -d DetourMatrix -d DistanceMatrix -d EState\n",
      "-d EccentricConnectivityIndex -d ExtendedTopochemicalAtom -d FragmentComplexity\n",
      "-d Framework -d GeometricalIndex -d GravitationalIndex -d HydrogenBond -d InformationContent\n",
      "-d KappaShapeIndex -d Lipinski -d LogS -d McGowanVolume -d MoeType\n",
      "-d MolecularDistanceEdge -d MolecularId -d PathCount -d Polarizability\n",
      "-d RingCount -d RotatableBond -d SLogP -d TopoPSA -d TopologicalCharge -d TopologicalIndex\n",
      "-d VdwVolumeABC -d VertexAdjacencyInformation -d WalkCount -d Weight -d WienerIndex\n",
      "-d ZagrebIndex\n",
      "178/19:\n",
      "!python -m mordred -t smi smiles/new_smiles.smi -o smiles/mordard.csv -d ABCIndex -d AcidBase -d AdjacencyMatrix -d Aromatic -d AtomCount -d Autocorrelation -d BCUT -d BalabanJ -d BaryszMatrix -d BertzCT -d BondCount -d CarbonTypes\n",
      "-d Chi -d Constitutional -d DetourMatrix -d DistanceMatrix -d EState\n",
      "-d EccentricConnectivityIndex -d ExtendedTopochemicalAtom -d FragmentComplexity\n",
      "-d Framework -d GeometricalIndex -d GravitationalIndex -d HydrogenBond -d InformationContent\n",
      "-d KappaShapeIndex -d Lipinski -d LogS -d McGowanVolume -d MoeType\n",
      "-d MolecularDistanceEdge -d MolecularId -d PathCount -d Polarizability\n",
      "-d RingCount -d RotatableBond -d SLogP -d TopoPSA -d TopologicalCharge -d TopologicalIndex\n",
      "-d VdwVolumeABC -d VertexAdjacencyInformation -d WalkCount -d Weight -d WienerIndex\n",
      "-d ZagrebIndex\n",
      "178/20:\n",
      "-d Chi -d Constitutional -d DetourMatrix -d DistanceMatrix -d EState\n",
      "-d EccentricConnectivityIndex -d ExtendedTopochemicalAtom -d FragmentComplexity\n",
      "-d Framework -d GeometricalIndex -d GravitationalIndex -d HydrogenBond -d InformationContent\n",
      "-d KappaShapeIndex -d Lipinski -d LogS -d McGowanVolume -d MoeType\n",
      "-d MolecularDistanceEdge -d MolecularId -d PathCount -d Polarizability\n",
      "-d RingCount -d RotatableBond -d SLogP -d TopoPSA -d TopologicalCharge -d TopologicalIndex\n",
      "-d VdwVolumeABC -d VertexAdjacencyInformation -d WalkCount -d Weight -d WienerIndex\n",
      "-d ZagrebIndex\n",
      "178/21:\n",
      "!python -m mordred -t smi smiles/new_smiles.smi -o smiles/mordard.csv -d ABCIndex -d AcidBase -d AdjacencyMatrix -d Aromatic -d AtomCount -d Autocorrelation -d BCUT -d BalabanJ -d BaryszMatrix -d BertzCT -d BondCount -d CarbonTypes\n",
      "    -d Chi -d Constitutional -d DetourMatrix -d DistanceMatrix -d EState\n",
      "    -d EccentricConnectivityIndex -d ExtendedTopochemicalAtom -d FragmentComplexity\n",
      "    -d Framework -d GeometricalIndex -d GravitationalIndex -d HydrogenBond -d InformationContent\n",
      "    -d KappaShapeIndex -d Lipinski -d LogS -d McGowanVolume -d MoeType\n",
      "    -d MolecularDistanceEdge -d MolecularId -d PathCount -d Polarizability\n",
      "    -d RingCount -d RotatableBond -d SLogP -d TopoPSA -d TopologicalCharge -d TopologicalIndex\n",
      "    -d VdwVolumeABC -d VertexAdjacencyInformation -d WalkCount -d Weight -d WienerIndex\n",
      "    -d ZagrebIndex\n",
      "178/22: !python -m mordred -t smi smiles/new_smiles.smi -o smiles/mordard.csv -d ABCIndex -d AcidBase -d AdjacencyMatrix -d Aromatic -d AtomCount -d Autocorrelation -d BCUT -d BalabanJ -d BaryszMatrix -d BertzCT -d BondCount -d CarbonTypes\n",
      "178/23: !python -m mordred -t smi smiles/new_smiles.smi -o smiles/mordard.csv -d ABCIndex -d AcidBase -d AdjacencyMatrix -d Aromatic -d AtomCount -d Autocorrelation -d BCUT -d BalabanJ -d BaryszMatrix -d BertzCT -d BondCount -d CarbonTypes -d Chi -d Constitutional -d DetourMatrix -d DistanceMatrix -d EState -d EccentricConnectivityIndex -d ExtendedTopochemicalAtom -d FragmentComplexity -d Framework -d GeometricalIndex -d GravitationalIndex -d HydrogenBond -d InformationContent -d KappaShapeIndex -d Lipinski -d LogS -d McGowanVolume -d MoeType -d MolecularDistanceEdge -d MolecularId -d PathCount -d Polarizability -d RingCount -d RotatableBond -d SLogP -d TopoPSA -d TopologicalCharge -d TopologicalIndex -d VdwVolumeABC -d VertexAdjacencyInformation -d WalkCount -d Weight -d WienerIndex -d ZagrebIndex\n",
      "178/24: !python -m mordred -t smi smiles/new_smiles.smi -o smiles/mordard.csv -d ABCIndex -d AcidBase -d AdjacencyMatrix -d Aromatic -d AtomCount -d Autocorrelation -d BCUT -d BalabanJ -d BertzCT -d BondCount -d CarbonTypes -d Chi -d Constitutional -d DetourMatrix -d DistanceMatrix -d EState -d EccentricConnectivityIndex -d ExtendedTopochemicalAtom -d FragmentComplexity -d Framework -d GeometricalIndex -d GravitationalIndex -d HydrogenBond -d InformationContent -d KappaShapeIndex -d Lipinski -d LogS -d McGowanVolume -d MoeType -d MolecularDistanceEdge -d MolecularId -d PathCount -d Polarizability -d RingCount -d RotatableBond -d SLogP -d TopoPSA -d TopologicalCharge -d TopologicalIndex -d VdwVolumeABC -d VertexAdjacencyInformation -d WalkCount -d Weight -d WienerIndex -d ZagrebIndex\n",
      "178/25: !python -m mordred -t smi smiles/new_smiles.smi -o smiles/mordard.csv -d ABCIndex -d AcidBase -d AdjacencyMatrix -d Aromatic -d AtomCount -d Autocorrelation -d BCUT -d BalabanJ -d BertzCT -d BondCount -d CarbonTypes -d Chi -d Constitutional -d DistanceMatrix -d EState -d EccentricConnectivityIndex -d ExtendedTopochemicalAtom -d FragmentComplexity -d Framework -d GeometricalIndex -d GravitationalIndex -d HydrogenBond -d InformationContent -d KappaShapeIndex -d Lipinski -d LogS -d McGowanVolume -d MoeType -d MolecularDistanceEdge -d MolecularId -d PathCount -d Polarizability -d RingCount -d RotatableBond -d SLogP -d TopoPSA -d TopologicalCharge -d TopologicalIndex -d VdwVolumeABC -d VertexAdjacencyInformation -d WalkCount -d Weight -d WienerIndex -d ZagrebIndex\n",
      "179/1:\n",
      "path = r'Fingerprint/'\n",
      "\n",
      "Fp1  = pd.read_csv(path + 'Fingerprinter.csv'               , header = 0)\n",
      "Fp2  = pd.read_csv(path + 'ExtendedFingerprinter.csv'       , header = 0)\n",
      "Fp3  = pd.read_csv(path + 'EStateFingerprinter.csv'         , header = 0)\n",
      "Fp4  = pd.read_csv(path + 'GraphOnlyFingerprinter.csv'      , header = 0)\n",
      "Fp5  = pd.read_csv(path + 'MACCSFingerprinter.csv'          , header = 0)\n",
      "Fp6  = pd.read_csv(path + 'PubchemFingerprinter.csv'        , header = 0)\n",
      "Fp7  = pd.read_csv(path + 'SubstructureFingerprinter.csv'   , header = 0)\n",
      "Fp8  = pd.read_csv(path + 'SubstructureFingerprintCount.csv', header = 0)\n",
      "Fp9  = pd.read_csv(path + 'KlekotaRothFingerprinter.csv'    , header = 0)\n",
      "Fp10 = pd.read_csv(path + 'KlekotaRothFingerprintCount.csv' , header = 0)\n",
      "Fp11 = pd.read_csv(path + 'AtomPairs2DFingerprinter.csv'    , header = 0)\n",
      "Fp12 = pd.read_csv(path + 'AtomPairs2DFingerprintCount.csv' , header = 0)\n",
      "Fp13  = pd.read_csv(path + 'mordard.csv'               , header = 0)\n",
      "179/2:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "179/3:\n",
      "path = r'Fingerprint/'\n",
      "\n",
      "Fp1  = pd.read_csv(path + 'Fingerprinter.csv'               , header = 0)\n",
      "Fp2  = pd.read_csv(path + 'ExtendedFingerprinter.csv'       , header = 0)\n",
      "Fp3  = pd.read_csv(path + 'EStateFingerprinter.csv'         , header = 0)\n",
      "Fp4  = pd.read_csv(path + 'GraphOnlyFingerprinter.csv'      , header = 0)\n",
      "Fp5  = pd.read_csv(path + 'MACCSFingerprinter.csv'          , header = 0)\n",
      "Fp6  = pd.read_csv(path + 'PubchemFingerprinter.csv'        , header = 0)\n",
      "Fp7  = pd.read_csv(path + 'SubstructureFingerprinter.csv'   , header = 0)\n",
      "Fp8  = pd.read_csv(path + 'SubstructureFingerprintCount.csv', header = 0)\n",
      "Fp9  = pd.read_csv(path + 'KlekotaRothFingerprinter.csv'    , header = 0)\n",
      "Fp10 = pd.read_csv(path + 'KlekotaRothFingerprintCount.csv' , header = 0)\n",
      "Fp11 = pd.read_csv(path + 'AtomPairs2DFingerprinter.csv'    , header = 0)\n",
      "Fp12 = pd.read_csv(path + 'AtomPairs2DFingerprintCount.csv' , header = 0)\n",
      "Fp13  = pd.read_csv(path + 'mordard.csv'               , header = 0)\n",
      "179/4: Fp1.shape\n",
      "179/5:\n",
      "df = pd.read_csv('df6_lipinski_kpu.csv')\n",
      "df\n",
      "179/6: name = df['compound']\n",
      "179/7: name.shape\n",
      "179/8:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df2 = df2.drop('Unnamed: 0', axis=1)\n",
      "df3 = df2.set_index('Name')\n",
      "df3\n",
      "179/9:\n",
      "LogKpu = df3['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']\n",
      "LogKpu = LogKpu.to_frame()\n",
      "LogKpu\n",
      "179/10:\n",
      "LogKpu = df3[['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]\n",
      "LogKpu = LogKpu.to_frame()\n",
      "LogKpu\n",
      "179/11:\n",
      "LogKpu = df3[['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]\n",
      "LogKpu\n",
      "179/12:\n",
      "Fp1.Name = name\n",
      "Fp2.Name = name\n",
      "Fp3.Name = name\n",
      "Fp4.Name = name\n",
      "Fp5.Name = name\n",
      "Fp6.Name = name\n",
      "Fp7.Name = name\n",
      "Fp8.Name = name\n",
      "Fp9.Name = name\n",
      "Fp10.Name = name\n",
      "Fp11.Name = name\n",
      "Fp12.Name = name\n",
      "Fp13.Name = name\n",
      "179/13: Fp13\n",
      "179/14: name = df['compound']\n",
      "179/15: name.shape\n",
      "179/16:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df2 = df2.drop('Unnamed: 0', axis=1)\n",
      "df3 = df2.set_index('Name')\n",
      "df3\n",
      "179/17:\n",
      "LogKpu = df3[['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]\n",
      "LogKpu\n",
      "179/18:\n",
      "Fp1.Name = name\n",
      "Fp2.Name = name\n",
      "Fp3.Name = name\n",
      "Fp4.Name = name\n",
      "Fp5.Name = name\n",
      "Fp6.Name = name\n",
      "Fp7.Name = name\n",
      "Fp8.Name = name\n",
      "Fp9.Name = name\n",
      "Fp10.Name = name\n",
      "Fp11.Name = name\n",
      "Fp12.Name = name\n",
      "Fp13.Name = name\n",
      "179/19:\n",
      "Fpp1 = Fp1.drop('Name', axis=1)\n",
      "Fpp2 = Fp2.drop('Name', axis=1)\n",
      "Fpp3 = Fp3.drop('Name', axis=1)\n",
      "Fpp4 = Fp4.drop('Name', axis=1)\n",
      "Fpp5 = Fp5.drop('Name', axis=1)\n",
      "Fpp6 = Fp6.drop('Name', axis=1)\n",
      "Fpp7 = Fp7.drop('Name', axis=1)\n",
      "Fpp8 = Fp8.drop('Name', axis=1)\n",
      "Fpp9 = Fp9.drop('Name', axis=1)\n",
      "Fpp10 = Fp10.drop('Name', axis=1)\n",
      "Fpp11 = Fp11.drop('Name', axis=1)\n",
      "Fpp12 = Fp12.drop('Name', axis=1)\n",
      "Fpp13 = Fp13.drop('Name', axis=1)\n",
      "179/20: Fp13\n",
      "179/21: Fp13.rename {name:Name}\n",
      "179/22: Fp13.rename({name:Name})\n",
      "179/23: Fp13.rename(name:Name)\n",
      "179/24: Fp13.rename(name: Name)\n",
      "179/25: Fp13.rename(name = Name)\n",
      "179/26: Fp13.rename({'name': 'Name'})\n",
      "179/27:\n",
      "Fp13 = Fp13.rename({'name': 'Name'})\n",
      "Fp13\n",
      "179/28:\n",
      "Fp13 = Fp13.rename({'Name': 'name'})\n",
      "Fp13\n",
      "179/29:\n",
      "Fp13 = Fp13.rename({'name': 'Mame'})\n",
      "Fp13\n",
      "179/30:\n",
      "Fp13 = Fp13.rename(columns={'name': 'Name'})\n",
      "Fp13\n",
      "179/31:\n",
      "Fp1.Name = name\n",
      "Fp2.Name = name\n",
      "Fp3.Name = name\n",
      "Fp4.Name = name\n",
      "Fp5.Name = name\n",
      "Fp6.Name = name\n",
      "Fp7.Name = name\n",
      "Fp8.Name = name\n",
      "Fp9.Name = name\n",
      "Fp10.Name = name\n",
      "Fp11.Name = name\n",
      "Fp12.Name = name\n",
      "Fp13.Name = name\n",
      "179/32:\n",
      "Fpp1 = Fp1.drop('Name', axis=1)\n",
      "Fpp2 = Fp2.drop('Name', axis=1)\n",
      "Fpp3 = Fp3.drop('Name', axis=1)\n",
      "Fpp4 = Fp4.drop('Name', axis=1)\n",
      "Fpp5 = Fp5.drop('Name', axis=1)\n",
      "Fpp6 = Fp6.drop('Name', axis=1)\n",
      "Fpp7 = Fp7.drop('Name', axis=1)\n",
      "Fpp8 = Fp8.drop('Name', axis=1)\n",
      "Fpp9 = Fp9.drop('Name', axis=1)\n",
      "Fpp10 = Fp10.drop('Name', axis=1)\n",
      "Fpp11 = Fp11.drop('Name', axis=1)\n",
      "Fpp12 = Fp12.drop('Name', axis=1)\n",
      "Fpp13 = Fp13.drop('Name', axis=1)\n",
      "179/33:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)\n",
      "    Fp_normalized\n",
      "    \n",
      "    return Fp_normalized\n",
      "179/34:\n",
      "Fp1_n  = normalized (Fpp1 )\n",
      "Fp2_n  = normalized (Fpp2 )\n",
      "Fp3_n  = normalized (Fpp3 )\n",
      "Fp4_n  = normalized (Fpp4 )\n",
      "Fp5_n  = normalized (Fpp5 )\n",
      "Fp6_n  = normalized (Fpp6 )\n",
      "Fp7_n  = normalized (Fpp7 )\n",
      "Fp8_n  = normalized (Fpp8 )\n",
      "Fp9_n  = normalized (Fpp9 )\n",
      "Fp10_n = normalized (Fpp10)\n",
      "Fp11_n = normalized (Fpp11)\n",
      "Fp12_n = normalized (Fpp12)\n",
      "Fp13_n = normalized (Fpp13)\n",
      "179/35: Fp13_n\n",
      "179/36:\n",
      "Fp1_n[\"Name\"] = Fp1.Name\n",
      "Fp2_n[\"Name\"] = Fp2.Name\n",
      "Fp3_n[\"Name\"] = Fp3.Name\n",
      "Fp4_n[\"Name\"] = Fp4.Name\n",
      "Fp5_n[\"Name\"] = Fp5.Name\n",
      "Fp6_n[\"Name\"] = Fp6.Name\n",
      "Fp7_n[\"Name\"] = Fp7.Name\n",
      "Fp8_n[\"Name\"] = Fp8.Name\n",
      "Fp9_n[\"Name\"] = Fp9.Name\n",
      "Fp10_n[\"Name\"] = Fp10.Name\n",
      "Fp11_n[\"Name\"] = Fp11.Name\n",
      "Fp12_n[\"Name\"] = Fp12.Name\n",
      "Fp13_n[\"Name\"] = Fp13.Name\n",
      "179/37: Fp1_n\n",
      "179/38:\n",
      "Fp1_n = Fp1_n.set_index('Name')\n",
      "Fp2_n = Fp2_n.set_index('Name')\n",
      "Fp3_n = Fp3_n.set_index('Name')\n",
      "Fp4_n = Fp4_n.set_index('Name')\n",
      "Fp5_n = Fp5_n.set_index('Name')\n",
      "Fp6_n = Fp6_n.set_index('Name')\n",
      "Fp7_n = Fp7_n.set_index('Name')\n",
      "Fp8_n = Fp8_n.set_index('Name')\n",
      "Fp9_n = Fp9_n.set_index('Name')\n",
      "Fp10_n = Fp10_n.set_index('Name')\n",
      "Fp11_n = Fp11_n.set_index('Name')\n",
      "Fp12_n = Fp12_n.set_index('Name')\n",
      "Fp13_n = Fp13_n.set_index('Name')\n",
      "179/39: Fp1_n\n",
      "179/40:\n",
      "Fp1_n .to_csv('Fp_normalized/Fingerprinter.csv'               , sep=',' ,index=False)\n",
      "Fp2_n .to_csv('Fp_normalized/ExtendedFingerprinter.csv'       , sep=',' ,index=False)\n",
      "Fp3_n .to_csv('Fp_normalized/EStateFingerprinter.csv'         , sep=',' ,index=False)\n",
      "Fp4_n .to_csv('Fp_normalized/GraphOnlyFingerprinter.csv'      , sep=',' ,index=False)\n",
      "Fp5_n .to_csv('Fp_normalized/MACCSFingerprinter.csv'          , sep=',' ,index=False)\n",
      "Fp6_n .to_csv('Fp_normalized/PubchemFingerprinter.csv'        , sep=',' ,index=False)\n",
      "Fp7_n .to_csv('Fp_normalized/SubstructureFingerprinter.csv'   , sep=',' ,index=False)\n",
      "Fp8_n .to_csv('Fp_normalized/SubstructureFingerprintCount.csv', sep=',' ,index=False)\n",
      "Fp9_n .to_csv('Fp_normalized/KlekotaRothFingerprinter.csv'    , sep=',' ,index=False)\n",
      "Fp10_n.to_csv('Fp_normalized/KlekotaRothFingerprintCount.csv' , sep=',' ,index=False)\n",
      "Fp11_n.to_csv('Fp_normalized/AtomPairs2DFingerprinter.csv'    , sep=',' ,index=False)\n",
      "Fp12_n.to_csv('Fp_normalized/AtomPairs2DFingerprintCount.csv' , sep=',' ,index=False)\n",
      "Fp13_n.to_csv('Fp_normalized/AtomPairs2DFingerprintCount.csv' , sep=',' ,index=False)\n",
      "179/41: raw13  = LogKpu.merge(Fp13_n, on='Name', how='outer')\n",
      "179/42: raw13\n",
      "179/43:\n",
      "\n",
      "raw13.to_csv('QSAR_Kpu/Mordard.csv' , sep=',' ,index=False)\n",
      "179/44:\n",
      "print (len(raw1 ),len(raw2 ),len(raw3 ),len(raw4 ),len(raw5 )\n",
      "      ,len(raw6 ),len(raw7 ),len(raw8 ),len(raw9 ),len(raw10)\n",
      "      ,len(raw11),len(raw12))\n",
      "179/45: print (len(raw13)\n",
      "179/46: print (len(raw13))\n",
      "179/47: print (len(raw13 .columns))\n",
      "177/1:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "177/2:\n",
      "df = pd.read_csv(\"QSAR_Kpu/Mordard.csv\")\n",
      "df\n",
      "177/3:\n",
      "X = df.drop(['LogKpu'], axis=1)\n",
      "X.shape\n",
      "177/4:\n",
      "y = df1['LogKp']\n",
      "y\n",
      "177/5:\n",
      "y = df['LogKp']\n",
      "y\n",
      "177/6:\n",
      "y = df['LogKpu']\n",
      "y\n",
      "177/7: y.shape\n",
      "177/8:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "X = sel.fit_transform(X)\n",
      "X.shape\n",
      "177/9:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.10))\n",
      "X = sel.fit_transform(X)\n",
      "X.shape\n",
      "177/10:\n",
      "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict\n",
      "kf =KFold(n_splits=10, shuffle=False)\n",
      "177/11:\n",
      "cnt = 1\n",
      "# split()  method generate indices to split data into training and test set.\n",
      "for train_index, test_index in kf.split(X, y):\n",
      "    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n",
      "    cnt += 1\n",
      "177/12:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "RF = RandomForestRegressor(n_estimators=400)\n",
      "y_pred = cross_val_predict(RF, X, y, cv=kf)\n",
      "177/13:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.10))\n",
      "X = sel.fit_transform(X)\n",
      "X.shape\n",
      "179/48: raw13.isnull()\n",
      "179/49: raw13.isnull(sum)\n",
      "179/50: raw13.isna()\n",
      "177/14: df.isnull()\n",
      "177/15: df.isnull().sum\n",
      "177/16: df.isna().sum\n",
      "177/17: df.dropna()\n",
      "177/18: df.dropna(column)\n",
      "177/19: df.fillna(0)\n",
      "177/20:\n",
      "X = df.drop(['LogKpu'], axis=1)\n",
      "X.shape\n",
      "177/21:\n",
      "y = df['LogKpu']\n",
      "y\n",
      "177/22: y.shape\n",
      "177/23:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.10))\n",
      "X = sel.fit_transform(X)\n",
      "X.shape\n",
      "177/24:\n",
      "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict\n",
      "kf =KFold(n_splits=10, shuffle=False)\n",
      "177/25:\n",
      "cnt = 1\n",
      "# split()  method generate indices to split data into training and test set.\n",
      "for train_index, test_index in kf.split(X, y):\n",
      "    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n",
      "    cnt += 1\n",
      "177/26:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "RF = RandomForestRegressor(n_estimators=400)\n",
      "y_pred = cross_val_predict(RF, X, y, cv=kf)\n",
      "177/27:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)\n",
      "    Fp_normalized\n",
      "    \n",
      "    return Fp_normalized\n",
      "177/28: normalized(df)\n",
      "177/29: df2 = normalized(df)\n",
      "177/30:\n",
      "X = df2.drop(['LogKpu'], axis=1)\n",
      "X.shape\n",
      "177/31:\n",
      "y = df2['LogKpu']\n",
      "y\n",
      "177/32:\n",
      "X = df.drop(['LogKpu'], axis=1)\n",
      "X.shape\n",
      "177/33:\n",
      "y = df['LogKpu']\n",
      "y\n",
      "177/34: df2 = normalized(X)\n",
      "177/35: X = normalized(X)\n",
      "177/36:\n",
      "X = normalized(X)\n",
      "X.shape\n",
      "177/37: X.isna()\n",
      "177/38:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.10))\n",
      "X = sel.fit_transform(X)\n",
      "X.shape\n",
      "177/39:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.10))\n",
      "X = sel.fit_transform(X)\n",
      "X\n",
      "177/40:\n",
      "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict\n",
      "kf =KFold(n_splits=10, shuffle=False)\n",
      "177/41:\n",
      "cnt = 1\n",
      "# split()  method generate indices to split data into training and test set.\n",
      "for train_index, test_index in kf.split(X, y):\n",
      "    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n",
      "    cnt += 1\n",
      "177/42:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "RF = RandomForestRegressor(n_estimators=400)\n",
      "y_pred = cross_val_predict(RF, X, y, cv=kf)\n",
      "177/43:\n",
      "X = normalized(X)\n",
      "X = np.nan_to_num(X)\n",
      "177/44: X = normalized(X)\n",
      "177/45:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)\n",
      "    Fp_normalized\n",
      "    \n",
      "    return Fp_normalized\n",
      "177/46: X = normalized(X)\n",
      "177/47: normalized(X)\n",
      "177/48:\n",
      "X = df.drop(['LogKpu'], axis=1)\n",
      "X.shape\n",
      "177/49:\n",
      "y = df['LogKpu']\n",
      "y\n",
      "177/50:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "177/51: pwd\n",
      "177/52:\n",
      "df = pd.read_csv(\"QSAR_Kpu/Mordard.csv\")\n",
      "df\n",
      "177/53: df.fillna(0)\n",
      "177/54:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)\n",
      "    Fp_normalized\n",
      "    \n",
      "    return Fp_normalized\n",
      "177/55:\n",
      "X = df.drop(['LogKpu'], axis=1)\n",
      "X.shape\n",
      "177/56:\n",
      "y = df['LogKpu']\n",
      "y\n",
      "177/57: y.shape\n",
      "177/58: normalized(X)\n",
      "177/59: X = normalized(X)\n",
      "177/60: X = np.nan_to_num(X)\n",
      "177/61:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.10))\n",
      "X = sel.fit_transform(X)\n",
      "X\n",
      "177/62:\n",
      "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict\n",
      "kf =KFold(n_splits=10, shuffle=False)\n",
      "177/63:\n",
      "cnt = 1\n",
      "# split()  method generate indices to split data into training and test set.\n",
      "for train_index, test_index in kf.split(X, y):\n",
      "    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n",
      "    cnt += 1\n",
      "177/64:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "RF = RandomForestRegressor(n_estimators=400)\n",
      "y_pred = cross_val_predict(RF, X, y, cv=kf)\n",
      "181/1: print('hello world')\n",
      "183/1:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "185/1:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "185/2: pwd\n",
      "185/3:\n",
      "df = pd.read_csv(\"QSAR_Kpu/Mordard.csv\")\n",
      "df\n",
      "185/4:\n",
      "df = pd.read_csv(\"QSAR_Kpu/Mordard.csv\")\n",
      "df\n",
      "189/1:\n",
      "#import important library\n",
      "import numpy as pd\n",
      "import pandas as pd\n",
      "189/2: df = pd.read_csv('df6_lipinski_Kpu.csv')\n",
      "189/3: df\n",
      "189/4: df.head(6)\n",
      "189/5: df.shape\n",
      "189/6: df = df.drop(df['Unnamed: 0'])\n",
      "189/7: df = df.drop(df['Unnamed: 0'])\n",
      "189/8: df\n",
      "189/9: df = df.drop(['Unnamed: 0'])\n",
      "189/10: df = df.drop(['Unnamed: 0'], axis = 1)\n",
      "189/11: df\n",
      "189/12: df.shape\n",
      "189/13: df = pd.read_csv('df6_lipinski_Kpu.csv')\n",
      "189/14: df.head(6)\n",
      "189/15: df.head(6)\n",
      "189/16: df.shape\n",
      "189/17: df1 = df.drop(['Unnamed: 0'], axis = 1)\n",
      "189/18: df\n",
      "189/19: df1\n",
      "189/20:\n",
      "#import important library\n",
      "import numpy as pd\n",
      "import pandas as pd\n",
      "from sklearn import svm, metrics, clone\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.neural_network import MLPClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "from sklearn.metrics import auc, accuracy_score, recall_score\n",
      "from sklearn.metrics import roc_curve, roc_auc_score\n",
      "import matplotlib.pyplot as plt\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import MACCSkeys\n",
      "from rdkit.Chem.AllChem import GetMorganFingerprintAsBitVect\n",
      "189/21: df1 = df[['compound', 'smiles'], 'LogKpu']\n",
      "189/22: df1 = df[['compound', 'smiles', 'LogKpu']]\n",
      "189/23: df1\n",
      "189/24:\n",
      "def smilestofp (smiles, method=\"maccs\", n_bits=2048):\n",
      "     # convert smiles to RDKit mol object\n",
      "    mol = Chem.MolFromSmiles(smiles)\n",
      "\n",
      "    if method == \"maccs\":\n",
      "        return np.array(MACCSkeys.GenMACCSKeys(mol))\n",
      "    if method == \"morgan2\":\n",
      "        return np.array(GetMorganFingerprintAsBitVect(mol, 2, nBits=n_bits))\n",
      "    if method == \"morgan3\":\n",
      "        return np.array(GetMorganFingerprintAsBitVect(mol, 3, nBits=n_bits))\n",
      "    else:\n",
      "        # NBVAL_CHECK_OUTPUT\n",
      "        print(f\"Warning: Wrong method specified: {method}. Default will be used instead.\")\n",
      "        return np.array(MACCSkeys.GenMACCSKeys(mol))\n",
      "189/25:\n",
      "# Add column for fingerprint\n",
      "df1[\"fp\"] = df1[\"smiles\"].apply(smiles_to_fp)\n",
      "compound_df.head(3)\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "189/26:\n",
      "def smiles_to_fp (smiles, method=\"maccs\", n_bits=2048):\n",
      "     # convert smiles to RDKit mol object\n",
      "    mol = Chem.MolFromSmiles(smiles)\n",
      "\n",
      "    if method == \"maccs\":\n",
      "        return np.array(MACCSkeys.GenMACCSKeys(mol))\n",
      "    if method == \"morgan2\":\n",
      "        return np.array(GetMorganFingerprintAsBitVect(mol, 2, nBits=n_bits))\n",
      "    if method == \"morgan3\":\n",
      "        return np.array(GetMorganFingerprintAsBitVect(mol, 3, nBits=n_bits))\n",
      "    else:\n",
      "        # NBVAL_CHECK_OUTPUT\n",
      "        print(f\"Warning: Wrong method specified: {method}. Default will be used instead.\")\n",
      "        return np.array(MACCSkeys.GenMACCSKeys(mol))\n",
      "189/27:\n",
      "# Add column for fingerprint\n",
      "df1[\"fp\"] = df1[\"smiles\"].apply(smiles_to_fp)\n",
      "compound_df.head(3)\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "189/28:\n",
      "#import important library\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn import svm, metrics, clone\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.neural_network import MLPClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "from sklearn.metrics import auc, accuracy_score, recall_score\n",
      "from sklearn.metrics import roc_curve, roc_auc_score\n",
      "import matplotlib.pyplot as plt\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import MACCSkeys\n",
      "from rdkit.Chem.AllChem import GetMorganFingerprintAsBitVect\n",
      "189/29:\n",
      "# Add column for fingerprint\n",
      "df1[\"fp\"] = df1[\"smiles\"].apply(smiles_to_fp)\n",
      "compound_df.head(3)\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "189/30:\n",
      "# Add column for fingerprint\n",
      "df1[\"fp\"] = df1[\"smiles\"].apply(smiles_to_fp)\n",
      "df1.head(3)\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "189/31:\n",
      "compound_df = df1.copy()\n",
      "# Add column for fingerprint\n",
      "compound_df[\"fp\"] = compound_df[\"smiles\"].apply(smiles_to_fp)\n",
      "compound_df.head(3)\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "189/32:\n",
      "# Fix seed for reproducible results\n",
      "SEED = 22\n",
      "seed_everything(SEED)\n",
      "189/33:\n",
      "# Fix seed for reproducible results\n",
      "import random\n",
      "\n",
      "random.seed(10)\n",
      "189/34:\n",
      "# Fix seed for reproducible results\n",
      "import random\n",
      "\n",
      "seed = random.seed(10)\n",
      "189/35:\n",
      "# Fix seed for reproducible results\n",
      "import random\n",
      "\n",
      "seed = random.seed(10)\n",
      "seed\n",
      "189/36:\n",
      "# Fix seed for reproducible results\n",
      "import random\n",
      "\n",
      "seed = random.seed(10)\n",
      "seed\n",
      "189/37: seed\n",
      "189/38:\n",
      "fingerprint_to_model = compound_df.fp.tolist()\n",
      "label_to_model = compound_df.LohKpu.tolist()\n",
      "train_internal, train_external, test_internal, train_external = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "189/39:\n",
      "fingerprint_to_model = compound_df.fp.tolist()\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "train_internal, train_external, test_internal, train_external = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "189/40: trian_internal.shape\n",
      "189/41: train_internal.shape\n",
      "189/42:\n",
      "fingerprint_to_model = compound_df.fp.tolist()\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "train_internal_x, train_external_y, test_internal_x, test_external-y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "print(\"Training data size:\", len(train))\n",
      "print(\"Test data size:\", len(test))\n",
      "189/43:\n",
      "fingerprint_to_model = compound_df.fp.tolist()\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "train_internal_x, train_external_y, test_internal_x, test_external_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "print(\"Training data size:\", len(train))\n",
      "print(\"Test data size:\", len(test))\n",
      "189/44:\n",
      "fingerprint_to_model = compound_df.fp.tolist()\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "train_internal_x, train_external_y, test_internal_x, test_external_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "print(\"Training data size:\", len(train_internal))\n",
      "print(\"Test data size:\", len(train_external))\n",
      "189/45:\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=SEED)\n",
      "189/46:\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=n_folds, shuffle=True, random_state=SEED)\n",
      "189/47:\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "189/48:\n",
      "from sklearn.linear_model import LinearRegression\n",
      "reg = LinearRegression()\n",
      "189/49:\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "cross_validation_score(reg, train_internal_x, train_external_y, cv=kf)\n",
      "189/50:\n",
      "from sklearn.model_selection import cross_validation_score\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "cross_validation_score(reg, train_internal_x, train_external_y, cv=kf)\n",
      "189/51:\n",
      "from sklearn.model_selection import cross_val_score\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "cross_validation_score(reg, train_internal_x, train_external_y, cv=kf)\n",
      "189/52:\n",
      "from sklearn.model_selection import cross_val_score\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "cross_val_score(reg, train_internal_x, train_external_y, cv=kf)\n",
      "189/53:\n",
      "from sklearn.model_selection import cross_val_score\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "cross_val_score(reg, train_internal_x, train_internal_y, cv=kf)\n",
      "189/54:\n",
      "fingerprint_to_model = compound_df.fp.tolist()\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "train_internal_x, train_internal_y, test_internal_x, test_external_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "print(\"Training data size:\", len(train_internal))\n",
      "print(\"Test data size:\", len(train_external))\n",
      "189/55:\n",
      "from sklearn.linear_model import LinearRegression\n",
      "reg = LinearRegression()\n",
      "189/56:\n",
      "from sklearn.model_selection import cross_val_score\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "cross_val_score(reg, train_internal_x, train_internal_y, cv=kf)\n",
      "189/57: train_internal_y\n",
      "189/58: train_internal_y.shape\n",
      "189/59: len(train_internal_y)\n",
      "189/60: len(train_internal_x)\n",
      "189/61: len(train_internal_y)\n",
      "189/62: train_internal_x\n",
      "189/63:\n",
      "fingerprint_to_model = compound_df.fp\n",
      "label_to_model = compound_df.LogKpu\n",
      "train_internal_x, train_internal_y, test_internal_x, test_external_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "print(\"Training data size:\", len(train_internal))\n",
      "print(\"Test data size:\", len(train_external))\n",
      "189/64:\n",
      "from sklearn.linear_model import LinearRegression\n",
      "reg = LinearRegression()\n",
      "189/65:\n",
      "from sklearn.model_selection import cross_val_score\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "cross_val_score(reg, train_internal_x, train_internal_y, cv=kf)\n",
      "189/66:\n",
      "fingerprint_to_model = compound_df.fp.tolist()\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "train_internal_x, train_internal_y, test_internal_x, test_external_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "print(\"Training data size:\", len(train_internal))\n",
      "print(\"Test data size:\", len(train_external))\n",
      "189/67:\n",
      "from sklearn.linear_model import LinearRegression\n",
      "reg = LinearRegression()\n",
      "189/68:\n",
      "from sklearn.model_selection import cross_val_score\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "cross_val_score(reg, train_internal_x, train_internal_y, cv=kf)\n",
      "189/69:\n",
      "fingerprint_to_model = compound_df.fp.tolist()\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "train_internal_x, train_external_y, test_internal_x, test_external_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "print(\"Training data size:\", len(train_internal))\n",
      "print(\"Test data size:\", len(train_external))\n",
      "189/70:\n",
      "from sklearn.linear_model import LinearRegression\n",
      "reg = LinearRegression()\n",
      "189/71:\n",
      "from sklearn.model_selection import cross_val_score\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "cross_val_score(reg, train_internal_x, train_internal_x, cv=kf)\n",
      "189/72:\n",
      "from sklearn.model_selection import cross_val_score\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "cross_val_score(reg, train_internal_x, train_internal_x, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "189/73:\n",
      "from sklearn.model_selection import cross_val_score\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "scores=cross_val_score(reg, train_internal_x, train_internal_x, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "189/74:\n",
      "def model_performance(ml_model, test_x, test_y, verbose=True):\n",
      "    \"\"\"\n",
      "    Helper function to calculate model performance\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    ml_model: sklearn model object\n",
      "        The machine learning model to train.\n",
      "    test_x: list\n",
      "        Molecular fingerprints for test set.\n",
      "    test_y: list\n",
      "        Associated activity labels for test set.\n",
      "    verbose: bool\n",
      "        Print performance measure (default = True)\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    tuple:\n",
      "        Accuracy, sensitivity, specificity, auc on test set.\n",
      "    \"\"\"\n",
      "\n",
      "    # Prediction probability on test set\n",
      "    test_prob = ml_model.predict_proba(test_x)[:, 1]\n",
      "\n",
      "    # Prediction class on test set\n",
      "    test_pred = ml_model.predict(test_x)\n",
      "\n",
      "    # Performance of model on test set\n",
      "    accuracy = accuracy_score(test_y, test_pred)\n",
      "    sens = recall_score(test_y, test_pred)\n",
      "    spec = recall_score(test_y, test_pred, pos_label=0)\n",
      "    auc = roc_auc_score(test_y, test_prob)\n",
      "\n",
      "    if verbose:\n",
      "        # Print performance results\n",
      "        # NBVAL_CHECK_OUTPUT        print(f\"Accuracy: {accuracy:.2}\")\n",
      "        print(f\"Sensitivity: {sens:.2f}\")\n",
      "        print(f\"Specificity: {spec:.2f}\")\n",
      "        print(f\"AUC: {auc:.2f}\")\n",
      "\n",
      "    return accuracy, sens, spec, auc\n",
      "189/75:\n",
      "from sklearn.model_selection import cross_val_score\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "scores=cross_val_score(reg, train_internal_x, train_internal_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "189/76:\n",
      "from sklearn.model_selection import cross_val_score\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "scores=cross_val_score(reg, train_internal_x, train_internal_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "189/77:\n",
      "from sklearn.model_selection import cross_val_score\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "scores=cross_val_score(reg, train_internal_x, train_external_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "189/78:\n",
      "fingerprint_to_model = compound_df.fp.tolist()\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "train_internal_x, train_external_x, test_internal_y, test_external_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "print(\"Training data size:\", len(train_internal))\n",
      "print(\"Test data size:\", len(train_external))\n",
      "189/79:\n",
      "from sklearn.model_selection import cross_val_score\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "scores=cross_val_score(reg, train_internal_x, train_internal_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "189/80:\n",
      "fingerprint_to_model = compound_df.fp.tolist()\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "print(\"Training data size:\", len(train_internal))\n",
      "print(\"Test data size:\", len(train_external))\n",
      "189/81:\n",
      "from sklearn.linear_model import LinearRegression\n",
      "reg = LinearRegression()\n",
      "189/82:\n",
      "from sklearn.model_selection import cross_val_score\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "scores=cross_val_score(reg, train_internal_x, train_internal_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "189/83:\n",
      "from sklearn.model_selection import cross_val_score\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "scores=cross_val_score(reg, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "189/84:\n",
      "from sklearn.model_selection import cross_val_score\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "scores=cross_val_score(reg, train_x, train_y, cv=kf)\n",
      "scores\n",
      "189/85:\n",
      "from sklearn.model_selection import cross_val_score\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "scores=cross_val_score(reg, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "189/86:\n",
      "from sklearn.model_selection import cross_val_score\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "scores=cross_val_score(reg, train_x, train_x, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "189/87:\n",
      "from sklearn.model_selection import cross_val_score\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "scores=cross_val_score(reg, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "189/88:\n",
      "fingerprint_to_model = compound_df.fp.tolist()\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(train_x))\n",
      "189/89:\n",
      "fingerprint_to_model = compound_df.fp.tolist()\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "189/90:\n",
      "from sklearn.model_selection import cross_val_score\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "scores=cross_val_score(reg, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "189/91: np.any(np.isnan(fingerprint_to_model))\n",
      "189/92: np.any(np.isnan(label_to_model))\n",
      "189/93: label_to_model\n",
      "189/94:\n",
      "df1 = df[['compound', 'smiles', 'LogKpu']]\n",
      "df2 =df1[df1.kp.notna()]\n",
      "189/95:\n",
      "df1 = df[['compound', 'smiles', 'LogKpu']]\n",
      "df2 =df1[df1.LogKpu.notna()]\n",
      "189/96: df2\n",
      "189/97:\n",
      "def smiles_to_fp (smiles, method=\"maccs\", n_bits=2048):\n",
      "     # convert smiles to RDKit mol object\n",
      "    mol = Chem.MolFromSmiles(smiles)\n",
      "\n",
      "    if method == \"maccs\":\n",
      "        return np.array(MACCSkeys.GenMACCSKeys(mol))\n",
      "    if method == \"morgan2\":\n",
      "        return np.array(GetMorganFingerprintAsBitVect(mol, 2, nBits=n_bits))\n",
      "    if method == \"morgan3\":\n",
      "        return np.array(GetMorganFingerprintAsBitVect(mol, 3, nBits=n_bits))\n",
      "    else:\n",
      "        # NBVAL_CHECK_OUTPUT\n",
      "        print(f\"Warning: Wrong method specified: {method}. Default will be used instead.\")\n",
      "        return np.array(MACCSkeys.GenMACCSKeys(mol))\n",
      "189/98:\n",
      "compound_df = df1.copy()\n",
      "# Add column for fingerprint\n",
      "compound_df[\"fp\"] = compound_df[\"smiles\"].apply(smiles_to_fp)\n",
      "compound_df.head(3)\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "189/99:\n",
      "fingerprint_to_model = compound_df.fp.tolist()\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "189/100: df2\n",
      "189/101:\n",
      "compound_df = df2.copy()\n",
      "# Add column for fingerprint\n",
      "compound_df[\"fp\"] = compound_df[\"smiles\"].apply(smiles_to_fp)\n",
      "compound_df.head(3)\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "189/102:\n",
      "fingerprint_to_model = compound_df.fp.tolist()\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "189/103:\n",
      "from sklearn.linear_model import LinearRegression\n",
      "reg = LinearRegression()\n",
      "189/104: np.any(np.isnan(fingerprint_to_model))\n",
      "189/105: np.any(np.isnan(label_to_model))\n",
      "189/106:\n",
      "from sklearn.model_selection import cross_val_score. cross_val_predict\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "scores=cross_val_score(reg, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "189/107:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "scores=cross_val_score(reg, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "189/108:\n",
      "def model_performance(ml_model, test_x, test_y, verbose=True):\n",
      "    \"\"\"\n",
      "    Helper function to calculate model performance\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    ml_model: sklearn model object\n",
      "        The machine learning model to train.\n",
      "    test_x: list\n",
      "        Molecular fingerprints for test set.\n",
      "    test_y: list\n",
      "        Associated activity labels for test set.\n",
      "    verbose: bool\n",
      "        Print performance measure (default = True)\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    tuple:\n",
      "        Accuracy, sensitivity, specificity, auc on test set.\n",
      "    \"\"\"\n",
      "\n",
      "    # Prediction probability on test set\n",
      "    test_prob = ml_model.predict_proba(test_x)[:, 1]\n",
      "\n",
      "    # Prediction class on test set\n",
      "    test_pred = ml_model.predict(test_x)\n",
      "\n",
      "    # Performance of model on test set\n",
      "    accuracy = accuracy_score(test_y, test_pred)\n",
      "    sens = recall_score(test_y, test_pred)\n",
      "    spec = recall_score(test_y, test_pred, pos_label=0)\n",
      "    auc = roc_auc_score(test_y, test_prob)\n",
      "\n",
      "    if verbose:\n",
      "        # Print performance results\n",
      "        # NBVAL_CHECK_OUTPUT        print(f\"Accuracy: {accuracy:.2}\")\n",
      "        print(f\"Sensitivity: {sens:.2f}\")\n",
      "        print(f\"Specificity: {spec:.2f}\")\n",
      "        print(f\"AUC: {auc:.2f}\")\n",
      "\n",
      "    return accuracy, sens, spec, auc\n",
      "189/109: model_performance(reg, test_x, test_y)\n",
      "189/110:\n",
      "def model_training_and_validation(ml_model, name, splits, verbose=True):\n",
      "    \"\"\"\n",
      "    Fit a machine learning model on a random train-test split of the data\n",
      "    and return the performance measures.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    ml_model: sklearn model object\n",
      "        The machine learning model to train.\n",
      "    name: str\n",
      "        Name of machine learning algorithm: RF, SVM, ANN\n",
      "    splits: list\n",
      "        List of desciptor and label data: train_x, test_x, train_y, test_y.\n",
      "    verbose: bool\n",
      "        Print performance info (default = True)\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    tuple:\n",
      "        Accuracy, sensitivity, specificity, auc on test set.\n",
      "\n",
      "    \"\"\"\n",
      "    train_x, test_x, train_y, test_y = splits\n",
      "\n",
      "    # Fit the model\n",
      "    ml_model.fit(train_x, train_y)\n",
      "\n",
      "    # Calculate model performance results\n",
      "    accuracy, sens, spec, auc = model_performance(ml_model, test_x, test_y, verbose)\n",
      "\n",
      "    return accuracy, sens, spec, auc\n",
      "189/111:\n",
      "def model_training_and_validation(ml_model, name, splits, verbose=True):\n",
      "    \"\"\"\n",
      "    Fit a machine learning model on a random train-test split of the data\n",
      "    and return the performance measures.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    ml_model: sklearn model object\n",
      "        The machine learning model to train.\n",
      "    name: str\n",
      "        Name of machine learning algorithm: RF, SVM, ANN\n",
      "    splits: list\n",
      "        List of desciptor and label data: train_x, test_x, train_y, test_y.\n",
      "    verbose: bool\n",
      "        Print performance info (default = True)\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    tuple:\n",
      "        Accuracy, sensitivity, specificity, auc on test set.\n",
      "\n",
      "    \"\"\"\n",
      "    train_x, test_x, train_y, test_y = splits\n",
      "\n",
      "    # Fit the model\n",
      "    ml_model.fit(train_x, train_y)\n",
      "\n",
      "    # Calculate model performance results\n",
      "    accuracy, sens, spec, auc = model_performance(ml_model, test_x, test_y, verbose)\n",
      "\n",
      "    return accuracy, sens, spec, auc\n",
      "189/112:\n",
      "def crossvalidation(ml_model, df, n_folds=5, verbose=False):\n",
      "    \"\"\"\n",
      "    Machine learning model training and validation in a cross-validation loop.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    ml_model: sklearn model object\n",
      "        The machine learning model to train.\n",
      "    df: pd.DataFrame\n",
      "        Data set with SMILES and their associated activity labels.\n",
      "    n_folds: int, optional\n",
      "        Number of folds for cross-validation.\n",
      "    verbose: bool, optional\n",
      "        Performance measures are printed.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    None\n",
      "\n",
      "    \"\"\"\n",
      "    t0 = time.time()\n",
      "    # Shuffle the indices for the k-fold cross-validation\n",
      "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=SEED)\n",
      "\n",
      "    # Results for each of the cross-validation folds\n",
      "    acc_per_fold = []\n",
      "    sens_per_fold = []\n",
      "    spec_per_fold = []\n",
      "    auc_per_fold = []\n",
      "\n",
      "    # Loop over the folds\n",
      "    for train_index, test_index in kf.split(df):\n",
      "        # clone model -- we want a fresh copy per fold!\n",
      "        fold_model = clone(ml_model)\n",
      "        # Training\n",
      "\n",
      "        # Convert the fingerprint and the label to a list\n",
      "        train_x = df.iloc[train_index].fp.tolist()\n",
      "        train_y = df.iloc[train_index].active.tolist()\n",
      "\n",
      "        # Fit the model\n",
      "        fold_model.fit(train_x, train_y)\n",
      "\n",
      "        # Testing\n",
      "\n",
      "        # Convert the fingerprint and the label to a list\n",
      "        test_x = df.iloc[test_index].fp.tolist()\n",
      "        test_y = df.iloc[test_index].active.tolist()\n",
      "\n",
      "        # Performance for each fold\n",
      "        accuracy, sens, spec, auc = model_performance(fold_model, test_x, test_y, verbose)\n",
      "\n",
      "        # Save results\n",
      "        acc_per_fold.append(accuracy)\n",
      "        sens_per_fold.append(sens)\n",
      "        spec_per_fold.append(spec)\n",
      "        auc_per_fold.append(auc)\n",
      "\n",
      "    # Print statistics of results\n",
      "    print(\n",
      "        f\"Mean accuracy: {np.mean(acc_per_fold):.2f} \\t\"\n",
      "        f\"and std : {np.std(acc_per_fold):.2f} \\n\"\n",
      "        f\"Mean sensitivity: {np.mean(sens_per_fold):.2f} \\t\"\n",
      "        f\"and std : {np.std(sens_per_fold):.2f} \\n\"\n",
      "        f\"Mean specificity: {np.mean(spec_per_fold):.2f} \\t\"\n",
      "        f\"and std : {np.std(spec_per_fold):.2f} \\n\"\n",
      "        f\"Mean AUC: {np.mean(auc_per_fold):.2f} \\t\"\n",
      "        f\"and std : {np.std(auc_per_fold):.2f} \\n\"\n",
      "        f\"Time taken : {time.time() - t0:.2f}s\\n\"\n",
      "    )\n",
      "\n",
      "    return acc_per_fold, sens_per_fold, spec_per_fold, auc_per_fold\n",
      "189/113:\n",
      "# Set model parameter for random forest\n",
      "param = {\n",
      "    \"n_estimators\": 100,  # number of trees to grows\n",
      "    \"criterion\": \"entropy\",  # cost function to be optimized for a split\n",
      "}\n",
      "model_RF = RandomForestClassifier(**param)\n",
      "189/114:\n",
      "# Fit model on single split\n",
      "performance_measures = model_training_and_validation(model_RF, \"RF\", splits)\n",
      "189/115:\n",
      "fingerprint_to_model = compound_df.fp.tolist()\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "(\n",
      "    static_train_x,\n",
      "    static_test_x,\n",
      "    static_train_y,\n",
      "    static_test_y,\n",
      ") = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "splits = [static_train_x, static_test_x, static_train_y, static_test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(static_train_x))\n",
      "print(\"Test data size:\", len(static_test_x))\n",
      "189/116:\n",
      "from sklearn.linear_model import LinearRegression\n",
      "reg = LinearRegression()\n",
      "189/117: np.any(np.isnan(fingerprint_to_model))\n",
      "189/118: np.any(np.isnan(label_to_model))\n",
      "189/119:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "scores=cross_val_score(reg, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "189/120:\n",
      "def model_performance(ml_model, test_x, test_y, verbose=True):\n",
      "    \"\"\"\n",
      "    Helper function to calculate model performance\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    ml_model: sklearn model object\n",
      "        The machine learning model to train.\n",
      "    test_x: list\n",
      "        Molecular fingerprints for test set.\n",
      "    test_y: list\n",
      "        Associated activity labels for test set.\n",
      "    verbose: bool\n",
      "        Print performance measure (default = True)\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    tuple:\n",
      "        Accuracy, sensitivity, specificity, auc on test set.\n",
      "    \"\"\"\n",
      "\n",
      "    # Prediction probability on test set\n",
      "    test_prob = ml_model.predict_proba(test_x)[:, 1]\n",
      "\n",
      "    # Prediction class on test set\n",
      "    test_pred = ml_model.predict(test_x)\n",
      "\n",
      "    # Performance of model on test set\n",
      "    accuracy = accuracy_score(test_y, test_pred)\n",
      "    sens = recall_score(test_y, test_pred)\n",
      "    spec = recall_score(test_y, test_pred, pos_label=0)\n",
      "    auc = roc_auc_score(test_y, test_prob)\n",
      "\n",
      "    if verbose:\n",
      "        # Print performance results\n",
      "        # NBVAL_CHECK_OUTPUT        print(f\"Accuracy: {accuracy:.2}\")\n",
      "        print(f\"Sensitivity: {sens:.2f}\")\n",
      "        print(f\"Specificity: {spec:.2f}\")\n",
      "        print(f\"AUC: {auc:.2f}\")\n",
      "\n",
      "    return accuracy, sens, spec, auc\n",
      "189/121:\n",
      "def model_training_and_validation(ml_model, name, splits, verbose=True):\n",
      "    \"\"\"\n",
      "    Fit a machine learning model on a random train-test split of the data\n",
      "    and return the performance measures.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    ml_model: sklearn model object\n",
      "        The machine learning model to train.\n",
      "    name: str\n",
      "        Name of machine learning algorithm: RF, SVM, ANN\n",
      "    splits: list\n",
      "        List of desciptor and label data: train_x, test_x, train_y, test_y.\n",
      "    verbose: bool\n",
      "        Print performance info (default = True)\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    tuple:\n",
      "        Accuracy, sensitivity, specificity, auc on test set.\n",
      "\n",
      "    \"\"\"\n",
      "    train_x, test_x, train_y, test_y = splits\n",
      "\n",
      "    # Fit the model\n",
      "    ml_model.fit(train_x, train_y)\n",
      "\n",
      "    # Calculate model performance results\n",
      "    accuracy, sens, spec, auc = model_performance(ml_model, test_x, test_y, verbose)\n",
      "\n",
      "    return accuracy, sens, spec, auc\n",
      "189/122:\n",
      "# Set model parameter for random forest\n",
      "param = {\n",
      "    \"n_estimators\": 100,  # number of trees to grows\n",
      "    \"criterion\": \"entropy\",  # cost function to be optimized for a split\n",
      "}\n",
      "model_RF = RandomForestClassifier(**param)\n",
      "189/123:\n",
      "# Fit model on single split\n",
      "performance_measures = model_training_and_validation(model_RF, \"RF\", splits)\n",
      "189/124:\n",
      "def model_performance(ml_model, test_x, test_y, verbose=True):\n",
      "     # Prediction probability on test set\n",
      "    test_prob = ml_model.predict_proba(test_x)[:, 1]\n",
      "\n",
      "    # Prediction class on test set\n",
      "    test_pred = ml_model.predict(test_x)\n",
      "\n",
      "    # Performance of model on test set\n",
      "    accuracy = accuracy_score(test_y, test_pred)\n",
      "    sens = recall_score(test_y, test_pred)\n",
      "    spec = recall_score(test_y, test_pred, pos_label=0)\n",
      "    auc = roc_auc_score(test_y, test_prob)\n",
      "\n",
      "    if verbose:\n",
      "        # Print performance results\n",
      "        # NBVAL_CHECK_OUTPUT        print(f\"Accuracy: {accuracy:.2}\")\n",
      "        print(f\"Sensitivity: {sens:.2f}\")\n",
      "        print(f\"Specificity: {spec:.2f}\")\n",
      "        print(f\"AUC: {auc:.2f}\")\n",
      "\n",
      "    return accuracy, sens, spec, auc\n",
      "189/125:\n",
      "def model_training_and_validation(ml_model, name, splits, verbose=True):\n",
      "    train_x, test_x, train_y, test_y = splits\n",
      "\n",
      "    # Fit the model\n",
      "    ml_model.fit(train_x, train_y)\n",
      "\n",
      "    # Calculate model performance results\n",
      "    accuracy, sens, spec, auc = model_performance(ml_model, test_x, test_y, verbose)\n",
      "\n",
      "    return accuracy, sens, spec, auc\n",
      "189/126:\n",
      "def crossvalidation(ml_model, df, n_folds=5, verbose=False):\n",
      "    t0 = time.time()\n",
      "    # Shuffle the indices for the k-fold cross-validation\n",
      "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=SEED)\n",
      "\n",
      "    # Results for each of the cross-validation folds\n",
      "    acc_per_fold = []\n",
      "    sens_per_fold = []\n",
      "    spec_per_fold = []\n",
      "    auc_per_fold = []\n",
      "\n",
      "    # Loop over the folds\n",
      "    for train_index, test_index in kf.split(df):\n",
      "        # clone model -- we want a fresh copy per fold!\n",
      "        fold_model = clone(ml_model)\n",
      "        # Training\n",
      "\n",
      "        # Convert the fingerprint and the label to a list\n",
      "        train_x = df.iloc[train_index].fp.tolist()\n",
      "        train_y = df.iloc[train_index].active.tolist()\n",
      "\n",
      "        # Fit the model\n",
      "        fold_model.fit(train_x, train_y)\n",
      "\n",
      "        # Testing\n",
      "\n",
      "        # Convert the fingerprint and the label to a list\n",
      "        test_x = df.iloc[test_index].fp.tolist()\n",
      "        test_y = df.iloc[test_index].active.tolist()\n",
      "\n",
      "        # Performance for each fold\n",
      "        accuracy, sens, spec, auc = model_performance(fold_model, test_x, test_y, verbose)\n",
      "\n",
      "        # Save results\n",
      "        acc_per_fold.append(accuracy)\n",
      "        sens_per_fold.append(sens)\n",
      "        spec_per_fold.append(spec)\n",
      "        auc_per_fold.append(auc)\n",
      "\n",
      "    # Print statistics of results\n",
      "    print(\n",
      "        f\"Mean accuracy: {np.mean(acc_per_fold):.2f} \\t\"\n",
      "        f\"and std : {np.std(acc_per_fold):.2f} \\n\"\n",
      "        f\"Mean sensitivity: {np.mean(sens_per_fold):.2f} \\t\"\n",
      "        f\"and std : {np.std(sens_per_fold):.2f} \\n\"\n",
      "        f\"Mean specificity: {np.mean(spec_per_fold):.2f} \\t\"\n",
      "        f\"and std : {np.std(spec_per_fold):.2f} \\n\"\n",
      "        f\"Mean AUC: {np.mean(auc_per_fold):.2f} \\t\"\n",
      "        f\"and std : {np.std(auc_per_fold):.2f} \\n\"\n",
      "        f\"Time taken : {time.time() - t0:.2f}s\\n\"\n",
      "    )\n",
      "\n",
      "    return acc_per_fold, sens_per_fold, spec_per_fold, auc_per_fold\n",
      "189/127:\n",
      "# Set model parameter for random forest\n",
      "param = {\n",
      "    \"n_estimators\": 100,  # number of trees to grows\n",
      "    \"criterion\": \"entropy\",  # cost function to be optimized for a split\n",
      "}\n",
      "model_RF = RandomForestClassifier(**param)\n",
      "189/128:\n",
      "# Fit model on single split\n",
      "performance_measures = model_training_and_validation(model_RF, \"RF\", splits)\n",
      "189/129:\n",
      "# Fit model on single split\n",
      "performance_measures = model_training_and_validation(model_RF, splits)\n",
      "189/130:\n",
      "# Fit model on single split\n",
      "performance_measures = model_training_and_validation(model_RF, 'rf', splits, verbose=True)\n",
      "189/131:\n",
      "#import important library\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn import svm, metrics, clone\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.neural_network import MLPClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "from sklearn.metrics import auc, accuracy_score, recall_score\n",
      "from sklearn.metrics import roc_curve, roc_auc_score\n",
      "import matplotlib.pyplot as plt\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import MACCSkeys\n",
      "from rdkit.Chem.AllChem import GetMorganFingerprintAsBitVect\n",
      "189/132:\n",
      "# Set model parameter for random forest regression\n",
      "param = {\n",
      "    \"n_estimators\": 100,  # number of trees to grows\n",
      "    \"criterion\": \"entropy\",  # cost function to be optimized for a split\n",
      "}\n",
      "model_RF = RandomForestRegressorR(**param)\n",
      "189/133:\n",
      "# Set model parameter for random forest regression\n",
      "param = {\n",
      "    \"n_estimators\": 100,  # number of trees to grows\n",
      "    \"criterion\": \"entropy\",  # cost function to be optimized for a split\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "189/134:\n",
      "# Fit model on single split\n",
      "performance_measures = model_training_and_validation(model_RF, 'rf', splits, verbose=True)\n",
      "189/135:\n",
      "# Set model parameter for random forest regression\n",
      "param = {\n",
      "    \"n_estimators\": 100,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "189/136:\n",
      "# Fit model on single split\n",
      "performance_measures = model_training_and_validation(model_RF, 'rf', splits, verbose=True)\n",
      "189/137: N_FOLDS = 5\n",
      "189/138:\n",
      "for model in models:\n",
      "    print(\"\\n======= \")\n",
      "    print(f\"{model['label']}\")\n",
      "    crossvalidation(model[\"model\"], compound_df, n_folds=N_FOLDS)\n",
      "189/139:\n",
      "# Initialize the list that stores all models. First one is RF.\n",
      "models = [{\"label\": \"Model_RF\", \"model\": model_RF}]\n",
      "189/140:\n",
      "for model in models:\n",
      "    print(\"\\n======= \")\n",
      "    print(f\"{model['label']}\")\n",
      "    crossvalidation(model[\"model\"], compound_df, n_folds=N_FOLDS)\n",
      "189/141:\n",
      "def crossvalidation(ml_model, df, n_folds=5, verbose=False):\n",
      "    # Shuffle the indices for the k-fold cross-validation\n",
      "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=SEED)\n",
      "\n",
      "    # Results for each of the cross-validation folds\n",
      "    acc_per_fold = []\n",
      "    sens_per_fold = []\n",
      "    spec_per_fold = []\n",
      "    auc_per_fold = []\n",
      "\n",
      "    # Loop over the folds\n",
      "    for train_index, test_index in kf.split(df):\n",
      "        # clone model -- we want a fresh copy per fold!\n",
      "        fold_model = clone(ml_model)\n",
      "        # Training\n",
      "\n",
      "        # Convert the fingerprint and the label to a list\n",
      "        train_x = df.iloc[train_index].fp.tolist()\n",
      "        train_y = df.iloc[train_index].active.tolist()\n",
      "\n",
      "        # Fit the model\n",
      "        fold_model.fit(train_x, train_y)\n",
      "\n",
      "        # Testing\n",
      "\n",
      "        # Convert the fingerprint and the label to a list\n",
      "        test_x = df.iloc[test_index].fp.tolist()\n",
      "        test_y = df.iloc[test_index].active.tolist()\n",
      "\n",
      "        # Performance for each fold\n",
      "        accuracy, sens, spec, auc = model_performance(fold_model, test_x, test_y, verbose)\n",
      "\n",
      "        # Save results\n",
      "        acc_per_fold.append(accuracy)\n",
      "        sens_per_fold.append(sens)\n",
      "        spec_per_fold.append(spec)\n",
      "        auc_per_fold.append(auc)\n",
      "\n",
      "    # Print statistics of results\n",
      "    print(\n",
      "        f\"Mean accuracy: {np.mean(acc_per_fold):.2f} \\t\"\n",
      "        f\"and std : {np.std(acc_per_fold):.2f} \\n\"\n",
      "        f\"Mean sensitivity: {np.mean(sens_per_fold):.2f} \\t\"\n",
      "        f\"and std : {np.std(sens_per_fold):.2f} \\n\"\n",
      "        f\"Mean specificity: {np.mean(spec_per_fold):.2f} \\t\"\n",
      "        f\"and std : {np.std(spec_per_fold):.2f} \\n\"\n",
      "        f\"Mean AUC: {np.mean(auc_per_fold):.2f} \\t\"\n",
      "        f\"and std : {np.std(auc_per_fold):.2f} \\n\"\n",
      "        f\"Time taken : {time.time() - t0:.2f}s\\n\"\n",
      "    )\n",
      "\n",
      "    return acc_per_fold, sens_per_fold, spec_per_fold, auc_per_fold\n",
      "189/142:\n",
      "# Set model parameter for random forest regression\n",
      "param = {\n",
      "    \"n_estimators\": 100,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "189/143: N_FOLDS = 5\n",
      "189/144:\n",
      "# Initialize the list that stores all models. First one is RF.\n",
      "models = [{\"label\": \"Model_RF\", \"model\": model_RF}]\n",
      "189/145:\n",
      "for model in models:\n",
      "    print(\"\\n======= \")\n",
      "    print(f\"{model['label']}\")\n",
      "    crossvalidation(model[\"model\"], compound_df, n_folds=N_FOLDS)\n",
      "189/146:\n",
      "def crossvalidation(ml_model, df, n_folds=5, verbose=False):\n",
      "    # Shuffle the indices for the k-fold cross-validation\n",
      "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=SEED)\n",
      "\n",
      "    # Results for each of the cross-validation folds\n",
      "    acc_per_fold = []\n",
      "    sens_per_fold = []\n",
      "    spec_per_fold = []\n",
      "    auc_per_fold = []\n",
      "\n",
      "    # Loop over the folds\n",
      "    for train_index, test_index in kf.split(df):\n",
      "        # clone model -- we want a fresh copy per fold!\n",
      "        fold_model = clone(ml_model)\n",
      "        # Training\n",
      "\n",
      "        # Convert the fingerprint and the label to a list\n",
      "        train_x = df.iloc[train_index].fp.tolist()\n",
      "        train_y = df.iloc[train_index].active.tolist()\n",
      "\n",
      "        # Fit the model\n",
      "        fold_model.fit(train_x, train_y)\n",
      "\n",
      "        # Testing\n",
      "\n",
      "        # Convert the fingerprint and the label to a list\n",
      "        test_x = df.iloc[test_index].fp.tolist()\n",
      "        test_y = df.iloc[test_index].LogKpu.tolist()\n",
      "\n",
      "        # Performance for each fold\n",
      "        accuracy, sens, spec, auc = model_performance(fold_model, test_x, test_y, verbose)\n",
      "\n",
      "        # Save results\n",
      "        acc_per_fold.append(accuracy)\n",
      "        sens_per_fold.append(sens)\n",
      "        spec_per_fold.append(spec)\n",
      "        auc_per_fold.append(auc)\n",
      "\n",
      "    # Print statistics of results\n",
      "    print(\n",
      "        f\"Mean accuracy: {np.mean(acc_per_fold):.2f} \\t\"\n",
      "        f\"and std : {np.std(acc_per_fold):.2f} \\n\"\n",
      "        f\"Mean sensitivity: {np.mean(sens_per_fold):.2f} \\t\"\n",
      "        f\"and std : {np.std(sens_per_fold):.2f} \\n\"\n",
      "        f\"Mean specificity: {np.mean(spec_per_fold):.2f} \\t\"\n",
      "        f\"and std : {np.std(spec_per_fold):.2f} \\n\"\n",
      "        f\"Mean AUC: {np.mean(auc_per_fold):.2f} \\t\"\n",
      "        f\"and std : {np.std(auc_per_fold):.2f} \\n\"\n",
      "        f\"Time taken : {time.time() - t0:.2f}s\\n\"\n",
      "    )\n",
      "\n",
      "    return acc_per_fold, sens_per_fold, spec_per_fold, auc_per_fold\n",
      "189/147:\n",
      "# Set model parameter for random forest regression\n",
      "param = {\n",
      "    \"n_estimators\": 100,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "189/148: N_FOLDS = 5\n",
      "189/149:\n",
      "# Initialize the list that stores all models. First one is RF.\n",
      "models = [{\"label\": \"Model_RF\", \"model\": model_RF}]\n",
      "189/150:\n",
      "for model in models:\n",
      "    print(\"\\n======= \")\n",
      "    print(f\"{model['label']}\")\n",
      "    crossvalidation(model[\"model\"], compound_df, n_folds=N_FOLDS)\n",
      "189/151:\n",
      "def crossvalidation(ml_model, df, n_folds=5, verbose=False):\n",
      "    # Shuffle the indices for the k-fold cross-validation\n",
      "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=SEED)\n",
      "\n",
      "    # Results for each of the cross-validation folds\n",
      "    acc_per_fold = []\n",
      "    sens_per_fold = []\n",
      "    spec_per_fold = []\n",
      "    auc_per_fold = []\n",
      "\n",
      "    # Loop over the folds\n",
      "    for train_index, test_index in kf.split(df):\n",
      "        # clone model -- we want a fresh copy per fold!\n",
      "        fold_model = clone(ml_model)\n",
      "        # Training\n",
      "\n",
      "        # Convert the fingerprint and the label to a list\n",
      "        train_x = df.iloc[train_index].fp.tolist()\n",
      "        train_y = df.iloc[train_index].LogKpu.tolist()\n",
      "\n",
      "        # Fit the model\n",
      "        fold_model.fit(train_x, train_y)\n",
      "\n",
      "        # Testing\n",
      "\n",
      "        # Convert the fingerprint and the label to a list\n",
      "        test_x = df.iloc[test_index].fp.tolist()\n",
      "        test_y = df.iloc[test_index].LogKpu.tolist()\n",
      "\n",
      "        # Performance for each fold\n",
      "        accuracy, sens, spec, auc = model_performance(fold_model, test_x, test_y, verbose)\n",
      "\n",
      "        # Save results\n",
      "        acc_per_fold.append(accuracy)\n",
      "        sens_per_fold.append(sens)\n",
      "        spec_per_fold.append(spec)\n",
      "        auc_per_fold.append(auc)\n",
      "\n",
      "    # Print statistics of results\n",
      "    print(\n",
      "        f\"Mean accuracy: {np.mean(acc_per_fold):.2f} \\t\"\n",
      "        f\"and std : {np.std(acc_per_fold):.2f} \\n\"\n",
      "        f\"Mean sensitivity: {np.mean(sens_per_fold):.2f} \\t\"\n",
      "        f\"and std : {np.std(sens_per_fold):.2f} \\n\"\n",
      "        f\"Mean specificity: {np.mean(spec_per_fold):.2f} \\t\"\n",
      "        f\"and std : {np.std(spec_per_fold):.2f} \\n\"\n",
      "        f\"Mean AUC: {np.mean(auc_per_fold):.2f} \\t\"\n",
      "        f\"and std : {np.std(auc_per_fold):.2f} \\n\"\n",
      "        f\"Time taken : {time.time() - t0:.2f}s\\n\"\n",
      "    )\n",
      "\n",
      "    return acc_per_fold, sens_per_fold, spec_per_fold, auc_per_fold\n",
      "189/152:\n",
      "# Set model parameter for random forest regression\n",
      "param = {\n",
      "    \"n_estimators\": 100,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "189/153: N_FOLDS = 5\n",
      "189/154:\n",
      "# Initialize the list that stores all models. First one is RF.\n",
      "models = [{\"label\": \"Model_RF\", \"model\": model_RF}]\n",
      "189/155:\n",
      "for model in models:\n",
      "    print(\"\\n======= \")\n",
      "    print(f\"{model['label']}\")\n",
      "    crossvalidation(model[\"model\"], compound_df, n_folds=N_FOLDS)\n",
      "189/156:\n",
      "# Set model parameter for random forest regression\n",
      "param = {\n",
      "    \"n_estimators\": 100,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "189/157:\n",
      "scores=cross_val_score(reg, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "189/158:\n",
      "def crossvalidation(ml_model, df, n_folds=5, verbose=False):\n",
      "    # Shuffle the indices for the k-fold cross-validation\n",
      "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=SEED)\n",
      "\n",
      "    # Results for each of the cross-validation folds\n",
      "    acc_per_fold = []\n",
      "    sens_per_fold = []\n",
      "    spec_per_fold = []\n",
      "    auc_per_fold = []\n",
      "\n",
      "    # Loop over the folds\n",
      "    for train_index, test_index in kf.split(df):\n",
      "        # clone model -- we want a fresh copy per fold!\n",
      "        fold_model = clone(ml_model)\n",
      "        # Training\n",
      "\n",
      "        # Convert the fingerprint and the label to a list\n",
      "        train_x = df.iloc[train_index].fp.tolist()\n",
      "        train_y = df.iloc[train_index].LogKpu.tolist()\n",
      "\n",
      "        # Fit the model\n",
      "        fold_model.fit(train_x, train_y)\n",
      "\n",
      "        # Testing\n",
      "\n",
      "        # Convert the fingerprint and the label to a list\n",
      "        test_x = df.iloc[test_index].fp.tolist()\n",
      "        test_y = df.iloc[test_index].LogKpu.tolist()\n",
      "\n",
      "        # Performance for each fold\n",
      "        accuracy, sens, spec, auc = model_performance(fold_model, test_x, test_y, verbose)\n",
      "\n",
      "        # Save results\n",
      "        acc_per_fold.append(accuracy)\n",
      "        sens_per_fold.append(sens)\n",
      "        spec_per_fold.append(spec)\n",
      "        auc_per_fold.append(auc)\n",
      "\n",
      "    # Print statistics of results\n",
      "    print(\n",
      "        f\"Mean accuracy: {np.mean(acc_per_fold):.2f} \\t\"\n",
      "        f\"and std : {np.std(acc_per_fold):.2f} \\n\"\n",
      "        f\"Mean sensitivity: {np.mean(sens_per_fold):.2f} \\t\"\n",
      "        f\"and std : {np.std(sens_per_fold):.2f} \\n\"\n",
      "        f\"Mean specificity: {np.mean(spec_per_fold):.2f} \\t\"\n",
      "        f\"and std : {np.std(spec_per_fold):.2f} \\n\"\n",
      "        f\"Mean AUC: {np.mean(auc_per_fold):.2f} \\t\"\n",
      "        f\"and std : {np.std(auc_per_fold):.2f} \\n\"\n",
      "        f\"Time taken : {time.time() - t0:.2f}s\\n\"\n",
      "    )\n",
      "\n",
      "    return acc_per_fold, sens_per_fold, spec_per_fold, auc_per_fold\n",
      "189/159:\n",
      "# Set model parameter for random forest regression\n",
      "param = {\n",
      "    \"n_estimators\": 100,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "189/160: N_FOLDS = 5\n",
      "189/161:\n",
      "# Initialize the list that stores all models. First one is RF.\n",
      "models = [{\"label\": \"Model_RF\", \"model\": model_RF}]\n",
      "189/162: from sklearn.cross_decomposition import PLSRegression\n",
      "189/163: model_PLS = PLSRegression(n_components=10)\n",
      "189/164:\n",
      "scores=cross_val_score(model_PLS, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "189/165:\n",
      "# Set model parameter for random forest regression\n",
      "param = {\n",
      "    \"n_estimators\": 200,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "189/166:\n",
      "# Set model parameter for random forest regression\n",
      "param = {\n",
      "    \"n_estimators\": 300,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "189/167:\n",
      "# Set model parameter for random forest regression\n",
      "param = {\n",
      "    \"n_estimators\": 1000,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "189/168:\n",
      "# Set model parameter for random forest regression\n",
      "param = {\n",
      "    \"n_estimators\": 100,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "189/169:\n",
      "# Set model parameter for random forest regression\n",
      "param = {\n",
      "    \"n_estimators\": 200,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "189/170:\n",
      "from sklearn.linear_model import Ridge\n",
      "Ridge = Ridge(alpha=1.0)\n",
      "189/171:\n",
      "scores=cross_val_score(Ridge, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "189/172:\n",
      "scores=cross_val_score(Ridge, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "189/173:\n",
      "scores=cross_val_score(Ridge, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "189/174:\n",
      "scores=cross_val_score(Ridge, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "189/175:\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "regr = MLPRegressor(random_state=1, max_iter=500, solver='lbfgs')\n",
      "scores=cross_val_score(model_PLS, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "189/176:\n",
      "y_pred = cross_val_predict(regr, train_x, train_y cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "189/177:\n",
      "y_pred = cross_val_predict(regr, train_x, train_y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "189/178:\n",
      "from sklearn.metrics import r2_score\n",
      "y_pred = cross_val_predict(regr, train_x, train_y, cv=kf)\n",
      "r2_score(y, y_pred)\n",
      "189/179:\n",
      "from sklearn.metrics import r2_score\n",
      "y_pred = cross_val_predict(regr, train_x, train_y, cv=kf)\n",
      "r2_score(train_y, y_pred)\n",
      "189/180:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='linear')\n",
      "189/181:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "189/182:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='poly')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "189/183:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='sigmoid')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "189/184:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='linear')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "189/185:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='precomputed')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "189/186:\n",
      "# Set model parameter for random forest regression\n",
      "param = {\n",
      "    \"n_estimators\": 3000,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "189/187:\n",
      "compound_df = df2.copy()\n",
      "# Add column for fingerprint\n",
      "compound_df[\"fp\"] = compound_df[\"smiles\"].apply(smiles_to_fp, args=(\"morgan3\",))\n",
      "compound_df.head(3)\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "189/188: compound_df.shape\n",
      "189/189:\n",
      "fingerprint_to_model = compound_df.fp.tolist()\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "(\n",
      "    static_train_x,\n",
      "    static_test_x,\n",
      "    static_train_y,\n",
      "    static_test_y,\n",
      ") = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "splits = [static_train_x, static_test_x, static_train_y, static_test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(static_train_x))\n",
      "print(\"Test data size:\", len(static_test_x))\n",
      "189/190:\n",
      "# Set model parameter for random forest regression\n",
      "param = {\n",
      "    \"n_estimators\": 3000,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "189/191:\n",
      "fingerprint_to_model = compound_df.fp.tolist()\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "(\n",
      "    train_x,\n",
      "    test_x,\n",
      "    train_y,\n",
      "    test_y,\n",
      ") = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(static_train_x))\n",
      "print(\"Test data size:\", len(static_test_x))\n",
      "189/192:\n",
      "# Set model parameter for random forest regression\n",
      "param = {\n",
      "    \"n_estimators\": 3000,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "189/193:\n",
      "# Set model parameter for random forest regression\n",
      "param = {\n",
      "    \"n_estimators\": 1000,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "189/194:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "189/195: #Or we should move to mordard fingerprint and calculate with prediction\n",
      "187/1:\n",
      "\n",
      "raw13.to_csv('QSAR_Kpu/Mordard.csv' , sep=',' ,index=True)\n",
      "187/2:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "187/3:\n",
      "path = r'Fingerprint/'\n",
      "\n",
      "Fp1  = pd.read_csv(path + 'Fingerprinter.csv'               , header = 0)\n",
      "Fp2  = pd.read_csv(path + 'ExtendedFingerprinter.csv'       , header = 0)\n",
      "Fp3  = pd.read_csv(path + 'EStateFingerprinter.csv'         , header = 0)\n",
      "Fp4  = pd.read_csv(path + 'GraphOnlyFingerprinter.csv'      , header = 0)\n",
      "Fp5  = pd.read_csv(path + 'MACCSFingerprinter.csv'          , header = 0)\n",
      "Fp6  = pd.read_csv(path + 'PubchemFingerprinter.csv'        , header = 0)\n",
      "Fp7  = pd.read_csv(path + 'SubstructureFingerprinter.csv'   , header = 0)\n",
      "Fp8  = pd.read_csv(path + 'SubstructureFingerprintCount.csv', header = 0)\n",
      "Fp9  = pd.read_csv(path + 'KlekotaRothFingerprinter.csv'    , header = 0)\n",
      "Fp10 = pd.read_csv(path + 'KlekotaRothFingerprintCount.csv' , header = 0)\n",
      "Fp11 = pd.read_csv(path + 'AtomPairs2DFingerprinter.csv'    , header = 0)\n",
      "Fp12 = pd.read_csv(path + 'AtomPairs2DFingerprintCount.csv' , header = 0)\n",
      "Fp13  = pd.read_csv(path + 'mordard.csv'               , header = 0)\n",
      "187/4: Fp1.shape\n",
      "187/5:\n",
      "df = pd.read_csv('df6_lipinski_kpu.csv')\n",
      "df\n",
      "187/6: name = df['compound']\n",
      "187/7: name.shape\n",
      "187/8: name = df['compound']\n",
      "187/9: name.shape\n",
      "187/10:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df2 = df2.drop('Unnamed: 0', axis=1)\n",
      "df3 = df2.set_index('Name')\n",
      "df3\n",
      "187/11:\n",
      "LogKpu = df3[['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]\n",
      "LogKpu\n",
      "187/12:\n",
      "Fp13 = Fp13.rename(columns={'name': 'Name'})\n",
      "Fp13\n",
      "187/13:\n",
      "Fp1.Name = name\n",
      "Fp2.Name = name\n",
      "Fp3.Name = name\n",
      "Fp4.Name = name\n",
      "Fp5.Name = name\n",
      "Fp6.Name = name\n",
      "Fp7.Name = name\n",
      "Fp8.Name = name\n",
      "Fp9.Name = name\n",
      "Fp10.Name = name\n",
      "Fp11.Name = name\n",
      "Fp12.Name = name\n",
      "Fp13.Name = name\n",
      "187/14:\n",
      "Fpp1 = Fp1.drop('Name', axis=1)\n",
      "Fpp2 = Fp2.drop('Name', axis=1)\n",
      "Fpp3 = Fp3.drop('Name', axis=1)\n",
      "Fpp4 = Fp4.drop('Name', axis=1)\n",
      "Fpp5 = Fp5.drop('Name', axis=1)\n",
      "Fpp6 = Fp6.drop('Name', axis=1)\n",
      "Fpp7 = Fp7.drop('Name', axis=1)\n",
      "Fpp8 = Fp8.drop('Name', axis=1)\n",
      "Fpp9 = Fp9.drop('Name', axis=1)\n",
      "Fpp10 = Fp10.drop('Name', axis=1)\n",
      "Fpp11 = Fp11.drop('Name', axis=1)\n",
      "Fpp12 = Fp12.drop('Name', axis=1)\n",
      "Fpp13 = Fp13.drop('Name', axis=1)\n",
      "187/15:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)\n",
      "    Fp_normalized\n",
      "    \n",
      "    return Fp_normalized\n",
      "187/16:\n",
      "Fp1_n  = normalized (Fpp1 )\n",
      "Fp2_n  = normalized (Fpp2 )\n",
      "Fp3_n  = normalized (Fpp3 )\n",
      "Fp4_n  = normalized (Fpp4 )\n",
      "Fp5_n  = normalized (Fpp5 )\n",
      "Fp6_n  = normalized (Fpp6 )\n",
      "Fp7_n  = normalized (Fpp7 )\n",
      "Fp8_n  = normalized (Fpp8 )\n",
      "Fp9_n  = normalized (Fpp9 )\n",
      "Fp10_n = normalized (Fpp10)\n",
      "Fp11_n = normalized (Fpp11)\n",
      "Fp12_n = normalized (Fpp12)\n",
      "Fp13_n = normalized (Fpp13)\n",
      "187/17:\n",
      "path = r'Fingerprint/'\n",
      "\n",
      "Fp13  = pd.read_csv(path + 'mordard.csv'               , header = 0)\n",
      "187/18: Fp1.shape\n",
      "187/19:\n",
      "df = pd.read_csv('df6_lipinski_kpu.csv')\n",
      "df\n",
      "187/20: name = df['compound']\n",
      "187/21: name.shape\n",
      "187/22:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df2 = df2.drop('Unnamed: 0', axis=1)\n",
      "df3 = df2.set_index('Name')\n",
      "df3\n",
      "187/23:\n",
      "LogKpu = df3[['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]\n",
      "LogKpu\n",
      "187/24:\n",
      "Fp13 = Fp13.rename(columns={'name': 'Name'})\n",
      "Fp13\n",
      "187/25:\n",
      "\n",
      "Fp13.Name = name\n",
      "187/26:\n",
      "\n",
      "Fpp13 = Fp13.drop('Name', axis=1)\n",
      "187/27:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)\n",
      "    Fp_normalized\n",
      "    \n",
      "    return Fp_normalized\n",
      "187/28:\n",
      "\n",
      "Fp13_n = normalized (Fpp13)\n",
      "187/29:\n",
      "\n",
      "Fpp13 = Fp13.drop('Name', axis=1)\n",
      "Fpp13\n",
      "187/30: Fpp13.fillna(value='0', method='ffill')\n",
      "187/31: Fpp13.fillna(value=0, method='ffill')\n",
      "187/32: Fpp13 = Fpp13.fillna(0)\n",
      "187/33:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)\n",
      "    Fp_normalized\n",
      "    \n",
      "    return Fp_normalized\n",
      "187/34:\n",
      "\n",
      "Fp13_n = normalized (Fpp13)\n",
      "187/35: Fp13_n\n",
      "187/36: np.any(np.isnan(Fp13_n))\n",
      "187/37: Fp13_n[\"Name\"] = Fp13.Name\n",
      "187/38: Fp1_n\n",
      "187/39: Fp13_n = Fp13_n.set_index('Name')\n",
      "187/40: Fp1_n\n",
      "187/41: Fp13_n\n",
      "187/42: Fp13_n.to_csv('Fp_normalized/Morderd.csv' , sep=',' ,index=True)\n",
      "187/43: raw13  = LogKpu.merge(Fp13_n, on='Name', how='outer')\n",
      "187/44: raw13\n",
      "187/45: raw13.isna()\n",
      "187/46:\n",
      "\n",
      "raw13.to_csv('QSAR_Kpu/Mordard.csv' , sep=',' ,index=True)\n",
      "187/47: print (len(raw13))\n",
      "187/48: print (len(raw13 .columns)\n",
      "187/49: print (len(raw13 .columns))\n",
      "187/50:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "X = sel.fit_transform(X)\n",
      "X.shape\n",
      "187/51: compound_df = raw13.copy()\n",
      "187/52:\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['LogKp'], axis=1)\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "187/53:\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['LogKpu'], axis=1)\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "187/54: compound_df.head(3)\n",
      "187/55:\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "187/56: compound_df.head(3)\n",
      "187/57: fingerprint_to_model.head(3)\n",
      "187/58:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "fingerprint_to_model = sel.fit_transform(X)\n",
      "label_to_model.shape\n",
      "187/59:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "fingerprint_to_model.shape\n",
      "187/60:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "fingerprint_to_model.shape\n",
      "187/61:\n",
      "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict\n",
      "kf =KFold(n_splits=10, shuffle=False)\n",
      "187/62:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "187/63:\n",
      "(\n",
      "    train_x,\n",
      "    test_x,\n",
      "    train_y,\n",
      "    test_y,\n",
      ") = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(static_train_x))\n",
      "print(\"Test data size:\", len(static_test_x))\n",
      "187/64:\n",
      "    train_x,\n",
      "    test_x,\n",
      "    train_y,\n",
      "    test_y,\n",
      "= train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(static_train_x))\n",
      "print(\"Test data size:\", len(static_test_x))\n",
      "187/65:\n",
      "    train_x,\n",
      "    test_x,\n",
      "    train_y,\n",
      "    test_y,\n",
      "    = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(static_train_x))\n",
      "print(\"Test data size:\", len(static_test_x))\n",
      "187/66:\n",
      "train_x,test_x,train_y,test_y= train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(static_train_x))\n",
      "print(\"Test data size:\", len(static_test_x))\n",
      "187/67:\n",
      "seed = random.seed(10)\n",
      "seed\n",
      "187/68:\n",
      "import random\n",
      "seed = random.seed(10)\n",
      "seed\n",
      "187/69:\n",
      "train_x,test_x,train_y,test_y= train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(static_train_x))\n",
      "print(\"Test data size:\", len(static_test_x))\n",
      "187/70:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "187/71:\n",
      "import random\n",
      "SEED = random.seed(10)\n",
      "seed\n",
      "187/72:\n",
      "train_x,test_x,train_y,test_y= train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(static_train_x))\n",
      "print(\"Test data size:\", len(static_test_x))\n",
      "187/73:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "187/74:\n",
      "train_x,test_x,train_y,test_y= train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "187/75:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "187/76:\n",
      "# Set model parameter for random forest regression\n",
      "param = {\n",
      "    \"n_estimators\": 3000,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "187/77:\n",
      "train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "187/78:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "187/79:\n",
      "# Set model parameter for random forest regression\n",
      "param = {\n",
      "    \"n_estimators\": 3000,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "187/80: np.any(np.isnan(raw13))\n",
      "187/81: raw13 =raw13[raw13.LogKpu.notna()]\n",
      "187/82:\n",
      "\n",
      "raw13.to_csv('QSAR_Kpu/Mordard.csv' , sep=',' ,index=True)\n",
      "187/83: print (len(raw13))\n",
      "187/84: print (len(raw13 .columns))\n",
      "187/85:\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "187/86: fingerprint_to_model.head(3)\n",
      "187/87:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "fingerprint_to_model.shape\n",
      "187/88:\n",
      "import random\n",
      "SEED = random.seed(10)\n",
      "seed\n",
      "187/89:\n",
      "train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "187/90:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "187/91:\n",
      "# Set model parameter for random forest regression\n",
      "param = {\n",
      "    \"n_estimators\": 3000,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "187/92:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "model_PLS = PLSRegression(n_components=10)\n",
      "scores=cross_val_score(model_PLS, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "187/93: fingerprint_to_model.head(3)\n",
      "187/94:\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "187/95: fingerprint_to_model.head(3)\n",
      "187/96:\n",
      "import random\n",
      "SEED = random.seed(10)\n",
      "seed\n",
      "187/97:\n",
      "#not reduce\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "187/98:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "fingerprint_to_model.shape\n",
      "187/99:\n",
      "import random\n",
      "SEED = random.seed(10)\n",
      "seed\n",
      "187/100:\n",
      "train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "187/101:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "187/102:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "187/103:\n",
      "#not reduce\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "187/104: fingerprint_to_model.shape\n",
      "187/105:\n",
      "train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "187/106: train_x.shape\n",
      "187/107:\n",
      "# Set model parameter for random forest regression\n",
      "param = {\n",
      "    \"n_estimators\": 1000,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "189/196:\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "regr = MLPRegressor(random_state=1, max_iter=500, solver='lbfgs')\n",
      "scores=cross_val_score(model_regr, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "189/197:\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "regr = MLPRegressor(random_state=1, max_iter=500, solver='lbfgs')\n",
      "scores=cross_val_score(regr, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "197/2:\n",
      "path = r'Fingerprint/'\n",
      "\n",
      "Fp13  = pd.read_csv(path + 'mordard.csv'               , header = 0)\n",
      "197/3: Fp1.shape\n",
      "197/4:\n",
      "df = pd.read_csv('df6_lipinski_kpu.csv')\n",
      "df\n",
      "197/5: Fp13.shape\n",
      "197/6:\n",
      "df = pd.read_csv('df6_lipinski_kpu.csv')\n",
      "df\n",
      "197/7: name = df['compound']\n",
      "197/8: name.shape\n",
      "197/9:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df2 = df2.drop('Unnamed: 0', axis=1)\n",
      "df3 = df2.set_index('Name')\n",
      "df3\n",
      "197/10:\n",
      "LogKpu = df3[['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]\n",
      "LogKpu\n",
      "197/11:\n",
      "Fp13 = Fp13.rename(columns={'name': 'Name'})\n",
      "Fp13\n",
      "197/12:\n",
      "\n",
      "Fp13.Name = name\n",
      "197/13:\n",
      "\n",
      "Fpp13 = Fp13.drop('Name', axis=1)\n",
      "Fpp13\n",
      "197/14: Fpp13 = Fpp13.fillna(0)\n",
      "197/15:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)\n",
      "    Fp_normalized\n",
      "    \n",
      "    return Fp_normalized\n",
      "197/16:\n",
      "\n",
      "Fp13_n = normalized (Fpp13)\n",
      "197/17: Fp13_n\n",
      "197/18: np.any(np.isnan(Fp13_n))\n",
      "197/19: Fp13_n[\"Name\"] = Fp13.Name\n",
      "197/20: Fp1_n\n",
      "197/21: Fp13_n\n",
      "197/22: Fp13_n = Fp13_n.set_index('Name')\n",
      "197/23: Fp13_n\n",
      "197/24: raw13  = LogKpu.merge(Fp13_n, on='Name', how='outer')\n",
      "197/25: raw13\n",
      "197/26: raw13 =raw13[raw13.LogKpu.notna()]\n",
      "197/27:\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "197/28: fingerprint_to_model.head(3)\n",
      "197/29:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "fingerprint_to_model.shape\n",
      "197/30:\n",
      "import random\n",
      "SEED = random.seed(10)\n",
      "seed\n",
      "197/31:\n",
      "train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "197/32:\n",
      "import random\n",
      "SEED = random.seed(10)\n",
      "SEED\n",
      "197/33:\n",
      "train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "197/34:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "197/35:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "197/36:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "197/37:\n",
      "# Set model parameter for random forest regression\n",
      "param = {\n",
      "    \"n_estimators\": 1000,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/38:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "model_PLS = PLSRegression(n_components=10)\n",
      "scores=cross_val_score(model_PLS, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/39:\n",
      "from sklearn.metrics import r2_score\n",
      "#Try cross_val_predict and calculate R2\n",
      "y_pred = cross_val_predict(RF, X, y, cv=kf)\n",
      "#Rsqure\n",
      "r2_score(y, y_pred)\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(y, y_pred)\n",
      "\n",
      "print('R2 between prediction is', '{:4}'.r2_score)\n",
      "print('RMSE between prediction is', '{:4}'.RMSE_pred)\n",
      "197/40:\n",
      "from sklearn.metrics import r2_score\n",
      "#Try cross_val_predict and calculate R2\n",
      "y_pred = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "#Rsqure\n",
      "r2_score(train_y, y_pred)\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "print('R2 between prediction is', '{:4}'.r2_score)\n",
      "print('RMSE between prediction is', '{:4}'.RMSE_pred)\n",
      "197/41:\n",
      "from sklearn.metrics import r2_score\n",
      "#Try cross_val_predict and calculate R2\n",
      "y_pred = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "#Rsqure\n",
      "r2_score(train_y, y_pred)\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "print(f'R2 between prediction is {r2_score}')\n",
      "print('RMSE between prediction is', '{:4}'.RMSE_pred)\n",
      "197/42:\n",
      "from sklearn.metrics import r2_score\n",
      "#Try cross_val_predict and calculate R2\n",
      "y_pred = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "#Rsqure\n",
      "r2_score(train_y, y_pred)\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "print(f'R2 between prediction is {r2_score}')\n",
      "print(f'RMSE between prediction is {RMSE_pred}')\n",
      "197/43:\n",
      "from sklearn.metrics import r2_score\n",
      "#Try cross_val_predict and calculate R2\n",
      "y_pred = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "#Rsqure\n",
      "r2_score = r2_score(train_y, y_pred)\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "print(f'R2 between prediction is {r2_score}')\n",
      "print(f'RMSE between prediction is {RMSE_pred}')\n",
      "197/44:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "model_PLS = PLSRegression(n_components=10)\n",
      "scores=cross_val_score(model_PLS, train_x, train_y, cv=kf)\n",
      "#Try cross_val_predict and calculate R2\n",
      "y_pred = cross_val_predict(model_PLS, train_x, train_y, cv=kf)\n",
      "#Rsqure\n",
      "r2_score = r2_score(train_y, y_pred)\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "print(f'R2 between prediction is {r2_score}')\n",
      "print(f'RMSE between prediction is {RMSE_pred}')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "198/1:\n",
      "#flowchart\n",
      "#download data\n",
      "#create Morgan Fingerprint\n",
      "#normalized fingerprint\n",
      "#create test and train set\n",
      "#create CV of train set\n",
      "#train CV of train set model\n",
      "#train using linear regressor\n",
      "#train using pls\n",
      "#train using randomforestregressor\n",
      "#select most most descripted model\n",
      "#select top 20 features\n",
      "#predict test set from trained model\n",
      "198/2:\n",
      "#import important library\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn import svm, metrics, clone\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.neural_network import MLPClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "from sklearn.metrics import auc, accuracy_score, recall_score\n",
      "from sklearn.metrics import roc_curve, roc_auc_score\n",
      "import matplotlib.pyplot as plt\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import MACCSkeys\n",
      "from rdkit.Chem.AllChem import GetMorganFingerprintAsBitVect\n",
      "198/3:\n",
      "# Fix seed for reproducible results\n",
      "import random\n",
      "\n",
      "seed = random.seed(10)\n",
      "seed\n",
      "198/4: seed\n",
      "198/5: df = pd.read_csv('df6_lipinski_Kpu.csv')\n",
      "198/6: df.head(6)\n",
      "198/7: df.shape\n",
      "198/8:\n",
      "df1 = df[['compound', 'smiles', 'LogKpu']]\n",
      "df2 =df1[df1.LogKpu.notna()]\n",
      "198/9: df2\n",
      "198/10:\n",
      "def smiles_to_fp (smiles, method=\"maccs\", n_bits=2048):\n",
      "     # convert smiles to RDKit mol object\n",
      "    mol = Chem.MolFromSmiles(smiles)\n",
      "\n",
      "    if method == \"maccs\":\n",
      "        return np.array(MACCSkeys.GenMACCSKeys(mol))\n",
      "    if method == \"morgan2\":\n",
      "        return np.array(GetMorganFingerprintAsBitVect(mol, 2, nBits=n_bits))\n",
      "    if method == \"morgan3\":\n",
      "        return np.array(GetMorganFingerprintAsBitVect(mol, 3, nBits=n_bits))\n",
      "    else:\n",
      "        # NBVAL_CHECK_OUTPUT\n",
      "        print(f\"Warning: Wrong method specified: {method}. Default will be used instead.\")\n",
      "        return np.array(MACCSkeys.GenMACCSKeys(mol))\n",
      "198/11:\n",
      "compound_df = df2.copy()\n",
      "# Add column for fingerprint\n",
      "compound_df[\"fp\"] = compound_df[\"smiles\"].apply(smiles_to_fp)\n",
      "compound_df.head(3)\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "198/12:\n",
      "def plot_roc_curves_for_models(models, test_x, test_y, save_png=False):\n",
      "    \"\"\"\n",
      "    Helper function to plot customized roc curve.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    models: dict\n",
      "        Dictionary of pretrained machine learning models.\n",
      "    test_x: list\n",
      "        Molecular fingerprints for test set.\n",
      "    test_y: list\n",
      "        Associated activity labels for test set.\n",
      "    save_png: bool\n",
      "        Save image to disk (default = False)\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    fig:\n",
      "        Figure.\n",
      "    \"\"\"\n",
      "\n",
      "    fig, ax = plt.subplots()\n",
      "\n",
      "    # Below for loop iterates through your models list\n",
      "    for model in models:\n",
      "        # Select the model\n",
      "        ml_model = model[\"model\"]\n",
      "        # Prediction probability on test set\n",
      "        test_prob = ml_model.predict_proba(test_x)[:, 1]\n",
      "        # Prediction class on test set\n",
      "        test_pred = ml_model.predict(test_x)\n",
      "        # Compute False postive rate and True positive rate\n",
      "        fpr, tpr, thresholds = metrics.roc_curve(test_y, test_prob)\n",
      "        # Calculate Area under the curve to display on the plot\n",
      "        auc = roc_auc_score(test_y, test_prob)\n",
      "        # Plot the computed values\n",
      "        ax.plot(fpr, tpr, label=(f\"{model['label']} AUC area = {auc:.2f}\"))\n",
      "\n",
      "    # Custom settings for the plot\n",
      "    ax.plot([0, 1], [0, 1], \"r--\")\n",
      "    ax.set_xlabel(\"False Positive Rate\")\n",
      "    ax.set_ylabel(\"True Positive Rate\")\n",
      "    ax.set_title(\"Receiver Operating Characteristic\")\n",
      "    ax.legend(loc=\"lower right\")\n",
      "    # Save plot\n",
      "    if save_png:\n",
      "        fig.savefig(f\"{DATA}/roc_auc\", dpi=300, bbox_inches=\"tight\", transparent=True)\n",
      "    return fig\n",
      "198/13:\n",
      "fingerprint_to_model = compound_df.fp.tolist()\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "(\n",
      "    train_x,\n",
      "    test_x,\n",
      "    train_y,\n",
      "    test_y,\n",
      ") = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(static_train_x))\n",
      "print(\"Test data size:\", len(static_test_x))\n",
      "198/14:\n",
      "fingerprint_to_model = compound_df.fp.tolist()\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "    train_x, test_x, train_y, test_y, = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(static_train_x))\n",
      "print(\"Test data size:\", len(static_test_x))\n",
      "198/15:\n",
      "fingerprint_to_model = compound_df.fp.tolist()\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "train_x, test_x, train_y, test_y, = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(static_train_x))\n",
      "print(\"Test data size:\", len(static_test_x))\n",
      "198/16:\n",
      "# Fix seed for reproducible results\n",
      "import random\n",
      "\n",
      "SEED = random.seed(10)\n",
      "SEED\n",
      "198/17:\n",
      "fingerprint_to_model = compound_df.fp.tolist()\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "train_x, test_x, train_y, test_y, = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(static_train_x))\n",
      "print(\"Test data size:\", len(static_test_x))\n",
      "198/18:\n",
      "fingerprint_to_model = compound_df.fp.tolist()\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "train_x, test_x, train_y, test_y, = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(static_train_x))\n",
      "print(\"Test data size:\", len(static_test_x))\n",
      "198/19:\n",
      "fingerprint_to_model = compound_df.fp.tolist()\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "train_x, test_x, train_y, test_y, = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "197/45:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "model_PLS = PLSRegression(n_components=10)\n",
      "scores=cross_val_score(model_PLS, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/46:\n",
      "from sklearn.metrics import r2_score\n",
      "#Try cross_val_predict and calculate R2\n",
      "y_pred = cross_val_predict(model_PLS, train_x, train_y, cv=kf)\n",
      "#Rsqure\n",
      "r2_score = r2_score(train_y, y_pred)\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "print(f'R2 between prediction is {r2_score}')\n",
      "print(f'RMSE between prediction is {RMSE_pred}')\n",
      "197/47:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/48:\n",
      "from sklearn.metrics import r2_score\n",
      "#Try cross_val_predict and calculate R2\n",
      "y_pred = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "#Rsqure\n",
      "r2_score = r2_score(train_y, y_pred)\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "print(f'R2 between prediction is {r2_score}')\n",
      "print(f'RMSE between prediction is {RMSE_pred}')\n",
      "197/49:\n",
      "from sklearn.linear_model import Ridge\n",
      "Ridge = Ridge(alpha=1.0)\n",
      "from sklearn.metrics import r2_score\n",
      "#Try cross_val_predict and calculate R2\n",
      "y_pred = cross_val_predict(Ridge, train_x, train_y, cv=kf)\n",
      "#Rsqure\n",
      "r2_score = r2_score(train_y, y_pred)\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "print(f'R2 between prediction is {r2_score}')\n",
      "print(f'RMSE between prediction is {RMSE_pred}')\n",
      "197/50:\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "regr = MLPRegressor(random_state=1, max_iter=500, solver='lbfgs')\n",
      "scores=cross_val_score(regr, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "from sklearn.metrics import r2_score\n",
      "#Try cross_val_predict and calculate R2\n",
      "y_pred = cross_val_predict(regr, train_x, train_y, cv=kf)\n",
      "#Rsqure\n",
      "r2_score = r2_score(train_y, y_pred)\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "print(f'R2 between prediction is {r2_score}')\n",
      "print(f'RMSE between prediction is {RMSE_pred}')\n",
      "197/51:\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "regr = MLPRegressor(random_state=1, max_iter=500, solver='lbfgs', alpha=1.0)\n",
      "scores=cross_val_score(regr, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "from sklearn.metrics import r2_score\n",
      "#Try cross_val_predict and calculate R2\n",
      "y_pred = cross_val_predict(regr, train_x, train_y, cv=kf)\n",
      "#Rsqure\n",
      "r2_score = r2_score(train_y, y_pred)\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "print(f'R2 between prediction is {r2_score}')\n",
      "print(f'RMSE between prediction is {RMSE_pred}')\n",
      "197/52:\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "regr = MLPRegressor(random_state=1, max_iter=1000, solver='lbfgs', alpha=1.0)\n",
      "scores=cross_val_score(regr, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "from sklearn.metrics import r2_score\n",
      "#Try cross_val_predict and calculate R2\n",
      "y_pred = cross_val_predict(regr, train_x, train_y, cv=kf)\n",
      "#Rsqure\n",
      "r2_score = r2_score(train_y, y_pred)\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "print(f'R2 between prediction is {r2_score}')\n",
      "print(f'RMSE between prediction is {RMSE_pred}')\n",
      "197/53:\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "regr = MLPRegressor(random_state=1, max_iter=500, solver='lbfgs', alpha=0.1)\n",
      "scores=cross_val_score(regr, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "from sklearn.metrics import r2_score\n",
      "#Try cross_val_predict and calculate R2\n",
      "y_pred = cross_val_predict(regr, train_x, train_y, cv=kf)\n",
      "#Rsqure\n",
      "r2_score = r2_score(train_y, y_pred)\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "print(f'R2 between prediction is {r2_score}')\n",
      "print(f'RMSE between prediction is {RMSE_pred}')\n",
      "197/54:\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "regr = MLPRegressor(random_state=1, max_iter=500, solver='lbfgs')\n",
      "scores=cross_val_score(regr, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "from sklearn.metrics import r2_score\n",
      "#Try cross_val_predict and calculate R2\n",
      "y_pred = cross_val_predict(regr, train_x, train_y, cv=kf)\n",
      "#Rsqure\n",
      "r2_score = r2_score(train_y, y_pred)\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "print(f'R2 between prediction is {r2_score}')\n",
      "print(f'RMSE between prediction is {RMSE_pred}')\n",
      "197/55:\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "regr = MLPRegressor(random_state=1, max_iter=1000, solver='lbfgs', alpha = 0.1)\n",
      "scores=cross_val_score(regr, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "from sklearn.metrics import r2_score\n",
      "#Try cross_val_predict and calculate R2\n",
      "y_pred = cross_val_predict(regr, train_x, train_y, cv=kf)\n",
      "#Rsqure\n",
      "r2_score = r2_score(train_y, y_pred)\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "print(f'R2 between prediction is {r2_score}')\n",
      "print(f'RMSE between prediction is {RMSE_pred}')\n",
      "197/56:\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "regr = MLPRegressor(random_state=1, max_iter=1000, solver='lbfgs')\n",
      "scores=cross_val_score(regr, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "from sklearn.metrics import r2_score\n",
      "#Try cross_val_predict and calculate R2\n",
      "y_pred = cross_val_predict(regr, train_x, train_y, cv=kf)\n",
      "#Rsqure\n",
      "r2_score = r2_score(train_y, y_pred)\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "print(f'R2 between prediction is {r2_score}')\n",
      "print(f'RMSE between prediction is {RMSE_pred}')\n",
      "197/57:\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "clf = LogisticRegression(random_state=1, solver='liblinear')\n",
      "197/58:\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "LR = LogisticRegression(random_state=1, solver='liblinear')\n",
      "scores=cross_val_score(LR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "from sklearn.metrics import r2_score\n",
      "#Try cross_val_predict and calculate R2\n",
      "y_pred = cross_val_predict(LR, train_x, train_y, cv=kf)\n",
      "#Rsqure\n",
      "r2_score = r2_score(train_y, y_pred)\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "print(f'R2 between prediction is {r2_score}')\n",
      "print(f'RMSE between prediction is {RMSE_pred}')\n",
      "197/59:\n",
      "from sklearn.linear_model import linear_model\n",
      "Lasso = linear_model.Lasso()\n",
      "scores=cross_val_score(Lasso, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "from sklearn.metrics import r2_score\n",
      "#Try cross_val_predict and calculate R2\n",
      "y_pred = cross_val_predict(Lasso, train_x, train_y, cv=kf)\n",
      "#Rsqure\n",
      "r2_score = r2_score(train_y, y_pred)\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "print(f'R2 between prediction is {r2_score}')\n",
      "print(f'RMSE between prediction is {RMSE_pred}')\n",
      "197/60:\n",
      "from sklearn import linear_model\n",
      "Lasso = linear_model.Lasso()\n",
      "scores=cross_val_score(Lasso, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "from sklearn.metrics import r2_score\n",
      "#Try cross_val_predict and calculate R2\n",
      "y_pred = cross_val_predict(Lasso, train_x, train_y, cv=kf)\n",
      "#Rsqure\n",
      "r2_score = r2_score(train_y, y_pred)\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "print(f'R2 between prediction is {r2_score}')\n",
      "print(f'RMSE between prediction is {RMSE_pred}')\n",
      "197/61:\n",
      "from sklearn.linear_model import LassoCV\n",
      "reg = LassoCV(cv=kf, random_state=0).fit(train_x, train_y)\n",
      "scores=reg.score(train_x, train_y)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/62:\n",
      "from sklearn.linear_model import LassoCV\n",
      "reg = LassoCV(cv=kf, random_state=1).fit(train_x, train_y)\n",
      "scores=reg.score(train_x, train_y)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/63:\n",
      "from sklearn.linear_model import LassoCV\n",
      "reg = LassoCV(cv=kf, random_state=1, tol=0.001).fit(train_x, train_y)\n",
      "scores=reg.score(train_x, train_y)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/64:\n",
      "from sklearn.linear_model import LassoCV\n",
      "reg = LassoCV(cv=kf, random_state=1, tol=0.1).fit(train_x, train_y)\n",
      "scores=reg.score(train_x, train_y)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/65:\n",
      "from sklearn.linear_model import LassoCV\n",
      "reg = LassoCV(cv=kf, random_state=1, tol=0.1).fit(train_x, train_y)\n",
      "scores=reg.score(train_x, train_y)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.5f\" % (scores.mean(), scores.std()))\n",
      "197/66:\n",
      "from sklearn.linear_model import LassoCV\n",
      "reg = LassoCV(cv=10, random_state=1, tol=0.1).fit(train_x, train_y)\n",
      "scores=reg.score(train_x, train_y)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.5f\" % (scores.mean(), scores.std()))\n",
      "y_pred = cross_val_predict(Lasso, train_x, train_y, cv=kf)\n",
      "197/67:\n",
      "from sklearn.linear_model import LassoCV\n",
      "reg = LassoCV(cv=10, random_state=1, tol=0.1)\n",
      "scores=cross_val_score(reg, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.5f\" % (scores.mean(), scores.std()))\n",
      "y_pred = cross_val_predict(Lasso, train_x, train_y, cv=kf)\n",
      "197/68:\n",
      "from sklearn.linear_model import LassoCV\n",
      "reg = LassoCV(cv=10, random_state=1, tol=0.1)\n",
      "scores=cross_val_score(reg, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.5f\" % (scores.mean(), scores.std()))\n",
      "y_pred = cross_val_predict(Lasso, train_x, train_y, cv=kf)\n",
      "#Rsqure\n",
      "r2_score = r2_score(train_y, y_pred)\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "print(f'R2 between prediction is {r2_score}')\n",
      "print(f'RMSE between prediction is {RMSE_pred}')\n",
      "197/69:\n",
      "from sklearn.linear_model import LassoCV\n",
      "reg = LassoCV(cv=10, random_state=1, tol=0.1)\n",
      "scores=cross_val_score(reg, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.5f\" % (scores.mean(), scores.std()))\n",
      "y_pred = cross_val_predict(Lasso, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure\n",
      "r2_score = r2_score(train_y, y_pred)\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "print(f'R2 between prediction is {r2_score}')\n",
      "print(f'RMSE between prediction is {RMSE_pred}')\n",
      "197/70:\n",
      "from sklearn.linear_model import LassoCV\n",
      "reg = LassoCV(cv=10, random_state=1, tol=0.1)\n",
      "scores=cross_val_score(reg, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.5f\" % (scores.mean(), scores.std()))\n",
      "y_pred = cross_val_predict(Lasso, train_x, train_y, cv=kf)\n",
      "197/71:\n",
      "from sklearn.linear_model import LassoCV\n",
      "reg = LassoCV(cv=10, random_state=1, tol=0.1)\n",
      "scores=cross_val_score(reg, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.5f\" % (scores.mean(), scores.std()))\n",
      "y_pred = cross_val_predict(Lasso, train_x, train_y, cv=kf)\n",
      "197/72:\n",
      "#not reduce\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "197/73:\n",
      "from sklearn.linear_model import LassoCV\n",
      "reg = LassoCV(cv=10, random_state=1, tol=0.1)\n",
      "scores=cross_val_score(reg, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.5f\" % (scores.mean(), scores.std()))\n",
      "y_pred = cross_val_predict(Lasso, train_x, train_y, cv=kf)\n",
      "197/74:\n",
      "#not reduce\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "197/75:\n",
      "from sklearn.linear_model import LassoCV\n",
      "reg = LassoCV(cv=10, random_state=1, tol=0.1)\n",
      "scores=cross_val_score(reg, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.5f\" % (scores.mean(), scores.std()))\n",
      "y_pred = cross_val_predict(Lasso, train_x, train_y, cv=kf)\n",
      "197/76:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=20)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/77:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=10)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/78:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=5)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/79:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=2)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/80:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=100)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/81:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=20)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/82:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=30)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/83:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=10)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/84:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/85:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=20, shuffle=True, random_state=SEED)\n",
      "197/86:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/87:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
      "197/88:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/89:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "197/90:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/91:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/92:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/93:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/94:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/95:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/96:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/97:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/98:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/99:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/100:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/101:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/102:\n",
      "from sklearn.linear_model import Ridge\n",
      "Ridge = Ridge(alpha=1.0)\n",
      "from sklearn.metrics import r2_score\n",
      "#Try cross_val_predict and calculate R2\n",
      "y_pred = cross_val_predict(Ridge, train_x, train_y, cv=kf)\n",
      "#Rsqure\n",
      "r2_score = r2_score(train_y, y_pred)\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "print(f'R2 between prediction is {r2_score}')\n",
      "print(f'RMSE between prediction is {RMSE_pred}')\n",
      "197/103:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/104:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/105:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/106:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/107:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/108:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/109:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/110:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/111:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/112:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/113:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/114:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='linear')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/115:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='linear')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/116:\n",
      "from sklearn.linear_model import Ridge\n",
      "Ridge = Ridge(alpha=1.0)\n",
      "from sklearn.metrics import r2_score\n",
      "#Try cross_val_predict and calculate R2\n",
      "y_pred = cross_val_predict(Ridge, train_x, train_y, cv=kf)\n",
      "#Rsqure\n",
      "r2_score = r2_score(train_y, y_pred)\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "print(f'R2 between prediction is {r2_score}')\n",
      "print(f'RMSE between prediction is {RMSE_pred}')\n",
      "197/117:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='linear')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/118:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='linear')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/119:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/120:\n",
      "#Try cross_val_predict and calculate R2\n",
      "y_pred = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "#Rsqure\n",
      "r2_score = r2_score(train_y, y_pred)\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "print(f'R2 between prediction is {r2_score}')\n",
      "print(f'RMSE between prediction is {RMSE_pred}')\n",
      "197/121:\n",
      "#Try cross_val_predict and calculate R2\n",
      "y_pred = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "#Rsqure\n",
      "r2_score(train_y, y_pred)\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "print(f'R2 between prediction is {r2_score}')\n",
      "print(f'RMSE between prediction is {RMSE_pred}')\n",
      "197/122:\n",
      "#Try cross_val_predict and calculate R2\n",
      "y_pred = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "197/123:\n",
      "#Try cross_val_predict and calculate R2\n",
      "y_pred = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "r2_score = r2_score(train_y, y_pred)\n",
      "197/124:\n",
      "from sklearn.metrics import r2_score\n",
      "#Try cross_val_predict and calculate R2\n",
      "y_pred = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "#Rsqure\n",
      "r2_score = r2_score(train_y, y_pred)\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "print(f'R2 between prediction is {r2_score}')\n",
      "print(f'RMSE between prediction is {RMSE_pred}')\n",
      "198/20:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "fingerprint_to_model.shape\n",
      "198/21:\n",
      "train_x, test_x, train_y, test_y, = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "198/22:\n",
      "# Set model parameter for random forest regression\n",
      "param = {\n",
      "    \"n_estimators\": 1000,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "198/23:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "198/24:\n",
      "# Set model parameter for random forest regression\n",
      "param = {\n",
      "    \"n_estimators\": 1000,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "198/25:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "198/26:\n",
      "# Set model parameter for random forest regression\n",
      "param = {\n",
      "    \"n_estimators\": 1000,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/125: Fp13_n.mid\n",
      "197/126: Fp13_n.mid()\n",
      "197/127: Fp13_n.middle()\n",
      "197/128: Fp13_n.head(20)\n",
      "197/129:\n",
      "\n",
      "raw13.to_csv('QSAR_Kpu/Mordard.csv' , sep=',' ,index=True)\n",
      "197/130:\n",
      "#not reduce\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "197/131: fingerprint_to_model.shape\n",
      "197/132:\n",
      "# Set model parameter for random forest regression\n",
      "param = {\n",
      "    \"n_estimators\": 1000,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/133:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "from sklearn.metrics import r2_score\n",
      "#Try cross_val_predict and calculate R2\n",
      "y_pred = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "#Rsqure\n",
      "r2_score = r2_score(train_y, y_pred)\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "print(f'R2 between prediction is {r2_score}')\n",
      "print(f'RMSE between prediction is {RMSE_pred}')\n",
      "197/134: SVR = SVR.predict(test_y)\n",
      "197/135: y_predic_external = SVR.predict(test_x)\n",
      "197/136:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "from sklearn.metrics import r2_score\n",
      "#Try cross_val_predict and calculate R2\n",
      "y_pred = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "#Rsqure\n",
      "r2_score = r2_score(train_y, y_pred)\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "print(f'R2 between prediction is {r2_score}')\n",
      "print(f'RMSE between prediction is {RMSE_pred}')\n",
      "197/137:\n",
      "from sklearn.svm import SVR\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "from sklearn.metrics import r2_score\n",
      "#Try cross_val_predict and calculate R2\n",
      "y_pred = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "#Rsqure\n",
      "r2_score = r2_score(train_y, y_pred)\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "RMSE_pred = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "print(f'R2 between prediction is {r2_score}')\n",
      "print(f'RMSE between prediction is {RMSE_pred}')\n",
      "197/138: y_predic_external = SVR.predict(test_x)\n",
      "197/139: y_predic_external = SVR.fit(train_x, train_y)\n",
      "197/140: SVR_fit = SVR.fit(train_x, train_y)\n",
      "197/141: SVR_fit.score\n",
      "197/142: SVR_fit.score()\n",
      "197/143: SVR_fit.score(train_x, train_y)\n",
      "197/144:\n",
      "r2_score = SVR_fit.score(train_x, train_y)\n",
      "print(f'R2 between train is {r2_score}')\n",
      "197/145: SVR_predict_y = SVR.predict(test_x)\n",
      "197/146: r2_score = r2_score(test_y, SVR_predict_y)\n",
      "197/147:\n",
      "from sklearn.metrics import r2_score\n",
      "r2_score = r2_score(test_y, SVR_predict_y)\n",
      "197/148: y_pred = SVR.predict(test_x)\n",
      "197/149:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "r2_score = r2_score(test_y, y_pred)\n",
      "RMSE_pred = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "print(f'R2 between prediction is {r2_score}')\n",
      "print(f'RMSE between prediction is {RMSE_pred}')\n",
      "197/150:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "r2_score = r2_score(test_y, y_pred)\n",
      "RMSE_pred = mean_squared_error(test_y, y_pred)\n",
      "\n",
      "print(f'R2 between prediction is {r2_score}')\n",
      "print(f'RMSE between prediction is {RMSE_pred}')\n",
      "197/151:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "r2_score = r2_score(test_y, y_pred)\n",
      "RMSE_pred = mean_squared_error(test_y, y_pred)\n",
      "\n",
      "print(f'Q2 between prediction and test is {r2_score}')\n",
      "print(f'RMSE between prediction is {RMSE_pred}')\n",
      "197/152:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(train_y, y_ext)\n",
      "RMSE_test = mean_squared_error(train_y, y_pred)\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/153:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/154:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/155:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/156: from sklearn.inspection import permutation_importance\n",
      "197/157:\n",
      "from sklearn.inspection import permutation_importance\n",
      "\n",
      "perm_importance = permutation_importance(SVC, train_x, train_y)\n",
      "peature_names = ['feature1', 'feature2', 'feature3', ...... ]\n",
      "features = np.array(feature_names)\n",
      "\n",
      "sorted_idx = perm_importance.importances_mean.argsort()\n",
      "plt.barh(features[sorted_idx], perm_importance.importances_mean[sorted_idx])\n",
      "plt.xlabel(\"Permutation Importance\")\n",
      "197/158:\n",
      "from sklearn.inspection import permutation_importance\n",
      "\n",
      "perm_importance = permutation_importance(SVC, train_x, train_y)\n",
      "peature_names = ['feature1', 'feature2', 'feature3', ......]\n",
      "features = np.array(feature_names)\n",
      "\n",
      "sorted_idx = perm_importance.importances_mean.argsort()\n",
      "plt.barh(features[sorted_idx], perm_importance.importances_mean[sorted_idx])\n",
      "plt.xlabel(\"Permutation Importance\")\n",
      "197/159:\n",
      "from sklearn.inspection import permutation_importance\n",
      "\n",
      "result = permutation_importance(SVC, train_x, train_y, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances[sorted_idx].T,\n",
      "           vert=False, labels=train_x.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (train set)\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "197/160:\n",
      "from sklearn.inspection import permutation_importance\n",
      "\n",
      "result = permutation_importance(SVR, train_x, train_y, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances[sorted_idx].T,\n",
      "           vert=False, labels=train_x.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (train set)\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "197/161:\n",
      "from sklearn.inspection import permutation_importance\n",
      "\n",
      "result = permutation_importance(SVR, train_x, train_y, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances[sorted_idx].T,\n",
      "           vert=False, labels=train_x.head[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (train set)\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "197/162:\n",
      "from sklearn.inspection import permutation_importance\n",
      "\n",
      "result = permutation_importance(SVR, train_x, train_y, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances[sorted_idx].T,\n",
      "           vert=False, labels=fingerprint_to_model.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (train set)\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "197/163:\n",
      "from sklearn.inspection import permutation_importance\n",
      "\n",
      "result = permutation_importance(SVR, train_x, train_y, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances[sorted_idx].T,\n",
      "           vert=False, labels=fingerprint_to_model.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (train set)\")\n",
      "fig.tight_layout()\n",
      "fig.set_size_inches(18.5, 10.5)\n",
      "plt.show()\n",
      "197/164:\n",
      "from sklearn.inspection import permutation_importance\n",
      "\n",
      "result = permutation_importance(SVR, train_x, train_y, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances[sorted_idx].T,\n",
      "           vert=False, labels=fingerprint_to_model.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (train set)\")\n",
      "fig.tight_layout()\n",
      "fig.set_size_inches(18.5, 20)\n",
      "plt.show()\n",
      "197/165:\n",
      "from sklearn.inspection import permutation_importance\n",
      "\n",
      "result = permutation_importance(SVR, train_x, train_y, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances[sorted_idx].T,\n",
      "           vert=False, labels=fingerprint_to_model.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (train set)\")\n",
      "fig.tight_layout()\n",
      "fig.set_size_inches(18.5, 25)\n",
      "plt.show()\n",
      "197/166: fingerprint_to_model.columns[sorted_idx]\n",
      "197/167: result.importances[sorted_idx].T\n",
      "197/168: result\n",
      "197/169: result.to_frame()\n",
      "197/170: pd.DataFrame(result.importances[sorted_idx], columns=fingerprint_to_model.columns[sorted_idx])\n",
      "197/171: pd.DataFrame(sorted_idx, columns=fingerprint_to_model.columns[sorted_idx])\n",
      "197/172: pd.DataFrame(result.importances_mean, columns=fingerprint_to_model.columns)\n",
      "197/173: pd.DataFrame(result.importances_mean, columns=fingerprint_to_model.columns[sorted_idx])\n",
      "197/174: pd.DataFrame(result.importances_mean.T, columns=fingerprint_to_model.columns[sorted_idx])\n",
      "197/175: result.importances_mean\n",
      "197/176: result\n",
      "197/177: result.shape\n",
      "197/178: len(result)\n",
      "197/179: result.importances\n",
      "197/180: pd.DataFrame(result, columns=fingerprint_to_model.columns[sorted_idx])\n",
      "197/181: pd.DataFrame(result.importances_mean, columns=fingerprint_to_model.columns[sorted_idx])\n",
      "197/182: result.importances_mean\n",
      "197/183: result.importances_mean.transpose\n",
      "197/184: result.importances_mean.transpose()\n",
      "197/185: result.importances_mean.transpose(X)\n",
      "197/186: result.importances_mean.transpose(1)\n",
      "197/187: result.importances_mean.transpose(1,1)\n",
      "197/188: result.importances_mean.transpose(0,1)\n",
      "197/189: result.importances_mean\n",
      "197/190: result.importances_mean.to_csv('importances_mean.csv')\n",
      "197/191: pd.DataFrame(result.importances_mean)\n",
      "197/192: importances_mean = pd.DataFrame(result.importances_mean)\n",
      "197/193: importances_mean.to_csv('importances_mean.csv')\n",
      "197/194: fingerprint_to_model.columns[sorted_idx]\n",
      "197/195: sorted_idx\n",
      "197/196: pd.DataFrame(result.importances_mean[sorted_idx], columns=fingerprint_to_model.columns[sorted_idx])\n",
      "197/197: pd.DataFrame(result.importances_mean[sorted_idx].T, columns=fingerprint_to_model.columns[sorted_idx])\n",
      "197/198: result.importances_mean[sorted_idx].T\n",
      "197/199: pd.DataFrame(result.importances_mean[sorted_idx].T, columns=fingerprint_to_model.columns[sorted_idx].T)\n",
      "197/200: result.importances_mean[sorted_idx]\n",
      "197/201: df = pd.DataFrame(fingerprint_to_model.columns[sorted_idx])\n",
      "197/202: df\n",
      "197/203: result.importances[sorted_idx]\n",
      "197/204: df2 = pd.DataFrame(result.importances_mean[sorted_idx])\n",
      "197/205: df2\n",
      "197/206: df3 = pd.concat([df, df2], axis=1)\n",
      "197/207: df3\n",
      "197/208: df3.to_csv('importances_mean.csv')\n",
      "197/209: df3\n",
      "197/210: df3.rename({0: Features}, {0: importnaces_mean})\n",
      "197/211: df3.rename({'0': 'Features'}, {'0': 'importnaces_mean'})\n",
      "197/212: df3.rename({'0':'Features'}, {'0':'importnaces_mean'})\n",
      "197/213: df3.rename(columns={'0':'Features', '0':'importnaces_mean'})\n",
      "197/214: df4 = df3.rename(columns={'0':'Features', '0':'importnaces_mean'})\n",
      "197/215:\n",
      "df4 = df3.rename(columns={'0':'Features', '0':'importnaces_mean'})\n",
      "df4\n",
      "197/216:\n",
      "df4 = df3.rename(columns={'Features':'0', '0':'importnaces_mean'})\n",
      "df4\n",
      "197/217:\n",
      "df4 = df3.rename(columns={'Features':'0', '0':'importnaces_mean'})\n",
      "df4\n",
      "197/218:\n",
      "df3 = pd.concat([df, df2], axis=1)\n",
      "df3\n",
      "197/219:\n",
      "df4 = df3.rename(columns={'0':'Features', '0':'importnaces_mean'})\n",
      "df4\n",
      "197/220: df5.read_csv('importances_mean.csv')\n",
      "197/221: df5 = pd.read_csv('importances_mean.csv')\n",
      "197/222: df5\n",
      "197/223: df5 = df5.drop(['Unnamed: 0'], axis=1)\n",
      "197/224:\n",
      "df5 = df5.drop(['Unnamed: 0'], axis=1)\n",
      "df5\n",
      "197/225:\n",
      "df6 = df5.drop(['Unnamed: 0'], axis=1)\n",
      "df6\n",
      "197/226:\n",
      "df5 = pd.read_csv('importances_mean.csv')\n",
      "df5\n",
      "197/227:\n",
      "df5 = pd.read_csv('importances_mean.csv')\n",
      "df5.head(3)\n",
      "197/228:\n",
      "df6 = df5.drop(['Unnamed: 0'], axis=1)\n",
      "df6\n",
      "197/229:\n",
      "df6 = df5.drop(['Unnamed: 0'], axis=1)\n",
      "df6.head(20)\n",
      "197/230:\n",
      "df6 = df5.drop(['Unnamed: 0'], axis=1)\n",
      "df6.tail(20)\n",
      "197/231: [selection] = ['ABC', 'ATS4pe', 'ATS4Z', 'ATS6Rare', 'nl', 'ATS3p', 'ATS2Z', 'ATS6v', 'ATS0m', 'ATS2m', 'ATS4v', 'ATS2se', 'ATS5pe', 'ATS4se', 'ATS4s', 'ATS8s', 'AATS3s', 'AATS2s', 'ATS1v', 'nH']\n",
      "197/232:\n",
      "[selection] = ['ABC', 'ATS4pe', 'ATS4Z', 'ATS6Rare', 'nl', 'ATS3p', 'ATS2Z', 'ATS6v', 'ATS0m', 'ATS2m', 'ATS4v', 'ATS2se', 'ATS5pe', 'ATS4se', 'ATS4s', 'ATS8s', 'AATS3s', 'AATS2s', 'ATS1v', 'nH']\n",
      "fingerprint_to_model = compound_df[selection]\n",
      "197/233: fingerprint_to_model\n",
      "197/234:\n",
      "[selection] = ['ABC', 'ATS4pe', 'ATS4Z', 'ATS6Rare', 'nl', 'ATS3p', 'ATS2Z', 'ATS6v', 'ATS0m']\n",
      "fingerprint_to_model = compound_df[selection]\n",
      "197/235:\n",
      "\n",
      "fingerprint_to_model = compound_df[['ABC', 'ATS4pe', 'ATS4Z', 'ATS6Rare', 'nl', 'ATS3p', 'ATS2Z', 'ATS6v', 'ATS0m',\n",
      "               'ATS2m', 'ATS4v', 'ATS2se', 'ATS5pe', 'ATS4se', 'ATS4s', 'ATS8s', 'AATS3s', 'AATS2s', 'ATS1v', 'nH']]\n",
      "197/236:\n",
      "fingerprint_to_model = compound_df[['ABC', 'ATS4pe', 'ATS4Z', 'ATS6are', 'nI', 'ATS3p', 'ATS2Z', 'ATS6v', 'ATS0m',\n",
      "               'ATS2m', 'ATS4v', 'ATS2se', 'ATS5pe', 'ATS4se', 'ATS4s', 'ATS8s', 'AATS3s', 'AATS2s', 'ATS1v', 'nH']]\n",
      "197/237: fingerprint_to_model\n",
      "197/238: fingerprint_to_model\n",
      "197/239: label_to_model = compound_df.LogKpu.tolist()\n",
      "197/240:\n",
      "train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "197/241:\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "fingerprint_to_modl.shape\n",
      "197/242:\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "fingerprint_to_model.shape\n",
      "197/243:\n",
      "train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "197/244:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/245:\n",
      "#not reduce\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "197/246: fingerprint_to_model.shape\n",
      "197/247:\n",
      "train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "197/248: train_x.shape\n",
      "197/249:\n",
      "# Set model parameter for random forest regression\n",
      "param = {\n",
      "    \"n_estimators\": 1000,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/250:\n",
      "# higher Features reduced performance?\n",
      "# yes\n",
      "197/251:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/252: y_cv.to_csv('y_cv.csv')\n",
      "197/253:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/254:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/255:\n",
      "#not reduce\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "197/256: fingerprint_to_model.shape\n",
      "197/257:\n",
      "train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "197/258: train_x.shape\n",
      "197/259:\n",
      "# Set model parameter for random forest regression\n",
      "param = {\n",
      "    \"n_estimators\": 1000,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/260:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/261:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/262:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/263:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/264:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/265:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/266:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/267:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "197/268:\n",
      "path = r'Fingerprint/'\n",
      "\n",
      "Fp13  = pd.read_csv(path + 'mordard.csv'               , header = 0)\n",
      "197/269: Fp13.shape\n",
      "197/270:\n",
      "df = pd.read_csv('df6_lipinski_kpu.csv')\n",
      "df\n",
      "197/271: name = df['compound']\n",
      "197/272: name.shape\n",
      "197/273:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df2 = df2.drop('Unnamed: 0', axis=1)\n",
      "df3 = df2.set_index('Name')\n",
      "df3\n",
      "197/274:\n",
      "LogKpu = df3[['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]\n",
      "LogKpu\n",
      "197/275:\n",
      "Fp13 = Fp13.rename(columns={'name': 'Name'})\n",
      "Fp13\n",
      "197/276:\n",
      "\n",
      "Fp13.Name = name\n",
      "197/277:\n",
      "\n",
      "Fpp13 = Fp13.drop('Name', axis=1)\n",
      "Fpp13\n",
      "197/278: Fpp13 = Fpp13.fillna(0)\n",
      "197/279:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)\n",
      "    Fp_normalized\n",
      "    \n",
      "    return Fp_normalized\n",
      "197/280:\n",
      "\n",
      "Fp13_n = normalized (Fpp13)\n",
      "197/281: Fp13_n\n",
      "197/282: np.any(np.isnan(Fp13_n))\n",
      "197/283: Fp13_n[\"Name\"] = Fp13.Name\n",
      "197/284: Fp13_n\n",
      "197/285: Fp13_n = Fp13_n.set_index('Name')\n",
      "197/286: Fp13_n.head(20)\n",
      "197/287: raw13  = LogKpu.merge(Fp13_n, on='Name', how='outer')\n",
      "197/288: raw13\n",
      "197/289: np.any(np.isnan(raw13))\n",
      "197/290: raw13 =raw13[raw13.LogKpu.notna()]\n",
      "197/291:\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "197/292:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "fingerprint_to_model.shape\n",
      "197/293:\n",
      "import random\n",
      "SEED = random.seed(10)\n",
      "SEED\n",
      "197/294:\n",
      "train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "197/295:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "197/296: fingerprint_to_model.shape\n",
      "197/297:\n",
      "train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "197/298: train_x.shape\n",
      "197/299:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/300:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/301:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/302:\n",
      "train_x.to_csv('y_train.csv')\n",
      "train_x.to_csv('x_train.csv')\n",
      "train_y.to_csv('y_train.csv')\n",
      "test_y.to_csv('y_test.csv')\n",
      "y_pred.to_csv('y_pred.csv')\n",
      "\n",
      "y_ext.to_csv('y_SVR_ext.csv')\n",
      "197/303: importances_mean = pd.DataFrame(result.importances_mean)\n",
      "197/304: fingerprint_to_model.columns[sorted_idx]\n",
      "197/305: df2 = pd.DataFrame(result.importances_mean[sorted_idx])\n",
      "197/306: df = pd.DataFrame(fingerprint_to_model.columns[sorted_idx])\n",
      "197/307:\n",
      "from sklearn.inspection import permutation_importance\n",
      "\n",
      "result = permutation_importance(SVR, train_x, train_y, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(T,\n",
      "           vert=False, labels=fingerprint_to_model.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (train set)\")\n",
      "fig.tight_layout()\n",
      "fig.set_size_inches(18.5, 25)\n",
      "plt.show()\n",
      "197/308:\n",
      "from sklearn.inspection import permutation_importance\n",
      "\n",
      "result = permutation_importance(SVR, train_x, train_y, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances_mean[sorted_idx].T,\n",
      "           vert=False, labels=fingerprint_to_model.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (train set)\")\n",
      "fig.tight_layout()\n",
      "fig.set_size_inches(18.5, 25)\n",
      "plt.show()\n",
      "197/309:\n",
      "from sklearn.inspection import permutation_importance\n",
      "\n",
      "result = permutation_importance(SVR, train_x, train_y, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances_mean[sorted_idx].T,\n",
      "           vert=False, labels=fingerprint_to_model.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (train set)\")\n",
      "fig.tight_layout()\n",
      "fig.set_size_inches(18.5, 25)\n",
      "plt.show()\n",
      "197/310: importances_mean = pd.DataFrame(result.importances_mean)\n",
      "197/311: fingerprint_to_model.columns[sorted_idx]\n",
      "197/312: df2 = pd.DataFrame(result.importances_mean[sorted_idx])\n",
      "197/313: df = pd.DataFrame(fingerprint_to_model.columns[sorted_idx])\n",
      "197/314: df\n",
      "197/315: df2\n",
      "197/316:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/317:\n",
      "train_x.to_csv('y_train.csv')\n",
      "train_x.to_csv('x_train.csv')\n",
      "train_y.to_csv('y_train.csv')\n",
      "test_y.to_csv('y_test.csv')\n",
      "y_pred.to_csv('y_pred.csv')\n",
      "\n",
      "y_ext.to_csv('y_SVR_ext.csv')\n",
      "197/318: plt.scatter(test_y, y_ext)\n",
      "197/319:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='black')\n",
      "plt.scatter(train_y, train_y, c='blue')\n",
      "197/320:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='black')\n",
      "197/321:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='black')\n",
      "m, b = np.polyfit(test_y, y_ext, 1)\n",
      "197/322:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='black')\n",
      "m, b = np.polyfit(test_y, y_ext, 1)\n",
      "plt.plot(x, m*x + b)\n",
      "197/323:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='black')\n",
      "m, b = np.polyfit(test_y, y_ext, 1)\n",
      "plt.plot(test_y, m*x + b)\n",
      "197/324:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='black')\n",
      "m, b = np.polyfit(test_y, y_ext, 1)\n",
      "plt.plot(test_y, m*test_y + b)\n",
      "197/325:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='black')\n",
      "197/326:\n",
      "pd.DataFrame(train_x).to_csv(\"Results/x_train.csv\")\n",
      "pd.DataFrame(test_x).to_csv(\"Results/x_test.csv\")\n",
      "pd.DataFrame(train_x).to_csv(\"Results/train_x.csv\")\n",
      "pd.DataFrame(train_y).to_csv(\"Results/train_y.csv\")\n",
      "pd.DataFrame(test_y).to_csv(\"Results/test_y.csv\")\n",
      "pd.DataFrame(y_pred).to_csv(\"Results/y_pred.csv\")\n",
      "pd.DataFrame(y_ext).to_csv(\"Results/y_SVR_ext.csv\")\n",
      "197/327:\n",
      "pd.DataFrame(train_x).to_csv(\"Result/x_train.csv\")\n",
      "pd.DataFrame(test_x).to_csv(\"Result/x_test.csv\")\n",
      "pd.DataFrame(train_x).to_csv(\"Result/train_x.csv\")\n",
      "pd.DataFrame(train_y).to_csv(\"Result/train_y.csv\")\n",
      "pd.DataFrame(test_y).to_csv(\"Result/test_y.csv\")\n",
      "pd.DataFrame(y_pred).to_csv(\"Result/y_pred.csv\")\n",
      "pd.DataFrame(y_ext).to_csv(\"Result/y_SVR_ext.csv\")\n",
      "202/1:\n",
      "#import important library\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn import svm, metrics, clone\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.neural_network import MLPClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "from sklearn.metrics import auc, accuracy_score, recall_score\n",
      "from sklearn.metrics import roc_curve, roc_auc_score\n",
      "import matplotlib.pyplot as plt\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import MACCSkeys\n",
      "from rdkit.Chem.AllChem import GetMorganFingerprintAsBitVect\n",
      "202/2:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "202/3:\n",
      "df = pd.read_csv('df6_lipinski_kpu.csv')\n",
      "df\n",
      "202/4: name = df['compound']\n",
      "202/5:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df2 = df2.drop('Unnamed: 0', axis=1)\n",
      "df3 = df2.set_index('Name')\n",
      "df3\n",
      "202/6:\n",
      "df = pd.read_csv('swissadme_kpu.csv')\n",
      "df\n",
      "202/7:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df2 = df2.drop('Canonical SMILES', axis=1)\n",
      "df3 = df2.set_index('Name')\n",
      "df3\n",
      "202/8:\n",
      "df_Fn = df3.drop(['smiles', 'Cells', 'LogKpu', 'Formula'], axis=1)\n",
      "df_Fn\n",
      "202/9:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)\n",
      "    Fp_normalized\n",
      "    \n",
      "    return Fp_normalized\n",
      "202/10:\n",
      "\n",
      "df_Fn_n = normalized (df_Fn)\n",
      "202/11:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df2 = df2.drop('Canonical SMILES', axis=1)\n",
      "df3 = df2.set_index('Name')\n",
      "df3\n",
      "202/12:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df2 = df2.drop('Canonical SMILES', axis=1)\n",
      "df3 = df2.set_index('Name')\n",
      "df3.head(3)\n",
      "202/13:\n",
      "ESOL Class = []\n",
      "for i in df_final.pIC50:\n",
      "  if float(i) >= 6:\n",
      "    bioactivity_class.append(\"active\")\n",
      "  elif float(i) <= 4:\n",
      "    bioactivity_class.append(\"inactive\")\n",
      "  else:\n",
      "    bioactivity_class.append(\"intermediate\")\n",
      "202/14:\n",
      "ESOL_Class = []\n",
      "for i in df3.ESOL_Class:\n",
      "\n",
      "    if i = 'Pooly soluble':\n",
      "        ESOL_Class.append(\"0\")\n",
      "    elif i = 'Moderately soluble':\n",
      "        ESOL_Class.append(\"1\")\n",
      "    elif i = 'Soluble':\n",
      "        ESOL_Class.append(\"2\")\n",
      "    else:\n",
      "        ESOL_Class.append(\"3\")\n",
      "202/15:\n",
      "ESOL_Class = []\n",
      "for i in df3.ESOL_Class:\n",
      "\n",
      "    if i == 'Pooly soluble':\n",
      "        ESOL_Class.append(\"0\")\n",
      "    elif i = 'Moderately soluble':\n",
      "        ESOL_Class.append(\"1\")\n",
      "    elif i = 'Soluble':\n",
      "        ESOL_Class.append(\"2\")\n",
      "    else:\n",
      "        ESOL_Class.append(\"3\")\n",
      "202/16:\n",
      "ESOL_Class = []\n",
      "for i in df3.ESOL_Class:\n",
      "\n",
      "    if i == 'Pooly soluble':\n",
      "        ESOL_Class.append(\"0\")\n",
      "    elif i == 'Moderately soluble':\n",
      "        ESOL_Class.append(\"1\")\n",
      "    elif i == 'Soluble':\n",
      "        ESOL_Class.append(\"2\")\n",
      "    else:\n",
      "        ESOL_Class.append(\"3\")\n",
      "202/17:\n",
      "ESOL_Class = []\n",
      "for i in df3.ESOL_Class:\n",
      "\n",
      "    if i == 'Pooly soluble':\n",
      "        ESOL_Class.append(\"0\")\n",
      "    elif i == 'Moderately soluble':\n",
      "        ESOL_Class.append(\"1\")\n",
      "    elif i == 'Soluble':\n",
      "        ESOL_Class.append(\"2\")\n",
      "    else:\n",
      "        ESOL_Class.append(\"3\")\n",
      "202/18:\n",
      "df = pd.read_csv('swissadme_kpu.csv')\n",
      "df\n",
      "202/19:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df2 = df2.drop('Canonical SMILES', axis=1)\n",
      "df3 = df2.set_index('Name')\n",
      "df3.head(3)\n",
      "202/20:\n",
      "ESOL_Class = []\n",
      "for i in df3.ESOL_Class:\n",
      "\n",
      "    if i == 'Pooly soluble':\n",
      "        ESOL_Class.append(\"0\")\n",
      "    elif i == 'Moderately soluble':\n",
      "        ESOL_Class.append(\"1\")\n",
      "    elif i == 'Soluble':\n",
      "        ESOL_Class.append(\"2\")\n",
      "    else:\n",
      "        ESOL_Class.append(\"3\")\n",
      "202/21:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df2 = df2.drop('Canonical_SMILES', axis=1)\n",
      "df3 = df2.set_index('Name')\n",
      "df3.head(3)\n",
      "202/22:\n",
      "ESOL_Class = []\n",
      "for i in df3.ESOL_Class:\n",
      "\n",
      "    if i == 'Pooly soluble':\n",
      "        ESOL_Class.append(\"0\")\n",
      "    elif i == 'Moderately soluble':\n",
      "        ESOL_Class.append(\"1\")\n",
      "    elif i == 'Soluble':\n",
      "        ESOL_Class.append(\"2\")\n",
      "    else:\n",
      "        ESOL_Class.append(\"3\")\n",
      "202/23: df3.head(3)\n",
      "202/24: df3.ESOL_Class(3)\n",
      "202/25: df3.ESOL_Class\n",
      "202/26: ESOL_Class\n",
      "202/27:\n",
      "Ali_Class = []\n",
      "for i in df3.Ali_Class:\n",
      "\n",
      "    if i == 'Pooly soluble':\n",
      "        Ali_Class.append(\"0\")\n",
      "    elif i == 'Moderately soluble':\n",
      "        Ali_Class.append(\"1\")\n",
      "    elif i == 'Soluble':\n",
      "        Ali_Class.append(\"2\")\n",
      "    else:\n",
      "        Ali_Class.append(\"3\")\n",
      "202/28:\n",
      "df = pd.read_csv('swissadme_kpu.csv')\n",
      "df\n",
      "202/29:\n",
      "ESOL_Class = []\n",
      "for i in df3.ESOL_Class:\n",
      "\n",
      "    if i == 'Poorly soluble':\n",
      "        ESOL_Class.append(\"1\")\n",
      "    elif i == 'Moderately soluble':\n",
      "        ESOL_Class.append(\"2\")\n",
      "    elif i == 'Soluble':\n",
      "        ESOL_Class.append(\"3\")\n",
      "    else:\n",
      "        ESOL_Class.append(\"4\")\n",
      "202/30:\n",
      "df = pd.read_csv('swissadme_kpu.csv')\n",
      "df\n",
      "202/31:\n",
      "df = pd.read_csv('swissadme_kpu.csv')\n",
      "df\n",
      "202/32:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df2 = df2.drop('Canonical_SMILES', axis=1)\n",
      "df3 = df2.set_index('Name')\n",
      "df3.head(3)\n",
      "202/33:\n",
      "ESOL_Class = []\n",
      "for i in df3.ESOL_Class:\n",
      "\n",
      "    if i == 'Poorly soluble':\n",
      "        ESOL_Class.append(\"1\")\n",
      "    elif i == 'Moderately soluble':\n",
      "        ESOL_Class.append(\"2\")\n",
      "    elif i == 'Soluble':\n",
      "        ESOL_Class.append(\"3\")\n",
      "    else:\n",
      "        ESOL_Class.append(\"4\")\n",
      "202/34:\n",
      "Ali_Class = []\n",
      "for i in df3.Ali_Class:\n",
      "\n",
      "    if i == 'insoluble':\n",
      "        Ali_Class.append(\"0\")\n",
      "    elif i == 'Poorly soluble':\n",
      "        Ali_Class.append(\"1\")\n",
      "    elif i == 'Moderately soluble':\n",
      "        Ali_Class.append(\"2\")\n",
      "    elif i == 'Soluble':\n",
      "        Ali_Class.append(\"3\")\n",
      "    else:\n",
      "        Ali_Class.append(\"4\")\n",
      "202/35:\n",
      "Ali_Class = []\n",
      "for i in df3.Silli:\n",
      "\n",
      "    if i == 'insoluble':\n",
      "        Ali_Class.append(\"0\")\n",
      "    elif i == 'Poorly soluble':\n",
      "        Ali_Class.append(\"1\")\n",
      "    elif i == 'Moderately soluble':\n",
      "        Ali_Class.append(\"2\")\n",
      "    elif i == 'Soluble':\n",
      "        Ali_Class.append(\"3\")\n",
      "    else:\n",
      "        Ali_Class.append(\"4\")\n",
      "202/36:\n",
      "Ali_Class = []\n",
      "for i in df3.Silicos-IT_class:\n",
      "\n",
      "    if i == 'insoluble':\n",
      "        Ali_Class.append(\"0\")\n",
      "    elif i == 'Poorly soluble':\n",
      "        Ali_Class.append(\"1\")\n",
      "    elif i == 'Moderately soluble':\n",
      "        Ali_Class.append(\"2\")\n",
      "    elif i == 'Soluble':\n",
      "        Ali_Class.append(\"3\")\n",
      "    else:\n",
      "        Ali_Class.append(\"4\")\n",
      "202/37:\n",
      "df = pd.read_csv('swissadme_kpu.csv')\n",
      "df\n",
      "202/38:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df2 = df2.drop('Canonical_SMILES', axis=1)\n",
      "df3 = df2.set_index('Name')\n",
      "df3.head(3)\n",
      "202/39:\n",
      "ESOL_Class = []\n",
      "for i in df3.ESOL_Class:\n",
      "\n",
      "    if i == 'Poorly soluble':\n",
      "        ESOL_Class.append(\"1\")\n",
      "    elif i == 'Moderately soluble':\n",
      "        ESOL_Class.append(\"2\")\n",
      "    elif i == 'Soluble':\n",
      "        ESOL_Class.append(\"3\")\n",
      "    else:\n",
      "        ESOL_Class.append(\"4\")\n",
      "202/40:\n",
      "Ali_Class = []\n",
      "for i in df3.Ali_Class:\n",
      "\n",
      "    if i == 'insoluble':\n",
      "        Ali_Class.append(\"0\")\n",
      "    elif i == 'Poorly soluble':\n",
      "        Ali_Class.append(\"1\")\n",
      "    elif i == 'Moderately soluble':\n",
      "        Ali_Class.append(\"2\")\n",
      "    elif i == 'Soluble':\n",
      "        Ali_Class.append(\"3\")\n",
      "    else:\n",
      "        Ali_Class.append(\"4\")\n",
      "202/41:\n",
      "Ali_Class = []\n",
      "for i in df3.Silicos_IT_class:\n",
      "\n",
      "    if i == 'insoluble':\n",
      "        Ali_Class.append(\"0\")\n",
      "    elif i == 'Poorly soluble':\n",
      "        Ali_Class.append(\"1\")\n",
      "    elif i == 'Moderately soluble':\n",
      "        Ali_Class.append(\"2\")\n",
      "    elif i == 'Soluble':\n",
      "        Ali_Class.append(\"3\")\n",
      "    else:\n",
      "        Ali_Class.append(\"4\")\n",
      "202/42:\n",
      "df = pd.read_csv('swissadme_kpu.csv')\n",
      "df\n",
      "202/43:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df2 = df2.drop('Canonical_SMILES', axis=1)\n",
      "df3 = df2.set_index('Name')\n",
      "df3.head(3)\n",
      "202/44: df3.columns()\n",
      "202/45: df3.columns\n",
      "202/46:\n",
      "Ali_Class = []\n",
      "for i in df3.'Silicos-IT_class':\n",
      "\n",
      "    if i == 'insoluble':\n",
      "        Ali_Class.append(\"0\")\n",
      "    elif i == 'Poorly soluble':\n",
      "        Ali_Class.append(\"1\")\n",
      "    elif i == 'Moderately soluble':\n",
      "        Ali_Class.append(\"2\")\n",
      "    elif i == 'Soluble':\n",
      "        Ali_Class.append(\"3\")\n",
      "    else:\n",
      "        Ali_Class.append(\"4\")\n",
      "202/47:\n",
      "df = pd.read_csv('swissadme_kpu.csv')\n",
      "df\n",
      "202/48:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df2 = df2.drop('Canonical_SMILES', axis=1)\n",
      "df3 = df2.set_index('Name')\n",
      "df3.head(3)\n",
      "202/49: df3.columns\n",
      "202/50:\n",
      "df = pd.read_csv('swissadme_kpu.csv')\n",
      "df\n",
      "202/51:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df2 = df2.drop('Canonical_SMILES', axis=1)\n",
      "df3 = df2.set_index('Name')\n",
      "df3.head(3)\n",
      "202/52:\n",
      "ESOL_Class = []\n",
      "for i in df3.ESOL_Class:\n",
      "\n",
      "    if i == 'Poorly soluble':\n",
      "        ESOL_Class.append(\"1\")\n",
      "    elif i == 'Moderately soluble':\n",
      "        ESOL_Class.append(\"2\")\n",
      "    elif i == 'Soluble':\n",
      "        ESOL_Class.append(\"3\")\n",
      "    else:\n",
      "        ESOL_Class.append(\"4\")\n",
      "202/53: df3.columns\n",
      "202/54:\n",
      "Ali_Class = []\n",
      "for i in df3.Ali_Class:\n",
      "\n",
      "    if i == 'insoluble':\n",
      "        Ali_Class.append(\"0\")\n",
      "    elif i == 'Poorly soluble':\n",
      "        Ali_Class.append(\"1\")\n",
      "    elif i == 'Moderately soluble':\n",
      "        Ali_Class.append(\"2\")\n",
      "    elif i == 'Soluble':\n",
      "        Ali_Class.append(\"3\")\n",
      "    else:\n",
      "        Ali_Class.append(\"4\")\n",
      "202/55:\n",
      "Ali_Class = []\n",
      "for i in df3.Silicos-IT_class:\n",
      "\n",
      "    if i == 'insoluble':\n",
      "        Ali_Class.append(\"0\")\n",
      "    elif i == 'Poorly soluble':\n",
      "        Ali_Class.append(\"1\")\n",
      "    elif i == 'Moderately soluble':\n",
      "        Ali_Class.append(\"2\")\n",
      "    elif i == 'Soluble':\n",
      "        Ali_Class.append(\"3\")\n",
      "    else:\n",
      "        Ali_Class.append(\"4\")\n",
      "202/56:\n",
      "Ali_Class = []\n",
      "for i in df3.Silicos_IT_class:\n",
      "\n",
      "    if i == 'insoluble':\n",
      "        Silicos_IT_class.append(\"0\")\n",
      "    elif i == 'Poorly soluble':\n",
      "        Silicos_IT_class.append(\"1\")\n",
      "    elif i == 'Moderately soluble':\n",
      "        Silicos_IT_class.append(\"2\")\n",
      "    elif i == 'Soluble':\n",
      "        Silicos_IT_class.append(\"3\")\n",
      "    else:\n",
      "        Silicos_IT_class.append(\"4\")\n",
      "202/57:\n",
      "Silicos_IT_class = []\n",
      "for i in df3.Silicos_IT_class:\n",
      "\n",
      "    if i == 'insoluble':\n",
      "        Silicos_IT_class.append(\"0\")\n",
      "    elif i == 'Poorly soluble':\n",
      "        Silicos_IT_class.append(\"1\")\n",
      "    elif i == 'Moderately soluble':\n",
      "        Silicos_IT_class.append(\"2\")\n",
      "    elif i == 'Soluble':\n",
      "        Silicos_IT_class.append(\"3\")\n",
      "    else:\n",
      "        Silicos_IT_class.append(\"4\")\n",
      "202/58:\n",
      "ESOL_Class = []\n",
      "for i in df3.ESOL_Class:\n",
      "\n",
      "    if i == 'Poorly soluble':\n",
      "        ESOL_Class.append(\"1\")\n",
      "    elif i == 'Moderately soluble':\n",
      "        ESOL_Class.append(\"2\")\n",
      "    elif i == 'Soluble':\n",
      "        ESOL_Class.append(\"3\")\n",
      "    else:\n",
      "        ESOL_Class.append(\"4\")\n",
      "202/59:\n",
      "Ali_Class = []\n",
      "for i in df3.Ali_Class:\n",
      "\n",
      "    if i == 'insoluble':\n",
      "        Ali_Class.append(\"0\")\n",
      "    elif i == 'Poorly soluble':\n",
      "        Ali_Class.append(\"1\")\n",
      "    elif i == 'Moderately soluble':\n",
      "        Ali_Class.append(\"2\")\n",
      "    elif i == 'Soluble':\n",
      "        Ali_Class.append(\"3\")\n",
      "    else:\n",
      "        Ali_Class.append(\"4\")\n",
      "202/60:\n",
      "Silicos_IT_class = []\n",
      "for i in df3.Silicos_IT_class:\n",
      "\n",
      "    if i == 'insoluble':\n",
      "        Silicos_IT_class.append(\"0\")\n",
      "    elif i == 'Poorly soluble':\n",
      "        Silicos_IT_class.append(\"1\")\n",
      "    elif i == 'Moderately soluble':\n",
      "        Silicos_IT_class.append(\"2\")\n",
      "    elif i == 'Soluble':\n",
      "        Silicos_IT_class.append(\"3\")\n",
      "    else:\n",
      "        Silicos_IT_class.append(\"4\")\n",
      "202/61:\n",
      "#drop 'GIabsorption','BBBpermeant', 'Pgpsubstrate', 'CYP1A2inhibitor', 'CYP2C19inhibitor','CYP2C9inhibitor', 'CYP2D6inhibitor', 'CYP3A4inhibitor'\n",
      "\n",
      "df4 =df3.drop(['GIabsorption','BBBpermeant', 'Pgpsubstrate', 'CYP1A2inhibitor', 'CYP2C19inhibitor','CYP2C9inhibitor', 'CYP2D6inhibitor', 'CYP3A4inhibitor'], axis=1)\n",
      "202/62: df4\n",
      "202/63:\n",
      "#drop 'smiles','Cells', 'GIabsorption','BBBpermeant', 'Pgpsubstrate', 'CYP1A2inhibitor', 'CYP2C19inhibitor','CYP2C9inhibitor', 'CYP2D6inhibitor', 'CYP3A4inhibitor'\n",
      "\n",
      "df4 =df3.drop(['smiles','Cells','GIabsorption','BBBpermeant', 'Pgpsubstrate', 'CYP1A2inhibitor', 'CYP2C19inhibitor','CYP2C9inhibitor', 'CYP2D6inhibitor', 'CYP3A4inhibitor'], axis=1)\n",
      "202/64: df4\n",
      "202/65:\n",
      "#drop 'smiles','Cells','Formula', 'GIabsorption','BBBpermeant', 'Pgpsubstrate', 'CYP1A2inhibitor', 'CYP2C19inhibitor','CYP2C9inhibitor', 'CYP2D6inhibitor', 'CYP3A4inhibitor'\n",
      "\n",
      "df4 =df3.drop(['smiles','Cells','Formula', 'GIabsorption','BBBpermeant', 'Pgpsubstrate', 'CYP1A2inhibitor', 'CYP2C19inhibitor','CYP2C9inhibitor', 'CYP2D6inhibitor', 'CYP3A4inhibitor'], axis=1)\n",
      "202/66: df4\n",
      "202/67:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)\n",
      "    Fp_normalized\n",
      "    \n",
      "    return Fp_normalized\n",
      "202/68: df4_n = normalized(df4)\n",
      "202/69:\n",
      "ESOL_Class = pd.DataFrame(ESOL_Class)\n",
      "Ali_Class = pd.DataFrame(Ali_Class)\n",
      "Silicos_IT_class = pd.DataFrame(Silicos_IT_class)\n",
      "202/70: df5 = pd.concat[[df4, ESOL_Class, Ali_Class, Silicos_IT_class]]\n",
      "202/71: df5 = pd.concat([df4, ESOL_Class, Ali_Class, Silicos_IT_class])\n",
      "202/72: df5\n",
      "202/73: df5 = pd.concat([df4, ESOL_Class, Ali_Class, Silicos_IT_class], axis=1)\n",
      "202/74: df5\n",
      "202/75:\n",
      "ESOL_Class = pd.DataFrame(ESOL_Class)\n",
      "Ali_Class = pd.DataFrame(Ali_Class)\n",
      "Silicos_IT_class = pd.DataFrame(Silicos_IT_class)\n",
      "ESOL_Class.shape\n",
      "202/76:\n",
      "ESOL_Class.shape\n",
      "Ali_Class.shape\n",
      "202/77:\n",
      "\n",
      "Ali_Class.shape\n",
      "202/78:\n",
      "ESOL_Class = pd.DataFrame(ESOL_Class)\n",
      "Ali_Class = pd.DataFrame(Ali_Class)\n",
      "Silicos_IT_class = pd.DataFrame(Silicos_IT_class)\n",
      "ESOL_Class.shape\n",
      "202/79: Silicos_IT_class.shape\n",
      "202/80: df5 = pd.concat([df4, ESOL_Class, Ali_Class, Silicos_IT_class], axis=True)\n",
      "202/81: df5 = pd.concat([df4, ESOL_Class, Ali_Class, Silicos_IT_class], index=True)\n",
      "202/82: df5 = pd.concat([df4, ESOL_Class, Ali_Class, Silicos_IT_class], axis=1)\n",
      "202/83: df5\n",
      "202/84: df4\n",
      "202/85: ESOL_Class\n",
      "202/86:\n",
      "ESOL_Class = pd.DataFrame(ESOL_Class, columns='ESOL_Class')\n",
      "Ali_Class = pd.DataFrame(Ali_Class, columns='Ali_Class')\n",
      "Silicos_IT_class = pd.DataFrame(Silicos_IT_class, columns='Silicos_IT_class')\n",
      "ESOL_Class.shape\n",
      "202/87:\n",
      "ESOL_Class = pd.DataFrame(ESOL_Class, columns='ESOL_Class')\n",
      "Ali_Class = pd.DataFrame(Ali_Class, columns='Ali_Class')\n",
      "Silicos_IT_class = pd.DataFrame(Silicos_IT_class, columns='Silicos_IT_class')\n",
      "ESOL_Class.shape\n",
      "202/88:\n",
      "ESOL_Class = pd.DataFrame(ESOL_Class, columns=['ESOL_Class'])\n",
      "Ali_Class = pd.DataFrame(Ali_Class, columns=['Ali_Class'])\n",
      "Silicos_IT_class = pd.DataFrame(Silicos_IT_class, columns=['Silicos_IT_class'])\n",
      "ESOL_Class.shape\n",
      "202/89: ESOL_Class\n",
      "202/90: df5 = pd.concat([df4, ESOL_Class, Ali_Class, Silicos_IT_class], axis=1)\n",
      "202/91: df5\n",
      "202/92: df_all = pd.concat([df4, ESOL_Class], axis=1)\n",
      "202/93: df_all\n",
      "202/94: df_all = pd.concat([df4, ESOL_Class], axis=0)\n",
      "202/95: df_all\n",
      "202/96: df_all = pd.concat([df4, ESOL_Class])\n",
      "202/97: df_all\n",
      "202/98: ESOL_Class\n",
      "202/99: ESOL_Class\n",
      "202/100:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df2 = df2.drop('Canonical_SMILES', axis=1)\n",
      "df3 = df2.set_index('Name')\n",
      "df3.head(3)\n",
      "202/101:\n",
      "ESOL_Class = []\n",
      "for i in df3.ESOL_Class:\n",
      "\n",
      "    if i == 'Poorly soluble':\n",
      "        ESOL_Class.append(\"1\")\n",
      "    elif i == 'Moderately soluble':\n",
      "        ESOL_Class.append(\"2\")\n",
      "    elif i == 'Soluble':\n",
      "        ESOL_Class.append(\"3\")\n",
      "    else:\n",
      "        ESOL_Class.append(\"4\")\n",
      "202/102:\n",
      "ESOL_Class = pd.DataFrame(ESOL_Class, columns=['ESOL_Class'])\n",
      "Ali_Class = pd.DataFrame(Ali_Class, columns=['Ali_Class'])\n",
      "Silicos_IT_class = pd.DataFrame(Silicos_IT_class, columns=['Silicos_IT_class'])\n",
      "ESOL_Class.shape\n",
      "202/103: ESOL_Class\n",
      "202/104: Silicos_IT_class.shape\n",
      "202/105:\n",
      "Ali_Class = []\n",
      "for i in df3.Ali_Class:\n",
      "\n",
      "    if i == 'insoluble':\n",
      "        Ali_Class.append(\"0\")\n",
      "    elif i == 'Poorly soluble':\n",
      "        Ali_Class.append(\"1\")\n",
      "    elif i == 'Moderately soluble':\n",
      "        Ali_Class.append(\"2\")\n",
      "    elif i == 'Soluble':\n",
      "        Ali_Class.append(\"3\")\n",
      "    else:\n",
      "        Ali_Class.append(\"4\")\n",
      "202/106:\n",
      "Silicos_IT_class = []\n",
      "for i in df3.Silicos_IT_class:\n",
      "\n",
      "    if i == 'insoluble':\n",
      "        Silicos_IT_class.append(\"0\")\n",
      "    elif i == 'Poorly soluble':\n",
      "        Silicos_IT_class.append(\"1\")\n",
      "    elif i == 'Moderately soluble':\n",
      "        Silicos_IT_class.append(\"2\")\n",
      "    elif i == 'Soluble':\n",
      "        Silicos_IT_class.append(\"3\")\n",
      "    else:\n",
      "        Silicos_IT_class.append(\"4\")\n",
      "202/107:\n",
      "#drop 'smiles','Cells','Formula', 'GIabsorption','BBBpermeant', 'Pgpsubstrate', 'CYP1A2inhibitor', 'CYP2C19inhibitor','CYP2C9inhibitor', 'CYP2D6inhibitor', 'CYP3A4inhibitor'\n",
      "\n",
      "df4 =df3.drop(['smiles','Cells','Formula', 'GIabsorption','BBBpermeant', 'Pgpsubstrate', 'CYP1A2inhibitor', 'CYP2C19inhibitor','CYP2C9inhibitor', 'CYP2D6inhibitor', 'CYP3A4inhibitor'], axis=1)\n",
      "202/108: df4\n",
      "202/109:\n",
      "ESOL_Class = pd.DataFrame(ESOL_Class, columns=['ESOL_Class'])\n",
      "Ali_Class = pd.DataFrame(Ali_Class, columns=['Ali_Class'])\n",
      "Silicos_IT_class = pd.DataFrame(Silicos_IT_class, columns=['Silicos_IT_class'])\n",
      "ESOL_Class.shape\n",
      "202/110: ESOL_Class\n",
      "202/111: Ali_Class\n",
      "202/112: Silicos_IT_class.shape\n",
      "202/113:\n",
      "\n",
      "Ali_Class.shape\n",
      "202/114: df4\n",
      "202/115: df_all = pd.concat([df4, ESOL_Class, Ali_Class, Silicos_IT_class])\n",
      "202/116: df_all\n",
      "202/117: df_all = pd.concat([df4, ESOL_Class, Ali_Class, Silicos_IT_class], ignore_index=True)\n",
      "202/118: df_all\n",
      "202/119:\n",
      "ESOL_Class = pd.DataFrame(ESOL_Class, columns=['ESOL_Class']).to_csv('ESOL_Class.csv')\n",
      "Ali_Class = pd.DataFrame(Ali_Class, columns=['Ali_Class']).to_csv('Ali_Class.csv')\n",
      "Silicos_IT_class = pd.DataFrame(Silicos_IT_class, columns=['Silicos_IT_class']).to_csv('Silicos_IT_class.csv')\n",
      "ESOL_Class.shape\n",
      "202/120:\n",
      "ESOL_Class = pd.DataFrame(ESOL_Class, columns=['ESOL_Class']).to_csv('ESOL_Class.csv')\n",
      "Ali_Class = pd.DataFrame(Ali_Class, columns=['Ali_Class']).to_csv('Ali_Class.csv')\n",
      "Silicos_IT_class = pd.DataFrame(Silicos_IT_class, columns=['Silicos_IT_class']).to_csv('Silicos_IT_class.csv')\n",
      "202/121:\n",
      "ESOL_Class = pd.DataFrame(ESOL_Class, columns=['ESOL_Class'])\n",
      "Ali_Class = pd.DataFrame(Ali_Class, columns=['Ali_Class'])\n",
      "Silicos_IT_class = pd.DataFrame(Silicos_IT_class, columns=['Silicos_IT_class'])\n",
      "202/122:\n",
      "ESOL_Class.to_csv('ESOL_Class.csv')\n",
      "Ali_Class.to_csv('Ali_Class.csv')\n",
      "Silicos_IT_class.to_csv('Silicos_IT_class.csv')\n",
      "202/123: ESOL_Class\n",
      "202/124:\n",
      "ESOL_Class = []\n",
      "for i in df3.ESOL_Class:\n",
      "\n",
      "    if i == 'Poorly soluble':\n",
      "        ESOL_Class.append(\"1\")\n",
      "    elif i == 'Moderately soluble':\n",
      "        ESOL_Class.append(\"2\")\n",
      "    elif i == 'Soluble':\n",
      "        ESOL_Class.append(\"3\")\n",
      "    else:\n",
      "        ESOL_Class.append(\"4\")\n",
      "202/125:\n",
      "Ali_Class = []\n",
      "for i in df3.Ali_Class:\n",
      "\n",
      "    if i == 'insoluble':\n",
      "        Ali_Class.append(\"0\")\n",
      "    elif i == 'Poorly soluble':\n",
      "        Ali_Class.append(\"1\")\n",
      "    elif i == 'Moderately soluble':\n",
      "        Ali_Class.append(\"2\")\n",
      "    elif i == 'Soluble':\n",
      "        Ali_Class.append(\"3\")\n",
      "    else:\n",
      "        Ali_Class.append(\"4\")\n",
      "202/126:\n",
      "Silicos_IT_class = []\n",
      "for i in df3.Silicos_IT_class:\n",
      "\n",
      "    if i == 'insoluble':\n",
      "        Silicos_IT_class.append(\"0\")\n",
      "    elif i == 'Poorly soluble':\n",
      "        Silicos_IT_class.append(\"1\")\n",
      "    elif i == 'Moderately soluble':\n",
      "        Silicos_IT_class.append(\"2\")\n",
      "    elif i == 'Soluble':\n",
      "        Silicos_IT_class.append(\"3\")\n",
      "    else:\n",
      "        Silicos_IT_class.append(\"4\")\n",
      "202/127:\n",
      "#drop 'smiles','Cells','Formula', 'GIabsorption','BBBpermeant', 'Pgpsubstrate', 'CYP1A2inhibitor', 'CYP2C19inhibitor','CYP2C9inhibitor', 'CYP2D6inhibitor', 'CYP3A4inhibitor'\n",
      "\n",
      "df4 =df3.drop(['smiles','Cells','Formula', 'GIabsorption','BBBpermeant', 'Pgpsubstrate', 'CYP1A2inhibitor', 'CYP2C19inhibitor','CYP2C9inhibitor', 'CYP2D6inhibitor', 'CYP3A4inhibitor'], axis=1)\n",
      "202/128: df4\n",
      "202/129:\n",
      "ESOL_Class = pd.DataFrame(ESOL_Class, columns=['ESOL_Class'])\n",
      "Ali_Class = pd.DataFrame(Ali_Class, columns=['Ali_Class'])\n",
      "Silicos_IT_class = pd.DataFrame(Silicos_IT_class, columns=['Silicos_IT_class'])\n",
      "202/130:\n",
      "ESOL_Class.to_csv('ESOL_Class.csv')\n",
      "Ali_Class.to_csv('Ali_Class.csv')\n",
      "Silicos_IT_class.to_csv('Silicos_IT_class.csv')\n",
      "202/131: ESOL_Class\n",
      "202/132:\n",
      "df = pd.read_csv('swissadme_kpu.csv')\n",
      "df\n",
      "202/133: df5 = pd.read_csv('swissadme_kpu.csv')\n",
      "202/134: df5\n",
      "202/135: df5[ESOL_Class]\n",
      "202/136: df5.ESOL_Class\n",
      "202/137: df5\n",
      "202/138: df6 = df5.drop(['Canonical_SMILES', 'smiles','Cells','Formula', 'GIabsorption','BBBpermeant', 'Pgpsubstrate', 'CYP1A2inhibitor', 'CYP2C19inhibitor','CYP2C9inhibitor', 'CYP2D6inhibitor', 'CYP3A4inhibitor'], axis=1)\n",
      "202/139: df6\n",
      "202/140: df7 =df6[df6.LogKpu.notna()]\n",
      "202/141: df7\n",
      "202/142: df8 = df6.drop(['compound', 'LogKpu'], axis=1)\n",
      "202/143: df8\n",
      "202/144: df8 = df6.drop(['LogKpu'], axis=1)\n",
      "202/145: df8\n",
      "202/146: df8.set_index(compound)\n",
      "202/147: df8.set_index('compound')\n",
      "202/148: df9 = df8.set_index('compound')\n",
      "202/149: df9\n",
      "202/150: df9 = df6.set_index('compound')\n",
      "202/151: df7 = df6.set_index('compound')\n",
      "202/152: df7\n",
      "202/153: label_to_model = df76.LogKpu.tolist()\n",
      "202/154: label_to_model = df6.LogKpu.tolist()\n",
      "202/155: label_to_model\n",
      "202/156: label_to_model = df7.LogKpu.tolist()\n",
      "202/157: df8_Fn = df7.drop(['LogKpu'], axis=1)\n",
      "202/158:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)\n",
      "    Fp_normalized\n",
      "    \n",
      "    return Fp_normalized\n",
      "202/159:\n",
      "\n",
      "df8_Fn_n = normalized (df8_Fn)\n",
      "202/160: df8_Fn_n\n",
      "202/161: fingerprint_to_model = df8_Fn_n\n",
      "202/162:\n",
      "import random\n",
      "SEED = random.seed(10)\n",
      "SEED\n",
      "202/163:\n",
      "train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "202/164:\n",
      "df7 = df6.set_index('compound')\n",
      "df7 = df6.LogKpu.notna\n",
      "202/165: df7\n",
      "202/166: df7 = df6.set_index('compound')\n",
      "202/167: df7\n",
      "202/168: df7\n",
      "202/169: df7 =df6[df6.LogKpu.notna()]\n",
      "202/170: df7\n",
      "202/171: df8 = df7.set_index('compound')\n",
      "202/172: df8\n",
      "202/173: label_to_model = df8.LogKpu.tolist()\n",
      "202/174: df8_Fn = df8.drop(['LogKpu'], axis=1)\n",
      "202/175:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)\n",
      "    Fp_normalized\n",
      "    \n",
      "    return Fp_normalized\n",
      "202/176: df8_Fn_n = normalized (df8_Fn)\n",
      "202/177: df8_Fn_n\n",
      "202/178: fingerprint_to_model = df8_Fn_n\n",
      "202/179:\n",
      "import random\n",
      "SEED = random.seed(10)\n",
      "SEED\n",
      "202/180:\n",
      "train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "202/181:\n",
      "# Set model parameter for random forest regression\n",
      "param = {\n",
      "    \"n_estimators\": 1000,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "202/182:\n",
      "#import important library\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn import svm, metrics, clone\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.neural_network import MLPClassifier\n",
      "from sklearn.model_selection import KFold, train_test_split\n",
      "from sklearn.metrics import auc, accuracy_score, recall_score\n",
      "from sklearn.metrics import roc_curve, roc_auc_score\n",
      "import matplotlib.pyplot as plt\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import MACCSkeys\n",
      "from rdkit.Chem.AllChem import GetMorganFingerprintAsBitVect\n",
      "\n",
      "# Set model parameter for random forest regression\n",
      "param = {\n",
      "    \"n_estimators\": 1000,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "202/183:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "# Set model parameter for random forest regression\n",
      "param = {\n",
      "    \"n_estimators\": 1000,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "202/184:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "202/185: fingerprint_to_model = df8_Fn\n",
      "202/186:\n",
      "import random\n",
      "SEED = random.seed(10)\n",
      "SEED\n",
      "202/187:\n",
      "train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "202/188:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "# Set model parameter for random forest regression\n",
      "param = {\n",
      "    \"n_estimators\": 1000,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "202/189:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "# Set model parameter for random forest regression\n",
      "param = {\n",
      "    \"n_estimators\": 100,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "202/190:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "202/191: fingerprint_to_model = df8_Fn_n\n",
      "202/192:\n",
      "import random\n",
      "SEED = random.seed(10)\n",
      "SEED\n",
      "202/193:\n",
      "train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "202/194:\n",
      "train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "202/195:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "# Set model parameter for random forest regression\n",
      "param = {\n",
      "    \"n_estimators\": 100,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "202/196:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "202/197:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "202/198:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/328:\n",
      "from sklearn.inspection import permutation_importance\n",
      "\n",
      "result = permutation_importance(SVR, train_x, train_y, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances_mean[sorted_idx].T,\n",
      "           vert=False, labels=fingerprint_to_model.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (train set)\")\n",
      "fig.tight_layout()\n",
      "fig.set_size_inches(18.5, 25)\n",
      "plt.show()\n",
      "197/329:\n",
      "#not reduce\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "197/330:\n",
      "from sklearn.inspection import permutation_importance\n",
      "\n",
      "result = permutation_importance(SVR, train_x, train_y, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances_mean[sorted_idx].T,\n",
      "           vert=False, labels=fingerprint_to_model.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (train set)\")\n",
      "fig.tight_layout()\n",
      "fig.set_size_inches(18.5, 25)\n",
      "plt.show()\n",
      "197/331: fingerprint_to_model\n",
      "197/332:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/333:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='linear')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/334:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='poly')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/335:\n",
      "from sklearn.inspection import permutation_importance\n",
      "\n",
      "result = permutation_importance(SVR, test_x, y_ext, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances_mean[sorted_idx].T,\n",
      "           vert=False, labels=fingerprint_to_model.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (train set)\")\n",
      "fig.tight_layout()\n",
      "fig.set_size_inches(18.5, 25)\n",
      "plt.show()\n",
      "197/336:\n",
      "from sklearn.inspection import permutation_importance\n",
      "\n",
      "result = permutation_importance(SVR, test_x, y_ext, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances_mean[sorted_idx].T,\n",
      "           vert=False, labels=fingerprint_to_model.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (train set)\")\n",
      "fig.tight_layout()\n",
      "fig.set_size_inches(18.5, 25)\n",
      "plt.show()\n",
      "197/337:\n",
      "from sklearn.inspection import permutation_importance\n",
      "\n",
      "result = permutation_importance(SVR, test_x, y_ext, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "197/338:\n",
      "from sklearn.inspection import permutation_importance\n",
      "\n",
      "result = permutation_importance(SVR, test_x, y_ext, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances_mean[sorted_idx].T,\n",
      "           vert=False, labels=fingerprint_to_model.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (train set)\")\n",
      "fig.tight_layout()\n",
      "fig.set_size_inches(18.5, 25)\n",
      "plt.show()\n",
      "197/339:\n",
      "from sklearn.inspection import permutation_importance\n",
      "\n",
      "result = permutation_importance(SVR, test_x, y_ext, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances_mean[sorted_idx].T,\n",
      "           vert=False, labels= fingerprint_to_model.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (train set)\")\n",
      "fig.tight_layout()\n",
      "fig.set_size_inches(18.5, 25)\n",
      "plt.show()\n",
      "197/340:\n",
      "from sklearn.inspection import permutation_importance\n",
      "\n",
      "result = permutation_importance(SVR, test_x, y_ext, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances_mean[sorted_idx].T,\n",
      "           vert=False, labels= test_x.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (train set)\")\n",
      "fig.tight_layout()\n",
      "fig.set_size_inches(18.5, 25)\n",
      "plt.show()\n",
      "197/341:\n",
      "from sklearn.inspection import permutation_importance\n",
      "\n",
      "result = permutation_importance(SVR, test_x, y_ext, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances_mean[sorted_idx].T,\n",
      "           vert=False, labels= fingerprint_to_model.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (train set)\")\n",
      "fig.tight_layout()\n",
      "fig.set_size_inches(18.5, 25)\n",
      "plt.show()\n",
      "197/342:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "param = {\n",
      "    \"n_estimators\": 1000,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/343:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "param = {\n",
      "    \"n_estimators\": 1000,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/344:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='black')\n",
      "197/345:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='black')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "197/346:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='black')\n",
      "plt.scatter(train_y, y_pred, c='blue')\n",
      "197/347:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "\n",
      "plt.scatter(train_y, y_pred, c='blue')\n",
      "197/348:\n",
      "pd.DataFrame(train_x, columns=['train_x']).to_csv(\"Result_RF/x_train.csv\")\n",
      "pd.DataFrame(test_x, columns=['test_x']).to_csv(\"Result_RF/x_test.csv\")\n",
      "pd.DataFrame(train_x, columns=['train_x']).to_csv(\"Result_RF/train_x.csv\")\n",
      "pd.DataFrame(train_y, columns=['train_y']).to_csv(\"Result_RF/train_y.csv\")\n",
      "pd.DataFrame(test_y, columns=['test_y']).to_csv(\"Result_RF/test_y.csv\")\n",
      "pd.DataFrame(y_pred, columns=['y_pred']).to_csv(\"Result_RF/y_pred.csv\")\n",
      "pd.DataFrame(y_ext, columns=['y_ext']).to_csv(\"Result_RF/y_SVR_ext.csv\")\n",
      "197/349: pd.DataFrame(train_x).to_csv(\"Result_RF/x_train.csv\")\n",
      "197/350:\n",
      "pd.DataFrame(train_x, columns='train_x').to_csv(\"Result_RF/x_train.csv\")\n",
      "pd.DataFrame(test_x, columns='test_x').to_csv(\"Result_RF/x_test.csv\")\n",
      "pd.DataFrame(train_x, columns='train_x').to_csv(\"Result_RF/train_x.csv\")\n",
      "pd.DataFrame(train_y, columns='train_y').to_csv(\"Result_RF/train_y.csv\")\n",
      "pd.DataFrame(test_y, columns='test_y').to_csv(\"Result_RF/test_y.csv\")\n",
      "pd.DataFrame(y_pred, columns='y_pred').to_csv(\"Result_RF/y_pred.csv\")\n",
      "pd.DataFrame(y_ext, columns='y_ext').to_csv(\"Result_RF/y_SVR_ext.csv\")\n",
      "197/351:\n",
      "pd.DataFrame(train_x, columns=['train_x']).to_csv(\"Result_RF/x_train.csv\")\n",
      "pd.DataFrame(test_x, columns='test_x').to_csv(\"Result_RF/x_test.csv\")\n",
      "pd.DataFrame(train_x, columns='train_x').to_csv(\"Result_RF/train_x.csv\")\n",
      "pd.DataFrame(train_y, columns='train_y').to_csv(\"Result_RF/train_y.csv\")\n",
      "pd.DataFrame(test_y, columns='test_y').to_csv(\"Result_RF/test_y.csv\")\n",
      "pd.DataFrame(y_pred, columns='y_pred').to_csv(\"Result_RF/y_pred.csv\")\n",
      "pd.DataFrame(y_ext, columns='y_ext').to_csv(\"Result_RF/y_SVR_ext.csv\")\n",
      "197/352:\n",
      "pd.DataFrame(train_x).to_csv(\"Result_RF/x_train.csv\")\n",
      "pd.DataFrame(test_x).to_csv(\"Result_RF/x_test.csv\")\n",
      "pd.DataFrame(train_x).to_csv(\"Result_RF/train_x.csv\")\n",
      "pd.DataFrame(train_y).to_csv(\"Result_RF/train_y.csv\")\n",
      "pd.DataFrame(test_y).to_csv(\"Result_RF/test_y.csv\")\n",
      "pd.DataFrame(y_pred).to_csv(\"Result_RF/y_pred.csv\")\n",
      "pd.DataFrame(y_ext).to_csv(\"Result_RF/y_SVR_ext.csv\")\n",
      "197/353: pd.DataFrame(train_x, columns=['train_x']).to_csv(\"Result_RF/x_train.csv\")\n",
      "197/354: pd.DataFrame(train_x, columns=['x']).to_csv(\"Result_RF/x_train.csv\")\n",
      "197/355:\n",
      "pd.DataFrame(train_x).to_csv(\"Result/x_train.csv\")\n",
      "pd.DataFrame(test_x).to_csv(\"Result/x_test.csv\")\n",
      "pd.DataFrame(y_cv).to_csv(\"Result/y_cv.csv\")\n",
      "pd.DataFrame(train_y).to_csv(\"Result/train_y.csv\")\n",
      "pd.DataFrame(test_y).to_csv(\"Result/test_y.csv\")\n",
      "pd.DataFrame(y_pred).to_csv(\"Result/y_pred.csv\")\n",
      "pd.DataFrame(y_ext).to_csv(\"Result/y_SVR_ext.csv\")\n",
      "197/356:\n",
      "#not reduce\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "197/357: fingerprint_to_model.shape\n",
      "197/358:\n",
      "train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "197/359: train_x.shape\n",
      "197/360:\n",
      "# Set model parameter for random forest regression\n",
      "param = {\n",
      "    \"n_estimators\": 1000,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/361:\n",
      "# higher Features reduced performance?\n",
      "# yes\n",
      "197/362:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/363:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "fingerprint_to_model.shape\n",
      "197/364:\n",
      "import random\n",
      "SEED = random.seed(10)\n",
      "SEED\n",
      "197/365:\n",
      "train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "197/366: train_x.shape\n",
      "197/367:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/368:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/369:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='black')\n",
      "197/370:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/371:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/372:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/373:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/374:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='black')\n",
      "197/375:\n",
      "import random\n",
      "SEED = random.seed(10)\n",
      "SEED\n",
      "197/376:\n",
      "train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "197/377:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "197/378:\n",
      "train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "197/379:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/380:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/381:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/382:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/383:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='black')\n",
      "197/384:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/385:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='black')\n",
      "197/386:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='black')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "197/387:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='black')\n",
      "plt.scatter(train_y, y_pred, c='blue')\n",
      "197/388:\n",
      "pd.DataFrame(train_x).to_csv(\"Result/x_train.csv\")\n",
      "pd.DataFrame(test_x).to_csv(\"Result/x_test.csv\")\n",
      "pd.DataFrame(y_cv).to_csv(\"Result/y_cv.csv\")\n",
      "pd.DataFrame(train_y).to_csv(\"Result/train_y.csv\")\n",
      "pd.DataFrame(test_y).to_csv(\"Result/test_y.csv\")\n",
      "pd.DataFrame(y_pred).to_csv(\"Result/y_pred.csv\")\n",
      "pd.DataFrame(y_ext).to_csv(\"Result/y_SVR_ext.csv\")\n",
      "197/389:\n",
      "from sklearn.inspection import permutation_importance\n",
      "\n",
      "result = permutation_importance(SVR, test_x, y_ext, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances_mean[sorted_idx].T,\n",
      "           vert=False, labels= fingerprint_to_model.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (train set)\")\n",
      "fig.tight_layout()\n",
      "fig.set_size_inches(18.5, 25)\n",
      "plt.show()\n",
      "197/390:\n",
      "from sklearn.inspection import permutation_importance\n",
      "\n",
      "result = permutation_importance(SVR, test_x, y_ext, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances_mean[sorted_idx].T,\n",
      "           vert=False, labels=fingerprint_to_model.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (train set)\")\n",
      "fig.tight_layout()\n",
      "fig.set_size_inches(18.5, 25)\n",
      "plt.show()\n",
      "197/391:\n",
      "pd.DataFrame(train_x, columns='t').to_csv(\"Result/x_train.csv\")\n",
      "pd.DataFrame(test_x).to_csv(\"Result/x_test.csv\")\n",
      "pd.DataFrame(y_cv).to_csv(\"Result/y_cv.csv\")\n",
      "pd.DataFrame(train_y).to_csv(\"Result/train_y.csv\")\n",
      "pd.DataFrame(test_y).to_csv(\"Result/test_y.csv\")\n",
      "pd.DataFrame(y_pred).to_csv(\"Result/y_pred.csv\")\n",
      "pd.DataFrame(y_ext).to_csv(\"Result/y_SVR_ext.csv\")\n",
      "197/392:\n",
      "pd.DataFrame(train_x, columns=[]'t').to_csv(\"Result/x_train.csv\")\n",
      "pd.DataFrame(test_x).to_csv(\"Result/x_test.csv\")\n",
      "pd.DataFrame(y_cv).to_csv(\"Result/y_cv.csv\")\n",
      "pd.DataFrame(train_y).to_csv(\"Result/train_y.csv\")\n",
      "pd.DataFrame(test_y).to_csv(\"Result/test_y.csv\")\n",
      "pd.DataFrame(y_pred).to_csv(\"Result/y_pred.csv\")\n",
      "pd.DataFrame(y_ext).to_csv(\"Result/y_SVR_ext.csv\")\n",
      "197/393:\n",
      "pd.DataFrame(train_x, columns=['t']).to_csv(\"Result/x_train.csv\")\n",
      "pd.DataFrame(test_x).to_csv(\"Result/x_test.csv\")\n",
      "pd.DataFrame(y_cv).to_csv(\"Result/y_cv.csv\")\n",
      "pd.DataFrame(train_y).to_csv(\"Result/train_y.csv\")\n",
      "pd.DataFrame(test_y).to_csv(\"Result/test_y.csv\")\n",
      "pd.DataFrame(y_pred).to_csv(\"Result/y_pred.csv\")\n",
      "pd.DataFrame(y_ext).to_csv(\"Result/y_SVR_ext.csv\")\n",
      "197/394:\n",
      "pd.DataFrame(train_x).to_csv(\"Result/x_train.csv\")\n",
      "pd.DataFrame(test_x).to_csv(\"Result/x_test.csv\")\n",
      "pd.DataFrame(y_cv).to_csv(\"Result/y_cv.csv\")\n",
      "pd.DataFrame(train_y).to_csv(\"Result/train_y.csv\")\n",
      "pd.DataFrame(test_y).to_csv(\"Result/test_y.csv\")\n",
      "pd.DataFrame(y_pred).to_csv(\"Result/y_pred.csv\")\n",
      "pd.DataFrame(y_ext).to_csv(\"Result/y_SVR_ext.csv\")\n",
      "197/395:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/396:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='linear')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/397:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "param = {\n",
      "    \"n_estimators\": 1000,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/398:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "\n",
      "plt.scatter(train_y, y_pred, c='blue')\n",
      "197/399:\n",
      "path = r'Fingerprint/'\n",
      "\n",
      "Fp13  = pd.read_csv(path + 'mordard.csv'               , header = 0)\n",
      "197/400: Fp13.shape\n",
      "197/401:\n",
      "df = pd.read_csv('df6_lipinski_kpu.csv')\n",
      "df\n",
      "197/402: name = df['compound']\n",
      "197/403: name.shape\n",
      "197/404:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df2 = df2.drop('Unnamed: 0', axis=1)\n",
      "df3 = df2.set_index('Name')\n",
      "df3\n",
      "197/405:\n",
      "LogKpu = df3[['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]\n",
      "LogKpu\n",
      "197/406:\n",
      "Fp13 = Fp13.rename(columns={'name': 'Name'})\n",
      "Fp13\n",
      "197/407:\n",
      "\n",
      "Fp13.Name = name\n",
      "197/408:\n",
      "\n",
      "Fpp13 = Fp13.drop('Name', axis=1)\n",
      "Fpp13\n",
      "197/409: Fpp13 = Fpp13.fillna(0)\n",
      "197/410:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)\n",
      "    Fp_normalized\n",
      "    \n",
      "    return Fp_normalized\n",
      "197/411:\n",
      "\n",
      "Fp13_n = normalized (Fpp13)\n",
      "197/412: Fp13_n\n",
      "197/413: np.any(np.isnan(Fp13_n))\n",
      "197/414: Fp13_n[\"Name\"] = Fp13.Name\n",
      "197/415: Fp13_n\n",
      "197/416: Fp13_n = Fp13_n.set_index('Name')\n",
      "197/417: Fp13_n.head(20)\n",
      "197/418: raw13  = LogKpu.merge(Fp13_n, on='Name', how='outer')\n",
      "197/419: raw13\n",
      "197/420: np.any(np.isnan(raw13))\n",
      "197/421: raw13 =raw13[raw13.LogKpu.notna()]\n",
      "197/422:\n",
      "\n",
      "raw13.to_csv('QSAR_Kpu/Mordard.csv' , sep=',' ,index=True)\n",
      "197/423: print (len(raw13))\n",
      "197/424: print (len(raw13 .columns))\n",
      "197/425:\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "197/426: fingerprint_to_model.head(3)\n",
      "197/427:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "fingerprint_to_model.shape\n",
      "197/428:\n",
      "import random\n",
      "SEED = random.seed(10)\n",
      "SEED\n",
      "197/429:\n",
      "train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "197/430:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "197/431:\n",
      "# Set model parameter for random forest regression\n",
      "param = {\n",
      "    \"n_estimators\": 1000,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/432:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "param = {\n",
      "    \"n_estimators\": 1000,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/433:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "\n",
      "plt.scatter(train_y, y_pred, c='blue')\n",
      "197/434:\n",
      "import random\n",
      "SEED = random.seed(10)\n",
      "SEED\n",
      "197/435:\n",
      "train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "197/436:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "197/437:\n",
      "# Set model parameter for random forest regression\n",
      "param = {\n",
      "    \"n_estimators\": 1000,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/438:\n",
      "# Set model parameter for random forest regression\n",
      "param = {\n",
      "    \"n_estimators\": 100,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/439:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "\n",
      "plt.scatter(train_y, y_pred, c='blue')\n",
      "197/440:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "param = {\n",
      "    \"n_estimators\": 1000,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/441: # SVR_linear_model\n",
      "197/442:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "\n",
      "plt.scatter(train_y, y_pred, c='blue')\n",
      "197/443:\n",
      "pd.DataFrame(train_x, columns=['train_x']).to_csv(\"Result_RF/x_train.csv\")\n",
      "pd.DataFrame(test_x, columns='test_x').to_csv(\"Result_RF/x_test.csv\")\n",
      "pd.DataFrame(train_x, columns='train_x').to_csv(\"Result_RF/train_x.csv\")\n",
      "pd.DataFrame(train_y, columns='train_y').to_csv(\"Result_RF/train_y.csv\")\n",
      "pd.DataFrame(test_y, columns='test_y').to_csv(\"Result_RF/test_y.csv\")\n",
      "pd.DataFrame(y_pred, columns='y_pred').to_csv(\"Result_RF/y_pred.csv\")\n",
      "pd.DataFrame(y_ext, columns='y_ext').to_csv(\"Result_RF/y_SVR_ext.csv\")\n",
      "197/444:\n",
      "pd.DataFrame(train_x).to_csv(\"Result_RF/x_train.csv\")\n",
      "pd.DataFrame(test_x).to_csv(\"Result_RF/x_test.csv\")\n",
      "pd.DataFrame(y_cv).to_csv(\"Result_RF/y_cv.csv\")\n",
      "pd.DataFrame(train_y).to_csv(\"Result_RF/train_y.csv\")\n",
      "pd.DataFrame(test_y).to_csv(\"Result_RF/test_y.csv\")\n",
      "pd.DataFrame(y_pred).to_csv(\"Result_RF/y_pred.csv\")\n",
      "pd.DataFrame(y_ext).to_csv(\"Result_RF/y_SVR_ext.csv\")\n",
      "197/445: X = pd.DataFrame(train_x)\n",
      "197/446: X\n",
      "197/447: fingerprint_to_model.head(3)\n",
      "197/448:\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "197/449: fingerprint_to_model.head(3)\n",
      "197/450:\n",
      "from sklearn.inspection import permutation_importance\n",
      "\n",
      "result = permutation_importance(SVR, test_x, y_ext, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances_mean[sorted_idx].T,\n",
      "           vert=False, labels=fingerprint_to_model.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (train set)\")\n",
      "fig.tight_layout()\n",
      "fig.set_size_inches(18.5, 25)\n",
      "plt.show()\n",
      "197/451: train_x\n",
      "197/452: X = pd.DataFrame(train_x, index = True)\n",
      "197/453: X = pd.DataFrame(train_x)\n",
      "197/454: train_x\n",
      "197/455: fingerprint_to_model\n",
      "197/456: X_train, X_test = X[train_index], X[test_index]\n",
      "197/457: train_x.index\n",
      "197/458: # RF_model\n",
      "197/459:\n",
      "train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=SEED)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "197/460: indices = fingerprint_to_model.index\n",
      "197/461:\n",
      "train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indicestest_size=0.2, random_state=SEED)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "197/462:\n",
      "train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=SEED)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "197/463: indices_train, indices_test\n",
      "197/464:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "197/465: fingerprint_to_model.shape\n",
      "197/466:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "fingerprint_to_model.shape\n",
      "197/467:\n",
      "import random\n",
      "SEED = random.seed(10)\n",
      "SEED\n",
      "197/468: indices = fingerprint_to_model.index\n",
      "197/469: indices = fingerprint_to_model.index\n",
      "197/470: fingerprint_to_model.head(3)\n",
      "197/471:\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "197/472: fingerprint_to_model.head(3)\n",
      "197/473: indices = fingerprint_to_model.index\n",
      "197/474:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "fingerprint_to_model.shape\n",
      "197/475:\n",
      "import random\n",
      "SEED = random.seed(10)\n",
      "SEED\n",
      "197/476:\n",
      "train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=SEED)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "197/477: indices_train, indices_test\n",
      "197/478:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "197/479:\n",
      "# Set model parameter for random forest regression\n",
      "param = {\n",
      "    \"n_estimators\": 100,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/480:\n",
      "# Set model parameter for random forest regression\n",
      "param = {\n",
      "    \"n_estimators\": 100,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/481:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/482:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/483:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='black')\n",
      "plt.scatter(train_y, y_pred, c='blue')\n",
      "197/484:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/485:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "197/486:\n",
      "# Set model parameter for random forest regression\n",
      "param = {\n",
      "    \"n_estimators\": 100,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/487:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "param = {\n",
      "    \"n_estimators\": 100,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/488:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "\n",
      "plt.scatter(train_y, y_pred, c='blue')\n",
      "197/489:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "197/490:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='blue')\n",
      "197/491:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "197/492:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/493:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='black')\n",
      "plt.scatter(train_y, y_pred, c='blue')\n",
      "197/494:\n",
      "from sklearn.inspection import permutation_importance\n",
      "\n",
      "result = permutation_importance(SVR, test_x, y_ext, n_repeats=10,\n",
      "                                random_state=42, n_jobs=2)\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.boxplot(result.importances_mean[sorted_idx].T,\n",
      "           vert=False, labels=fingerprint_to_model.columns[sorted_idx])\n",
      "ax.set_title(\"Permutation Importances (train set)\")\n",
      "fig.tight_layout()\n",
      "fig.set_size_inches(18.5, 25)\n",
      "plt.show()\n",
      "197/495:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='linear')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/496:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "param = {\n",
      "    \"n_estimators\": 100,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/497:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "param = {\n",
      "    \"n_estimators\": 100,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/498:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "197/499:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "param = {\n",
      "    \"n_estimators\": 100,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/500:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "197/501: train_x.index\n",
      "197/502: fingerprint_to_model\n",
      "197/503: train_x.shape\n",
      "197/504:\n",
      "#not reduce\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "197/505: fingerprint_to_model\n",
      "197/506:\n",
      "train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=SEED)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "197/507:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/508:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/509:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/510:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "param = {\n",
      "    \"n_estimators\": 100,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/511: # non reduce RF\n",
      "197/512:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "197/513:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "fingerprint_to_model.shape\n",
      "197/514:\n",
      "import random\n",
      "SEED = random.seed(10)\n",
      "SEED\n",
      "197/515:\n",
      "train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=SEED)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "197/516: indices_train, indices_test\n",
      "197/517:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "197/518:\n",
      "# Set model parameter for random forest regression\n",
      "param = {\n",
      "    \"n_estimators\": 100,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "197/519:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/520:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='black')\n",
      "plt.scatter(train_y, y_pred, c='blue')\n",
      "197/521:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='linear')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/522:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "param = {\n",
      "    \"n_estimators\": 100,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/523:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "197/524:\n",
      "pd.DataFrame(train_x).to_csv(\"Result_RF/x_train.csv\")\n",
      "pd.DataFrame(test_x).to_csv(\"Result_RF/x_test.csv\")\n",
      "pd.DataFrame(y_cv).to_csv(\"Result_RF/y_cv.csv\")\n",
      "pd.DataFrame(train_y).to_csv(\"Result_RF/train_y.csv\")\n",
      "pd.DataFrame(test_y).to_csv(\"Result_RF/test_y.csv\")\n",
      "pd.DataFrame(y_pred).to_csv(\"Result_RF/y_pred.csv\")\n",
      "pd.DataFrame(y_ext).to_csv(\"Result_RF/y_SVR_ext.csv\")\n",
      "197/525:\n",
      "pd.DataFrame(train_x).to_csv(\"Result/x_train.csv\")\n",
      "pd.DataFrame(test_x).to_csv(\"Result/x_test.csv\")\n",
      "pd.DataFrame(y_cv).to_csv(\"Result/y_cv.csv\")\n",
      "pd.DataFrame(train_y).to_csv(\"Result/train_y.csv\")\n",
      "pd.DataFrame(test_y).to_csv(\"Result/test_y.csv\")\n",
      "pd.DataFrame(y_pred).to_csv(\"Result/y_pred.csv\")\n",
      "pd.DataFrame(y_ext).to_csv(\"Result/y_SVR_ext.csv\")\n",
      "197/526:\n",
      "pd.DataFrame(train_x, indices_train).to_csv(\"Result/x_train.csv\")\n",
      "pd.DataFrame(test_x).to_csv(\"Result/x_test.csv\")\n",
      "pd.DataFrame(y_cv).to_csv(\"Result/y_cv.csv\")\n",
      "pd.DataFrame(train_y).to_csv(\"Result/train_y.csv\")\n",
      "pd.DataFrame(test_y).to_csv(\"Result/test_y.csv\")\n",
      "pd.DataFrame(y_pred).to_csv(\"Result/y_pred.csv\")\n",
      "pd.DataFrame(y_ext).to_csv(\"Result/y_SVR_ext.csv\")\n",
      "197/527:\n",
      "pd.DataFrame(train_x, indices_train).to_csv(\"Result/x_train.csv\")\n",
      "pd.DataFrame(test_x, indices_test).to_csv(\"Result/x_test.csv\")\n",
      "pd.DataFrame(y_cv, indices_train).to_csv(\"Result/y_cv.csv\")\n",
      "pd.DataFrame(train_y, indices_train).to_csv(\"Result/train_y.csv\")\n",
      "pd.DataFrame(test_y).to_csv(\"Result/test_y.csv\")\n",
      "pd.DataFrame(y_pred, indices_train).to_csv(\"Result/y_pred.csv\")\n",
      "pd.DataFrame(y_ext, indices_test).to_csv(\"Result/y_SVR_ext.csv\")\n",
      "197/528:\n",
      "pd.DataFrame(train_x, indices_train).to_csv(\"Result/x_train.csv\")\n",
      "pd.DataFrame(test_x, indices_test).to_csv(\"Result/x_test.csv\")\n",
      "pd.DataFrame(y_cv, indices_train).to_csv(\"Result/y_cv.csv\")\n",
      "pd.DataFrame(train_y, indices_train).to_csv(\"Result/train_y.csv\")\n",
      "pd.DataFrame(test_y, indices_test).to_csv(\"Result/test_y.csv\")\n",
      "pd.DataFrame(y_pred, indices_train).to_csv(\"Result/y_pred.csv\")\n",
      "pd.DataFrame(y_ext, indices_test).to_csv(\"Result/y_SVR_ext.csv\")\n",
      "197/529:\n",
      "pd.DataFrame(train_x, indices_train).to_csv(\"Result_RF/x_train.csv\")\n",
      "pd.DataFrame(test_x, indices_test).to_csv(\"Result_RF/x_test.csv\")\n",
      "pd.DataFrame(y_cv, indices_train).to_csv(\"Result_RF/y_cv.csv\")\n",
      "pd.DataFrame(train_y, indices_train).to_csv(\"Result_RF/train_y.csv\")\n",
      "pd.DataFrame(test_y, indices_test).to_csv(\"Result_RF/test_y.csv\")\n",
      "pd.DataFrame(y_pred, indices_train).to_csv(\"Result_RF/y_pred.csv\")\n",
      "pd.DataFrame(y_ext, indices_test).to_csv(\"Result_RF/y_SVR_ext.csv\")\n",
      "197/530:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/531:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='black')\n",
      "plt.scatter(train_y, y_pred, c='blue')\n",
      "197/532:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='black')\n",
      "plt.scatter(train_y, y_pred, c='blue')\n",
      "197/533:\n",
      "pd.DataFrame(train_x, indices_train).to_csv(\"Result/x_train.csv\")\n",
      "pd.DataFrame(test_x, indices_test).to_csv(\"Result/x_test.csv\")\n",
      "pd.DataFrame(y_cv, indices_train).to_csv(\"Result/y_cv.csv\")\n",
      "pd.DataFrame(train_y, indices_train).to_csv(\"Result/train_y.csv\")\n",
      "pd.DataFrame(test_y, indices_test).to_csv(\"Result/test_y.csv\")\n",
      "pd.DataFrame(y_pred, indices_train).to_csv(\"Result/y_pred.csv\")\n",
      "pd.DataFrame(y_ext, indices_test).to_csv(\"Result/y_SVR_ext.csv\")\n",
      "197/534:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "param = {\n",
      "    \"n_estimators\": 100,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/535:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "param = {\n",
      "    \"n_estimators\": 100,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/536:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "param = {\n",
      "    \"n_estimators\": 100,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/537:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "197/538:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "param = {\n",
      "    \"n_estimators\": 100,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/539:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "param = {\n",
      "    \"n_estimators\": 200,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "197/540:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "197/541:\n",
      "pd.DataFrame(train_x, indices_train).to_csv(\"Result_RF/x_train.csv\")\n",
      "pd.DataFrame(test_x, indices_test).to_csv(\"Result_RF/x_test.csv\")\n",
      "pd.DataFrame(y_cv, indices_train).to_csv(\"Result_RF/y_cv.csv\")\n",
      "pd.DataFrame(train_y, indices_train).to_csv(\"Result_RF/train_y.csv\")\n",
      "pd.DataFrame(test_y, indices_test).to_csv(\"Result_RF/test_y.csv\")\n",
      "pd.DataFrame(y_pred, indices_train).to_csv(\"Result_RF/y_pred.csv\")\n",
      "pd.DataFrame(y_ext, indices_test).to_csv(\"Result_RF/y_SVR_ext.csv\")\n",
      "207/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "207/2:\n",
      "path = r'Fingerprint/'\n",
      "\n",
      "Fp13  = pd.read_csv(path + 'mordard.csv'               , header = 0)\n",
      "207/3: Fp13.shape\n",
      "207/4:\n",
      "df = pd.read_csv('df6_lipinski_kpu.csv')\n",
      "df\n",
      "207/5: name = df['compound']\n",
      "207/6: name.shape\n",
      "207/7:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df2 = df2.drop('Unnamed: 0', axis=1)\n",
      "df3 = df2.set_index('Name')\n",
      "df3\n",
      "207/8:\n",
      "LogKpu = df3[['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]\n",
      "LogKpu\n",
      "207/9:\n",
      "Fp13 = Fp13.rename(columns={'name': 'Name'})\n",
      "Fp13\n",
      "207/10: Fpp13 = Fpp13.fillna(0)\n",
      "207/11:\n",
      "\n",
      "Fpp13 = Fp13.drop('Name', axis=1)\n",
      "Fpp13\n",
      "207/12: Fpp13 = Fpp13.fillna(0)\n",
      "207/13:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)\n",
      "    Fp_normalized\n",
      "    \n",
      "    return Fp_normalized\n",
      "207/14:\n",
      "\n",
      "Fp13_n = normalized (Fpp13)\n",
      "207/15: Fp13_n\n",
      "207/16: np.any(np.isnan(Fp13_n))\n",
      "207/17: Fp13_n[\"Name\"] = Fp13.Name\n",
      "207/18: Fp13_n\n",
      "207/19: Fp13_n = Fp13_n.set_index('Name')\n",
      "207/20: Fp13_n.head(20)\n",
      "207/21: Fp13_n.head(20)\n",
      "207/22:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "207/23:\n",
      "path = r'Fingerprint/'\n",
      "\n",
      "Fp13  = pd.read_csv(path + 'mordard.csv'               , header = 0)\n",
      "207/24: Fp13.shape\n",
      "207/25:\n",
      "df = pd.read_csv('df6_lipinski_kpu.csv')\n",
      "df\n",
      "207/26: name = df['compound']\n",
      "207/27: name.shape\n",
      "207/28:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df2 = df2.drop('Unnamed: 0', axis=1)\n",
      "df3 = df2.set_index('Name')\n",
      "df3\n",
      "207/29:\n",
      "LogKpu = df3[['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]\n",
      "LogKpu\n",
      "207/30:\n",
      "Fp13 = Fp13.rename(columns={'name': 'Name'})\n",
      "Fp13\n",
      "207/31: Fp13\n",
      "207/32:\n",
      "df = pd.read_csv('df6_lipinski_kpu.csv')\n",
      "df\n",
      "207/33: name = df['compound']\n",
      "207/34: name.shape\n",
      "207/35:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df2 = df2.drop('Unnamed: 0', axis=1)\n",
      "df3 = df2.set_index('Name')\n",
      "df3\n",
      "207/36:\n",
      "Fp13 = Fp13.rename(columns={'name': 'Name'})\n",
      "Fp13\n",
      "207/37:\n",
      "\n",
      "Fp13.Name = name\n",
      "207/38:\n",
      "\n",
      "Fpp13 = Fp13.drop('Name', axis=1)\n",
      "Fpp13\n",
      "207/39: Fpp13 = Fpp13.fillna(0)\n",
      "207/40:\n",
      "\n",
      "Fp13_n = normalized (Fpp13)\n",
      "207/41: Fp13_n\n",
      "207/42: np.any(np.isnan(Fp13_n))\n",
      "207/43: Fp13_n[\"Name\"] = Fp13.Name\n",
      "207/44: Fp13_n\n",
      "207/45: Fp13_n = Fp13_n.set_index('Name')\n",
      "207/46: Fp13_n.head(20)\n",
      "207/47: raw13  = LogKpu.merge(Fp13_n, on='Name', how='outer')\n",
      "207/48: raw13\n",
      "207/49: np.any(np.isnan(raw13))\n",
      "207/50: raw13 =raw13[raw13.LogKpu.notna()]\n",
      "207/51: print (len(raw13))\n",
      "207/52: print (len(raw13 .columns))\n",
      "207/53:\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "207/54: fingerprint_to_model.head(3)\n",
      "207/55: indices = fingerprint_to_model.index\n",
      "207/56:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "fingerprint_to_model.shape\n",
      "207/57:\n",
      "import random\n",
      "SEED = random.seed(10)\n",
      "SEED\n",
      "207/58:\n",
      "train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=SEED)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "207/59: indices_train, indices_test\n",
      "207/60:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "207/61:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "param = {\n",
      "    \"n_estimators\": 200,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "207/62:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "207/63:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "207/64:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "param = {\n",
      "    \"n_estimators\": 200,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "207/65:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "param = {\n",
      "    \"n_estimators\": 200,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "207/66:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "207/67:\n",
      "result = permutation_importance(\n",
      "    model_RF, test_x, test_y, n_repeats=kf, random_state=SEED, n_jobs=2)\n",
      "\n",
      "forest_importances = pd.Series(result.importances_mean, index=feature_names)\n",
      "207/68:\n",
      "from sklearn.inspection import permutation_importance\n",
      "result = permutation_importance(\n",
      "    model_RF, test_x, test_y, n_repeats=kf, random_state=SEED, n_jobs=2)\n",
      "feature_names = [f'feature {i}' for i in range(fingerprint_to_model.shape[1])]\n",
      "\n",
      "forest_importances = pd.Series(result.importances_mean, index=feature_names)\n",
      "207/69:\n",
      "from sklearn.inspection import permutation_importance\n",
      "result = permutation_importance(model_RF, test_x, test_y, n_repeats=10, random_state=SEED, n_jobs=2)\n",
      "feature_names = [f'feature {i}' for i in range(fingerprint_to_model.shape[1])]\n",
      "\n",
      "forest_importances = pd.Series(result.importances_mean, index=feature_names)\n",
      "207/70:\n",
      "from sklearn.inspection import permutation_importance\n",
      "result = permutation_importance(model_RF, test_x, test_y, n_repeats=10, random_state=SEED, n_jobs=2)\n",
      "feature_names = [f'feature {i}' for i in range(fingerprint_to_model.shape[1])]\n",
      "\n",
      "forest_importances = pd.Series(result.importances_mean, index=feature_names)\n",
      "fig, ax = plt.subplots()\n",
      "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
      "ax.set_title(\"Feature importances using permutation on full model\")\n",
      "ax.set_ylabel(\"Mean accuracy decrease\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "211/1:\n",
      "from sklearn.inspection import permutation_importance\n",
      "result = permutation_importance(model_RF, test_x, test_y, n_repeats=10, random_state=SEED, n_jobs=2)\n",
      "feature_names = [f'feature {i}' for i in range(fingerprint_to_model.shape[1])]\n",
      "\n",
      "forest_importances = pd.Series(result.importances_mean, index=feature_names)\n",
      "fig, ax = plt.subplots()\n",
      "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
      "ax.set_title(\"Feature importances using permutation on full model\")\n",
      "ax.set_ylabel(\"Mean accuracy decrease\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "211/2:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "211/3:\n",
      "path = r'Fingerprint/'\n",
      "\n",
      "Fp13  = pd.read_csv(path + 'mordard.csv'               , header = 0)\n",
      "211/4: Fp13.shape\n",
      "211/5: Fp13\n",
      "211/6:\n",
      "df = pd.read_csv('df6_lipinski_kpu.csv')\n",
      "df\n",
      "211/7: name = df['compound']\n",
      "211/8: name.shape\n",
      "211/9:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df2 = df2.drop('Unnamed: 0', axis=1)\n",
      "df3 = df2.set_index('Name')\n",
      "df3\n",
      "211/10:\n",
      "LogKpu = df3[['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]\n",
      "LogKpu\n",
      "211/11:\n",
      "Fp13 = Fp13.rename(columns={'name': 'Name'})\n",
      "Fp13\n",
      "211/12:\n",
      "\n",
      "Fp13.Name = name\n",
      "211/13:\n",
      "\n",
      "Fpp13 = Fp13.drop('Name', axis=1)\n",
      "Fpp13\n",
      "211/14: Fpp13 = Fpp13.fillna(0)\n",
      "211/15:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)\n",
      "    Fp_normalized\n",
      "    \n",
      "    return Fp_normalized\n",
      "211/16:\n",
      "\n",
      "Fp13_n = normalized (Fpp13)\n",
      "211/17: Fp13_n\n",
      "211/18: np.any(np.isnan(Fp13_n))\n",
      "211/19: Fp13_n[\"Name\"] = Fp13.Name\n",
      "211/20: Fp13_n\n",
      "211/21: Fp13_n = Fp13_n.set_index('Name')\n",
      "211/22: Fp13_n.head(20)\n",
      "211/23: raw13  = LogKpu.merge(Fp13_n, on='Name', how='outer')\n",
      "211/24: raw13\n",
      "211/25: np.any(np.isnan(raw13))\n",
      "211/26: raw13 =raw13[raw13.LogKpu.notna()]\n",
      "211/27: print (len(raw13))\n",
      "211/28: print (len(raw13 .columns))\n",
      "211/29:\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "211/30: fingerprint_to_model.head(3)\n",
      "211/31: indices = fingerprint_to_model.index\n",
      "211/32:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "fingerprint_to_model.shape\n",
      "211/33:\n",
      "import random\n",
      "SEED = random.seed(10)\n",
      "SEED\n",
      "211/34:\n",
      "train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=SEED)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "211/35: indices_train, indices_test\n",
      "211/36:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "211/37:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "211/38:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "211/39:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "211/40:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "211/41:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='black')\n",
      "plt.scatter(train_y, y_pred, c='blue')\n",
      "211/42:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "211/43:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "211/44:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "211/45:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='black')\n",
      "plt.scatter(train_y, y_pred, c='blue')\n",
      "211/46:\n",
      "pd.DataFrame(train_x, indices_train).to_csv(\"Result/x_train.csv\")\n",
      "pd.DataFrame(test_x, indices_test).to_csv(\"Result/x_test.csv\")\n",
      "pd.DataFrame(y_cv, indices_train).to_csv(\"Result/y_cv.csv\")\n",
      "pd.DataFrame(train_y, indices_train).to_csv(\"Result/train_y.csv\")\n",
      "pd.DataFrame(test_y, indices_test).to_csv(\"Result/test_y.csv\")\n",
      "pd.DataFrame(y_pred, indices_train).to_csv(\"Result/y_pred.csv\")\n",
      "pd.DataFrame(y_ext, indices_test).to_csv(\"Result/y_SVR_ext.csv\")\n",
      "211/47:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "param = {\n",
      "    \"n_estimators\": 200,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "211/48:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "param = {\n",
      "    \"n_estimators\": 200,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "211/49:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "211/50:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "param = {\n",
      "    \"n_estimators\": 200,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "211/51:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "param = {\n",
      "    \"n_estimators\": 200,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "211/52:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "param = {\n",
      "    \"n_estimators\": 200,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "211/53:\n",
      "train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "211/54: indices_train, indices_test\n",
      "211/55:\n",
      "train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "211/56: indices_train, indices_test\n",
      "211/57:\n",
      "train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "211/58:\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "211/59: fingerprint_to_model.head(3)\n",
      "211/60: indices = fingerprint_to_model.index\n",
      "211/61:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "fingerprint_to_model.shape\n",
      "211/62:\n",
      "train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "211/63: indices_train, indices_test\n",
      "211/64:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "211/65:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "211/66:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "211/67:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='black')\n",
      "plt.scatter(train_y, y_pred, c='blue')\n",
      "211/68:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "211/69:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "211/70:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='black')\n",
      "plt.scatter(train_y, y_pred, c='blue')\n",
      "211/71:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='linear')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "211/72:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "param = {\n",
      "    \"n_estimators\": 200,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "211/73:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "param = {\n",
      "    \"n_estimators\": 200,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "211/74:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "211/75:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "param = {\n",
      "    \"n_estimators\": 200,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "211/76:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "211/77: indices_train, indices_test\n",
      "211/78:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "211/79: fingerprint_to_model\n",
      "211/80: fingerprint_to_model.shape\n",
      "211/81:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "211/82:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "211/83:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "211/84:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "211/85:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='black')\n",
      "plt.scatter(train_y, y_pred, c='blue')\n",
      "211/86:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='linear')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "211/87:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "param = {\n",
      "    \"n_estimators\": 200,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "211/88:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "param = {\n",
      "    \"n_estimators\": 200,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "211/89:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "211/90:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "211/91:\n",
      "pd.DataFrame(train_x, indices_train).to_csv(\"Result_RF/x_train.csv\")\n",
      "pd.DataFrame(test_x, indices_test).to_csv(\"Result_RF/x_test.csv\")\n",
      "pd.DataFrame(y_cv, indices_train).to_csv(\"Result_RF/y_cv.csv\")\n",
      "pd.DataFrame(train_y, indices_train).to_csv(\"Result_RF/train_y.csv\")\n",
      "pd.DataFrame(test_y, indices_test).to_csv(\"Result_RF/test_y.csv\")\n",
      "pd.DataFrame(y_pred, indices_train).to_csv(\"Result_RF/y_pred.csv\")\n",
      "pd.DataFrame(y_ext, indices_test).to_csv(\"Result_RF/y_SVR_ext.csv\")\n",
      "211/92:\n",
      "from sklearn.inspection import permutation_importance\n",
      "result = permutation_importance(model_RF, test_x, test_y, n_repeats=10, random_state=SEED, n_jobs=2)\n",
      "feature_names = [f'feature {i}' for i in range(fingerprint_to_model.shape[1])]\n",
      "\n",
      "forest_importances = pd.Series(result.importances_mean, index=feature_names)\n",
      "fig, ax = plt.subplots()\n",
      "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
      "ax.set_title(\"Feature importances using permutation on full model\")\n",
      "ax.set_ylabel(\"Mean accuracy decrease\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "211/93: forest_importances\n",
      "211/94:\n",
      "from sklearn.inspection import permutation_importance\n",
      "result = permutation_importance(model_RF, test_x, test_y, n_repeats=10, random_state=42, n_jobs=2)\n",
      "feature_names = [f'feature {i}' for i in range(fingerprint_to_model.shape[1])]\n",
      "\n",
      "forest_importances = pd.Series(result.importances_mean, index=feature_names)\n",
      "fig, ax = plt.subplots()\n",
      "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
      "ax.set_title(\"Feature importances using permutation on full model\")\n",
      "ax.set_ylabel(\"Mean accuracy decrease\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "211/95:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "param = {\n",
      "    \"n_estimators\": 200,  # number of trees to grows\n",
      "    'random'\n",
      "}\n",
      "model_RF = RandomForestRegressor(n_estimators=200, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "211/96:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestRegressor(n_estimators=200, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "211/97:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestRegressor(n_estimators=200, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "211/98:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "211/99:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestRegressor(n_estimators=200, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "211/100:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "211/101:\n",
      "pd.DataFrame(train_x, indices_train).to_csv(\"Result_RF/x_train.csv\")\n",
      "pd.DataFrame(test_x, indices_test).to_csv(\"Result_RF/x_test.csv\")\n",
      "pd.DataFrame(y_cv, indices_train).to_csv(\"Result_RF/y_cv.csv\")\n",
      "pd.DataFrame(train_y, indices_train).to_csv(\"Result_RF/train_y.csv\")\n",
      "pd.DataFrame(test_y, indices_test).to_csv(\"Result_RF/test_y.csv\")\n",
      "pd.DataFrame(y_pred, indices_train).to_csv(\"Result_RF/y_pred.csv\")\n",
      "pd.DataFrame(y_ext, indices_test).to_csv(\"Result_RF/y_SVR_ext.csv\")\n",
      "211/102:\n",
      "from sklearn.inspection import permutation_importance\n",
      "result = permutation_importance(model_RF, test_x, test_y, n_repeats=10, random_state=42, n_jobs=2)\n",
      "feature_names = [f'feature {i}' for i in range(fingerprint_to_model.shape[1])]\n",
      "\n",
      "forest_importances = pd.Series(result.importances_mean, index=feature_names)\n",
      "fig, ax = plt.subplots()\n",
      "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
      "ax.set_title(\"Feature importances using permutation on full model\")\n",
      "ax.set_ylabel(\"Mean accuracy decrease\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "211/103: forest_importances\n",
      "211/104: result\n",
      "211/105: feature_names\n",
      "211/106:\n",
      "#Train features\n",
      "from sklearn.inspection import permutation_importance\n",
      "result = permutation_importance(model_RF, train_x, train_y, n_repeats=10, random_state=42, n_jobs=2)\n",
      "feature_names = [f'feature {i}' for i in range(fingerprint_to_model.shape[1])]\n",
      "\n",
      "forest_importances = pd.Series(result.importances_mean, index=feature_names)\n",
      "fig, ax = plt.subplots()\n",
      "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
      "ax.set_title(\"Feature importances using permutation on full model\")\n",
      "ax.set_ylabel(\"Mean accuracy decrease\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "211/107: forest_importances\n",
      "211/108: FI_mean = pd.DataFrame(forest_importances).to_csv('forest_importances.csv')\n",
      "211/109: pd.DataFrame(forest_importances).to_csv('forest_importances.csv')\n",
      "211/110: FI_mean = pd.DataFrame(forest_importances)\n",
      "211/111:\n",
      "FI_mean = pd.DataFrame(forest_importances)\n",
      "FI_mean.to_csv('forest_importances.csv')\n",
      "211/112:\n",
      "FI_mean = pd.DataFrame(forest_importances)\n",
      "FI_mean.to_csv('forest_importances.csv')\n",
      "FI_mean\n",
      "211/113:\n",
      "FI_mean = pd.DataFrame(forest_importances, columns=['Feature', 'Mean'])\n",
      "FI_mean.to_csv('forest_importances.csv')\n",
      "FI_mean\n",
      "211/114:\n",
      "FI_mean = pd.DataFrame(forest_importances, head=['Feature', 'Mean'])\n",
      "FI_mean.to_csv('forest_importances.csv')\n",
      "FI_mean\n",
      "211/115:\n",
      "FI_mean = pd.DataFrame(forest_importances, head=['Mean'])\n",
      "FI_mean.to_csv('forest_importances.csv')\n",
      "FI_mean\n",
      "211/116:\n",
      "FI_mean = pd.DataFrame(forest_importances)\n",
      "FI_mean.to_csv('forest_importances.csv')\n",
      "FI_mean\n",
      "211/117:\n",
      "FI_mean = pd.DataFrame(forest_importances)\n",
      "FI_mean.to_csv('forest_importances_RF.csv')\n",
      "FI_mean\n",
      "211/118: fingerprint_to_model.shape[1]\n",
      "211/119: fingerprint_to_model\n",
      "211/120: fingerprint_to_model_header = pd.DataFrame(fingerprint_to_model)\n",
      "211/121:\n",
      "fingerprint_to_model_header = pd.DataFrame(fingerprint_to_model)\n",
      "fingerprint_to_model_header\n",
      "211/122:\n",
      "FI_RF_mean = pd.DataFrame(forest_importances)\n",
      "FI_RF_mean.to_csv('forest_importances_RF.csv')\n",
      "FI_RF_mean\n",
      "211/123:\n",
      "FI_RF_train_mean = pd.DataFrame(forest_importances)\n",
      "FI_RF_train_mean.to_csv('forest_importances_train_RF.csv')\n",
      "FI_RF_train_mean\n",
      "211/124:\n",
      "#Test features\n",
      "from sklearn.inspection import permutation_importance\n",
      "result = permutation_importance(model_RF, test_x, test_y, n_repeats=10, random_state=42, n_jobs=2)\n",
      "feature_names = [f'feature {i}' for i in range(fingerprint_to_model.shape[1])]\n",
      "\n",
      "forest_importances = pd.Series(result.importances_mean, index=feature_names)\n",
      "fig, ax = plt.subplots()\n",
      "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
      "ax.set_title(\"Feature importances using permutation on full model\")\n",
      "ax.set_ylabel(\"Mean accuracy decrease\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "211/125:\n",
      "FI_RF_test_mean = pd.DataFrame(forest_importances)\n",
      "FI_RF_test_mean.to_csv('forest_importances_test_RF.csv')\n",
      "FI_RF_test_mean\n",
      "211/126:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "211/127:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "211/128:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='black')\n",
      "plt.scatter(train_y, y_pred, c='blue')\n",
      "211/129:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='black')\n",
      "plt.scatter(train_y, y_pred, c='blue')\n",
      "211/130:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "211/131:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "211/132:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "211/133:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "211/134:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "211/135:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='linear')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "211/136:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "model_PLS = PLSRegression(n_components=10)\n",
      "scores=cross_val_score(model_PLS, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "model_fit = model_PLS.fit(train_x, train_y)\n",
      "y_pred = model_PLS.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_PLS, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_PLS.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "211/137:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "model_PLS = PLSRegression(n_components=10)\n",
      "scores=cross_val_score(model_PLS, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "model_fit = model_PLS.fit(train_x, train_y)\n",
      "y_pred = model_PLS.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_PLS, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_PLS.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "211/138:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "model_PLS = PLSRegression(n_components=7)\n",
      "scores=cross_val_score(model_PLS, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "model_fit = model_PLS.fit(train_x, train_y)\n",
      "y_pred = model_PLS.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_PLS, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_PLS.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "211/139:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "\n",
      "model_PLS = PLSRegression(n_components=7)\n",
      "scores=cross_val_score(model_PLS, train_x, train_y, cv=kf)\n",
      "\n",
      "#calculate R2\n",
      "model_fit = model_PLS.fit(train_x, train_y)\n",
      "y_pred = model_PLS.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_PLS, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_PLS.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "211/140:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "\n",
      "model_PLS = PLSRegression(n_components=10)\n",
      "scores=cross_val_score(model_PLS, train_x, train_y, cv=kf)\n",
      "\n",
      "#calculate R2\n",
      "model_fit = model_PLS.fit(train_x, train_y)\n",
      "y_pred = model_PLS.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_PLS, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_PLS.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "211/141:\n",
      "from sklearn.linear_model import Ridge\n",
      "Ridge = Ridge(alpha=1.0)\n",
      "\n",
      "#calculate R2\n",
      "model_fit = Ridge.fit(train_x, train_y)\n",
      "y_pred = Ridge.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(Ridge, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = Ridge.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "218/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "218/2:\n",
      "path = r'Fingerprint/'\n",
      "\n",
      "Fp13  = pd.read_csv(path + 'mordard.csv'               , header = 0)\n",
      "218/3: Fp13.shape\n",
      "218/4: Fp13\n",
      "218/5:\n",
      "df = pd.read_csv('df6_lipinski_kpu.csv')\n",
      "df\n",
      "218/6: name = df['compound']\n",
      "218/7: name.shape\n",
      "218/8:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df2 = df2.drop('Unnamed: 0', axis=1)\n",
      "df3 = df2.set_index('Name')\n",
      "df3\n",
      "218/9:\n",
      "LogKpu = df3[['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]\n",
      "LogKpu\n",
      "218/10:\n",
      "Fp13 = Fp13.rename(columns={'name': 'Name'})\n",
      "Fp13\n",
      "218/11:\n",
      "\n",
      "Fp13.Name = name\n",
      "218/12:\n",
      "\n",
      "Fpp13 = Fp13.drop('Name', axis=1)\n",
      "Fpp13\n",
      "218/13: Fpp13 = Fpp13.fillna(0)\n",
      "218/14:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)\n",
      "    Fp_normalized\n",
      "    \n",
      "    return Fp_normalized\n",
      "218/15:\n",
      "\n",
      "Fp13_n = normalized (Fpp13)\n",
      "218/16: Fp13_n\n",
      "218/17: np.any(np.isnan(Fp13_n))\n",
      "218/18: Fp13_n[\"Name\"] = Fp13.Name\n",
      "218/19: Fp13_n\n",
      "218/20: Fp13_n = Fp13_n.set_index('Name')\n",
      "218/21: Fp13_n.head(20)\n",
      "218/22: raw13  = LogKpu.merge(Fp13_n, on='Name', how='outer')\n",
      "218/23: raw13\n",
      "218/24: np.any(np.isnan(raw13))\n",
      "218/25: raw13 =raw13[raw13.LogKpu.notna()]\n",
      "218/26: print (len(raw13))\n",
      "218/27: print (len(raw13 .columns))\n",
      "218/28:\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "218/29: fingerprint_to_model.head(3)\n",
      "218/30: indices = fingerprint_to_model.index\n",
      "218/31:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "fingerprint_to_model.shape\n",
      "218/32:\n",
      "import random\n",
      "SEED = random.seed(10)\n",
      "SEED\n",
      "218/33:\n",
      "train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "218/34: indices_train, indices_test\n",
      "218/35:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "218/36:\n",
      "# Set model parameter for random forest regression\n",
      "param = {\n",
      "    \"n_estimators\": 100,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "218/37:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "218/38:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='black')\n",
      "plt.scatter(train_y, y_pred, c='blue')\n",
      "218/39:\n",
      "pd.DataFrame(train_x, indices_train).to_csv(\"Result/x_train.csv\")\n",
      "pd.DataFrame(test_x, indices_test).to_csv(\"Result/x_test.csv\")\n",
      "pd.DataFrame(y_cv, indices_train).to_csv(\"Result/y_cv.csv\")\n",
      "pd.DataFrame(train_y, indices_train).to_csv(\"Result/train_y.csv\")\n",
      "pd.DataFrame(test_y, indices_test).to_csv(\"Result/test_y.csv\")\n",
      "pd.DataFrame(y_pred, indices_train).to_csv(\"Result/y_pred.csv\")\n",
      "pd.DataFrame(y_ext, indices_test).to_csv(\"Result/y_SVR_ext.csv\")\n",
      "218/40:\n",
      "pd.DataFrame(train_x, indices_train).to_csv(\"Result/x_train.csv\")\n",
      "pd.DataFrame(test_x, indices_test).to_csv(\"Result/x_test.csv\")\n",
      "pd.DataFrame(y_cv, indices_train).to_csv(\"Result/y_cv.csv\")\n",
      "pd.DataFrame(train_y, indices_train).to_csv(\"Result/train_y.csv\")\n",
      "pd.DataFrame(test_y, indices_test).to_csv(\"Result/test_y.csv\")\n",
      "pd.DataFrame(y_pred, indices_train).to_csv(\"Result/y_pred.csv\")\n",
      "pd.DataFrame(y_ext, indices_test).to_csv(\"Result/y_SVR_ext.csv\")\n",
      "218/41:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='linear')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "218/42:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestRegressor(n_estimators=200, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "218/43:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "218/44:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestRegressor(n_estimators=200, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "218/45:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "218/46:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "218/47:\n",
      "pd.DataFrame(train_x, indices_train).to_csv(\"Result_RF/x_train.csv\")\n",
      "pd.DataFrame(test_x, indices_test).to_csv(\"Result_RF/x_test.csv\")\n",
      "pd.DataFrame(y_cv, indices_train).to_csv(\"Result_RF/y_cv.csv\")\n",
      "pd.DataFrame(train_y, indices_train).to_csv(\"Result_RF/train_y.csv\")\n",
      "pd.DataFrame(test_y, indices_test).to_csv(\"Result_RF/test_y.csv\")\n",
      "pd.DataFrame(y_pred, indices_train).to_csv(\"Result_RF/y_pred.csv\")\n",
      "pd.DataFrame(y_ext, indices_test).to_csv(\"Result_RF/y_SVR_ext.csv\")\n",
      "218/48:\n",
      "#Test features RF\n",
      "from sklearn.inspection import permutation_importance\n",
      "result = permutation_importance(model_RF, test_x, test_y, n_repeats=10, random_state=42, n_jobs=2)\n",
      "feature_names = [f'feature {i}' for i in range(fingerprint_to_model.shape[1])]\n",
      "\n",
      "forest_importances = pd.Series(result.importances_mean, index=feature_names)\n",
      "fig, ax = plt.subplots()\n",
      "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
      "ax.set_title(\"Feature importances using permutation on full model\")\n",
      "ax.set_ylabel(\"Mean accuracy decrease\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "218/49: fingerprint_to_model\n",
      "218/50:\n",
      "FI_RF_test_mean = pd.DataFrame(forest_importances)\n",
      "FI_RF_test_mean.to_csv('Result_RF/forest_importances_test_RF.csv')\n",
      "FI_RF_test_mean\n",
      "218/51:\n",
      "#Train features\n",
      "from sklearn.inspection import permutation_importance\n",
      "result = permutation_importance(model_RF, train_x, train_y, n_repeats=10, random_state=42, n_jobs=2)\n",
      "feature_names = [f'feature {i}' for i in range(fingerprint_to_model.shape[1])]\n",
      "\n",
      "forest_importances = pd.Series(result.importances_mean, index=feature_names)\n",
      "fig, ax = plt.subplots()\n",
      "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
      "ax.set_title(\"Feature importances using permutation on full model\")\n",
      "ax.set_ylabel(\"Mean accuracy decrease\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "218/52:\n",
      "from sklearn.linear_model import Ridge\n",
      "Ridge = Ridge(alpha=1.0)\n",
      "\n",
      "#calculate R2\n",
      "model_fit = Ridge.fit(train_x, train_y)\n",
      "y_pred = Ridge.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(Ridge, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = Ridge.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "218/53:\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "MLP = MLPRegressor(activation='relu' random_state=42, max_iter=1000, solver='lbfgs')\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "model_fit = MLP.fit(train_x, train_y)\n",
      "y_pred = MLP.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(MLP, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = MLP.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "218/54:\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "MLP = MLPRegressor(activation='relu', random_state=42, max_iter=1000, solver='lbfgs')\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "model_fit = MLP.fit(train_x, train_y)\n",
      "y_pred = MLP.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(MLP, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = MLP.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "218/55:\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "218/56: fingerprint_to_model.head(3)\n",
      "218/57:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "fingerprint_to_model.shape\n",
      "218/58: fingerprint_to_model\n",
      "218/59: fingerprint_to_model.shape\n",
      "218/60: fingerprint_to_model.to_frame\n",
      "218/61: fingerprint_to_model.to_frame()\n",
      "218/62: pd.DataFrame(Ffingerprint_to_model)\n",
      "218/63: pd.DataFrame(fingerprint_to_model)\n",
      "218/64:\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "218/65: fingerprint_to_model.head(3)\n",
      "218/66: indices = fingerprint_to_model.index\n",
      "218/67:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "fingerprint_to_model2 = sel.fit_transform(fingerprint_to_model)\n",
      "fingerprint_to_model.shape\n",
      "218/68:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "fingerprint_to_model2 = sel.fit_transform(fingerprint_to_model)\n",
      "fingerprint_to_model2.shape\n",
      "218/69: fingerprint_to_model2_df = pd.DataFrame(fingerprint_to_model2, index=indices, columns=fingerprint_to_model.columns)\n",
      "218/70: fingerprint_to_model2_df = pd.DataFrame(fingerprint_to_model2, index=fingerprint_to_model.index, columns=fingerprint_to_model.columns)\n",
      "218/71: fingerprint_to_model2_df = pd.DataFrame(fingerprint_to_model2, index=fingerprint_to_model.index, columns=fingerprint_to_model.columns)\n",
      "218/72: fingerprint_to_model2_df = pd.DataFrame(fingerprint_to_model2, index=fingerprint_to_model2.index, columns=fingerprint_to_model.columns)\n",
      "218/73:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "fingerprint_to_model.shape\n",
      "218/74:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "fingerprint_to_modelo_model.get_support(index=True)\n",
      "218/75:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "fingerprint_to_modelo.get_support(index=True)\n",
      "218/76:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "fingerprint_to_model.get_support(index=True)\n",
      "218/77:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05)).get_support(index=True)\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "fingerprint_to_model.get_support(index=True)\n",
      "218/78:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05)).get_support(index=True)\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "fingerprint_to_model\n",
      "218/79:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "fingerprint_to_model\n",
      "218/80:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model).get_support(indices=True)\n",
      "fingerprint_to_model\n",
      "218/81:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "fingerprint_to_model\n",
      "218/82:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)\n",
      "fingerprint_to_model = sel.fit(fingerprint_to_model)\n",
      "fingerprint_to_model\n",
      "218/83:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "fingerprint_to_model\n",
      "218/84:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "fingerprint_to_model\n",
      "218/85:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "fingerprint_to_model.get_support(indices=True)\n",
      "218/86:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model).get_support(indices=True)\n",
      "fingerprint_to_model\n",
      "218/87:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05),get_support(indices=True))\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "fingerprint_to_model\n",
      "218/88:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05), get_support(indices=True))\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "fingerprint_to_model\n",
      "218/89:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "fingerprint_to_model\n",
      "218/90:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05))\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "\n",
      "\n",
      "def VarianceThreshold(df, threshold):\n",
      "    sel = VarianceThreshold(threshold)\n",
      "    sel.fit(df)\n",
      "    df = df.loc[:, sel.get_support()]\n",
      "    return df\n",
      "218/91:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05))\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "\n",
      "\n",
      "def VarianceThreshold_transform(df, threshold):\n",
      "    sel = VarianceThreshold(threshold)\n",
      "    sel.fit(df)\n",
      "    df = df.loc[:, sel.get_support()]\n",
      "    return df\n",
      "218/92:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05))\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "\n",
      "\n",
      "def VarianceThreshold_transform(df, threshold):\n",
      "    sel = VarianceThreshold(threshold)\n",
      "    sel.fit(df)\n",
      "    df = df.loc[:, sel.get_support(indices=True)]\n",
      "    return df\n",
      "218/93: fingerprint_to_model = VarianceThreshold_transform(fingerprint_to_model)\n",
      "218/94: fingerprint_to_model = VarianceThreshold_transform(fingerprint_to_model, 0.5)\n",
      "218/95: fingerprint_to_model = VarianceThreshold_transform(fingerprint_to_model, 0.05)\n",
      "218/96:\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "218/97: fingerprint_to_model.head(3)\n",
      "218/98: fingerprint_to_model = VarianceThreshold_transform(fingerprint_to_model, 0.05)\n",
      "218/99:\n",
      "sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)\n",
      "fingerprint_to_model = sel.fit(fingerprint_to_model)\n",
      "fingerprint_to_model\n",
      "218/100:\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "218/101: fingerprint_to_model.head(3)\n",
      "218/102:\n",
      "sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)\n",
      "fingerprint_to_model = sel.fit(fingerprint_to_model)\n",
      "fingerprint_to_model\n",
      "218/103:\n",
      "sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "fingerprint_to_model\n",
      "218/104:\n",
      "sel = VarianceThreshold(threshold=(0.05)).get_support()\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "fingerprint_to_model\n",
      "218/105:\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "fingerprint_to_model\n",
      "218/106:\n",
      "sel = VarianceThreshold(threshold=(0.05)).get_support()\n",
      "fingerprint_to_model = sel.fit(fingerprint_to_model)\n",
      "fingerprint_to_model\n",
      "218/107:\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "fingerprint_to_model\n",
      "218/108:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05))\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "\n",
      "\n",
      "def VarianceThreshold_transform(df, threshold):\n",
      "    sel = VarianceThreshold(threshold)\n",
      "    sel.fit(df)\n",
      "    df2 = df.loc[:, sel.get_support(indices=True)]\n",
      "    return df2\n",
      "218/109:\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "218/110: VarianceThreshold_transform(fingerprint_to_model, 0.05)\n",
      "218/111:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05))\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "\n",
      "\n",
      "def VarianceThreshold_selector(data):\n",
      "\n",
      "    #Select Model\n",
      "    selector = VarianceThreshold(0.05) #Defaults to 0.0, e.g. only remove features with the same value in all samples\n",
      "\n",
      "    #Fit the Model\n",
      "    selector.fit(data)\n",
      "    features = selector.get_support(indices = True) #returns an array of integers corresponding to nonremoved features\n",
      "    features = [column for column in data[features]] #Array of all nonremoved features' names\n",
      "\n",
      "    #Format and Return\n",
      "    selector = pd.DataFrame(selector.transform(data))\n",
      "    selector.columns = features\n",
      "    return selector\n",
      "218/112:\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "218/113: VarianceThreshold_selector(fingerprint_to_model)\n",
      "218/114:\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "218/115: fingerprint_to_model.head(3)\n",
      "218/116:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05))\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "\n",
      "\n",
      "selector = VarianceThreshold(0.05) #Defaults to 0.0, e.g. only remove features with the same value in all samples\n",
      "elector.fit(fingerprint_to_model)\n",
      "features = selector.get_support(indices = True) #returns an array of integers corresponding to nonremoved features\n",
      "features = [column for column in data[features]] #Array of all nonremoved features' names\n",
      "selector_df = pd.DataFrame(selector.transform(data), columns=features)\n",
      "218/117:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05))\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "\n",
      "\n",
      "selector = VarianceThreshold(0.05) #Defaults to 0.0, e.g. only remove features with the same value in all samples\n",
      "selector.fit(fingerprint_to_model)\n",
      "features = selector.get_support(indices = True) #returns an array of integers corresponding to nonremoved features\n",
      "features = [column for column in data[features]] #Array of all nonremoved features' names\n",
      "selector_df = pd.DataFrame(selector.transform(data), columns=features)\n",
      "218/118:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05))\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "\n",
      "\n",
      "selector = VarianceThreshold(0.05) #Defaults to 0.0, e.g. only remove features with the same value in all samples\n",
      "selector.fit(fingerprint_to_model)\n",
      "features = selector.get_support(indices = True) #returns an array of integers corresponding to nonremoved features\n",
      "features = [column for column in fingerprint_to_model[features]] #Array of all nonremoved features' names\n",
      "selector_df = pd.DataFrame(selector.transform(fingerprint_to_model), columns=features)\n",
      "218/119:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05))\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "\n",
      "\n",
      "selector = VarianceThreshold(0.05) #Defaults to 0.0, e.g. only remove features with the same value in all samples\n",
      "selector.fit(fingerprint_to_model)\n",
      "features = selector.get_support(indices = True) #returns an array of integers corresponding to nonremoved features\n",
      "features = [column for column in fingerprint_to_model[features]] #Array of all nonremoved features' names\n",
      "selector_df = pd.DataFrame(selector.transform(fingerprint_to_model), columns=features)\n",
      "218/120:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05))\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "\n",
      "\n",
      "selector = VarianceThreshold(0.05) #Defaults to 0.0, e.g. only remove features with the same value in all samples\n",
      "selector.fit(fingerprint_to_model)\n",
      "features = selector.get_support(indices = True) #returns an array of integers corresponding to nonremoved features\n",
      "selector_df = pd.DataFrame(selector.transform(fingerprint_to_model), columns=features)\n",
      "218/121: selector_df\n",
      "218/122:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05))\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "\n",
      "\n",
      "selector = VarianceThreshold(0.05) #Defaults to 0.0, e.g. only remove features with the same value in all samples\n",
      "selector.fit(fingerprint_to_model)\n",
      "features = selector.get_support(indices = True) #returns an array of integers corresponding to nonremoved features\n",
      "features = fingerprint_to_model[features] #Array of all nonremoved features' names\n",
      "selector_df = pd.DataFrame(selector.transform(fingerprint_to_model), columns=features)\n",
      "218/123:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05))\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "\n",
      "\n",
      "selector = VarianceThreshold(0.05) #Defaults to 0.0, e.g. only remove features with the same value in all samples\n",
      "selector.fit(fingerprint_to_model)\n",
      "features = selector.get_support(indices = True) #returns an array of integers corresponding to nonremoved features\n",
      "selector_df = pd.DataFrame(selector.transform(fingerprint_to_model), columns=fingerprint_to_model[features])\n",
      "218/124:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05))\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "\n",
      "\n",
      "selector = VarianceThreshold(0.05) #Defaults to 0.0, e.g. only remove features with the same value in all samples\n",
      "selector.fit(fingerprint_to_model)\n",
      "features = selector.get_support(indices = True) #returns an array of integers corresponding to nonremoved features\n",
      "selector_df = pd.DataFrame(selector.transform(fingerprint_to_model), columns=fingerprint_to_model)\n",
      "218/125:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05))\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "\n",
      "\n",
      "selector = VarianceThreshold(0.05) #Defaults to 0.0, e.g. only remove features with the same value in all samples\n",
      "selector.fit(fingerprint_to_model)\n",
      "features = selector.get_support(indices = True) #returns an array of integers corresponding to nonremoved features\n",
      "selector_df = pd.DataFrame(selector.transform(fingerprint_to_model), columns=fingerprint_to_model[features])\n",
      "218/126:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05))\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "\n",
      "\n",
      "selector = VarianceThreshold(0.05) #Defaults to 0.0, e.g. only remove features with the same value in all samples\n",
      "selector.fit(fingerprint_to_model)\n",
      "features = selector.get_support(indices = True) #returns an array of integers corresponding to nonremoved features\n",
      "selector_df = pd.DataFrame(selector.transform(fingerprint_to_model), columns=fingerprint_to_model)\n",
      "218/127:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05))\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "\n",
      "\n",
      "selector = VarianceThreshold(0.05) #Defaults to 0.0, e.g. only remove features with the same value in all samples\n",
      "selector2 = selector.fit(fingerprint_to_model)\n",
      "features = selector.get_support(indices = True) #returns an array of integers corresponding to nonremoved features\n",
      "selector_df = pd.DataFrame(selector.transform(fingerprint_to_model), columns=fingerprint_to_model)\n",
      "218/128:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05))\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "\n",
      "\n",
      "selector = VarianceThreshold(0.05) #Defaults to 0.0, e.g. only remove features with the same value in all samples\n",
      "selector2 = selector.fit(fingerprint_to_model)\n",
      "features = selector.get_support(indices = True) #returns an array of integers corresponding to nonremoved features\n",
      "selector_df = pd.DataFrame(selector2.transform(fingerprint_to_model), columns=fingerprint_to_model)\n",
      "218/129:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05))\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "\n",
      "\n",
      "selector = VarianceThreshold(0.05) #Defaults to 0.0, e.g. only remove features with the same value in all samples\n",
      "selector2 = selector.fit(fingerprint_to_model)\n",
      "features = selector.get_support(indices = True) #returns an array of integers corresponding to nonremoved features\n",
      "selector_df = pd.DataFrame(selector, columns=fingerprint_to_model)\n",
      "218/130:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05))\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "\n",
      "\n",
      "selector = VarianceThreshold(0.05) #Defaults to 0.0, e.g. only remove features with the same value in all samples\n",
      "selector2 = selector.fit(fingerprint_to_model)\n",
      "features = selector.get_support(indices = True) #returns an array of integers corresponding to nonremoved features\n",
      "selector_df = pd.DataFrame(selector2, columns=fingerprint_to_model)\n",
      "218/131:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05))\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "\n",
      "\n",
      "selector = VarianceThreshold(0.05) #Defaults to 0.0, e.g. only remove features with the same value in all samples\n",
      "selector2 = selector.fit(fingerprint_to_model)\n",
      "features = selector.get_support(indices = True) #returns an array of integers corresponding to nonremoved features\n",
      "selector_df = pd.DataFrame(selector2, columns=fingerprint_to_model)\n",
      "218/132: selector_df\n",
      "218/133: VarianceThreshold_transform(fingerprint_to_model, 0.05)\n",
      "218/134:\n",
      "import random\n",
      "SEED = random.seed(10)\n",
      "SEED\n",
      "218/135:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05))\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "\n",
      "\n",
      "selector = VarianceThreshold(0.05) #Defaults to 0.0, e.g. only remove features with the same value in all samples\n",
      "selector2 = selector.fit_transform(fingerprint_to_model)\n",
      "features = selector.get_support(indices = True) #returns an array of integers corresponding to nonremoved features\n",
      "selector_df = pd.DataFrame(selector2, columns=fingerprint_to_model)\n",
      "218/136:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05))\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "\n",
      "\n",
      "selector = VarianceThreshold(0.05) #Defaults to 0.0, e.g. only remove features with the same value in all samples\n",
      "selector2 = selector.fit_transform(fingerprint_to_model)\n",
      "features = selector.get_support(indices = True) #returns an array of integers corresponding to nonremoved features\n",
      "selector_df = pd.DataFrame(selector2, columns=fingerprint_to_model[column])\n",
      "218/137:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05))\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "\n",
      "\n",
      "selector = VarianceThreshold(0.05) #Defaults to 0.0, e.g. only remove features with the same value in all samples\n",
      "selector2 = selector.fit_transform(fingerprint_to_model)\n",
      "features = selector.get_support(indices = True) #returns an array of integers corresponding to nonremoved features\n",
      "selector_df = pd.DataFrame(selector2, columns=fingerprint_to_model.columns)\n",
      "218/138:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05))\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "\n",
      "\n",
      "selector = VarianceThreshold(0.05) #Defaults to 0.0, e.g. only remove features with the same value in all samples\n",
      "selector2 = selector.fit_transform(fingerprint_to_model)\n",
      "features = selector.get_support(indices = True) #returns an array of integers corresponding to nonremoved features\n",
      "selector_df = pd.DataFrame(selector2, columns=fingerprint_to_model[features].columns)\n",
      "218/139:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "fingerprint_to_model\n",
      "218/140:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "sel.get_support(indices=True)\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "fingerprint_to_model\n",
      "218/141:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "sel.get_support(indices=True)\n",
      "fingerprint_to_model = sel.fit(fingerprint_to_model)\n",
      "fingerprint_to_model\n",
      "218/142:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "\n",
      "fingerprint_to_model.get_support(indices=True)\n",
      "218/143:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model).get_support(indices=True)\n",
      "fingerprint_to_model\n",
      "218/144:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "sel.fit_transform(fingerprint_to_model).get_support(indices=True)\n",
      "fingerprint_to_model\n",
      "218/145:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model).get_support(indices=True)\n",
      "fingerprint_to_model\n",
      "218/146:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05))..get_support(indices=True)\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "fingerprint_to_model\n",
      "218/147:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "fingerprint_to_model\n",
      "218/148:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "\n",
      "selector.get_support()\n",
      "218/149:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "fingerprint_to_model\n",
      "sel.get_support()\n",
      "218/150:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "fingerprint_to_model\n",
      "sel.get_support()\n",
      "218/151:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "fingerprint_to_model\n",
      "218/152:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "fingerprint_to_model\n",
      "sel.get_support(indices=True)\n",
      "218/153:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "select = sel.fit(fingerprint_to_model)\n",
      "fingerprint_to_model\n",
      "sel.get_support(indices=True)\n",
      "218/154:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "sel = sel.fit(fingerprint_to_model)\n",
      "mask = sel.get_support()\n",
      "218/155: fingerprint_to_model = fingerprint_to_model.loc[:, mask]\n",
      "218/156:\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "218/157: fingerprint_to_model.head(3)\n",
      "218/158: indices = fingerprint_to_model.index\n",
      "218/159:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "sel = sel.fit(fingerprint_to_model)\n",
      "mask = sel.get_support()\n",
      "218/160: fingerprint_to_model = fingerprint_to_model.loc[:, mask]\n",
      "218/161:\n",
      "fingerprint_to_model = fingerprint_to_model.loc[:, mask]\n",
      "fingerprint_to_model\n",
      "218/162:\n",
      "fingerprint_to_model = sel.loc[:, mask]\n",
      "fingerprint_to_model\n",
      "218/163:\n",
      "fingerprint_to_model = fingerprint_to_model.loc[:, mask]\n",
      "fingerprint_to_model\n",
      "218/164:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "sel = sel.fit(fingerprint_to_model)\n",
      "mask = sel.get_support()\n",
      "218/165:\n",
      "fingerprint_to_model2 = fingerprint_to_model.loc[:, mask]\n",
      "fingerprint_to_model2\n",
      "218/166:\n",
      "train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "218/167: indices_train, indices_test\n",
      "218/168:\n",
      "train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model2, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "218/169: indices_train, indices_test\n",
      "218/170:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestRegressor(n_estimators=200, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "218/171:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "218/172:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestRegressor(n_estimators=200, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "218/173:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "218/174:\n",
      "#Test features RF\n",
      "from sklearn.inspection import permutation_importance\n",
      "result = permutation_importance(model_RF, test_x, test_y, n_repeats=10, random_state=42, n_jobs=2)\n",
      "feature_names = [f'feature {i}' for i in range(fingerprint_to_model.shape[1])]\n",
      "\n",
      "forest_importances = pd.Series(result.importances_mean, index=feature_names)\n",
      "fig, ax = plt.subplots()\n",
      "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
      "ax.set_title(\"Feature importances using permutation on full model\")\n",
      "ax.set_ylabel(\"Mean accuracy decrease\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "218/175:\n",
      "#Test features RF\n",
      "from sklearn.inspection import permutation_importance\n",
      "result = permutation_importance(model_RF, test_x, test_y, n_repeats=10, random_state=42, n_jobs=2)\n",
      "feature_names = [f'feature {i}' for i in range(fingerprint_to_model2.shape[1])]\n",
      "\n",
      "forest_importances = pd.Series(result.importances_mean, index=feature_names)\n",
      "fig, ax = plt.subplots()\n",
      "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
      "ax.set_title(\"Feature importances using permutation on full model\")\n",
      "ax.set_ylabel(\"Mean accuracy decrease\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "218/176:\n",
      "FI_RF_test_mean = pd.DataFrame(forest_importances)\n",
      "FI_RF_test_mean.to_csv('Result_RF/forest_importances_test_RF.csv')\n",
      "FI_RF_test_mean\n",
      "218/177:\n",
      "#Test features RF\n",
      "from sklearn.inspection import permutation_importance\n",
      "result = permutation_importance(model_RF, test_x, test_y, n_repeats=10, random_state=42, n_jobs=2)\n",
      "feature_names = fingerprint_to_model2.columns\n",
      "\n",
      "forest_importances = pd.Series(result.importances_mean, index=feature_names)\n",
      "fig, ax = plt.subplots()\n",
      "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
      "ax.set_title(\"Feature importances using permutation on full model\")\n",
      "ax.set_ylabel(\"Mean accuracy decrease\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "218/178:\n",
      "FI_RF_test_mean = pd.DataFrame(forest_importances)\n",
      "FI_RF_test_mean.to_csv('Result_RF/forest_importances_test_RF.csv')\n",
      "FI_RF_test_mean\n",
      "218/179:\n",
      "#Train features\n",
      "from sklearn.inspection import permutation_importance\n",
      "result = permutation_importance(model_RF, train_x, train_y, n_repeats=10, random_state=42, n_jobs=2)\n",
      "feature_names = fingerprint_to_model2.column\n",
      "\n",
      "forest_importances = pd.Series(result.importances_mean, index=feature_names)\n",
      "fig, ax = plt.subplots()\n",
      "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
      "ax.set_title(\"Feature importances using permutation on full model\")\n",
      "ax.set_ylabel(\"Mean accuracy decrease\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "218/180:\n",
      "#Train features\n",
      "from sklearn.inspection import permutation_importance\n",
      "result = permutation_importance(model_RF, train_x, train_y, n_repeats=10, random_state=42, n_jobs=2)\n",
      "feature_names = fingerprint_to_model2.columns\n",
      "\n",
      "forest_importances = pd.Series(result.importances_mean, index=feature_names)\n",
      "fig, ax = plt.subplots()\n",
      "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
      "ax.set_title(\"Feature importances using permutation on full model\")\n",
      "ax.set_ylabel(\"Mean accuracy decrease\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "218/181:\n",
      "FI_RF_train_mean = pd.DataFrame(forest_importances)\n",
      "FI_RF_train_mean.to_csv('Result_RF/forest_importances_train_RF.csv')\n",
      "FI_RF_train_mean\n",
      "218/182:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "sel = VarianceThreshold(threshold=(0.0))\n",
      "sel = sel.fit(fingerprint_to_model)\n",
      "mask = sel.get_support()\n",
      "218/183:\n",
      "fingerprint_to_model2 = fingerprint_to_model.loc[:, mask]\n",
      "fingerprint_to_model2\n",
      "218/184:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "sel = sel.fit(fingerprint_to_model)\n",
      "mask = sel.get_support()\n",
      "218/185:\n",
      "fingerprint_to_model2 = fingerprint_to_model.loc[:, mask]\n",
      "fingerprint_to_model2\n",
      "218/186:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "sel = VarianceThreshold(threshold=(0))\n",
      "sel = sel.fit(fingerprint_to_model)\n",
      "mask = sel.get_support()\n",
      "218/187:\n",
      "fingerprint_to_model2 = fingerprint_to_model.loc[:, mask]\n",
      "fingerprint_to_model2\n",
      "223/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "223/2:\n",
      "path = r'Fingerprint/'\n",
      "\n",
      "Fp13  = pd.read_csv(path + 'mordard.csv'               , header = 0)\n",
      "223/3: Fp13.shape\n",
      "223/4: Fp13\n",
      "223/5:\n",
      "df = pd.read_csv('df6_lipinski_kpu.csv')\n",
      "df\n",
      "223/6: name = df['compound']\n",
      "223/7: name.shape\n",
      "223/8:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df2 = df2.drop('Unnamed: 0', axis=1)\n",
      "df3 = df2.set_index('Name')\n",
      "df3\n",
      "223/9:\n",
      "\n",
      "raw13.to_csv('QSAR_Kpu/Mordard_Kpu.csv' , sep=',' ,index=True)\n",
      "223/10:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df2 = df2.drop('Unnamed: 0', axis=1)\n",
      "df3 = df2.set_index('Name')\n",
      "df3\n",
      "223/11:\n",
      "LogKpu = df3[['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]\n",
      "LogKpu\n",
      "223/12:\n",
      "Fp13 = Fp13.rename(columns={'name': 'Name'})\n",
      "Fp13\n",
      "223/13:\n",
      "\n",
      "Fp13.Name = name\n",
      "223/14:\n",
      "\n",
      "Fpp13 = Fp13.drop('Name', axis=1)\n",
      "Fpp13\n",
      "223/15: Fpp13 = Fpp13.fillna(0)\n",
      "223/16:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)\n",
      "    Fp_normalized\n",
      "    \n",
      "    return Fp_normalized\n",
      "223/17:\n",
      "\n",
      "Fp13_n = normalized (Fpp13)\n",
      "223/18: Fp13_n\n",
      "223/19: np.any(np.isnan(Fp13_n))\n",
      "223/20: Fp13_n[\"Name\"] = Fp13.Name\n",
      "223/21: Fp13_n\n",
      "223/22: raw13  = LogKpu.merge(Fp13_n, on='Name', how='outer')\n",
      "223/23: raw13\n",
      "223/24: np.any(np.isnan(raw13))\n",
      "223/25: raw13  = LogKpu.merge(Fp13_n, on='Name', how='outer')\n",
      "223/26: raw13\n",
      "223/27: np.any(np.isnan(raw13))\n",
      "223/28:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "223/29:\n",
      "path = r'Fingerprint/'\n",
      "\n",
      "Fp13  = pd.read_csv(path + 'mordard.csv'               , header = 0)\n",
      "223/30: Fp13.shape\n",
      "223/31: Fp13\n",
      "223/32:\n",
      "df = pd.read_csv('df6_lipinski_kpu.csv')\n",
      "df\n",
      "223/33: name = df['compound']\n",
      "223/34: name.shape\n",
      "223/35:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df2 = df2.drop('Unnamed: 0', axis=1)\n",
      "df3 = df2.set_index('Name')\n",
      "df3\n",
      "223/36:\n",
      "LogKpu = df3[['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]\n",
      "LogKpu\n",
      "223/37:\n",
      "Fp13 = Fp13.rename(columns={'name': 'Name'})\n",
      "Fp13\n",
      "223/38:\n",
      "\n",
      "Fp13.Name = name\n",
      "223/39:\n",
      "\n",
      "Fpp13 = Fp13.drop('Name', axis=1)\n",
      "Fpp13\n",
      "223/40: Fpp13 = Fpp13.fillna(0)\n",
      "223/41:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)\n",
      "    Fp_normalized\n",
      "    \n",
      "    return Fp_normalized\n",
      "223/42:\n",
      "\n",
      "Fp13_n = normalized (Fpp13)\n",
      "223/43: Fp13_n\n",
      "223/44: np.any(np.isnan(Fp13_n))\n",
      "223/45: Fp13_n[\"Name\"] = Fp13.Name\n",
      "223/46: Fp13_n\n",
      "223/47: Fp13_n = Fp13_n.set_index('Name')\n",
      "223/48: Fp13_n.head(20)\n",
      "223/49: raw13  = LogKpu.merge(Fp13_n, on='Name', how='outer')\n",
      "223/50: raw13\n",
      "223/51: np.any(np.isnan(raw13))\n",
      "223/52: raw13 =raw13[raw13.LogKpu.notna()]\n",
      "223/53:\n",
      "\n",
      "raw13.to_csv('QSAR_Kpu/Mordard_Kpu.csv' , sep=',' ,index=True)\n",
      "230/1:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "230/2: pwd\n",
      "230/3: df = pd.read_csv('rawdataqsar_sm.csv')\n",
      "230/4:\n",
      "df = pd.read_csv('rawdataqsar_sm.csv')\n",
      "df\n",
      "230/5:\n",
      "df2 = df.drop_duplicates(subset=['smiles'])\n",
      "df2\n",
      "230/6:\n",
      "df3 = df2.set_index(['compound'], drop=True)\n",
      "df3\n",
      "230/7: d4 =df3[df3.kp.notna()]\n",
      "230/8:\n",
      "d4 = df3[df3.kp.notna()]\n",
      "d4\n",
      "230/9:\n",
      "d5 = df4[df3.kpu.notna()]\n",
      "d5\n",
      "230/10:\n",
      "d5 = df4[df4.kpu.notna()]\n",
      "d5\n",
      "230/11:\n",
      "d4 = df3[df3.kp.notna()]\n",
      "d4\n",
      "230/12:\n",
      "d5 = df4[df4.kpu.notna()]\n",
      "d5\n",
      "230/13:\n",
      "df4 = df3[df3.kp.notna()]\n",
      "df4\n",
      "230/14:\n",
      "d5 = df4[df4.kpu.notna()]\n",
      "d5\n",
      "230/15:\n",
      "def LogKpu(input):\n",
      "    LogKpu = []\n",
      "\n",
      "    for i in input['kpu']:\n",
      "        LogKpu.append(np.log10(i))\n",
      "\n",
      "    input['LogKpu'] = LogKpu\n",
      "    x = input.drop('kpu', 1)   \n",
      "    return x\n",
      "230/16:\n",
      "df5 = df4[df4.kpu.notna()]\n",
      "df5\n",
      "230/17: df6 = LogKpu(df5)\n",
      "230/18: df6 = LogKpu(df5)\n",
      "230/19: df6\n",
      "230/20:\n",
      "def LogKpu(input):\n",
      "    LogKpu = []\n",
      "\n",
      "    for i in input['kpu']:\n",
      "        LogKpu.append(np.log10(i))\n",
      "\n",
      "    input['LogKpu'] = LogKpu\n",
      "    x = input   \n",
      "    return x\n",
      "230/21: df6 = LogKpu(df5)\n",
      "230/22: df6\n",
      "230/23:\n",
      "# Calculate Molecular Descriptors\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Descriptors, Lipinski\n",
      "\n",
      "def lipinski(smiles, verbose=False):\n",
      "\n",
      "    moldata= []\n",
      "    for elem in smiles:\n",
      "        mol=Chem.MolFromSmiles(elem) \n",
      "        moldata.append(mol)\n",
      "       \n",
      "    baseData= np.arange(1,1)\n",
      "    i=0  \n",
      "    for mol in moldata:        \n",
      "       \n",
      "        desc_MolWt = Descriptors.MolWt(mol)\n",
      "        desc_TPSA = Descriptors.TPSA(mol)\n",
      "        desc_MolLogP = Descriptors.MolLogP(mol)\n",
      "        desc_NumHDonors = Lipinski.NumHDonors(mol)\n",
      "        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)\n",
      "        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)\n",
      "        desc_FpDensityMorgan1 = Descriptors.FpDensityMorgan1(mol)\n",
      "        desc_FpDensityMorgan2 = Descriptors.FpDensityMorgan2(mol)\n",
      "        desc_FpDensityMorgan3 = Descriptors.FpDensityMorgan3(mol)\n",
      "        desc_NumRadicalElectrons = Descriptors.NumRadicalElectrons(mol)\n",
      "        desc_NumValenceElectrons = Descriptors.NumValenceElectrons(mol)\n",
      "           \n",
      "        row = np.array([desc_MolWt,\n",
      "                        desc_TPSA,\n",
      "                        desc_MolLogP,\n",
      "                        desc_NumHDonors,\n",
      "                        desc_NumHAcceptors,\n",
      "                        desc_NumRotatableBonds,\n",
      "                        desc_FpDensityMorgan1,\n",
      "                        desc_FpDensityMorgan2,\n",
      "                        desc_FpDensityMorgan3,\n",
      "                        desc_NumRadicalElectrons,\n",
      "                        desc_NumValenceElectrons])   \n",
      "    \n",
      "        if(i==0):\n",
      "            baseData=row\n",
      "        else:\n",
      "            baseData=np.vstack([baseData, row])\n",
      "        i=i+1      \n",
      "    \n",
      "    columnNames=[\"MW\",\"TPSA\", \"LogP\",\"NumHDonors\",\"NumHAcceptors\", \"NumRotatableBonds\", 'FpDensityMorgan1', 'FpDensityMorgan2', 'FpDensityMorgan3', 'NumRadicalElectrons', 'NumValenceElectrons']   \n",
      "    descriptors = pd.DataFrame(data=baseData,columns=columnNames)\n",
      "    \n",
      "    return descriptors\n",
      "230/24:\n",
      "df6_lipinski = lipinski(df6.smiles)\n",
      "df6_lipinski\n",
      "230/25:\n",
      "data = pd.concat([df6, df6_lipinski], axis=1)\n",
      "data.to_csv('df6_lipinski_Kpu_Kp_cells.csv')\n",
      "data\n",
      "230/26:\n",
      "df6_lipinski = lipinski(df6.smiles)\n",
      "df6_lipinski.set_index(df6['compound'])\n",
      "230/27:\n",
      "df6_lipinski = lipinski(df6.smiles)\n",
      "df6_lipinski.set_index(df6[compound])\n",
      "230/28:\n",
      "df6_lipinski = lipinski(df6.smiles)\n",
      "df6_lipinski.set_index(df6['compound'])\n",
      "230/29: indices = df6.index\n",
      "230/30:\n",
      "df6_lipinski = lipinski(df6.smiles)\n",
      "df6_lipinski.set_index([indices])\n",
      "230/31:\n",
      "data = pd.concat([df6, df6_lipinski], axis=1)\n",
      "data.to_csv('df6_lipinski_Kpu_Kp_cells.csv')\n",
      "data\n",
      "230/32:\n",
      "df6_lipinski = lipinski(df6.smiles)\n",
      "df6_lipinski = df6_lipinski.set_index([indices])\n",
      "230/33:\n",
      "data = pd.concat([df6, df6_lipinski], axis=1)\n",
      "data.to_csv('df6_lipinski_Kpu_Kp_cells.csv')\n",
      "data\n",
      "230/34:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2) = plt.subplots(1, 1)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "# Histogram for Kp\n",
      "plt1.hist(data['kp'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)\n",
      "plt1.set_xlabel('Kp', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt1.set_ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Kpu\n",
      "plt2.hist(data['kpu'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)\n",
      "plt2.set_xlabel(\"Kpu\", fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt2.set_ylim(0, 50)\n",
      "plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Descriptor_outlier.pdf', dpi=300)\n",
      "230/35:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2)) = plt.subplots(1, 1)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "# Histogram for Kp\n",
      "plt1.hist(data['kp'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)\n",
      "plt1.set_xlabel('Kp', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt1.set_ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Kpu\n",
      "plt2.hist(data['kpu'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)\n",
      "plt2.set_xlabel(\"Kpu\", fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt2.set_ylim(0, 50)\n",
      "plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Descriptor_outlier.pdf', dpi=300)\n",
      "230/36:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2)) = plt.subplots(2, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "# Histogram for Kp\n",
      "plt1.hist(data['kp'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)\n",
      "plt1.set_xlabel('Kp', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt1.set_ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Kpu\n",
      "plt2.hist(data['kpu'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)\n",
      "plt2.set_xlabel(\"Kpu\", fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt2.set_ylim(0, 50)\n",
      "plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Descriptor_outlier.pdf', dpi=300)\n",
      "230/37:\n",
      "data = pd.concat([df6, df6_lipinski], axis=1)\n",
      "data.to_csv('df6_lipinski_Kpu_Kp_cells.csv')\n",
      "data\n",
      "230/38:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2)) = plt.subplots(2, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "# Histogram for Kp\n",
      "plt1.hist(data['kp'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)\n",
      "plt1.set_xlabel('Kp', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt1.set_ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Kpu\n",
      "plt2.hist(data['kpu'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)\n",
      "plt2.set_xlabel(\"Kpu\", fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt2.set_ylim(0, 50)\n",
      "plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Descriptor_outlier.pdf', dpi=300)\n",
      "230/39: data.kp\n",
      "230/40:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2)) = plt.subplots(2, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "# Histogram for Kp\n",
      "plt1.hist(data['kp'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)\n",
      "plt1.set_xlabel('Kp', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt1.set_ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Kpu\n",
      "plt2.hist(data['kpu'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)\n",
      "plt2.set_xlabel(\"Kpu\", fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt2.set_ylim(0, 50)\n",
      "plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Descriptor_outlier.pdf', dpi=300)\n",
      "230/41: dat\n",
      "230/42: data\n",
      "230/43:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "# Histogram for Kp\n",
      "plt1.hist(data['kp'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)\n",
      "plt1.set_xlabel('Kp', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt1.set_ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Kpu\n",
      "plt2.hist(data['kpu'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)\n",
      "plt2.set_xlabel(\"Kpu\", fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt2.set_ylim(0, 50)\n",
      "plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Descriptor_outlier.pdf', dpi=300)\n",
      "230/44:\n",
      "#Here we are ploting the data\n",
      "%matplotlib inline\n",
      "import matplotlib as mpl\n",
      "import matplotlib.mlab as mlab\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "figure, ((plt1,plt2)) = plt.subplots(2, 2)\n",
      "figure.set_size_inches(15,15)\n",
      "\n",
      "# Histogram for Kp\n",
      "plt1.his(data['kp'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)\n",
      "plt1.set_xlabel('Kp', fontsize=16, fontweight='bold')\n",
      "plt1.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt1.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt1.set_ylim(0, 50)\n",
      "#plt1.grid(True)\n",
      "plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "#Histogram for Kpu\n",
      "plt2.hist(data['kpu'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)\n",
      "plt2.set_xlabel(\"Kpu\", fontsize=16, fontweight='bold')\n",
      "plt2.set_ylabel(\"Frequency\", fontsize=16, fontweight='bold')\n",
      "plt2.tick_params(axis='both', which='major', labelsize=14)\n",
      "plt2.set_ylim(0, 50)\n",
      "plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)\n",
      "\n",
      "\n",
      "\n",
      "#save file\n",
      "plt.tight_layout()\n",
      "plt.savefig('Descriptor_outlier.pdf', dpi=300)\n",
      "230/45: data.boxplot(lp)\n",
      "230/46: data.boxplot(kp)\n",
      "230/47: data.boxplot(kp)\n",
      "230/48: data.boxplot('kp')\n",
      "230/49: q3 = data.kp.quantile(.75)\n",
      "230/50:\n",
      "q3 = data.kp.quantile(.75)\n",
      "q1 = data.kp.quantile(.25)\n",
      "230/51:\n",
      "q3 = data.kp.quantile(.75)\n",
      "q1 = data.kp.quantile(.25)\n",
      "iqr = q3-q1\n",
      "230/52:\n",
      "q3 = data.kp.quantile(.75)\n",
      "q1 = data.kp.quantile(.25)\n",
      "iqr = q3-q1\n",
      "upper = q3 + (1.5*iqr)\n",
      "lower = q1 - (1.5*iqr)\n",
      "print('q1 = {}'. format(q1))\n",
      "230/53:\n",
      "q3 = data.kp.quantile(.75)\n",
      "q1 = data.kp.quantile(.25)\n",
      "iqr = q3-q1\n",
      "upper = q3 + (1.5*iqr)\n",
      "lower = q1 - (1.5*iqr)\n",
      "print('q1 = {}'. format(q1))\n",
      "print('q3 = {}'. format(q3))\n",
      "230/54:\n",
      "q3 = data.kp.quantile(.75)\n",
      "q1 = data.kp.quantile(.25)\n",
      "iqr = q3-q1\n",
      "upper = q3 + (1.5*iqr)\n",
      "lower = q1 - (1.5*iqr)\n",
      "print('q1 = {}'. format(q1))\n",
      "print('q3 = {}'. format(q3))\n",
      "print('iqr = {}'. format(iqr))\n",
      "231/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "231/2:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "231/3: Fp13.shape\n",
      "231/4:\n",
      "path = r'Fingerprint/'\n",
      "\n",
      "Fp13  = pd.read_csv(path + 'mordard.csv'               , header = 0)\n",
      "231/5: Fp13.shape\n",
      "231/6: Fp13\n",
      "231/7:\n",
      "df = pd.read_csv('df6_lipinski_kpu.csv')\n",
      "df\n",
      "231/8: name = df['compound']\n",
      "231/9: name.shape\n",
      "231/10:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df2 = df2.drop('Unnamed: 0', axis=1)\n",
      "df3 = df2.set_index('Name')\n",
      "df3\n",
      "230/55:\n",
      "q3 = data.kp.quantile(.75)\n",
      "q1 = data.kp.quantile(.25)\n",
      "iqr = q3-q1\n",
      "upper = q3 + (1.5*iqr)\n",
      "lower = q1 - (1.5*iqr)\n",
      "print('q1 = {}'. format(q1))\n",
      "print('q3 = {}'. format(q3))\n",
      "print('iqr = {}'. format(iqr))\n",
      "print('upper = {}'. format(upper))\n",
      "print('lower = {}'. format(lower))\n",
      "230/56: df7 = data[data.kp>314.55]\n",
      "230/57: df7\n",
      "231/11:\n",
      "df = pd.read_csv('df6_lipinski.csv')\n",
      "df\n",
      "231/12: name = df['compound']\n",
      "231/13: name.shape\n",
      "231/14:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df2 = df2.drop('Unnamed: 0', axis=1)\n",
      "df3 = df2.set_index('Name')\n",
      "df3\n",
      "231/15:\n",
      "LogKpu = df3[['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]\n",
      "LogKpu\n",
      "231/16:\n",
      "LogKp = df3[['LogKp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]\n",
      "LogKp\n",
      "231/17:\n",
      "Fp13 = Fp13.rename(columns={'name': 'Name'})\n",
      "Fp13\n",
      "231/18:\n",
      "\n",
      "Fpp13 = Fp13.drop('Name', axis=1)\n",
      "Fpp13\n",
      "231/19:\n",
      "\n",
      "Fp13.Name = name\n",
      "231/20:\n",
      "\n",
      "Fpp13 = Fp13.drop('Name', axis=1)\n",
      "Fpp13\n",
      "231/21: Fpp13 = Fpp13.fillna(0)\n",
      "231/22:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)\n",
      "    Fp_normalized\n",
      "    \n",
      "    return Fp_normalized\n",
      "231/23:\n",
      "\n",
      "Fp13_n = normalized (Fpp13)\n",
      "231/24: Fp13_n\n",
      "231/25: np.any(np.isnan(Fp13_n))\n",
      "231/26: Fp13_n[\"Name\"] = Fp13.Name\n",
      "231/27: Fp13_n\n",
      "231/28: Fp13_n = Fp13_n.set_index('Name')\n",
      "231/29: Fp13_n.head(20)\n",
      "231/30: raw13  = LogKp.merge(Fp13_n, on='Name', how='outer')\n",
      "231/31: raw13\n",
      "231/32: np.any(np.isnan(raw13))\n",
      "231/33: raw13 =raw13[raw13.LogKpu.notna()]\n",
      "231/34: raw13 =raw13[raw13.LogKp.notna()]\n",
      "231/35:\n",
      "\n",
      "raw13.to_csv('QSAR_Kp/Mordard_Kp.csv' , sep=',' ,index=True)\n",
      "231/36:\n",
      "\n",
      "raw13.to_csv('QSAR_Kp/Mordard_Kp.csv' , sep=',' ,index=True)\n",
      "231/37: print (len(raw13))\n",
      "231/38: print (len(raw13 .columns))\n",
      "231/39:\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['LogKp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)\n",
      "label_to_model = compound_df.LogKp.tolist()\n",
      "231/40: fingerprint_to_model.head(3)\n",
      "231/41: indices = fingerprint_to_model.index\n",
      "231/42:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "sel = VarianceThreshold(threshold=(0))\n",
      "sel = sel.fit(fingerprint_to_model)\n",
      "mask = sel.get_support()\n",
      "231/43:\n",
      "fingerprint_to_model2 = fingerprint_to_model.loc[:, mask]\n",
      "fingerprint_to_model2\n",
      "231/44:\n",
      "train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model2, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "231/45: indices_train, indices_test\n",
      "231/46:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
      "231/47:\n",
      "import random\n",
      "SEED = random.seed(10)\n",
      "SEED\n",
      "231/48:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
      "231/49:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "231/50:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='black')\n",
      "plt.scatter(train_y, y_pred, c='blue')\n",
      "223/54: # remove SM SMI SMO\n",
      "223/55: raw13\n",
      "223/56: raw14 = raw.drop['sesamol', 'sesamin', 'sesamolin']\n",
      "223/57: raw14 = raw13.drop['sesamol', 'sesamin', 'sesamolin']\n",
      "223/58: raw14 = raw13.drop(['sesamol', 'sesamin', 'sesamolin'])\n",
      "223/59: raw14\n",
      "223/60:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
      "223/61:\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)\n",
      "label_to_model = compound_df.LogKpu.tolist()\n",
      "indices = fingerprint_to_model.index\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "sel = VarianceThreshold(threshold=(0))\n",
      "sel = sel.fit(fingerprint_to_model)\n",
      "mask = sel.get_support()\n",
      "fingerprint_to_model2 = fingerprint_to_model.loc[:, mask]\n",
      "train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model2, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "indices_train, indices_test\n",
      "223/62:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
      "223/63:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "223/64:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestRegressor(n_estimators=200, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "231/51:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestRegressor(n_estimators=200, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "231/52:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "231/53:\n",
      "raw13 =raw13[raw13.LogKp.notna()]\n",
      "raw13 =raw13.drop(['GSK236090'])\n",
      "231/54:\n",
      "raw13 =raw13[raw13.LogKp.notna()]\n",
      "raw13 =raw13.drop(['GSK326090'])\n",
      "231/55:\n",
      "raw13 =raw13[raw13.LogKp.notna()]\n",
      "raw13 =raw13.drop(['GSK326093'])\n",
      "231/56:\n",
      "raw13 =raw13[raw13.LogKp.notna()]\n",
      "raw13 =raw13.drop(['GSK326090'])\n",
      "231/57:\n",
      "raw13 =raw13[raw13.LogKp.notna()]\n",
      "raw13 =raw13.drop(['GSK326090'], axis=0)\n",
      "231/58: raw13.GSK326090\n",
      "231/59: raw13.index\n",
      "231/60: print(raw13.index)\n",
      "231/61:\n",
      "raw13 =raw13[raw13.LogKp.notna()]\n",
      "raw13\n",
      "231/62: raw13.LogKp.max\n",
      "231/63: raw13.LogKp.max()\n",
      "231/64:\n",
      "raw13 =raw13[raw13.LogKp.notna()]\n",
      "raw13 =raw.drop(['GSK326090A'])\n",
      "231/65:\n",
      "raw13 =raw13[raw13.LogKp.notna()]\n",
      "raw13 =raw13.drop(['GSK326090A'])\n",
      "231/66:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='black')\n",
      "plt.scatter(train_y, y_pred, c='blue')\n",
      "223/65:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "231/67:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestRegressor(n_estimators=200, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "231/68:\n",
      "raw13 =raw13[raw13.LogKp.notna()]\n",
      "raw13 =raw13.drop([GSK326090A])\n",
      "231/69:\n",
      "#Test features RF\n",
      "from sklearn.inspection import permutation_importance\n",
      "result = permutation_importance(model_RF, test_x, test_y, n_repeats=10, random_state=42, n_jobs=2)\n",
      "feature_names = fingerprint_to_model2.columns\n",
      "\n",
      "forest_importances = pd.Series(result.importances_mean, index=feature_names)\n",
      "fig, ax = plt.subplots()\n",
      "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
      "ax.set_title(\"Feature importances using permutation on full model\")\n",
      "ax.set_ylabel(\"Mean accuracy decrease\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "231/70:\n",
      "pd.DataFrame(train_x, indices_train).to_csv(\"Result_RF/x_train_kp.csv\")\n",
      "pd.DataFrame(test_x, indices_test).to_csv(\"Result_RF/x_test_kp.csv\")\n",
      "pd.DataFrame(y_cv, indices_train).to_csv(\"Result_RF/y_cv_kp.csv\")\n",
      "pd.DataFrame(train_y, indices_train).to_csv(\"Result_RF/train_y_kp.csv\")\n",
      "pd.DataFrame(test_y, indices_test).to_csv(\"Result_RF/test_y_kp.csv\")\n",
      "pd.DataFrame(y_pred, indices_train).to_csv(\"Result_RF/y_pred_kp.csv\")\n",
      "pd.DataFrame(y_ext, indices_test).to_csv(\"Result_RF/y_SVR_ext_kp.csv\")\n",
      "231/71: feature_names\n",
      "231/72:\n",
      "#Train features\n",
      "from sklearn.inspection import permutation_importance\n",
      "result = permutation_importance(model_RF, train_x, train_y, n_repeats=10, random_state=42, n_jobs=2)\n",
      "feature_names = fingerprint_to_model2.columns\n",
      "\n",
      "forest_importances = pd.Series(result.importances_mean, index=feature_names)\n",
      "fig, ax = plt.subplots()\n",
      "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
      "ax.set_title(\"Feature importances using permutation on full model\")\n",
      "ax.set_ylabel(\"Mean accuracy decrease\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "230/58:\n",
      "q3 = data.kp.quantile(.75)\n",
      "q1 = data.kp.quantile(.25)\n",
      "iqr = q3-q1\n",
      "upper = q3 + (1.5*iqr)\n",
      "lower = q1 - (1.5*iqr)\n",
      "print('q1 = {}'. format(q1))\n",
      "print('q3 = {}'. format(q3))\n",
      "print('iqr = {}'. format(iqr))\n",
      "print('upper = {}'. format(upper))\n",
      "print('lower = {}'. format(lower))\n",
      "230/59:\n",
      "selection = ['smiles']\n",
      "smiles = data[selection]\n",
      "smiles.to_csv(r'smiles/new_smiles_Kp.smi', sep='\\t', index=False, header=False)\n",
      "225/1: !python -m mordred -t smi smiles/new_smiles_Kp.smi -o smiles/mordard_Kp.csv -d ABCIndex -d AcidBase -d AdjacencyMatrix -d Aromatic -d AtomCount -d Autocorrelation -d BCUT -d BalabanJ -d BertzCT -d BondCount -d CarbonTypes -d Chi -d Constitutional -d DistanceMatrix -d EState -d EccentricConnectivityIndex -d ExtendedTopochemicalAtom -d FragmentComplexity -d Framework -d GeometricalIndex -d GravitationalIndex -d HydrogenBond -d InformationContent -d KappaShapeIndex -d Lipinski -d LogS -d McGowanVolume -d MoeType -d MolecularDistanceEdge -d MolecularId -d PathCount -d Polarizability -d RingCount -d RotatableBond -d SLogP -d TopoPSA -d TopologicalCharge -d TopologicalIndex -d VdwVolumeABC -d VertexAdjacencyInformation -d WalkCount -d Weight -d WienerIndex -d ZagrebIndex\n",
      "231/73:\n",
      "path = r'Fingerprint/'\n",
      "\n",
      "Fp13  = pd.read_csv(path + 'mordard_Kp.csv'               , header = 0)\n",
      "231/74:\n",
      "path = r'smiles/'\n",
      "\n",
      "Fp13  = pd.read_csv(path + 'mordard_Kp.csv'               , header = 0)\n",
      "231/75: Fp13.shape\n",
      "231/76: Fp13\n",
      "231/77:\n",
      "df = pd.read_csv('df6_lipinski_Kpu_Kp_cells.csv')\n",
      "df\n",
      "231/78: name = df['compound']\n",
      "231/79: name.shape\n",
      "231/80:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df2 = df2.drop('Unnamed: 0', axis=1)\n",
      "df3 = df2.set_index('Name')\n",
      "df3\n",
      "231/81:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df3 = df2.set_index('Name')\n",
      "df3\n",
      "231/82:\n",
      "LogKp = df3[['LogKp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]\n",
      "LogKp\n",
      "231/83:\n",
      "LogKp = df3[['logkp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]\n",
      "LogKp\n",
      "231/84:\n",
      "Fp13 = Fp13.rename(columns={'name': 'Name'})\n",
      "Fp13\n",
      "231/85:\n",
      "\n",
      "Fp13.Name = name\n",
      "231/86:\n",
      "\n",
      "Fpp13 = Fp13.drop('Name', axis=1)\n",
      "Fpp13\n",
      "231/87: Fpp13 = Fpp13.fillna(0)\n",
      "231/88:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)\n",
      "    Fp_normalized\n",
      "    \n",
      "    return Fp_normalized\n",
      "231/89:\n",
      "\n",
      "Fp13_n = normalized (Fpp13)\n",
      "231/90: Fp13_n\n",
      "231/91: np.any(np.isnan(Fp13_n))\n",
      "231/92: Fp13_n[\"Name\"] = Fp13.Name\n",
      "231/93: Fp13_n\n",
      "231/94: Fp13_n = Fp13_n.set_index('Name')\n",
      "231/95: Fp13_n.head(20)\n",
      "231/96: Fp13_n.head(3)\n",
      "231/97: raw13  = LogKp.merge(Fp13_n, on='Name', how='outer')\n",
      "231/98: raw13\n",
      "231/99: np.any(np.isnan(raw13))\n",
      "231/100:\n",
      "raw13 =raw13[raw13.LogKp.notna()]\n",
      "raw13 =raw13.drop([GSK326090A])\n",
      "231/101:\n",
      "raw13 =raw13[raw13.logkp.notna()]\n",
      "raw13 =raw13.drop([GSK326090A])\n",
      "231/102:\n",
      "raw13 =raw13[raw13.logkp.notna()]\n",
      "raw13.shape\n",
      "231/103: print (len(raw13))\n",
      "231/104: print (len(raw13 .columns))\n",
      "231/105: raw13.to_csv('QSAR_Kp/Mordard_Kp_144.csv' , sep=',' ,index=True)\n",
      "231/106:\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['logkp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)\n",
      "label_to_model = compound_df.LogKp.tolist()\n",
      "231/107:\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['logkp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)\n",
      "label_to_model = compound_df.logkp.tolist()\n",
      "231/108: indices = fingerprint_to_model.index\n",
      "231/109:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "sel = VarianceThreshold(threshold=(0))\n",
      "sel = sel.fit(fingerprint_to_model)\n",
      "mask = sel.get_support()\n",
      "231/110:\n",
      "fingerprint_to_model2 = fingerprint_to_model.loc[:, mask]\n",
      "fingerprint_to_model2\n",
      "231/111:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "sel = sel.fit(fingerprint_to_model)\n",
      "mask = sel.get_support()\n",
      "231/112:\n",
      "fingerprint_to_model2 = fingerprint_to_model.loc[:, mask]\n",
      "fingerprint_to_model2\n",
      "231/113:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "sel = VarianceThreshold(threshold=(0))\n",
      "sel = sel.fit(fingerprint_to_model)\n",
      "mask = sel.get_support()\n",
      "231/114:\n",
      "fingerprint_to_model2 = fingerprint_to_model.loc[:, mask]\n",
      "fingerprint_to_model2\n",
      "231/115:\n",
      "train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model2, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "231/116: indices_train, indices_test\n",
      "231/117: indices_train, indices_test\n",
      "231/118:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
      "231/119: fingerprint_to_model.shape\n",
      "231/120:\n",
      "fingerprint_to_model2 = fingerprint_to_model.loc[:, mask]\n",
      "fingerprint_to_model2\n",
      "231/121:\n",
      "train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model2, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "231/122: indices_train, indices_test\n",
      "231/123:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
      "231/124: fingerprint_to_model2.shape\n",
      "231/125: train_x.shape\n",
      "231/126:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "231/127:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='black')\n",
      "plt.scatter(train_y, y_pred, c='blue')\n",
      "231/128:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "231/129:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='linear')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "231/130:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestRegressor(n_estimators=200, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "231/131:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "231/132:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "\n",
      "model_PLS = PLSRegression(n_components=10)\n",
      "scores=cross_val_score(model_PLS, train_x, train_y, cv=kf)\n",
      "\n",
      "#calculate R2\n",
      "model_fit = model_PLS.fit(train_x, train_y)\n",
      "y_pred = model_PLS.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_PLS, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_PLS.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "231/133:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "\n",
      "model_PLS = PLSRegression(n_components=10)\n",
      "scores=cross_val_score(model_PLS, train_x, train_y, cv=kf)\n",
      "\n",
      "#calculate R2\n",
      "model_fit = model_PLS.fit(train_x, train_y)\n",
      "y_pred = model_PLS.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_PLS, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_PLS.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "231/134:\n",
      "from sklearn.linear_model import Ridge\n",
      "Ridge = Ridge(alpha=1.0)\n",
      "\n",
      "#calculate R2\n",
      "model_fit = Ridge.fit(train_x, train_y)\n",
      "y_pred = Ridge.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(Ridge, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = Ridge.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "231/135:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestRegressor(n_estimators=200, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "231/136:\n",
      "from sklearn.linear_model import Ridge\n",
      "Ridge = Ridge(alpha=1.0)\n",
      "\n",
      "#calculate R2\n",
      "model_fit = Ridge.fit(train_x, train_y)\n",
      "y_pred = Ridge.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(Ridge, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = Ridge.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "231/137:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "\n",
      "model_PLS = PLSRegression(n_components=10)\n",
      "scores=cross_val_score(model_PLS, train_x, train_y, cv=kf)\n",
      "\n",
      "#calculate R2\n",
      "model_fit = model_PLS.fit(train_x, train_y)\n",
      "y_pred = model_PLS.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_PLS, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_PLS.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "231/138:\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "MLP = MLPRegressor(activation='relu', random_state=42, max_iter=1000, solver='lbfgs')\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "model_fit = MLP.fit(train_x, train_y)\n",
      "y_pred = MLP.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(MLP, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = MLP.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "231/139:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "sel = sel.fit(fingerprint_to_model)\n",
      "mask = sel.get_support()\n",
      "231/140:\n",
      "fingerprint_to_model2 = fingerprint_to_model.loc[:, mask]\n",
      "fingerprint_to_model2\n",
      "231/141: fingerprint_to_model2.columns\n",
      "231/142:\n",
      "train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model2, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "231/143: indices_train, indices_test\n",
      "231/144:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
      "231/145: fingerprint_to_model2.shape\n",
      "231/146: train_x.shape\n",
      "231/147:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "231/148:\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "231/149:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "231/150:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='linear')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "231/151:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestRegressor(n_estimators=200, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "231/152:\n",
      "#Test features RF\n",
      "from sklearn.inspection import permutation_importance\n",
      "result = permutation_importance(model_RF, test_x, test_y, n_repeats=10, random_state=42, n_jobs=2)\n",
      "feature_names = fingerprint_to_model2.columns\n",
      "forest_importances = pd.Series(result.importances_mean, index=feature_names)\n",
      "fig, ax = plt.subplots()\n",
      "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
      "ax.set_title(\"Feature importances using permutation on full model\")\n",
      "ax.set_ylabel(\"Mean accuracy decrease\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "231/153: feature_names\n",
      "231/154:\n",
      "FI_RF_test_mean = pd.DataFrame(forest_importances)\n",
      "FI_RF_test_mean.to_csv('Result_RF/forest_importances_test_RF_Kp_156.csv')\n",
      "FI_RF_test_mean\n",
      "231/155:\n",
      "#Train features\n",
      "from sklearn.inspection import permutation_importance\n",
      "result = permutation_importance(model_RF, train_x, train_y, n_repeats=10, random_state=42, n_jobs=2)\n",
      "feature_names = fingerprint_to_model2.columns\n",
      "\n",
      "forest_importances = pd.Series(result.importances_mean, index=feature_names)\n",
      "fig, ax = plt.subplots()\n",
      "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
      "ax.set_title(\"Feature importances using permutation on full model\")\n",
      "ax.set_ylabel(\"Mean accuracy decrease\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "237/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "237/2:\n",
      "path = r'smiles/'\n",
      "\n",
      "Fp13  = pd.read_csv(path + 'mordard_Kp.csv'               , header = 0)\n",
      "237/3: Fp13.shape\n",
      "237/4: Fp13\n",
      "237/5:\n",
      "df = pd.read_csv('df6_lipinski_Kpu_Kp_cells.csv')\n",
      "df\n",
      "237/6: name = df['compound']\n",
      "237/7: name.shape\n",
      "237/8:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df3 = df2.set_index('Name')\n",
      "df3\n",
      "237/9:\n",
      "LogKp = df3[['logkp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]\n",
      "LogKp\n",
      "237/10:\n",
      "Fp13 = Fp13.rename(columns={'name': 'Name'})\n",
      "Fp13\n",
      "237/11:\n",
      "\n",
      "Fp13.Name = name\n",
      "237/12:\n",
      "\n",
      "Fpp13 = Fp13.drop('Name', axis=1)\n",
      "Fpp13\n",
      "237/13: Fpp13 = Fpp13.fillna(0)\n",
      "237/14:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)\n",
      "    Fp_normalized\n",
      "    \n",
      "    return Fp_normalized\n",
      "237/15:\n",
      "\n",
      "Fp13_n = normalized (Fpp13)\n",
      "237/16: Fp13_n\n",
      "237/17: np.any(np.isnan(Fp13_n))\n",
      "237/18: Fp13_n[\"Name\"] = Fp13.Name\n",
      "237/19: Fp13_n\n",
      "237/20: Fp13_n = Fp13_n.set_index('Name')\n",
      "237/21: Fp13_n.head(3)\n",
      "237/22: raw13  = LogKp.merge(Fp13_n, on='Name', how='outer')\n",
      "237/23: raw13\n",
      "237/24: np.any(np.isnan(raw13))\n",
      "237/25:\n",
      "raw13 =raw13[raw13.logkp.notna()]\n",
      "raw13.shape\n",
      "237/26: print (len(raw13))\n",
      "237/27: print (len(raw13 .columns))\n",
      "237/28:\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['logkp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)\n",
      "label_to_model = compound_df.logkp.tolist()\n",
      "237/29: fingerprint_to_model.head(3)\n",
      "237/30: indices = fingerprint_to_model.index\n",
      "237/31:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "sel = sel.fit(fingerprint_to_model)\n",
      "mask = sel.get_support()\n",
      "237/32:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "sel = VarianceThreshold(threshold=(0.00))\n",
      "sel = sel.fit(fingerprint_to_model)\n",
      "mask = sel.get_support()\n",
      "237/33:\n",
      "fingerprint_to_model2 = fingerprint_to_model.loc[:, mask]\n",
      "fingerprint_to_model2\n",
      "237/34:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "sel = sel.fit(fingerprint_to_model)\n",
      "mask = sel.get_support()\n",
      "237/35:\n",
      "fingerprint_to_model2 = fingerprint_to_model.loc[:, mask]\n",
      "fingerprint_to_model2\n",
      "237/36: fingerprint_to_model2.set_option('display.max_columns\", None)\n",
      "237/37: fingerprint_to_model2.columns\n",
      "237/38:\n",
      "import random\n",
      "SEED = random.seed(10)\n",
      "SEED\n",
      "237/39:\n",
      "train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model2, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "237/40: indices_train, indices_test\n",
      "237/41:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
      "237/42: fingerprint_to_model2.shape\n",
      "237/43: train_x.shape\n",
      "237/44:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "237/45:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='linear')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "237/46:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestRegressor(n_estimators=200, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "246/1:\n",
      "from rdkit import Chem #RDKit Chemistry\n",
      "from rdkit.Chem.Draw import IPythonConsole #RDKit drawing\n",
      "from rdkit.Chem import Draw #RDKit drawing\n",
      "# A few settings to improve the quality of structures \n",
      "from rdkit.Chem import rdDepictor\n",
      "IPythonConsole.ipython_useSVG = True\n",
      "rdDepictor.SetPreferCoordGen(True)\n",
      "from rdkit.Chem import PandasTools #Add the ability to add a molecule to a dataframegrid\n",
      "import mols2grid #The mols2grid library provides a convenient way of displaying molecules in a grid\n",
      "246/2: ! pip install mol2grid\n",
      "246/3:\n",
      "from rdkit import Chem #RDKit Chemistry\n",
      "from rdkit.Chem.Draw import IPythonConsole #RDKit drawing\n",
      "from rdkit.Chem import Draw #RDKit drawing\n",
      "# A few settings to improve the quality of structures \n",
      "from rdkit.Chem import rdDepictor\n",
      "IPythonConsole.ipython_useSVG = True\n",
      "rdDepictor.SetPreferCoordGen(True)\n",
      "from rdkit.Chem import PandasTools #Add the ability to add a molecule to a dataframegrid\n",
      "import mols2grid #The mols2grid library provides a convenient way of displaying molecules in a grid\n",
      "246/4: ! pip install mols2grid\n",
      "246/5:\n",
      "from rdkit import Chem #RDKit Chemistry\n",
      "from rdkit.Chem.Draw import IPythonConsole #RDKit drawing\n",
      "from rdkit.Chem import Draw #RDKit drawing\n",
      "# A few settings to improve the quality of structures \n",
      "from rdkit.Chem import rdDepictor\n",
      "IPythonConsole.ipython_useSVG = True\n",
      "rdDepictor.SetPreferCoordGen(True)\n",
      "from rdkit.Chem import PandasTools #Add the ability to add a molecule to a dataframegrid\n",
      "import mols2grid #The mols2grid library provides a convenient way of displaying molecules in a grid\n",
      "246/6: mol = Chem.MolFromSmiles(\"c1ccccc1\")\n",
      "246/7: mol\n",
      "246/8: mol = Chem.MolFromSmiles(\"c1=ccccc1\")\n",
      "246/9:\n",
      "mol = Chem.MolFromSmiles(\"c1=ccccc1\")\n",
      "mol\n",
      "246/10:\n",
      "mol = Chem.MolFromSmiles(\"c1(=0)cccc1\")\n",
      "mol\n",
      "246/11:\n",
      "mol = Chem.MolFromSmiles(\"c(=0)cccc1\")\n",
      "mol\n",
      "246/12:\n",
      "mol = Chem.MolFromSmiles(\"c(=0)ccc1\")\n",
      "mol\n",
      "246/13:\n",
      "mol = Chem.MolFromSmiles(\"c(=0)cc\")\n",
      "mol\n",
      "246/14:\n",
      "mol = Chem.MolFromSmiles(\"c(=o)cc\")\n",
      "mol\n",
      "246/15:\n",
      "mol = Chem.MolFromSmiles(\"c(=o)c\")\n",
      "mol\n",
      "246/16:\n",
      "mol = Chem.MolFromSmiles(\"cc(=o)c\")\n",
      "mol\n",
      "246/17:\n",
      "mol = Chem.MolFromSmiles(\"c1c=oc\")\n",
      "mol\n",
      "246/18:\n",
      "mol = Chem.MolFromSmiles(\"c1ccc1\")\n",
      "mol\n",
      "246/19:\n",
      "mol = Chem.MolFromSmiles(\"c1cccc1\")\n",
      "mol\n",
      "246/20:\n",
      "mol = Chem.MolFromSmiles(\"c1ccccc1\")\n",
      "mol\n",
      "246/21:\n",
      "mol = Chem.MolFromSmiles(\"c1cccccc1\")\n",
      "mol\n",
      "246/22:\n",
      "mol = Chem.MolFromSmiles(\"c1cccc1\")\n",
      "mol\n",
      "246/23:\n",
      "mol = Chem.MolFromSmiles(\"c1ccc1\")\n",
      "mol\n",
      "246/24:\n",
      "mol = Chem.MolFromSmiles(\"c1ccc\")\n",
      "mol\n",
      "246/25:\n",
      "mol = Chem.MolFromSmiles(\"c1cc1c\")\n",
      "mol\n",
      "246/26:\n",
      "mol = Chem.MolFromSmiles(\"c1ccc1\")\n",
      "mol\n",
      "246/27:\n",
      "mol = Chem.MolFromSmiles(\"c1cccc1\")\n",
      "mol\n",
      "246/28:\n",
      "mol = Chem.MolFromSmiles(\"c1ccccn1\")\n",
      "mol\n",
      "246/29: glvc = Chem.MolFromSmiles(\"CN1CCN(Cc2ccc(cc2)C(=O)Nc3ccc(C)c(Nc4nccc(n4)c5cccnc5)c3)CC1\")\n",
      "246/30: glvc\n",
      "246/31: mols = [x for x in Chem.SDMolSupplier(\"example_compounds.sdf\")]\n",
      "246/32: mols\n",
      "246/33: Draw.MolsToGridImage(mols,molsPerRow=4,useSVG=True)\n",
      "246/34: mols2grid.display(mols)\n",
      "247/1:\n",
      "import pandas as pd #data table manipulation\n",
      "from rdkit import Chem # basic cheminformatics\n",
      "from rdkit.Chem.Descriptors import MolWt, MolLogP, NumAromaticRings, NumHDonors, NumHAcceptors\n",
      "import math #needed for log10\n",
      "import seaborn as sns #plotting\n",
      "from sklearn.tree import DecisionTreeClassifier, plot_tree # descision trees \n",
      "from sklearn.model_selection import train_test_split # split a dataset\n",
      "from tqdm import tqdm # progress bar\n",
      "from matplotlib import pyplot as plt # plotting\n",
      "from dtreeviz.trees import * #plotting decision trees\n",
      "from sklearn.metrics import roc_auc_score, matthews_corrcoef, cohen_kappa_score, plot_roc_curve, plot_confusion_matrix # model stats\n",
      "247/2: !pip install dtreeviz\n",
      "247/3:\n",
      "import pandas as pd #data table manipulation\n",
      "from rdkit import Chem # basic cheminformatics\n",
      "from rdkit.Chem.Descriptors import MolWt, MolLogP, NumAromaticRings, NumHDonors, NumHAcceptors\n",
      "import math #needed for log10\n",
      "import seaborn as sns #plotting\n",
      "from sklearn.tree import DecisionTreeClassifier, plot_tree # descision trees \n",
      "from sklearn.model_selection import train_test_split # split a dataset\n",
      "from tqdm import tqdm # progress bar\n",
      "from matplotlib import pyplot as plt # plotting\n",
      "from dtreeviz.trees import * #plotting decision trees\n",
      "from sklearn.metrics import roc_auc_score, matthews_corrcoef, cohen_kappa_score, plot_roc_curve, plot_confusion_matrix # model stats\n",
      "247/4: tqdm.pandas()\n",
      "247/5:\n",
      "def calc_descriptors(smi):\n",
      "    mol = Chem.MolFromSmiles(smi)\n",
      "    if mol:\n",
      "        mw, logp, num_arom_rings, hbd, hba = [x(mol) for x in [MolWt, MolLogP, NumAromaticRings, NumHDonors, NumHAcceptors]]\n",
      "        res = [mw, logp, num_arom_rings, hbd, hba]\n",
      "    else:\n",
      "        res = [None] * 5\n",
      "    return res\n",
      "247/6: df = pd.read_csv(\"delaney.csv\")\n",
      "247/7: df.columns\n",
      "247/8:\n",
      "df = pd.read_csv(\"delaney.csv\")\n",
      "df\n",
      "247/9: df.columns\n",
      "247/10:\n",
      "cols = list(df.columns)\n",
      "cols[1] = 'LogS'\n",
      "df.columns = cols\n",
      "247/11: df['IsSol'] = df.LogS > math.log10(200 * 1e-6)\n",
      "247/12: sns.displot(x='LogS',hue=\"IsSol\",data=df)\n",
      "247/13: df['desc'] = df.SMILES.progress_apply(calc_descriptors)\n",
      "247/14:\n",
      "desc_cols = ['MW','LogP','NumAromatic','HBD','HBA']\n",
      "df[desc_cols] = df.desc.to_list()\n",
      "247/15: df.drop(\"desc\",axis=1,inplace=True)\n",
      "247/16: df\n",
      "247/17:\n",
      "desc_cols = ['MW','LogP','NumAromatic','HBD','HBA']\n",
      "df[desc_cols] = df.desc.to_list()\n",
      "247/18:\n",
      "desc_cols = ['MW','LogP','NumAromatic','HBD','HBA']\n",
      "df[desc_cols] = df.desc.to_list()\n",
      "247/19: df\n",
      "247/20: train, test = train_test_split(df)\n",
      "247/21:\n",
      "train_X = train[desc_cols]\n",
      "train_y = train.IsSol\n",
      "test_X = test[desc_cols]\n",
      "test_y = test.IsSol\n",
      "247/22:\n",
      "cls = DecisionTreeClassifier(max_depth=2)\n",
      "cls.fit(train_X,train_y)\n",
      "247/23:\n",
      "pred = cls.predict(test_X)\n",
      "auc, mcc, kappa = roc_auc_score(test_y, pred),matthews_corrcoef(test_y,pred),cohen_kappa_score(test_y,pred)\n",
      "print(f\"ROC AUC = {auc:.2f}\")\n",
      "print(f\"Matthews Correlation Coefficient = {mcc:.2f}\")\n",
      "print(f\"Cohen\\'s Kappa = {kappa:.2f}\")\n",
      "247/24: plot_confusion_matrix(cls,test_X,test_y)\n",
      "247/25: plot_roc_curve(cls,test_X,test_y)\n",
      "247/26:\n",
      "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
      "plot_tree(cls,feature_names=desc_cols)\n",
      "247/27:\n",
      "viz = dtreeviz(cls, train_X, train_y, feature_names = desc_cols, \n",
      "               target_name = \"Solubility\",class_names=[\"False\",\"True\"],scale=2)\n",
      "viz\n",
      "247/28:\n",
      "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
      "plot_tree(cls,feature_names=desc_cols)\n",
      "247/29:\n",
      "import pandas as pd #data table manipulation\n",
      "from rdkit import Chem # basic cheminformatics\n",
      "from rdkit.Chem.Descriptors import MolWt, MolLogP, NumAromaticRings, NumHDonors, NumHAcceptors\n",
      "import math #needed for log10\n",
      "import seaborn as sns #plotting\n",
      "from sklearn.tree import DecisionTreeClassifier, plot_tree # descision trees \n",
      "from sklearn.model_selection import train_test_split # split a dataset\n",
      "from tqdm import tqdm # progress bar\n",
      "from matplotlib import pyplot as plt # plotting\n",
      "from dtreeviz.trees import * #plotting decision trees\n",
      "from sklearn.metrics import roc_auc_score, matthews_corrcoef, cohen_kappa_score, plot_roc_curve, plot_confusion_matrix # model stats\n",
      "247/30:\n",
      "viz = dtreeviz(cls, train_X, train_y, feature_names = desc_cols, \n",
      "               target_name = \"Solubility\",class_names=[\"False\",\"True\"],scale=2)\n",
      "viz\n",
      "253/1:\n",
      "from rdkit import Chem #RDKit Chemistry\n",
      "from rdkit.Chem.Draw import IPythonConsole #RDKit drawing\n",
      "from rdkit.Chem import Draw #RDKit drawing\n",
      "# A few settings to improve the quality of structures \n",
      "from rdkit.Chem import rdDepictor\n",
      "IPythonConsole.ipython_useSVG = True\n",
      "rdDepictor.SetPreferCoordGen(True)\n",
      "from rdkit.Chem import PandasTools #Add the ability to add a molecule to a dataframegrid\n",
      "import mols2grid #The mols2grid library provides a convenient way of displaying molecules in a grid\n",
      "253/2: mol = Chem.MolFromSmiles(\"c1cccc1\")\n",
      "253/3: mol = Chem.MolFromSmiles(\"c1ccc1\")\n",
      "253/4: mol\n",
      "273/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "273/2:\n",
      "path = r'smiles/'\n",
      "\n",
      "Fp13  = pd.read_csv(path + 'mordard_Kp.csv'               , header = 0)\n",
      "273/3: Fp13.shape\n",
      "273/4: Fp13\n",
      "273/5:\n",
      "df = pd.read_csv('df6_lipinski_Kpu_Kp_cells.csv')\n",
      "df\n",
      "273/6: Fp13\n",
      "273/7:\n",
      "df = pd.read_csv('df6_lipinski_Kpu_Kp_cells.csv')\n",
      "df\n",
      "273/8: name.shape\n",
      "273/9: name = df['compound']\n",
      "273/10: name.shape\n",
      "273/11:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df3 = df2.set_index('Name')\n",
      "df3\n",
      "273/12:\n",
      "LogKp = df3[['logkp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]\n",
      "LogKp\n",
      "273/13:\n",
      "Fp13 = Fp13.rename(columns={'name': 'Name'})\n",
      "Fp13\n",
      "273/14:\n",
      "\n",
      "Fp13.Name = name\n",
      "273/15:\n",
      "\n",
      "Fpp13 = Fp13.drop('Name', axis=1)\n",
      "Fpp13\n",
      "273/16: Fpp13 = Fpp13.fillna(0)\n",
      "273/17:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)\n",
      "    Fp_normalized\n",
      "    \n",
      "    return Fp_normalized\n",
      "273/18:\n",
      "\n",
      "Fp13_n = normalized (Fpp13)\n",
      "273/19: Fp13_n\n",
      "273/20: np.any(np.isnan(Fp13_n))\n",
      "273/21: Fp13_n[\"Name\"] = Fp13.Name\n",
      "273/22: Fp13_n\n",
      "273/23: Fp13_n = Fp13_n.set_index('Name')\n",
      "273/24: Fp13_n.head(3)\n",
      "273/25: raw13  = LogKp.merge(Fp13_n, on='Name', how='outer')\n",
      "273/26: raw13\n",
      "273/27: np.any(np.isnan(raw13))\n",
      "273/28:\n",
      "raw13 =raw13[raw13.logkp.notna()]\n",
      "raw13.shape\n",
      "273/29: print (len(raw13))\n",
      "273/30: print (len(raw13 .columns))\n",
      "273/31:\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['logkp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)\n",
      "label_to_model = compound_df.logkp.tolist()\n",
      "273/32: fingerprint_to_model.head(3)\n",
      "273/33:\n",
      "train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model2, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "273/34: indices = fingerprint_to_model.index\n",
      "273/35:\n",
      "train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "273/36:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "sel = sel.fit(train_x)\n",
      "mask = sel.get_support()\n",
      "273/37:\n",
      "fingerprint_to_model2 = sel.loc[:, mask]\n",
      "fingerprint_to_model2\n",
      "273/38:\n",
      "fingerprint_to_model2 = train_x.loc[:, mask]\n",
      "fingerprint_to_model2\n",
      "273/39:\n",
      "train_x_selected = train_x.loc[:, mask]\n",
      "train_x_selected\n",
      "273/40: train_x_selected.columns\n",
      "273/41: indices_train, indices_test\n",
      "273/42: indices_train, indices_test\n",
      "273/43:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
      "273/44: train_x_selected.shape\n",
      "273/45:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x_selected, train_y)\n",
      "y_pred = SVR.predict(train_x_selected)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "273/46:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x_selected, train_y)\n",
      "y_pred = SVR.predict(train_x_selected)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "\n",
      "test_x_selected = sel.transform(test_x)\n",
      "\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "273/47:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x_selected, train_y)\n",
      "y_pred = SVR.predict(train_x_selected)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "\n",
      "test_x_selected = sel.transform(test_x)\n",
      "\n",
      "y_ext = SVR.predict(test_x_selected)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "273/48:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x_selected, train_y)\n",
      "y_pred = SVR.predict(train_x_selected)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "\n",
      "test_x_selected = sel.transform(test_x)\n",
      "\n",
      "y_ext = SVR.predict(test_x_selected)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "Test_accuracy = accuracy_score(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f CV accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "print(f'Test accuracy is {Test_accuracy}')\n",
      "print('===================================')\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "273/49:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x_selected, train_y)\n",
      "y_pred = SVR.predict(train_x_selected)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "\n",
      "test_x_selected = sel.transform(test_x)\n",
      "\n",
      "y_ext = SVR.predict(test_x_selected)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f CV accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "273/50: test_x_selected = sel.transform(test_x)\n",
      "273/51:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='linear')\n",
      "scores=cross_val_score(SVR, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x_selected, train_y)\n",
      "y_pred = SVR.predict(train_x_selected)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "\n",
      "y_ext = SVR.predict(test_x_selected)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "273/52:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='linear')\n",
      "scores=cross_val_score(SVR, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x_selected, train_y)\n",
      "y_pred = SVR.predict(train_x_selected)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "\n",
      "y_ext = SVR.predict(test_x_selected)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "273/53:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestRegressor(n_estimators=200, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x_selected, train_y)\n",
      "y_pred = model_RF.predict(train_x_selected)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x_selected)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "273/54:\n",
      "#Train features\n",
      "from sklearn.inspection import permutation_importance\n",
      "result = permutation_importance(model_RF, train_x_selected, train_y, n_repeats=10, random_state=42, n_jobs=2)\n",
      "feature_names = fingerprint_to_model2.columns\n",
      "\n",
      "forest_importances = pd.Series(result.importances_mean, index=feature_names)\n",
      "fig, ax = plt.subplots()\n",
      "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
      "ax.set_title(\"Feature importances using permutation on full model\")\n",
      "ax.set_ylabel(\"Mean accuracy decrease\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "273/55:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestRegressor(n_estimators=200, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "model_RF_fit = model_RF.fit(train_x_selected, train_y)\n",
      "y_pred = model_RF.predict(train_x_selected)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x_selected)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "273/56: model_RF_fit\n",
      "273/57: model_RF\n",
      "273/58:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestRegressor(n_estimators=200, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "\n",
      "y_pred = model_RF.predict(train_x_selected)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x_selected)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "273/59:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestRegressor(n_estimators=200, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "model_RF_fit = model_RF.fit(train_x_selected, train_y)\n",
      "y_pred = model_RF.predict(train_x_selected)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x_selected)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "273/60:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "\n",
      "model_PLS = PLSRegression(n_components=10)\n",
      "scores=cross_val_score(model_PLS, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "#calculate R2\n",
      "model_fit = model_PLS.fit(train_x_selected, train_y)\n",
      "y_pred = model_PLS.predict(train_x_selected)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_PLS, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_PLS.predict(test_x_selected)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "273/61:\n",
      "from sklearn.linear_model import Ridge\n",
      "Ridge = Ridge(alpha=1.0)\n",
      "\n",
      "#calculate R2\n",
      "model_fit = Ridge.fit(train_x_selected, train_y)\n",
      "y_pred = Ridge.predict(train_x_selected)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(Ridge, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = Ridge.predict(test_x_selected)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "273/62:\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "MLP = MLPRegressor(activation='relu', random_state=42, max_iter=1000, solver='lbfgs')\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "model_fit = MLP.fit(train_x_selected, train_y)\n",
      "y_pred = MLP.predict(train_x_selected)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(MLP, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = MLP.predict(test_x_selected)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "273/63:\n",
      "from sklearn.linear_model import Ridge\n",
      "Ridge = Ridge(alpha=1.0)\n",
      "\n",
      "#calculate R2\n",
      "model_fit = Ridge.fit(train_x_selected, train_y)\n",
      "y_pred = Ridge.predict(train_x_selected)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(Ridge, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = Ridge.predict(test_x_selected)\n",
      "#Rsqure_ext\n",
      "r2_ext = score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "273/64:\n",
      "from sklearn.linear_model import Ridge\n",
      "Ridge = Ridge(alpha=1.0)\n",
      "\n",
      "#calculate R2\n",
      "model_fit = Ridge.fit(train_x_selected, train_y)\n",
      "y_pred = Ridge.predict(train_x_selected)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(Ridge, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = Ridge.predict(test_x_selected)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "273/65:\n",
      "pd.DataFrame(train_x, indices_train).to_csv(\"Result_RF/x_train_kp.csv\")\n",
      "pd.DataFrame(test_x, indices_test).to_csv(\"Result_RF/x_test_kp.csv\")\n",
      "pd.DataFrame(y_cv, indices_train).to_csv(\"Result_RF/y_cv_kp.csv\")\n",
      "pd.DataFrame(train_y, indices_train).to_csv(\"Result_RF/train_y_kp.csv\")\n",
      "pd.DataFrame(test_y, indices_test).to_csv(\"Result_RF/test_y_kp.csv\")\n",
      "pd.DataFrame(y_pred, indices_train).to_csv(\"Result_RF/y_pred_kp.csv\")\n",
      "pd.DataFrame(y_ext, indices_test).to_csv(\"Result_RF/y_SVR_ext_kp.csv\")\n",
      "273/66:\n",
      "FI_RF_train_mean = pd.DataFrame(forest_importances)\n",
      "FI_RF_train_mean.to_csv('Result_RF/forest_importances_train_RF_Kp_181.csv')\n",
      "FI_RF_train_mean\n",
      "273/67:\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['logkp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)\n",
      "label_to_model = compound_df.logkp.tolist()\n",
      "273/68: fingerprint_to_model.head(3)\n",
      "273/69: indices = fingerprint_to_model.index\n",
      "273/70:\n",
      "train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "273/71: indices_train, indices_test\n",
      "273/72: train_x.columns\n",
      "273/73:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
      "273/74: train_x.shape\n",
      "273/75:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f CV accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "273/76:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='linear')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "273/77:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestRegressor(n_estimators=200, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "model_RF_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "273/78: train_x.shape\n",
      "273/79:\n",
      "#Train features\n",
      "from sklearn.inspection import permutation_importance\n",
      "result = permutation_importance(model_RF, train_x, train_y, n_repeats=10, random_state=42, n_jobs=2)\n",
      "feature_names = fingerprint_to_model2.columns\n",
      "\n",
      "forest_importances = pd.Series(result.importances_mean, index=feature_names)\n",
      "fig, ax = plt.subplots()\n",
      "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
      "ax.set_title(\"Feature importances using permutation on full model\")\n",
      "ax.set_ylabel(\"Mean accuracy decrease\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "276/1:\n",
      "#Train features\n",
      "from sklearn.inspection import permutation_importance\n",
      "result = permutation_importance(model_RF, train_x_selected, train_y, n_repeats=10, random_state=42, n_jobs=2)\n",
      "feature_names = train_x_selected.columns\n",
      "\n",
      "forest_importances = pd.Series(result.importances_mean, index=feature_names)\n",
      "fig, ax = plt.subplots()\n",
      "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
      "ax.set_title(\"Feature importances using permutation on full model\")\n",
      "ax.set_ylabel(\"Mean accuracy decrease\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "276/2:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestRegressor(n_estimators=200, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "model_RF_fit = model_RF.fit(train_x_selected, train_y)\n",
      "y_pred = model_RF.predict(train_x_selected)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x_selected)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "276/3:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "276/4:\n",
      "path = r'smiles/'\n",
      "\n",
      "Fp13  = pd.read_csv(path + 'mordard_Kp.csv'               , header = 0)\n",
      "276/5: Fp13.shape\n",
      "276/6: Fp13\n",
      "276/7:\n",
      "df = pd.read_csv('df6_lipinski_Kpu_Kp_cells.csv')\n",
      "df\n",
      "276/8: name = df['compound']\n",
      "276/9: name.shape\n",
      "276/10:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df3 = df2.set_index('Name')\n",
      "df3\n",
      "276/11:\n",
      "LogKp = df3[['logkp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]\n",
      "LogKp\n",
      "276/12:\n",
      "Fp13 = Fp13.rename(columns={'name': 'Name'})\n",
      "Fp13\n",
      "276/13:\n",
      "\n",
      "Fp13.Name = name\n",
      "276/14:\n",
      "\n",
      "Fpp13 = Fp13.drop('Name', axis=1)\n",
      "Fpp13\n",
      "276/15: Fpp13 = Fpp13.fillna(0)\n",
      "276/16:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)\n",
      "    Fp_normalized\n",
      "    \n",
      "    return Fp_normalized\n",
      "276/17:\n",
      "\n",
      "Fp13_n = normalized (Fpp13)\n",
      "276/18: Fp13_n\n",
      "276/19: np.any(np.isnan(Fp13_n))\n",
      "276/20: Fp13_n[\"Name\"] = Fp13.Name\n",
      "276/21: Fp13_n\n",
      "276/22: Fp13_n = Fp13_n.set_index('Name')\n",
      "276/23: Fp13_n.head(3)\n",
      "276/24: raw13  = LogKp.merge(Fp13_n, on='Name', how='outer')\n",
      "276/25: raw13\n",
      "276/26: np.any(np.isnan(raw13))\n",
      "276/27:\n",
      "raw13 =raw13[raw13.logkp.notna()]\n",
      "raw13.shape\n",
      "276/28: print (len(raw13))\n",
      "276/29: print (len(raw13 .columns))\n",
      "276/30:\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['logkp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)\n",
      "label_to_model = compound_df.logkp.tolist()\n",
      "276/31: fingerprint_to_model.head(3)\n",
      "276/32: indices = fingerprint_to_model.index\n",
      "276/33:\n",
      "train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "276/34: indices_train, indices_test\n",
      "276/35:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "sel = sel.fit(train_x)\n",
      "mask = sel.get_support()\n",
      "276/36:\n",
      "train_x_selected = train_x.loc[:, mask]\n",
      "train_x_selected\n",
      "276/37: train_x_selected.columns\n",
      "276/38:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
      "276/39: train_x_selected.shape\n",
      "276/40: test_x_selected = sel.transform(test_x)\n",
      "276/41:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestRegressor(n_estimators=200, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "model_RF_fit = model_RF.fit(train_x_selected, train_y)\n",
      "y_pred = model_RF.predict(train_x_selected)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x_selected)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "276/42:\n",
      "#Train features\n",
      "from sklearn.inspection import permutation_importance\n",
      "result = permutation_importance(model_RF, train_x_selected, train_y, n_repeats=10, random_state=42, n_jobs=2)\n",
      "feature_names = train_x_selected.columns\n",
      "\n",
      "forest_importances = pd.Series(result.importances_mean, index=feature_names)\n",
      "fig, ax = plt.subplots()\n",
      "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
      "ax.set_title(\"Feature importances using permutation on full model\")\n",
      "ax.set_ylabel(\"Mean accuracy decrease\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "273/80:\n",
      "#Train features\n",
      "from sklearn.inspection import permutation_importance\n",
      "result = permutation_importance(model_RF, train_x, train_y, n_repeats=10, random_state=42, n_jobs=2)\n",
      "feature_names = train_x.columns\n",
      "\n",
      "forest_importances = pd.Series(result.importances_mean, index=feature_names)\n",
      "fig, ax = plt.subplots()\n",
      "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
      "ax.set_title(\"Feature importances using permutation on full model\")\n",
      "ax.set_ylabel(\"Mean accuracy decrease\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "276/43:\n",
      "FI_RF_train_mean = pd.DataFrame(forest_importances)\n",
      "FI_RF_train_mean.to_csv('Result_RF/forest_importances_train_RF_Kp_181.csv')\n",
      "FI_RF_train_mean\n",
      "273/81:\n",
      "FI_RF_train_mean = pd.DataFrame(forest_importances)\n",
      "FI_RF_train_mean.to_csv('Result_RF/forest_importances_train_RF_Kp_1493_full.csv')\n",
      "FI_RF_train_mean\n",
      "273/82:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "\n",
      "model_PLS = PLSRegression(n_components=10)\n",
      "scores=cross_val_score(model_PLS, train_x, train_y, cv=kf)\n",
      "\n",
      "#calculate R2\n",
      "model_fit = model_PLS.fit(train_x, train_y)\n",
      "y_pred = model_PLS.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_PLS, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_PLS.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "273/83:\n",
      "from sklearn.linear_model import Ridge\n",
      "Ridge = Ridge(alpha=1.0)\n",
      "\n",
      "#calculate R2\n",
      "model_fit = Ridge.fit(train_x, train_y)\n",
      "y_pred = Ridge.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(Ridge, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = Ridge.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "273/84:\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "MLP = MLPRegressor(activation='relu', random_state=42, max_iter=1000, solver='lbfgs')\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "model_fit = MLP.fit(train_x, train_y)\n",
      "y_pred = MLP.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(MLP, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = MLP.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "273/85:\n",
      "pd.DataFrame(train_x, indices_train).to_csv(\"Result_RF/RF_full_features/x_train_kp.csv\")\n",
      "pd.DataFrame(test_x, indices_test).to_csv(\"Result_RF/RF_full_features/x_test_kp.csv\")\n",
      "pd.DataFrame(y_cv, indices_train).to_csv(\"Result_RF/RF_full_features/y_cv_kp.csv\")\n",
      "pd.DataFrame(train_y, indices_train).to_csv(\"Result_RF/RF_full_features/train_y_kp.csv\")\n",
      "pd.DataFrame(test_y, indices_test).to_csv(\"Result_RF/RF_full_features/test_y_kp.csv\")\n",
      "pd.DataFrame(y_pred, indices_train).to_csv(\"Result_RF/RF_full_features/y_pred_kp.csv\")\n",
      "pd.DataFrame(y_ext, indices_test).to_csv(\"Result_RF/RF_full_features/y_SVR_ext_kp.csv\")\n",
      "273/86:\n",
      "from sklearn.decomposition import PCA\n",
      "pca = PCA(n_components=7)\n",
      "pca.fit(fingerprint_to_model)\n",
      "273/87:\n",
      "from sklearn.decomposition import PCA\n",
      "pca = PCA(n_components=7)\n",
      "pca.fit(fingerprint_to_model)\n",
      "print(pca.explained_variance_ratio_)\n",
      "273/88:\n",
      "from sklearn.decomposition import PCA\n",
      "pca = PCA(n_components=7)\n",
      "pca.fit(train_x)\n",
      "print(pca.explained_variance_ratio_)\n",
      "276/44:\n",
      "from sklearn.decomposition import PCA\n",
      "pca = PCA(n_components=7)\n",
      "pca.fit(train_x_selected)\n",
      "print(pca.explained_variance_ratio_)\n",
      "285/1: from scipy import stats\n",
      "285/2:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from scipy import stats\n",
      "285/3: pwd\n",
      "285/4: df = pd.read_csv('LogCMratio.csv')\n",
      "285/5: df = pd.read_csv('LogCMratio.csv')\n",
      "285/6: df\n",
      "285/7: stats.pearsonr(df[Log_CM_SK_37], df[Log_CM_SK_4])\n",
      "285/8: df\n",
      "285/9: stats.pearsonr(df['Log_CM_SK_37'], df['Log_CM_SK_4'])\n",
      "285/10: stats.pearsonr(df['Log_CM_Ve_37'], df['Log_CM_Ve_4'])\n",
      "287/1:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "287/2: df = pd.read_csv('rawcorrelation.csv')\n",
      "287/3: df\n",
      "287/4: df2 = df.drop(columns=['Unnamed: 0'])\n",
      "287/5: df2\n",
      "287/6:\n",
      "from scipy.stats import pearsonr\n",
      "rho = df2.corr()\n",
      "pval = df2.corr(method=lambda x, y: pearsonr(x, y)[1]) - np.eye(*rho.shape)\n",
      "p = pval.applymap(lambda x: ''.join(['*' for t in [0.001,0.01,0.05] if x<=t]))\n",
      "Res_pearson_sig = rho.round(3).astype(str) + p\n",
      "Res_pearson_sig.to_csv('Res_pearson_sig.csv')\n",
      "Res_pearson_sig\n",
      "287/7:\n",
      "Res_pearson = df2.corr(method='pearson').round(3)\n",
      "Res_pearson.to_csv('Res_pearson.csv')\n",
      "Res_pearson\n",
      "287/8: pval\n",
      "287/9:\n",
      "from scipy.stats import pearsonr\n",
      "rho = df2.corr()\n",
      "pval = df2.corr(method=lambda x, y: pearsonr(x, y)[1]) - np.eye(*rho.shape)\n",
      "p = pval.applymap(lambda x: ''.join(['*' for t in [0.01,0.05,0.1] if x<=t]))\n",
      "Res_pearson_sig = rho.round(3).astype(str) + p\n",
      "Res_pearson_sig.to_csv('Res_pearson_sig.csv')\n",
      "Res_pearson_sig\n",
      "287/10: pval\n",
      "287/11:\n",
      "from scipy.stats import pearsonr\n",
      "rho = df2.corr()\n",
      "pval = df2.corr(method=lambda x, y: pearsonr(x, y)[1]) - np.eye(*rho.shape)\n",
      "p = pval.applymap(lambda x: ''.join(['*' for t in [0.001,0.01,0.05] if x<=t]))\n",
      "Res_pearson_sig = rho.round(3).astype(str) + p\n",
      "Res_pearson_sig.to_csv('Res_pearson_sig.csv')\n",
      "Res_pearson_sig\n",
      "287/12: pval\n",
      "290/1: df1 = pd.read_csv('spheroid.csv')\n",
      "290/2:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from scipy import stats\n",
      "290/3: pwd\n",
      "290/4: df = pd.read_csv('LogCMratio.csv')\n",
      "290/5: df\n",
      "290/6: stats.pearsonr(df['Log_CM_SK_37'], df['Log_CM_SK_4'])\n",
      "290/7: stats.pearsonr(df['Log_CM_Ve_37'], df['Log_CM_Ve_4'])\n",
      "290/8: df1 = pd.read_csv('spheroid.csv')\n",
      "290/9: df1\n",
      "290/10: shapiro_test = stats.shapiro(df1[volume])\n",
      "290/11: shapiro_test = stats.shapiro(df1[Volume])\n",
      "290/12: shapiro_test = stats.shapiro(df1['Volume'])\n",
      "290/13: shapiro_test\n",
      "290/14: shapiro_test = stats.shapiro(df1['CTCF_Green'])\n",
      "290/15: shapiro_test\n",
      "290/16: shapiro_test = stats.shapiro(df1['CTCF_Red'])\n",
      "290/17: shapiro_test\n",
      "290/18: df1['Volume']\n",
      "290/19: df2 = df1.loc[df1['Samples'] == 'Control']\n",
      "290/20: shapiro_test = stats.shapiro(df2['Volume'])\n",
      "290/21: shapiro_test\n",
      "290/22:\n",
      "df2 = df1.loc[df1['Samples'] == 'Control']\n",
      "df2\n",
      "290/23: shapiro_test = stats.shapiro(df2['CTCF_Green'])\n",
      "290/24: shapiro_test\n",
      "290/25: shapiro_test = stats.shapiro(df2['CTCF_Red'])\n",
      "290/26: shapiro_test\n",
      "290/27:\n",
      "df3 = df1.loc[df1['Samples'] == '3 mM']\n",
      "df3\n",
      "290/28: shapiro_test = stats.shapiro(df3['Volume'])\n",
      "290/29:\n",
      "shapiro_test = stats.shapiro(df3['Volume'])\n",
      "shapiro_test\n",
      "290/30:\n",
      "shapiro_test = stats.shapiro(df3['CTCF_Green'])\n",
      "shapiro_test\n",
      "290/31:\n",
      "shapiro_test = stats.shapiro(df3['CTCF_Red'])\n",
      "shapiro_test\n",
      "290/32:\n",
      "df4 = df1.loc[df1['Samples'] == '6 mM']\n",
      "df4\n",
      "290/33:\n",
      "shapiro_test = stats.shapiro(df4['Volume'])\n",
      "shapiro_test\n",
      "290/34:\n",
      "shapiro_test = stats.shapiro(df4['CTCF_Green'])\n",
      "shapiro_test\n",
      "290/35:\n",
      "shapiro_test = stats.shapiro(df4['CTCF_Red'])\n",
      "shapiro_test\n",
      "290/36: stats.kruskal(df1['Samples'], df1['Volume'])\n",
      "290/37: stats.kruskal(df1['Volume'])\n",
      "290/38: stats.kruskal(df2['Volume'], df3['Volume'], df4['Volume'])\n",
      "290/39: stats.kruskal(df2['Volume'], df3['Volume'])\n",
      "290/40: stats.kruskal(df2['Volume'], df3['Volume'])\n",
      "290/41: stats.kruskal(df2['Volume'], df4['Volume'])\n",
      "290/42: stats.kruskal(df2['Volume'], df4['Volume'])\n",
      "290/43: stats.kruskal(df3['Volume'], df4['Volume'])\n",
      "307/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "307/2:\n",
      "path = r'smiles/'\n",
      "\n",
      "Fp13  = pd.read_csv(path + 'mordard_Kp.csv'               , header = 0)\n",
      "307/3: Fp13.shape\n",
      "307/4: Fp13\n",
      "307/5:\n",
      "df = pd.read_csv('df6_lipinski_Kpu_Kp_cells.csv')\n",
      "df\n",
      "307/6: name = df['compound']\n",
      "307/7: name.shape\n",
      "307/8:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df3 = df2.set_index('Name')\n",
      "df3\n",
      "307/9:\n",
      "LogKp = df3[['logkp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]\n",
      "LogKp\n",
      "307/10:\n",
      "Fp13 = Fp13.rename(columns={'name': 'Name'})\n",
      "Fp13\n",
      "306/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from scipy import stats\n",
      "306/2: pwd\n",
      "306/3: df = pd.read_csv('/Result_RF/forest_importances_train_RF_Kp_181.csv')\n",
      "306/4: pwd\n",
      "306/5: df = pd.read_csv('OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Result_RF/forest_importances_train_RF_Kp_181.csv')\n",
      "306/6: df = pd.read_csv('r/Result_RF/forest_importances_train_RF_Kp_181.csv')\n",
      "306/7: df = pd.read_csv('rResult_RF/forest_importances_train_RF_Kp_181.csv')\n",
      "306/8: df = pd.read_csv(r'Result_RF/forest_importances_train_RF_Kp_181.csv')\n",
      "306/9: df\n",
      "307/11:\n",
      "Fp13 = Fp13.rename(columns={'name': 'Name'})\n",
      "Fp13\n",
      "307/12: name = df['compound']\n",
      "307/13: name.shape\n",
      "307/14:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df3 = df2.set_index('Name')\n",
      "df3\n",
      "307/15:\n",
      "LogKp = df3[['logkp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]\n",
      "LogKp\n",
      "307/16:\n",
      "Fp13 = Fp13.rename(columns={'name': 'Name'})\n",
      "Fp13\n",
      "307/17:\n",
      "\n",
      "Fp13.Name = name\n",
      "307/18:\n",
      "\n",
      "Fpp13 = Fp13.drop('Name', axis=1)\n",
      "Fpp13\n",
      "307/19: Fpp13 = Fpp13.fillna(0)\n",
      "307/20:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)\n",
      "    Fp_normalized\n",
      "    \n",
      "    return Fp_normalized\n",
      "307/21:\n",
      "\n",
      "Fp13_n = normalized (Fpp13)\n",
      "307/22: Fp13_n\n",
      "307/23: np.any(np.isnan(Fp13_n))\n",
      "307/24: Fp13_n[\"Name\"] = Fp13.Name\n",
      "307/25: Fp13_n\n",
      "307/26: Fp13_n = Fp13_n.set_index('Name')\n",
      "307/27: Fp13_n.head(3)\n",
      "307/28: raw13  = LogKp.merge(Fp13_n, on='Name', how='outer')\n",
      "307/29: raw13\n",
      "307/30: np.any(np.isnan(raw13))\n",
      "307/31:\n",
      "raw13 =raw13[raw13.logkp.notna()]\n",
      "raw13.shape\n",
      "307/32: raw13.to_csv('QSAR_Kp/Mordard_Kp_144.csv' , sep=',' ,index=True)\n",
      "307/33: print (len(raw13))\n",
      "307/34: print (len(raw13 .columns))\n",
      "307/35:\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['logkp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)\n",
      "label_to_model = compound_df.logkp.tolist()\n",
      "307/36: fingerprint_to_model.head(3)\n",
      "307/37: features_20 = fingerprint_to_model[['nFaRing', 'n9FaHRing', \"n9FaRing\", 'VSA_EState3', 'MAXaaaC', 'AXp-4dv', 'ATSC1m', 'SRW05', 'ATSC1Z', 'SlogP_VSA3', 'GGI5', 'PEOE_VSA6', 'C3SP2', 'MDEN-13', 'MINsCH3', 'EState_VSA9', 'GGI6', 'SaasN', 'VSA_EState8', 'EState_VSA4]]\n",
      "307/38:\n",
      "features_20 = fingerprint_to_model[['nFaRing', 'n9FaHRing', \"n9FaRing\", 'VSA_EState3', 'MAXaaaC', 'AXp-4dv', \n",
      "                                    'ATSC1m', 'SRW05', 'ATSC1Z', 'SlogP_VSA3', 'GGI5', 'PEOE_VSA6', 'C3SP2', \n",
      "                                    'MDEN-13', 'MINsCH3', 'EState_VSA9', 'GGI6', 'SaasN', 'VSA_EState8', 'EState_VSA4']]\n",
      "307/39: features_20\n",
      "307/40: indices = fingerprint_to_model.index\n",
      "307/41:\n",
      "train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(fingerprints_20, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "307/42:\n",
      "train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(fingerprint_20, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "307/43:\n",
      "train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(features_20, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "307/44:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "308/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "308/2:\n",
      "path = r'smiles/'\n",
      "\n",
      "Fp13  = pd.read_csv(path + 'mordard_Kp.csv'               , header = 0)\n",
      "308/3: Fp13.shape\n",
      "308/4: Fp13\n",
      "308/5:\n",
      "df = pd.read_csv('df6_lipinski_Kpu_Kp_cells.csv')\n",
      "df\n",
      "308/6: name = df['compound']\n",
      "308/7: name.shape\n",
      "308/8:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df3 = df2.set_index('Name')\n",
      "df3\n",
      "308/9:\n",
      "LogKp = df3[['logkp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]\n",
      "LogKp\n",
      "308/10:\n",
      "Fp13 = Fp13.rename(columns={'name': 'Name'})\n",
      "Fp13\n",
      "308/11: Fp13.Name = name\n",
      "308/12:\n",
      "\n",
      "Fpp13 = Fp13.drop('Name', axis=1)\n",
      "Fpp13\n",
      "308/13: Fpp13 = Fpp13.fillna(0)\n",
      "308/14:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)\n",
      "    Fp_normalized\n",
      "    \n",
      "    return Fp_normalized\n",
      "308/15: Fp13_n = normalized (Fpp13)\n",
      "308/16: Fp13_n\n",
      "308/17: np.any(np.isnan(Fp13_n))\n",
      "308/18: Fp13_n[\"Name\"] = Fp13.Name\n",
      "308/19: Fp13_n\n",
      "308/20: Fp13_n = Fp13_n.set_index('Name')\n",
      "308/21: Fp13_n.head(3)\n",
      "308/22: Fp13_n.to_csv('Fp_normalized/Morderd_LogKp.csv' , sep=',' ,index=True)\n",
      "308/23: raw13  = LogKp.merge(Fp13_n, on='Name', how='outer')\n",
      "308/24: raw13\n",
      "308/25: np.any(np.isnan(raw13))\n",
      "308/26: raw13\n",
      "308/27: np.any(np.isnan(raw13))\n",
      "308/28:\n",
      "raw13 =raw13[raw13.logkp.notna()]\n",
      "raw13.shape\n",
      "308/29: print (len(raw13))\n",
      "308/30: print (len(raw13 .columns))\n",
      "308/31:\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['logkp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)\n",
      "label_to_model = compound_df.logkp.tolist()\n",
      "308/32: fingerprint_to_model.head(3)\n",
      "308/33: indices = fingerprint_to_model.index\n",
      "308/34: indices = fingerprint_to_model.index\n",
      "308/35:\n",
      "features_20 = fingerprint_to_model[['nFaRing', 'n9FaHRing', \"n9FaRing\", 'VSA_EState3', 'MAXaaaC', 'AXp-4dv', \n",
      "                                    'ATSC1m', 'SRW05', 'ATSC1Z', 'SlogP_VSA3', 'GGI5', 'PEOE_VSA6', 'C3SP2', \n",
      "                                    'MDEN-13', 'MINsCH3', 'EState_VSA9', 'GGI6', 'SaasN', 'VSA_EState8', 'EState_VSA4']]\n",
      "308/36: features_20\n",
      "308/37:\n",
      "train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(features_20, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "308/38: ##RF\n",
      "308/39:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestRegressor(n_estimators=200, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "model_RF_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "308/40:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
      "308/41:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestRegressor(n_estimators=200, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "model_RF_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "308/42:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "\n",
      "test_x_selected = sel.transform(test_x)\n",
      "\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f CV accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "308/43:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "\n",
      "test_x = sel.transform(test_x)\n",
      "\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f CV accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "308/44:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f CV accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "308/45: features_20.tail(3)\n",
      "308/46:\n",
      "df = features_20.tail(3)\n",
      "df\n",
      "308/47: df.to_csv('Result_RF/Features_20_SM.csv')\n",
      "308/48: indices = fingerprint_to_model.index\n",
      "308/49:\n",
      "train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "308/50: indices_train, indices_test\n",
      "308/51:\n",
      "train_x_selected = train_x.loc[:, mask]\n",
      "train_x_selected\n",
      "308/52:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "sel = sel.fit(train_x)\n",
      "mask = sel.get_support()\n",
      "308/53:\n",
      "train_x_selected = train_x.loc[:, mask]\n",
      "train_x_selected\n",
      "308/54: train_x_selected.columns\n",
      "308/55:\n",
      "import random\n",
      "SEED = random.seed(10)\n",
      "SEED\n",
      "308/56:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
      "308/57: train_x_selected.shape\n",
      "308/58: test_x_selected = sel.transform(test_x)\n",
      "308/59:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x_selected, train_y)\n",
      "y_pred = SVR.predict(train_x_selected)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "\n",
      "test_x_selected = sel.transform(test_x)\n",
      "\n",
      "y_ext = SVR.predict(test_x_selected)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f CV accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "308/60:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "\n",
      "model_PLS = PLSRegression(n_components=10)\n",
      "scores=cross_val_score(model_PLS, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "#calculate R2\n",
      "model_fit = model_PLS.fit(train_x_selected, train_y)\n",
      "y_pred = model_PLS.predict(train_x_selected)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_PLS, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_PLS.predict(test_x_selected)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "308/61: print(f\"PLS r-squared {pls.score(test_x, test_y):.3f}\")\n",
      "308/62: print(f\"PLS r-squared {model_fit.score(test_x, test_y):.3f}\")\n",
      "308/63: print(f\"PLS r-squared {model_fit.score(test_y, y_exxt):.3f}\")\n",
      "308/64: print(f\"PLS r-squared {model_fit.score(test_y, y_ext):.3f}\")\n",
      "308/65: model_fit.score(test_y, y_ext)\n",
      "308/66:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "\n",
      "model_PLS = PLSRegression(n_components=10)\n",
      "scores=cross_val_score(model_PLS, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "#calculate R2\n",
      "model_fit = model_PLS.fit(train_x_selected, train_y)\n",
      "y_pred = model_PLS.predict(train_x_selected)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_PLS, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_PLS.predict(test_x_selected)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "308/67: y_exxt\n",
      "308/68: y_ext\n",
      "308/69: test_y\n",
      "308/70: y_ext\n",
      "308/71: model_PLS.score\n",
      "308/72: model_PLS.score(train_x, train_y)\n",
      "308/73: model_PLS.score(train_x_selected, train_y)\n",
      "308/74: model_PLS.score(test_x_selected, y_ext)\n",
      "308/75: model_PLS.score(test_x_selected, y_test)\n",
      "308/76: model_PLS.score(test_x_selected, test_y)\n",
      "309/1:\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split\n",
      "309/2: pwd\n",
      "309/3:\n",
      "list_Fp = ['Descriptor.xml', 'Fingerprinter.xml', 'ExtendedFingerprinter.xml', 'EStateFingerprinter.xml',\n",
      "           'GraphOnlyFingerprinter.xml', 'MACCSFingerprinter.xml', 'PubchemFingerprinter.xml',\n",
      "           'SubstructureFingerprinter.xml', 'KlekotaRothFingerprinter.xml', 'AtomPairs2DFingerprinter.xml', 'AtomPairs2DFingerprintCount.xml', \n",
      "           'KlekotaRothFingerprintCount.xml',\n",
      "           'SubstructureFingerprintCount.xml' ]\n",
      "309/4:\n",
      "def Fingerprint(name , Fp):\n",
      "    \n",
      "    group = name\n",
      "    group = [name.replace(\".smi\", \".csv\") for name in group]\n",
      "    result = ', '.join(group)\n",
      "    \n",
      "    Fp_name = Fp\n",
      "    Fp_name = Fp.replace(\".xml\", \"_\")\n",
      "    \n",
      "    print (name)\n",
      "    print (result)\n",
      "    print (Fp)\n",
      "    print (Fp_name)\n",
      "\n",
      "    ! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/Fingerprinter.xml' \\\n",
      "    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/$name' \\\n",
      "    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/$Fp_name$result'\n",
      "309/5:\n",
      "! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \\\n",
      "    -removesalt -standardizenitro  \\\n",
      "    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/SubstructureFingerprintCount.xml' \\\n",
      "    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles_Kp.smi' \\\n",
      "    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/SubstructureFingerprintCount_Kp.csv'\n",
      "318/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "318/2:\n",
      "path = r'smiles/'\n",
      "\n",
      "Fp13  = pd.read_csv(path + 'SubstructureFingerprintCount_Kp.csv'               , header = 0)\n",
      "318/3:\n",
      "path = r'Fingerprint/'\n",
      "\n",
      "Fp13  = pd.read_csv(path + 'SubstructureFingerprintCount_Kp.csv'               , header = 0)\n",
      "318/4: Fp13.shape\n",
      "318/5: Fp13\n",
      "318/6:\n",
      "df = pd.read_csv('df6_lipinski_Kpu_Kp_cells.csv')\n",
      "df\n",
      "318/7: name = df['compound']\n",
      "318/8: name.shape\n",
      "318/9:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df3 = df2.set_index('Name')\n",
      "df3\n",
      "318/10:\n",
      "LogKp = df3[['logkp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]\n",
      "LogKp\n",
      "318/11:\n",
      "Fp13 = Fp13.rename(columns={'name': 'Name'})\n",
      "Fp13\n",
      "318/12: Fp13.Name = name\n",
      "318/13:\n",
      "Fpp13 = Fp13.drop('Name', axis=1)\n",
      "Fpp13\n",
      "318/14: Fpp13 = Fpp13.fillna(0)\n",
      "318/15: Fpp13 = Fpp13.fillna(0)\n",
      "318/16:\n",
      "#dont need to normalization becuase it only one class of finggerprint\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)\n",
      "    Fp_normalized\n",
      "    \n",
      "    return Fp_normalized\n",
      "318/17: #Fp13_n = normalized (Fpp13)\n",
      "318/18: #Fp13_n\n",
      "318/19: np.any(np.isnan(Fp13))\n",
      "318/20: np.any(np.isnan(Fpp13))\n",
      "318/21:\n",
      "#Fp13_n = normalized (Fpp13)\n",
      "Fp13_n = Fpp13\n",
      "318/22: Fp13_n\n",
      "318/23: np.any(np.isnan(Fp13_n))\n",
      "318/24: Fp13_n[\"Name\"] = Fp13.Name\n",
      "318/25: Fp13_n\n",
      "318/26: Fp13_n = Fp13_n.set_index('Name')\n",
      "318/27: Fp13_n.head(3)\n",
      "318/28: raw13  = LogKp.merge(Fp13_n, on='Name', how='outer')\n",
      "318/29: raw13\n",
      "318/30: Fp13_n.shape\n",
      "318/31: np.any(np.isnan(raw13))\n",
      "318/32: raw13.shape\n",
      "318/33: Fp13_n.shape\n",
      "318/34: print (len(raw13))\n",
      "318/35: print (len(raw13 .columns))\n",
      "318/36:\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['logkp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)\n",
      "label_to_model = compound_df.logkp.tolist()\n",
      "318/37: fingerprint_to_model.head(3)\n",
      "318/38:\n",
      "LogKp = df3[['logkp']]\n",
      "LogKp\n",
      "318/39:\n",
      "Fp13 = Fp13.rename(columns={'name': 'Name'})\n",
      "Fp13\n",
      "318/40: Fp13.Name = name\n",
      "318/41:\n",
      "Fpp13 = Fp13.drop('Name', axis=1)\n",
      "Fpp13\n",
      "318/42: Fpp13 = Fpp13.fillna(0)\n",
      "318/43:\n",
      "#dont need to normalization becuase it only one class of finggerprint\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)\n",
      "    Fp_normalized\n",
      "    \n",
      "    return Fp_normalized\n",
      "318/44:\n",
      "#Fp13_n = normalized (Fpp13)\n",
      "Fp13_n = Fpp13\n",
      "318/45: Fp13_n\n",
      "318/46: np.any(np.isnan(Fp13_n))\n",
      "318/47: Fp13_n[\"Name\"] = Fp13.Name\n",
      "318/48: Fp13_n\n",
      "318/49: Fp13_n = Fp13_n.set_index('Name')\n",
      "318/50: Fp13_n.head(3)\n",
      "318/51: Fp13_n.shape\n",
      "318/52: raw13  = LogKp.merge(Fp13_n, on='Name', how='outer')\n",
      "318/53: raw13\n",
      "318/54: np.any(np.isnan(raw13))\n",
      "318/55: raw13.shape\n",
      "318/56: raw13.to_csv('QSAR_Kp/Substructure_Kp_144.csv' , sep=',' ,index=True)\n",
      "318/57: print (len(raw13))\n",
      "318/58: print (len(raw13 .columns))\n",
      "318/59:\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['logkp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)\n",
      "label_to_model = compound_df.logkp.tolist()\n",
      "318/60:\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['logkp'], axis=1)\n",
      "label_to_model = compound_df.logkp.tolist()\n",
      "318/61:\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['logkp'], axis=1)\n",
      "label_to_model = compound_df.logkp.tolist()\n",
      "318/62: fingerprint_to_model.head(3)\n",
      "318/63: indices = fingerprint_to_model.index\n",
      "318/64:\n",
      "train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "318/65: indices_train, indices_test\n",
      "318/66:\n",
      "import random\n",
      "SEED = random.seed(10)\n",
      "SEED\n",
      "318/67:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
      "318/68:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "\n",
      "test_x_selected = sel.transform(test_x)\n",
      "\n",
      "y_ext = SVR.predict(test_x_selected)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f CV accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "318/69:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "\n",
      "test_x = sel.transform(test_x)\n",
      "\n",
      "y_ext = SVR.predict(test_x_selected)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f CV accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "318/70:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "\n",
      "\n",
      "y_ext = SVR.predict(test_x_selected)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f CV accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "318/71:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "\n",
      "\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f CV accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "318/72:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestRegressor(n_estimators=200, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "model_RF_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "318/73:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f CV accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "318/74:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "sel = VarianceThreshold(threshold=(0.05))\n",
      "sel = sel.fit(train_x)\n",
      "mask = sel.get_support()\n",
      "318/75:\n",
      "train_x_selected = train_x.loc[:, mask]\n",
      "train_x_selected\n",
      "318/76: train_x_selected.columns\n",
      "318/77:\n",
      "import random\n",
      "SEED = random.seed(10)\n",
      "SEED\n",
      "318/78:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
      "318/79: train_x_selected.shape\n",
      "318/80: test_x_selected = sel.transform(test_x)\n",
      "318/81:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x_selected, train_y)\n",
      "y_pred = SVR.predict(train_x_selected)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "\n",
      "test_x_selected = sel.transform(test_x)\n",
      "\n",
      "y_ext = SVR.predict(test_x_selected)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f CV accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "318/82:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='linear')\n",
      "scores=cross_val_score(SVR, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x_selected, train_y)\n",
      "y_pred = SVR.predict(train_x_selected)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "\n",
      "y_ext = SVR.predict(test_x_selected)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "318/83:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestRegressor(n_estimators=200, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "model_RF_fit = model_RF.fit(train_x_selected, train_y)\n",
      "y_pred = model_RF.predict(train_x_selected)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x_selected)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "318/84:\n",
      "#Train features\n",
      "from sklearn.inspection import permutation_importance\n",
      "result = permutation_importance(model_RF, train_x_selected, train_y, n_repeats=10, random_state=42, n_jobs=2)\n",
      "feature_names = train_x_selected.columns\n",
      "\n",
      "forest_importances = pd.Series(result.importances_mean, index=feature_names)\n",
      "fig, ax = plt.subplots()\n",
      "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
      "ax.set_title(\"Feature importances using permutation on training model\")\n",
      "ax.set_ylabel(\"Mean accuracy decrease\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "318/85:\n",
      "FI_RF_train_mean = pd.DataFrame(forest_importances)\n",
      "FI_RF_train_mean.to_csv('Result_RF/forest_importances_train_RF_Kp_Substructure44.csv')\n",
      "FI_RF_train_mean\n",
      "318/86:\n",
      "from sklearn.decomposition import PCA\n",
      "pca = PCA(n_components=7)\n",
      "pca.fit(train_x_selected)\n",
      "print(pca.explained_variance_ratio_)\n",
      "318/87:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "\n",
      "model_PLS = PLSRegression(n_components=10)\n",
      "scores=cross_val_score(model_PLS, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "#calculate R2\n",
      "model_fit = model_PLS.fit(train_x_selected, train_y)\n",
      "y_pred = model_PLS.predict(train_x_selected)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_PLS, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_PLS.predict(test_x_selected)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "318/88: model_PLS.score(test_x_selected, test_y)\n",
      "318/89:\n",
      "model_PLS.score(test_x_selected, test_y)\n",
      "pls = PLSRegression(n_components=7)\n",
      "pls.fit(train_x_selected)\n",
      "print(pca.explained_variance_ratio_)\n",
      "318/90:\n",
      "model_PLS.score(test_x_selected, test_y)\n",
      "pls = PLSRegression(n_components=7)\n",
      "pls.fit(train_x_selected, train_y)\n",
      "print(pca.explained_variance_ratio_)\n",
      "318/91:\n",
      "model_PLS.score(test_x_selected, test_y)\n",
      "pls = PLSRegression(n_components=7)\n",
      "print(pls.fit(train_x_selected, train_y))\n",
      "print(pls.explained_variance_ratio_)\n",
      "318/92:\n",
      "model_PLS.score(test_x_selected, test_y)\n",
      "pls = PLSRegression(n_components=7)\n",
      "print(pls.fit(train_x_selected, train_y))\n",
      "print(pls.explained_variance)\n",
      "318/93:\n",
      "model_PLS.score(test_x_selected, test_y)\n",
      "pls = PLSRegression(n_components=7)\n",
      "print(pls.fit(train_x_selected, train_y))\n",
      "318/94: model_PLS.score(test_x_selected, test_y)\n",
      "318/95: model_PLS.score(train_x_selected, train_y)\n",
      "318/96:\n",
      "from sklearn.linear_model import Ridge\n",
      "Ridge = Ridge(alpha=1.0)\n",
      "\n",
      "#calculate R2\n",
      "model_fit = Ridge.fit(train_x_selected, train_y)\n",
      "y_pred = Ridge.predict(train_x_selected)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(Ridge, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = Ridge.predict(test_x_selected)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "318/97:\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "MLP = MLPRegressor(activation='relu', random_state=42, max_iter=1000, solver='lbfgs')\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "model_fit = MLP.fit(train_x_selected, train_y)\n",
      "y_pred = MLP.predict(train_x_selected)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(MLP, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = MLP.predict(test_x_selected)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "308/77:\n",
      "features_20 = fingerprint_to_model[['nFaRing', 'n9FaHRing', \"n9FaRing\", 'VSA_EState3', 'MAXaaaC', 'AXp-4dv', \n",
      "                                    'ATSC1m', 'SRW05', 'ATSC1Z', 'SlogP_VSA3', 'GGI5', 'PEOE_VSA6', 'C3SP2', \n",
      "                                    'MDEN-13', 'MINsCH3', 'EState_VSA9', 'GGI6', 'SaasN', 'VSA_EState8', 'EState_VSA4']]\n",
      "308/78: features_20\n",
      "308/79:\n",
      "df = features_20.tail(3)\n",
      "df\n",
      "308/80:\n",
      "train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(features_20, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "308/81:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestRegressor(n_estimators=200, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "model_RF_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "308/82:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f CV accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "308/83:\n",
      "features_10 = fingerprint_to_model[['nFaRing', 'n9FaHRing', \"n9FaRing\", 'VSA_EState3', 'MAXaaaC', 'AXp-4dv', \n",
      "                                    'ATSC1m', 'SRW05', 'ATSC1Z', 'SlogP_VSA3']]\n",
      "308/84:\n",
      "df = features_10.tail(3)\n",
      "df\n",
      "308/85:\n",
      "train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(features_20, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "308/86:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestRegressor(n_estimators=200, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "model_RF_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "308/87: train_x.shape\n",
      "308/88:\n",
      "train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(features_10, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "308/89: train_x.shape\n",
      "308/90:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestRegressor(n_estimators=200, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "model_RF_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "308/91:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "\n",
      "model_PLS = PLSRegression(n_components=10)\n",
      "scores=cross_val_score(model_PLS, train_x, train_y, cv=kf)\n",
      "\n",
      "#calculate R2\n",
      "model_PLS.fit(train_x_selected, train_y)\n",
      "y_pred = model_PLS.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_PLS, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_PLS.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "308/92:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "\n",
      "model_PLS = PLSRegression(n_components=10)\n",
      "scores=cross_val_score(model_PLS, train_x, train_y, cv=kf)\n",
      "\n",
      "#calculate R2\n",
      "model_PLS.fit(train_x, train_y)\n",
      "y_pred = model_PLS.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_PLS, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_PLS.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "319/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "319/2:\n",
      "path = r'smiles/'\n",
      "\n",
      "Fp13  = pd.read_csv(path + 'mordard_Kp.csv'               , header = 0)\n",
      "319/3: Fp13.shape\n",
      "319/4: Fp13\n",
      "319/5:\n",
      "df = pd.read_csv('df6_lipinski_Kpu_Kp_cells.csv')\n",
      "df\n",
      "319/6: name = df['compound']\n",
      "319/7: name.shape\n",
      "319/8:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df3 = df2.set_index('Name')\n",
      "df3\n",
      "319/9:\n",
      "LogKp = df3[['logkp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]\n",
      "LogKp\n",
      "319/10:\n",
      "Fp13 = Fp13.rename(columns={'name': 'Name'})\n",
      "Fp13\n",
      "319/11:\n",
      "\n",
      "Fp13.Name = name\n",
      "319/12:\n",
      "\n",
      "Fpp13 = Fp13.drop('Name', axis=1)\n",
      "Fpp13\n",
      "319/13: Fpp13 = Fpp13.fillna(0)\n",
      "319/14:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)\n",
      "    Fp_normalized\n",
      "    \n",
      "    return Fp_normalized\n",
      "319/15:\n",
      "\n",
      "Fp13_n = normalized (Fpp13)\n",
      "319/16: Fp13_n\n",
      "319/17: np.any(np.isnan(Fp13_n))\n",
      "319/18: Fp13_n[\"Name\"] = Fp13.Name\n",
      "319/19: Fp13_n\n",
      "319/20: Fp13_n = Fp13_n.set_index('Name')\n",
      "319/21: Fp13_n.head(3)\n",
      "319/22: raw13  = LogKp.merge(Fp13_n, on='Name', how='outer')\n",
      "319/23: raw13\n",
      "319/24: np.any(np.isnan(raw13))\n",
      "319/25:\n",
      "raw13 =raw13[raw13.logkp.notna()]\n",
      "raw13.shape\n",
      "319/26: print (len(raw13))\n",
      "319/27: print (len(raw13 .columns))\n",
      "319/28:\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['logkp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)\n",
      "label_to_model = compound_df.logkp.tolist()\n",
      "319/29: fingerprint_to_model.head(3)\n",
      "319/30: indices = fingerprint_to_model.index\n",
      "319/31:\n",
      "train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "319/32:\n",
      "#not reduce\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['logkpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)\n",
      "label_to_model = compound_df.logkpu.tolist()\n",
      "319/33:\n",
      "train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "319/34:\n",
      "train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "319/35:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "param = {\n",
      "    \"n_estimators\": 100,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "319/36: indices_train, indices_test\n",
      "319/37:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
      "319/38:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "param = {\n",
      "    \"n_estimators\": 100,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "319/39:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "param = {\n",
      "    \"n_estimators\": 100,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "319/40:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "param = {\n",
      "    \"n_estimators\": 100,  # number of trees to grows\n",
      "}\n",
      "model_RF = RandomForestRegressor(**param)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "320/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "320/2:\n",
      "path = r'smiles/'\n",
      "\n",
      "Fp13  = pd.read_csv(path + 'mordard_Kp.csv'               , header = 0)\n",
      "320/3: Fp13.shape\n",
      "320/4: Fp13\n",
      "320/5:\n",
      "df = pd.read_csv('df6_lipinski_Kpu_Kp_cells.csv')\n",
      "df\n",
      "320/6: name = df['compound']\n",
      "320/7: name.shape\n",
      "320/8:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df3 = df2.set_index('Name')\n",
      "df3\n",
      "320/9:\n",
      "LogKp = df3[['logkp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]\n",
      "LogKp\n",
      "320/10:\n",
      "Fp13 = Fp13.rename(columns={'name': 'Name'})\n",
      "Fp13\n",
      "320/11: Fp13.Name = name\n",
      "320/12:\n",
      "Fpp13 = Fp13.drop('Name', axis=1)\n",
      "Fpp13\n",
      "320/13: Fpp13 = Fpp13.fillna(0)\n",
      "320/14:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)\n",
      "    Fp_normalized\n",
      "    \n",
      "    return Fp_normalized\n",
      "320/15: Fp13_n = normalized (Fpp13)\n",
      "320/16: Fp13_n\n",
      "320/17: np.any(np.isnan(Fp13_n))\n",
      "320/18: Fp13_n[\"Name\"] = Fp13.Name\n",
      "320/19: Fp13_n\n",
      "320/20: Fp13_n = Fp13_n.set_index('Name')\n",
      "320/21: Fp13_n.head(3)\n",
      "320/22: raw13  = LogKp.merge(Fp13_n, on='Name', how='outer')\n",
      "320/23: raw13\n",
      "320/24: np.any(np.isnan(raw13))\n",
      "320/25:\n",
      "path = r'smiles/'\n",
      "\n",
      "Fp13  = pd.read_csv(path + 'mordard_Kp.csv'               , header = 0)\n",
      "320/26: Fp13.shape\n",
      "320/27: Fp13\n",
      "320/28:\n",
      "df = pd.read_csv('df6_lipinski_Kpu_Kp_cells.csv')\n",
      "df\n",
      "320/29: name = df['compound']\n",
      "320/30: name.shape\n",
      "320/31:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df3 = df2.set_index('Name')\n",
      "df3\n",
      "320/32:\n",
      "LogKp = df3[['logkp']]\n",
      "LogKp\n",
      "320/33:\n",
      "Fp13 = Fp13.rename(columns={'name': 'Name'})\n",
      "Fp13\n",
      "320/34: Fp13.Name = name\n",
      "320/35:\n",
      "Fpp13 = Fp13.drop('Name', axis=1)\n",
      "Fpp13\n",
      "320/36: Fpp13 = Fpp13.fillna(0)\n",
      "320/37:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)\n",
      "    Fp_normalized\n",
      "    \n",
      "    return Fp_normalized\n",
      "320/38: Fp13_n = normalized (Fpp13)\n",
      "320/39: Fp13_n\n",
      "320/40: np.any(np.isnan(Fp13_n))\n",
      "320/41: Fp13_n[\"Name\"] = Fp13.Name\n",
      "320/42: Fp13_n\n",
      "320/43: Fp13_n = Fp13_n.set_index('Name')\n",
      "320/44: Fp13_n.head(3)\n",
      "320/45: raw13  = LogKp.merge(Fp13_n, on='Name', how='outer')\n",
      "320/46:\n",
      "LogKp = df3[['logkp']]\n",
      "LogKp\n",
      "320/47:\n",
      "Fp13 = Fp13.rename(columns={'name': 'Name'})\n",
      "Fp13\n",
      "320/48: Fp13.Name = name\n",
      "320/49:\n",
      "Fpp13 = Fp13.drop('Name', axis=1)\n",
      "Fpp13\n",
      "320/50: Fpp13 = Fpp13.fillna(0)\n",
      "320/51:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)\n",
      "    Fp_normalized\n",
      "    \n",
      "    return Fp_normalized\n",
      "320/52: Fp13_n = normalized (Fpp13)\n",
      "320/53: Fp13_n\n",
      "320/54: np.any(np.isnan(Fp13_n))\n",
      "320/55: Fp13_n[\"Name\"] = Fp13.Name\n",
      "320/56: Fp13_n\n",
      "320/57: Fp13_n = Fp13_n.set_index('Name')\n",
      "320/58: Fp13_n.head(3)\n",
      "320/59: raw13  = LogKp.merge(Fp13_n, on='Name', how='outer')\n",
      "320/60: raw13\n",
      "320/61: np.any(np.isnan(raw13))\n",
      "320/62:\n",
      "raw13 =raw13[raw13.logkp.notna()]\n",
      "raw13.shape\n",
      "320/63: print (len(raw13))\n",
      "320/64: print (len(raw13 .columns))\n",
      "320/65:\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['logkp'], axis=1)\n",
      "label_to_model = compound_df.logkp.tolist()\n",
      "320/66: fingerprint_to_model.head(3)\n",
      "320/67: indices = fingerprint_to_model.index\n",
      "320/68:\n",
      "train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "320/69: indices_train, indices_test\n",
      "320/70:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "\n",
      "#sel = VarianceThreshold(threshold=(0.05))\n",
      "#sel = sel.fit(train_x)\n",
      "#mask = sel.get_support()\n",
      "320/71:\n",
      "#train_x_selected = train_x.loc[:, mask]\n",
      "#train_x_selected\n",
      "320/72: train_x_selected.columns\n",
      "320/73: #train_x_selected.columns\n",
      "320/74:\n",
      "import random\n",
      "SEED = random.seed(10)\n",
      "SEED\n",
      "320/75:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
      "320/76: #train_x_selected.shape\n",
      "320/77: #test_x_selected = sel.transform(test_x)\n",
      "320/78:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "\n",
      "test_x = sel.transform(test_x)\n",
      "\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f CV accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "320/79:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "SVR = SVR(kernel='rbf')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f CV accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "320/80:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='linear')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x_selected)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "320/81:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='linear')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "320/82:\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "SVR = SVR(kernel='linear')\n",
      "scores=cross_val_score(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "SVR_fit = SVR.fit(train_x, train_y)\n",
      "y_pred = SVR.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "\n",
      "y_ext = SVR.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "320/83:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestRegressor(n_estimators=200, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "model_RF_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "320/84:\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestRegressor(n_estimators=200, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "model_RF_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "320/85:\n",
      "#Train features\n",
      "from sklearn.inspection import permutation_importance\n",
      "result = permutation_importance(model_RF, train_x, train_y, n_repeats=10, random_state=42, n_jobs=2)\n",
      "feature_names = train_x.columns\n",
      "\n",
      "forest_importances = pd.Series(result.importances_mean, index=feature_names)\n",
      "fig, ax = plt.subplots()\n",
      "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
      "ax.set_title(\"Feature importances using permutation on full model\")\n",
      "ax.set_ylabel(\"Mean accuracy decrease\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "320/86:\n",
      "FI_RF_train_mean = pd.DataFrame(forest_importances)\n",
      "FI_RF_train_mean.to_csv('Result_RF/forest_importances_train_RF_Kp_1493.csv')\n",
      "FI_RF_train_mean\n",
      "320/87:\n",
      "from sklearn.decomposition import PCA\n",
      "pca = PCA(n_components=7)\n",
      "pca.fit(train_x)\n",
      "print(pca.explained_variance_ratio_)\n",
      "320/88:\n",
      "from sklearn.cross_decomposition import PLSRegression\n",
      "\n",
      "model_PLS = PLSRegression(n_components=10)\n",
      "scores=cross_val_score(model_PLS, train_x, train_y, cv=kf)\n",
      "\n",
      "#calculate R2\n",
      "model_PLS.fit(train_x, train_y)\n",
      "y_pred = model_PLS.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_PLS, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_PLS.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "320/89: model_PLS.score(test_x, test_y)\n",
      "320/90:\n",
      "from sklearn.linear_model import Ridge\n",
      "Ridge = Ridge(alpha=1.0)\n",
      "\n",
      "#calculate R2\n",
      "model_fit = Ridge.fit(train_x_selected, train_y)\n",
      "y_pred = Ridge.predict(train_x_selected)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(Ridge, train_x_selected, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = Ridge.predict(test_x_selected)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "320/91:\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "MLP = MLPRegressor(activation='relu', random_state=42, max_iter=1000, solver='lbfgs')\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "model_fit = MLP.fit(train_x, train_y)\n",
      "y_pred = MLP.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(MLP, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = MLP.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "320/92:\n",
      "forest_importances_sd = pd.Series(result.importances_std, index=feature_names)\n",
      "FI_RF_train_mean = pd.DataFrame(forest_importances)\n",
      "FI_RF_train_sd = pd.DataFrame(forest_importances)\n",
      "FI_RF_train_all = pd.concat(FI_RF_train_mean, FI_RF_train_sd, axis = 1)\n",
      "FI_RF_train_mean.to_csv('Result_RF/forest_importances_train_RF_Kp_1493.csv')\n",
      "FI_RF_train_mean\n",
      "320/93:\n",
      "forest_importances_sd = pd.Series(result.importances_std, index=feature_names)\n",
      "FI_RF_train_mean = pd.DataFrame(forest_importances)\n",
      "FI_RF_train_sd = pd.DataFrame(forest_importances)\n",
      "FI_RF_train_all = pd.concat(FI_RF_train_mean, FI_RF_train_sd, axis=1)\n",
      "FI_RF_train_mean.to_csv('Result_RF/forest_importances_train_RF_Kp_1493.csv')\n",
      "FI_RF_train_mean\n",
      "320/94:\n",
      "forest_importances_sd = pd.Series(result.importances_std, index=feature_names)\n",
      "FI_RF_train_mean = pd.DataFrame(forest_importances)\n",
      "FI_RF_train_sd = pd.DataFrame(forest_importances)\n",
      "FI_RF_train_all = pd.concat([FI_RF_train_mean, FI_RF_train_sd], axis=1)\n",
      "FI_RF_train_mean.to_csv('Result_RF/forest_importances_train_RF_Kp_1493.csv')\n",
      "FI_RF_train_mean\n",
      "320/95:\n",
      "forest_importances_sd = pd.Series(result.importances_std, index=feature_names)\n",
      "FI_RF_train_mean = pd.DataFrame(forest_importances)\n",
      "FI_RF_train_sd = pd.DataFrame(forest_importances)\n",
      "FI_RF_train_all = pd.concat([FI_RF_train_mean, FI_RF_train_sd], axis=1)\n",
      "FI_RF_train_mean.to_csv('Result_RF/forest_importances_train_RF_Kp_1493.csv')\n",
      "FI_RF_train_all\n",
      "320/96:\n",
      "forest_importances_sd = pd.Series(result.importances_std, index=feature_names, name=SD)\n",
      "forest_importances = pd.Series(result.importances_mean, index=feature_names, name=Mean)\n",
      "FI_RF_train_mean = pd.DataFrame(forest_importances)\n",
      "FI_RF_train_sd = pd.DataFrame(forest_importances_sd)\n",
      "FI_RF_train_all = pd.concat([FI_RF_train_mean, FI_RF_train_sd], axis=1)\n",
      "FI_RF_train_all.to_csv('Result_RF/forest_importances_train_RF_Kp_1493.csv')\n",
      "FI_RF_train_all\n",
      "320/97:\n",
      "forest_importances_sd = pd.Series(result.importances_std, index=feature_names, name='SD')\n",
      "forest_importances = pd.Series(result.importances_mean, index=feature_names, name='Mean')\n",
      "FI_RF_train_mean = pd.DataFrame(forest_importances)\n",
      "FI_RF_train_sd = pd.DataFrame(forest_importances_sd)\n",
      "FI_RF_train_all = pd.concat([FI_RF_train_mean, FI_RF_train_sd], axis=1)\n",
      "FI_RF_train_all.to_csv('Result_RF/forest_importances_train_RF_Kp_1493.csv')\n",
      "FI_RF_train_all\n",
      "320/98: FI_RF_train_all.sort_values(by=['Mean'])\n",
      "320/99: FI_20 = FI_RF_train_all.sort_values(by=['Mean'])\n",
      "320/100:\n",
      "FI_20 = FI_RF_train_all.sort_values(by=['Mean'])\n",
      "FI_20\n",
      "320/101:\n",
      "FI_20 = FI_RF_train_all.sort_values(by=['Mean'])\n",
      "FI_20.tail(20)\n",
      "320/102:\n",
      "FI = FI_RF_train_all.sort_values(by=['Mean'])\n",
      "FI_20 = FI.tail(20)\n",
      "FI_20\n",
      "320/103:\n",
      "features_20 = fingerprint_to_model[['PEOE_VSA10', 'AATS8p', \"ATSC4are\", 'AATSC4se', 'ATSC3s', 'SlogP_VSA10', \n",
      "                                    'VSA_EState4', 'nFaRing', 'ATSC4c', 'GATS3c']]\n",
      "320/104:\n",
      "features_10 = fingerprint_to_model[['PEOE_VSA10', 'AATS8p', \"ATSC4are\", 'AATSC4se', 'ATSC3s', 'SlogP_VSA10', \n",
      "                                    'VSA_EState4', 'nFaRing', 'ATSC4c', 'GATS3c']]\n",
      "320/105:\n",
      "FI = FI_RF_train_all.sort_values(by=['Mean'])\n",
      "FI_10 = FI.tail(10)\n",
      "FI_10\n",
      "320/106: features_10\n",
      "320/107:\n",
      "df = features_10.tail(3)\n",
      "df\n",
      "320/108:\n",
      "train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(features_10, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "320/109:\n",
      "train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(features_10, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "320/110: train_x.shape\n",
      "320/111:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestRegressor(n_estimators=200, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "model_RF_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "320/112:\n",
      "FI_20 = FI.tail(20)\n",
      "FI_20\n",
      "320/113:\n",
      "features_20 = fingerprint_to_model[['PEOE_VSA10', 'AATS8p', \"ATSC4are\", 'AATSC4se', 'ATSC3s', 'SlogP_VSA10', \n",
      "                                    'VSA_EState4', 'nFaRing', 'ATSC4c', 'GATS3c', 'MATS1c', 'AATSC4pe', \n",
      "                                    'AATSC4are', 'ETA_dAlpha_B', 'AATSC1c', 'MAXaaaC', 'MATS4c', 'GATS3se',\n",
      "                                   'MATS2Z', 'ATSC1m']]\n",
      "320/114:\n",
      "df = features_20.tail(3)\n",
      "df\n",
      "320/115:\n",
      "train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(features_20, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "320/116: train_x.shape\n",
      "320/117:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestRegressor(n_estimators=200, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "model_RF_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "320/118: indices = fingerprint_to_model.index\n",
      "320/119:\n",
      "train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "320/120: indices_train, indices_test\n",
      "320/121:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
      "320/122:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestRegressor(n_estimators=200, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "model_RF_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "320/123:\n",
      "train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(features_20, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "320/124: train_x.shape\n",
      "320/125:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestRegressor(n_estimators=200, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "model_RF_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "320/126:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "model_RF_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "320/127:\n",
      "#Train features\n",
      "from sklearn.inspection import permutation_importance\n",
      "result = permutation_importance(model_RF, train_x, train_y, n_repeats=10, random_state=42, n_jobs=-1)\n",
      "feature_names = train_x.columns\n",
      "forest_importances_sd = pd.Series(result.importances_std, index=feature_names, name='SD')\n",
      "forest_importances = pd.Series(result.importances_mean, index=feature_names, name='Mean')\n",
      "fig, ax = plt.subplots()\n",
      "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
      "ax.set_title(\"Feature importances using permutation on 20 features model\")\n",
      "ax.set_ylabel(\"Mean accuracy decrease\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "FI_RF_train_mean = pd.DataFrame(forest_importances)\n",
      "FI_RF_train_sd = pd.DataFrame(forest_importances_sd)\n",
      "FI_RF_train_all = pd.concat([FI_RF_train_mean, FI_RF_train_sd], axis=1)\n",
      "FI_RF_train_all.to_csv('Result_RF/forest_importances_train_RF_Kp_20.csv')\n",
      "FI_RF_train_all\n",
      "320/128: FI_RF_train_all.sort_values(by=['Mean'])\n",
      "320/129: model_RF.feature_importances_\n",
      "321/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "from rdkit import Chem\n",
      "from rdkit.Chem import Draw\n",
      "from rdkit.Chem.Draw import IPythonConsole\n",
      "from rdkit.Chem import Descriptors\n",
      "from rdkit.Chem import AllChem\n",
      "from rdkit import DataStructs\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import r2_score\n",
      "from collections import defaultdict\n",
      "321/2:\n",
      "path = r'smiles/'\n",
      "\n",
      "Fp13  = pd.read_csv(path + 'mordard_Kp.csv'               , header = 0)\n",
      "321/3: Fp13.shape\n",
      "321/4: Fp13\n",
      "321/5: Fp13\n",
      "321/6:\n",
      "df = pd.read_csv('df6_lipinski_Kpu_Kp_cells.csv')\n",
      "df\n",
      "321/7: name = df['compound']\n",
      "321/8: name.shape\n",
      "321/9:\n",
      "df2 = df.rename({'compound': 'Name'}, axis=1)\n",
      "df3 = df2.set_index('Name')\n",
      "df3\n",
      "321/10:\n",
      "LogKp = df3[['logkp']]\n",
      "LogKp\n",
      "321/11:\n",
      "Fp13 = Fp13.rename(columns={'name': 'Name'})\n",
      "Fp13\n",
      "321/12: Fp13.Name = name\n",
      "321/13:\n",
      "Fpp13 = Fp13.drop('Name', axis=1)\n",
      "Fpp13\n",
      "321/14: Fpp13 = Fpp13.fillna(0)\n",
      "321/15:\n",
      "from sklearn import preprocessing\n",
      "\n",
      "def normalized (Fp):\n",
      "    \n",
      "    min_max_scaler = preprocessing.MinMaxScaler()\n",
      "    np_scaled = min_max_scaler.fit_transform(Fp)\n",
      "    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)\n",
      "    Fp_normalized\n",
      "    \n",
      "    return Fp_normalized\n",
      "321/16: Fp13_n = normalized (Fpp13)\n",
      "321/17: Fp13_n\n",
      "321/18: np.any(np.isnan(Fp13_n))\n",
      "321/19: Fp13_n[\"Name\"] = Fp13.Name\n",
      "321/20: Fp13_n\n",
      "321/21: Fp13_n = Fp13_n.set_index('Name')\n",
      "321/22: Fp13_n.head(3)\n",
      "321/23: Fp13_n.to_csv('Fp_normalized/Morderd_LogKp.csv' , sep=',' ,index=True)\n",
      "321/24: raw13  = LogKp.merge(Fp13_n, on='Name', how='outer')\n",
      "321/25: raw13\n",
      "321/26: np.any(np.isnan(raw13))\n",
      "321/27:\n",
      "raw13 =raw13[raw13.logkp.notna()]\n",
      "raw13.shape\n",
      "321/28: raw13.to_csv('QSAR_Kp/Mordard_Kp_144_full.csv' , sep=',' ,index=True)\n",
      "321/29: print (len(raw13))\n",
      "321/30: print (len(raw13 .columns))\n",
      "321/31:\n",
      "compound_df = raw13.copy()\n",
      "fingerprint_to_model = compound_df.drop(['logkp'], axis=1)\n",
      "label_to_model = compound_df.logkp.tolist()\n",
      "321/32: fingerprint_to_model.head(3)\n",
      "321/33: indices = fingerprint_to_model.index\n",
      "321/34:\n",
      "train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "321/35: indices_train, indices_test\n",
      "321/36:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "\n",
      "#sel = VarianceThreshold(threshold=(0.05))\n",
      "#sel = sel.fit(train_x)\n",
      "#mask = sel.get_support()\n",
      "321/37:\n",
      "#train_x_selected = train_x.loc[:, mask]\n",
      "#train_x_selected\n",
      "321/38: #train_x_selected.columns\n",
      "321/39:\n",
      "import random\n",
      "SEED = random.seed(10)\n",
      "SEED\n",
      "321/40:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
      "321/41: #train_x_selected.shape\n",
      "321/42: #test_x_selected = sel.transform(test_x)\n",
      "321/43:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestRegressor(n_estimators=200, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "model_RF_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "321/44:\n",
      "pd.DataFrame(train_x, indices_train).to_csv(\"Result_RF/1493_features/x_train_kp.csv\")\n",
      "pd.DataFrame(test_x, indices_test).to_csv(\"Result_RF/1493_features/x_test_kp.csv\")\n",
      "pd.DataFrame(y_cv, indices_train).to_csv(\"Result_RF/1493_features/y_cv_kp.csv\")\n",
      "pd.DataFrame(train_y, indices_train).to_csv(\"Result_RF/1493_features/train_y_kp.csv\")\n",
      "pd.DataFrame(test_y, indices_test).to_csv(\"Result_RF/1493_features/test_y_kp.csv\")\n",
      "pd.DataFrame(y_pred, indices_train).to_csv(\"Result_RF/1493_features/y_pred_kp.csv\")\n",
      "pd.DataFrame(y_ext, indices_test).to_csv(\"Result_RF/1493_features/y_SVR_ext_kp.csv\")\n",
      "321/45: plt.barh(feature_names , model_RF.feature_importances_)\n",
      "321/46:\n",
      "#Train features\n",
      "from sklearn.inspection import permutation_importance\n",
      "result = permutation_importance(model_RF, train_x, train_y, n_repeats=10, random_state=42, n_jobs=-1)\n",
      "feature_names = train_x.columns\n",
      "forest_importances_sd = pd.Series(result.importances_std, index=feature_names, name='SD')\n",
      "forest_importances = pd.Series(result.importances_mean, index=feature_names, name='Mean')\n",
      "fig, ax = plt.subplots()\n",
      "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
      "ax.set_title(\"Feature importances using permutation on 20 features model\")\n",
      "ax.set_ylabel(\"Mean accuracy decrease\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "FI_RF_train_mean = pd.DataFrame(forest_importances)\n",
      "FI_RF_train_sd = pd.DataFrame(forest_importances_sd)\n",
      "FI_RF_train_all = pd.concat([FI_RF_train_mean, FI_RF_train_sd], axis=1)\n",
      "FI_RF_train_all.to_csv('Result_RF/forest_importances_train_RF_Kp_20.csv')\n",
      "FI_RF_train_all\n",
      "321/47:\n",
      "FI = FI_RF_train_all.sort_values(by=['Mean'])\n",
      "FI_10 = FI.tail(10)\n",
      "FI_10\n",
      "321/48:\n",
      "features_10 = fingerprint_to_model[['PEOE_VSA10', 'AATS8p', \"ATSC4are\", 'AATSC4se', 'ATSC3s', 'SlogP_VSA10', \n",
      "                                    'VSA_EState4', 'nFaRing', 'ATSC4c', 'GATS3c']]\n",
      "321/49: features_10\n",
      "321/50:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestRegressor(n_estimators=200, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "model_RF_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "321/51: plt.barh(feature_names , model_RF.feature_importances_)\n",
      "321/52:\n",
      "forest_importances_sd = pd.Series(result.importances_std, index=feature_names, name='SD')\n",
      "forest_importances = pd.Series(result.importances_mean, index=feature_names, name='Mean')\n",
      "FI_RF_train_mean = pd.DataFrame(forest_importances)\n",
      "FI_RF_train_sd = pd.DataFrame(forest_importances_sd)\n",
      "FI_RF_train_all = pd.concat([FI_RF_train_mean, FI_RF_train_sd], axis=1)\n",
      "FI_RF_train_all.to_csv('Result_RF/forest_importances_train_RF_Kp_1493.csv')\n",
      "FI_RF_train_all\n",
      "321/53:\n",
      "FI = FI_RF_train_all.sort_values(by=['Mean'])\n",
      "FI_10 = FI.tail(10)\n",
      "FI_10\n",
      "321/54:\n",
      "features_10 = fingerprint_to_model[['PEOE_VSA10', 'AATS8p', \"ATSC4are\", 'AATSC4se', 'ATSC3s', 'SlogP_VSA10', \n",
      "                                    'VSA_EState4', 'nFaRing', 'ATSC4c', 'GATS3c']]\n",
      "321/55: features_10\n",
      "321/56:\n",
      "df = features_10.tail(3)\n",
      "df\n",
      "321/57: df.to_csv('Result_RF/Features_10_SM.csv')\n",
      "321/58:\n",
      "train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(features_10, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "321/59: train_x.shape\n",
      "321/60:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestRegressor(n_estimators=200, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "model_RF_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "321/61: plt.barh(feature_names , model_RF.feature_importances_)\n",
      "321/62: model_RF.feature_importances_\n",
      "321/63: plt.barh(feature_names, model_RF.feature_importances_)\n",
      "321/64:\n",
      "feature_names = train_x.columns\n",
      "model_RF.feature_importances_\n",
      "321/65: plt.barh(feature_names, model_RF.feature_importances_)\n",
      "321/66:\n",
      "sorted_idx = model_RF.feature_importances_.argsort()\n",
      "plt.barh(feature_names[sorted_idx], model_RF.feature_importances_[sorted_idx])\n",
      "plt.xlabel(\"Random Forest Feature Importance\")\n",
      "321/67:\n",
      "pd.DataFrame(train_x, indices_train).to_csv(\"Result_RF/10_features/x_train_kp.csv\")\n",
      "pd.DataFrame(test_x, indices_test).to_csv(\"Result_RF/10_features/x_test_kp.csv\")\n",
      "pd.DataFrame(y_cv, indices_train).to_csv(\"Result_RF/10_features/y_cv_kp.csv\")\n",
      "pd.DataFrame(train_y, indices_train).to_csv(\"Result_RF/10_features/train_y_kp.csv\")\n",
      "pd.DataFrame(test_y, indices_test).to_csv(\"Result_RF/10_features/test_y_kp.csv\")\n",
      "pd.DataFrame(y_pred, indices_train).to_csv(\"Result_RF/10_features/y_pred_kp.csv\")\n",
      "pd.DataFrame(y_ext, indices_test).to_csv(\"Result_RF/10_features/y_SVR_ext_kp.csv\")\n",
      "321/68:\n",
      "features_20 = fingerprint_to_model[['PEOE_VSA10', 'AATS8p', \"ATSC4are\", 'AATSC4se', 'ATSC3s', 'SlogP_VSA10', \n",
      "                                    'VSA_EState4', 'nFaRing', 'ATSC4c', 'GATS3c', 'MATS1c', 'AATSC4pe', \n",
      "                                    'AATSC4are', 'ETA_dAlpha_B', 'AATSC1c', 'MAXaaaC', 'MATS4c', 'GATS3se',\n",
      "                                   'MATS2Z', 'ATSC1m']]\n",
      "321/69:\n",
      "df = features_20.tail(3)\n",
      "df\n",
      "321/70: df.to_csv('Result_RF/Features_20_SM.csv')\n",
      "321/71:\n",
      "train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(features_20, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "321/72: train_x.shape\n",
      "321/73:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "model_RF_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "321/74:\n",
      "pd.DataFrame(train_x, indices_train).to_csv(\"Result_RF/20_features/x_train_kp.csv\")\n",
      "pd.DataFrame(test_x, indices_test).to_csv(\"Result_RF/20_features/x_test_kp.csv\")\n",
      "pd.DataFrame(y_cv, indices_train).to_csv(\"Result_RF/20_features/y_cv_kp.csv\")\n",
      "pd.DataFrame(train_y, indices_train).to_csv(\"Result_RF/20_features/train_y_kp.csv\")\n",
      "pd.DataFrame(test_y, indices_test).to_csv(\"Result_RF/20_features/test_y_kp.csv\")\n",
      "pd.DataFrame(y_pred, indices_train).to_csv(\"Result_RF/20_features/y_pred_kp.csv\")\n",
      "pd.DataFrame(y_ext, indices_test).to_csv(\"Result_RF/20_features/y_SVR_ext_kp.csv\")\n",
      "321/75:\n",
      "#Train features\n",
      "from sklearn.inspection import permutation_importance\n",
      "result = permutation_importance(model_RF, train_x, train_y, n_repeats=10, random_state=42, n_jobs=-1)\n",
      "feature_names = train_x.columns\n",
      "forest_importances_sd = pd.Series(result.importances_std, index=feature_names, name='SD')\n",
      "forest_importances = pd.Series(result.importances_mean, index=feature_names, name='Mean')\n",
      "fig, ax = plt.subplots()\n",
      "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
      "ax.set_title(\"Feature importances using permutation on 20 features model\")\n",
      "ax.set_ylabel(\"Mean accuracy decrease\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "FI_RF_train_mean = pd.DataFrame(forest_importances)\n",
      "FI_RF_train_sd = pd.DataFrame(forest_importances_sd)\n",
      "FI_RF_train_all = pd.concat([FI_RF_train_mean, FI_RF_train_sd], axis=1)\n",
      "FI_RF_train_all.to_csv('Result_RF/forest_importances_train_RF_Kp_20.csv')\n",
      "FI_RF_train_all\n",
      "321/76:\n",
      "#Train features\n",
      "from sklearn.inspection import permutation_importance\n",
      "result = permutation_importance(model_RF, train_x, train_y, n_repeats=10, random_state=42, n_jobs=-1)\n",
      "feature_names = train_x.columns\n",
      "forest_importances_sd = pd.Series(result.importances_std, index=feature_names, name='SD')\n",
      "forest_importances = pd.Series(result.importances_mean, index=feature_names, name='Mean')\n",
      "fig, ax = plt.subplots()\n",
      "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
      "ax.set_title(\"Feature importances using permutation on 20 features model\")\n",
      "ax.set_ylabel(\"Mean accuracy decrease\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "FI_RF_train_mean = pd.DataFrame(forest_importances)\n",
      "FI_RF_train_sd = pd.DataFrame(forest_importances_sd)\n",
      "FI_RF_train_all = pd.concat([FI_RF_train_mean, FI_RF_train_sd], axis=1)\n",
      "FI_RF_train_all.to_csv('Result_RF/20_features/forest_importances_train_RF_Kp_20.csv')\n",
      "FI_RF_train_all\n",
      "321/77: FI_RF_train_all.sort_values(by=['Mean'])\n",
      "321/78: model_RF.feature_importances\n",
      "321/79: model_RF.feature_importances_\n",
      "321/80:\n",
      "sorted_idx = model_RF.feature_importances_.argsort()\n",
      "plt.barh(feature_names[sorted_idx], model_RF.feature_importances_[sorted_idx])\n",
      "plt.xlabel(\"Random Forest Feature Importance\")\n",
      "321/81:\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "plt.barh(feature_names[sorted_idx], result.importances_mean[sorted_idx])\n",
      "plt.xlabel(\"Random Forest Feature Importance\")\n",
      "321/82:\n",
      "explainer = shap.TreeExplainer(model_RF)\n",
      "shap_values = explainer.shap_values(test_x)\n",
      "321/83:\n",
      "import shap\n",
      "explainer = shap.TreeExplainer(model_RF)\n",
      "shap_values = explainer.shap_values(test_x)\n",
      "321/84: shap.summary_plot(shap_values, test_x, plot_type=\"bar\")\n",
      "321/85: shap.summary_plot(shap_values, test_x)\n",
      "321/86:\n",
      "import shap\n",
      "explainer = shap.TreeExplainer(model_RF)\n",
      "shap_values = explainer.shap_values(train_x)\n",
      "321/87: shap.summary_plot(shap_values, train_x, plot_type=\"bar\")\n",
      "321/88:\n",
      "train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "321/89: indices_train, indices_test\n",
      "321/90:\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)\n",
      "#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)\n",
      "#fingerprint_to_model\n",
      "\n",
      "#sel = VarianceThreshold(threshold=(0.05))\n",
      "#sel = sel.fit(train_x)\n",
      "#mask = sel.get_support()\n",
      "321/91:\n",
      "#train_x_selected = train_x.loc[:, mask]\n",
      "#train_x_selected\n",
      "321/92: #train_x_selected.columns\n",
      "321/93:\n",
      "import random\n",
      "SEED = random.seed(10)\n",
      "SEED\n",
      "321/94:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
      "321/95: #train_x_selected.shape\n",
      "321/96: #test_x_selected = sel.transform(test_x)\n",
      "321/97:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestRegressor(n_estimators=200, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "model_RF_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "321/98:\n",
      "pd.DataFrame(train_x, indices_train).to_csv(\"Result_RF/1493_features/x_train_kp.csv\")\n",
      "pd.DataFrame(test_x, indices_test).to_csv(\"Result_RF/1493_features/x_test_kp.csv\")\n",
      "pd.DataFrame(y_cv, indices_train).to_csv(\"Result_RF/1493_features/y_cv_kp.csv\")\n",
      "pd.DataFrame(train_y, indices_train).to_csv(\"Result_RF/1493_features/train_y_kp.csv\")\n",
      "pd.DataFrame(test_y, indices_test).to_csv(\"Result_RF/1493_features/test_y_kp.csv\")\n",
      "pd.DataFrame(y_pred, indices_train).to_csv(\"Result_RF/1493_features/y_pred_kp.csv\")\n",
      "pd.DataFrame(y_ext, indices_test).to_csv(\"Result_RF/1493_features/y_SVR_ext_kp.csv\")\n",
      "321/99:\n",
      "train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(features_10, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "321/100: train_x.shape\n",
      "321/101:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestRegressor(n_estimators=200, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "model_RF_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "321/102:\n",
      "#Train features\n",
      "from sklearn.inspection import permutation_importance\n",
      "result = permutation_importance(model_RF, train_x, train_y, n_repeats=10, random_state=42, n_jobs=-1)\n",
      "feature_names = train_x.columns\n",
      "forest_importances_sd = pd.Series(result.importances_std, index=feature_names, name='SD')\n",
      "forest_importances = pd.Series(result.importances_mean, index=feature_names, name='Mean')\n",
      "fig, ax = plt.subplots()\n",
      "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
      "ax.set_title(\"Feature importances using permutation on 20 features model\")\n",
      "ax.set_ylabel(\"Mean accuracy decrease\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "FI_RF_train_mean = pd.DataFrame(forest_importances)\n",
      "FI_RF_train_sd = pd.DataFrame(forest_importances_sd)\n",
      "FI_RF_train_all = pd.concat([FI_RF_train_mean, FI_RF_train_sd], axis=1)\n",
      "FI_RF_train_all.to_csv('Result_RF/20_features/forest_importances_train_RF_Kp_10.csv')\n",
      "FI_RF_train_all\n",
      "321/103:\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "plt.barh(feature_names[sorted_idx], result.importances_mean[sorted_idx])\n",
      "plt.xlabel(\"Permutation Feature Importance\")\n",
      "321/104:\n",
      "import shap\n",
      "explainer = shap.TreeExplainer(model_RF)\n",
      "shap_values = explainer.shap_values(train_x)\n",
      "321/105: shap.summary_plot(shap_values, train_x, plot_type=\"bar\")\n",
      "321/106: shap.summary_plot(shap_values, test_x)\n",
      "321/107: shap.summary_plot(shap_values, train_x)\n",
      "321/108:\n",
      "#Train features\n",
      "from sklearn.inspection import permutation_importance\n",
      "result = permutation_importance(model_RF, train_x, train_y, n_repeats=10, random_state=42, n_jobs=-1)\n",
      "feature_names = train_x.columns\n",
      "forest_importances_sd = pd.Series(result.importances_std, index=feature_names, name='SD')\n",
      "forest_importances = pd.Series(result.importances_mean, index=feature_names, name='Mean')\n",
      "fig, ax = plt.subplots()\n",
      "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
      "ax.set_title(\"Feature importances using permutation on 20 features model\")\n",
      "ax.set_ylabel(\"Mean accuracy decrease\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "FI_RF_train_mean = pd.DataFrame(forest_importances)\n",
      "FI_RF_train_sd = pd.DataFrame(forest_importances_sd)\n",
      "FI_RF_train_all = pd.concat([FI_RF_train_mean, FI_RF_train_sd], axis=1)\n",
      "FI_RF_train_all.to_csv('Result_RF/10_features/forest_importances_train_RF_Kp_10.csv')\n",
      "FI_RF_train_all\n",
      "321/109:\n",
      "sorted_idx = result.importances_mean.argsort()\n",
      "plt.barh(feature_names[sorted_idx], result.importances_mean[sorted_idx])\n",
      "plt.xlabel(\"Permutation Feature Importance\")\n",
      "321/110: -0.6375/-1.9\n",
      "321/111: -0.6375/-1.9*100\n",
      "321/112:\n",
      "FI = FI_RF_train_all.sort_values(by=['Mean'])\n",
      "FI_20 = FI.tail(20)\n",
      "FI_20\n",
      "321/113:\n",
      "FI = FI_RF_train_all.sort_values(by=['Mean'])\n",
      "FI_10 = FI.tail(10)\n",
      "FI_10\n",
      "321/114:\n",
      "train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(features_10, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "321/115: train_x.shape\n",
      "321/116:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestRegressor(n_estimators=200, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "model_RF_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "321/117:\n",
      "pd.DataFrame(train_x, indices_train).to_csv(\"Result_RF/10_features/x_train_kp.csv\", name='x_train')\n",
      "pd.DataFrame(test_x, indices_test).to_csv(\"Result_RF/10_features/x_test_kp.csv\", name='x_test')\n",
      "pd.DataFrame(y_cv, indices_train).to_csv(\"Result_RF/10_features/y_cv_kp.csv\", name='y_cv')\n",
      "pd.DataFrame(train_y, indices_train).to_csv(\"Result_RF/10_features/train_y_kp.csv\", name='train_y')\n",
      "pd.DataFrame(test_y, indices_test).to_csv(\"Result_RF/10_features/test_y_kp.csv\", name='test_y')\n",
      "pd.DataFrame(y_pred, indices_train).to_csv(\"Result_RF/10_features/y_pred_kp.csv\", name='y_pred')\n",
      "pd.DataFrame(y_ext, indices_test).to_csv(\"Result_RF/10_features/y_ext_kp.csv\", name='y_ext')\n",
      "321/118:\n",
      "pd.DataFrame(train_x, indices_train, colume='x_train').to_csv(\"Result_RF/10_features/x_train_kp.csv\")\n",
      "pd.DataFrame(test_x, indices_test, colume='x_test').to_csv(\"Result_RF/10_features/x_test_kp.csv\")\n",
      "pd.DataFrame(y_cv, indices_train, colume='y_cv').to_csv(\"Result_RF/10_features/y_cv_kp.csv\")\n",
      "pd.DataFrame(train_y, indices_train, colume='train_y').to_csv(\"Result_RF/10_features/train_y_kp.csv\")\n",
      "pd.DataFrame(test_y, indices_test, colume='test_y').to_csv(\"Result_RF/10_features/test_y_kp.csv\")\n",
      "pd.DataFrame(y_pred, indices_train, colume='y_pred').to_csv(\"Result_RF/10_features/y_pred_kp.csv\")\n",
      "pd.DataFrame(y_ext, indices_test, colume='y_ext').to_csv(\"Result_RF/10_features/y_ext_kp.csv\")\n",
      "321/119:\n",
      "pd.DataFrame(train_x, indices_train, columes='x_train').to_csv(\"Result_RF/10_features/x_train_kp.csv\")\n",
      "pd.DataFrame(test_x, indices_test, columes='x_test').to_csv(\"Result_RF/10_features/x_test_kp.csv\")\n",
      "pd.DataFrame(y_cv, indices_train, columes='y_cv').to_csv(\"Result_RF/10_features/y_cv_kp.csv\")\n",
      "pd.DataFrame(train_y, indices_train, columes='train_y').to_csv(\"Result_RF/10_features/train_y_kp.csv\")\n",
      "pd.DataFrame(test_y, indices_test, columes='test_y').to_csv(\"Result_RF/10_features/test_y_kp.csv\")\n",
      "pd.DataFrame(y_pred, indices_train, columes='y_pred').to_csv(\"Result_RF/10_features/y_pred_kp.csv\")\n",
      "pd.DataFrame(y_ext, indices_test, columes='y_ext').to_csv(\"Result_RF/10_features/y_ext_kp.csv\")\n",
      "321/120: pd.DataFrame(train_x, indices_train, columns=c['Name', 'x_train']).to_csv(\"Result_RF/10_features/x_train_kp.csv\")\n",
      "321/121: pd.DataFrame(train_x, indices_train, columns=['Name', 'x_train']).to_csv(\"Result_RF/10_features/x_train_kp.csv\")\n",
      "321/122:\n",
      "pd.DataFrame(train_x, indices_train).to_csv(\"Result_RF/10_features/x_train_kp.csv\")\n",
      "pd.DataFrame(test_x, indices_test).to_csv(\"Result_RF/10_features/x_test_kp.csv\")\n",
      "pd.DataFrame(y_cv, indices_train).to_csv(\"Result_RF/10_features/y_cv_kp.csv\")\n",
      "pd.DataFrame(train_y, indices_train).to_csv(\"Result_RF/10_features/train_y_kp.csv\")\n",
      "pd.DataFrame(test_y, indices_test).to_csv(\"Result_RF/10_features/test_y_kp.csv\")\n",
      "pd.DataFrame(y_pred, indices_train).to_csv(\"Result_RF/10_features/y_pred_kp.csv\")\n",
      "pd.DataFrame(y_ext, indices_test).to_csv(\"Result_RF/10_features/y_ext_kp.csv\")\n",
      "321/123:\n",
      "#Train features\n",
      "from sklearn.inspection import permutation_importance\n",
      "result = permutation_importance(model_RF, train_x, train_y, n_repeats=10, random_state=42, n_jobs=-1)\n",
      "feature_names = train_x.columns\n",
      "forest_importances_sd = pd.Series(result.importances_std, index=feature_names, name='SD')\n",
      "forest_importances = pd.Series(result.importances_mean, index=feature_names, name='Mean')\n",
      "fig, ax = plt.subplots()\n",
      "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
      "ax.set_title(\"Feature importances using permutation on 10 features model\")\n",
      "ax.set_ylabel(\"Mean accuracy decrease\")\n",
      "fig.tight_layout()\n",
      "plt.show()\n",
      "FI_RF_train_mean = pd.DataFrame(forest_importances)\n",
      "FI_RF_train_sd = pd.DataFrame(forest_importances_sd)\n",
      "FI_RF_train_all = pd.concat([FI_RF_train_mean, FI_RF_train_sd], axis=1)\n",
      "FI_RF_train_all.to_csv('Result_RF/10_features/forest_importances_train_RF_Kp_10.csv')\n",
      "FI_RF_train_all\n",
      "321/124:\n",
      "df = pd.read_csv('df6_lipinski_Kpu_Kp_cells.csv')\n",
      "df_logkp = df[['compound', 'smiles', 'kp', 'logkp']]\n",
      "321/125:\n",
      "df = pd.read_csv('df6_lipinski_Kpu_Kp_cells.csv')\n",
      "df_logkp = df[['compound', 'smiles', 'kp', 'logkp']]\n",
      "df_logkp\n",
      "321/126:\n",
      "df_logkp = df[['compound', 'smiles', 'kp', 'logkp']]\n",
      "df_logkp.to_csv('Supplementary_File_2.csv')\n",
      "df_logkp\n",
      "321/127: raw13.to_csv('QSAR_Kp/Mordard_Kp_144_1493_features.csv' , sep=',' ,index=True)\n",
      "321/128: from rdkit import Chem\n",
      "321/129: Chem.MolFromSmiles('Oc1ccc2c(c1)OCO2')\n",
      "321/130: Chem.MolFromSmiles('C1Oc2c(O1)cc(cc2)C1OCC2C1COC2c1ccc2c(c1)OCO2')\n",
      "321/131:\n",
      "m = Chem.MolFromSmiles('C1Oc2c(O1)cc(cc2)C1OCC2C1COC2c1ccc2c(c1)OCO2')\n",
      "substructure = Chem.MolFromSmarts('Oc1ccc2c(c1)OC')\n",
      "print(m.GetSubstructMatches(substructure))\n",
      "321/132:\n",
      "m = Chem.MolFromSmiles('C1Oc2c(O1)cc(cc2)C1OCC2C1COC2c1ccc2c(c1)OCO2')\n",
      "substructure = Chem.MolFromSmarts('Oc1ccc2c(c1)O')\n",
      "print(m.GetSubstructMatches(substructure))\n",
      "321/133:\n",
      "m = Chem.MolFromSmiles('C1Oc2c(O1)cc(cc2)C1OCC2C1COC2c1ccc2c(c1)OCO2')\n",
      "substructure = Chem.MolFromSmarts('Oc1ccc2c(c1)OCO2')\n",
      "print(m.GetSubstructMatches(substructure))\n",
      "321/134:\n",
      "m = Chem.MolFromSmiles('C1Oc2c(O1)cc(cc2)C1OCC2C1COC2c1ccc2c(c1)OCO2')\n",
      "substructure = Chem.MolFromSmarts('Oc1ccc2c(c1)OCO')\n",
      "print(m.GetSubstructMatches(substructure))\n",
      "321/135:\n",
      "m = Chem.MolFromSmiles('C1Oc2c(O1)cc(cc2)C1OCC2C1COC2c1ccc2c(c1)OCO2')\n",
      "substructure = Chem.MolFromSmarts('c1ccc2c(c1)OCO2')\n",
      "print(m.GetSubstructMatches(substructure))\n",
      "321/136: m\n",
      "321/137:\n",
      "SMO = Chem.MolFromSmiles('C1Oc2c(O1)cc(cc2)OC1OCC2C1COC2c1ccc2c(c1)OCO2')\n",
      "substructure = Chem.MolFromSmarts('c1ccc2c(c1)OCO2')\n",
      "print(m.GetSubstructMatches(substructure))\n",
      "321/138: SMO\n",
      "321/139:\n",
      "SMO = Chem.MolFromSmiles('C1Oc2c(O1)cc(cc2)OC1OCC2C1COC2c1ccc2c(c1)OCO2')\n",
      "substructure = Chem.MolFromSmarts('c1ccc2c(c1)OCO2')\n",
      "print(m.GetSubstructMatches(substructure))\n",
      "321/140: SMO\n",
      "321/141:\n",
      "SMO = Chem.MolFromSmiles('C1Oc2c(O1)cc(cc2)OC1OCC2C1COC2c1ccc2c(c1)OCO2')\n",
      "substructure = Chem.MolFromSmarts('c1ccc2c(c1)OCO2')\n",
      "print(SMO.GetSubstructMatches(substructure))\n",
      "321/142: SMO\n",
      "321/143:\n",
      "SMO = Chem.MolFromSmiles('C1Oc2c(O1)cc(cc2)OC1OCC2C1COC2c1ccc2c(c1)OCO2')\n",
      "substructure = Chem.MolFromSmarts('c1ccc2c(c1)OCO2')\n",
      "SMO.GetSubstructMatches(substructure)\n",
      "321/144:\n",
      "SM = Chem.MolFromSmiles('Oc1ccc2c(c1)OCO')\n",
      "substructure = Chem.MolFromSmarts('Oc1ccc2c(c1)OCO')\n",
      "print(SM.GetSubstructMatches(substructure))\n",
      "SM\n",
      "321/145: from rdkit import Chem\n",
      "321/146:\n",
      "SM = Chem.MolFromSmiles('Oc1ccc2c(c1)OCO')\n",
      "substructure = Chem.MolFromSmarts('c1ccc2c(c1)OCO')\n",
      "print(SM.GetSubstructMatches(substructure))\n",
      "SM\n",
      "321/147:\n",
      "SM = Chem.MolFromSmiles('Oc1ccc2c(c1)OCO')\n",
      "substructure = Chem.MolFromSmarts('c1ccc2c(c1)OCO')\n",
      "print(SM.GetSubstructMatches(substructure))\n",
      "321/148:\n",
      "SM = Chem.MolFromSmiles('Oc1ccc2c(c1)OCO2')\n",
      "substructure = Chem.MolFromSmarts('c1ccc2c(c1)OCO2')\n",
      "print(SM.GetSubstructMatches(substructure))\n",
      "321/149:\n",
      "SM = Chem.MolFromSmiles('Oc1ccc2c(c1)OCO2')\n",
      "substructure = Chem.MolFromSmarts('c1ccc2c(c1)OCO2')\n",
      "print(SM.GetSubstructMatches(substructure))\n",
      "SM\n",
      "321/150:\n",
      "SM = Chem.MolFromSmiles('Oc1ccc2c(c1)OCO2')\n",
      "substructure = Chem.MolFromSmarts('c1ccc2c(c1)OCO2')\n",
      "substructure = Chem.MolFromSmarts('O')\n",
      "print(SM.GetSubstructMatches(substructure))\n",
      "SM\n",
      "321/151:\n",
      "SM = Chem.MolFromSmiles('Oc1ccc2c(c1)OCO2')\n",
      "substructure = Chem.MolFromSmarts('c1ccc2c(c1)OCO2')\n",
      "substructure = Chem.MolFromSmarts('Oc1')\n",
      "print(SM.GetSubstructMatches(substructure))\n",
      "SM\n",
      "321/152:\n",
      "SM = Chem.MolFromSmiles('Oc1ccc2c(c1)OCO2')\n",
      "substructure = Chem.MolFromSmarts('c1ccc2c(c1)OCO2')\n",
      "print(SM.GetSubstructMatches(substructure))\n",
      "SM\n",
      "321/153:\n",
      "SM = Chem.MolFromSmiles('Oc1ccc2c(c1)OCO2')\n",
      "substructure = Chem.MolFromSmarts('c1ccc2c(c1)OCO2')\n",
      "print(SM.GetSubstructMatches(substructure))\n",
      "SM\n",
      "SM.Chem.Draw.MolToFile\n",
      "321/154:\n",
      "SM = Chem.MolFromSmiles('Oc1ccc2c(c1)OCO2')\n",
      "substructure = Chem.MolFromSmarts('c1ccc2c(c1)OCO2')\n",
      "print(SM.GetSubstructMatches(substructure))\n",
      "SM\n",
      "Chem.Draw.MolToFile(SM)\n",
      "321/155:\n",
      "SM = Chem.MolFromSmiles('Oc1ccc2c(c1)OCO2')\n",
      "substructure = Chem.MolFromSmarts('c1ccc2c(c1)OCO2')\n",
      "print(SM.GetSubstructMatches(substructure))\n",
      "SM\n",
      "Chem.Draw.MolToFile(SM, 'sesamol.png')\n",
      "321/156:\n",
      "SM = Chem.MolFromSmiles('Oc1ccc2c(c1)OCO2')\n",
      "substructure = Chem.MolFromSmarts('c1ccc2c(c1)OCO2')\n",
      "print(SM.GetSubstructMatches(substructure))\n",
      "SM\n",
      "321/157:\n",
      "SM = Chem.MolFromSmiles('Oc1ccc2c(c1)OCO2')\n",
      "substructure = Chem.MolFromSmarts('c1ccc2c(c1)OCO2')\n",
      "SM.GetSubstructMatches(substructure)\n",
      "SM\n",
      "321/158: Chem.MolFromSmiles('Oc1ccc2c(c1)OCO2')\n",
      "341/1: print('hello world')\n",
      "341/2:\n",
      "from rdkit import DataStructs\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "341/3:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "341/4: df = pd.read_csv('antihyperglycemia.csv')\n",
      "341/5: df\n",
      "341/6: df = pd.read_csv('antihyperglycemia.csv')\n",
      "341/7: df\n",
      "341/8: df = df.drop['Name_Thai']\n",
      "341/9: df\n",
      "341/10: df = df.drop[['Name_Thai']]\n",
      "341/11: df = df.drop(['Name_Thai'])\n",
      "341/12: df = df.drop('Name_Thai')\n",
      "341/13: df = df.drop(['Name_Thai'], axis=1)\n",
      "341/14:\n",
      "df = df.drop(['Name_Thai'], axis=1)\n",
      "fd\n",
      "341/15:\n",
      "df = df.drop(['Name_Thai'], axis=1)\n",
      "df\n",
      "341/16: df = pd.read_csv('antihyperglycemia.csv')\n",
      "341/17: df\n",
      "341/18:\n",
      "df = df.drop(['Number', 'Name_Thai'], axis=1)\n",
      "df\n",
      "341/19:\n",
      "from scipy.stats import pearsonr\n",
      "rho = df.corr()\n",
      "pval = df.corr(method=lambda x, y: pearsonr(x, y)[1]) - np.eye(*rho.shape)\n",
      "p = pval.applymap(lambda x: ''.join(['*' for t in [0.001,0.01,0.05] if x<=t]))\n",
      "Res_pearson_sig = rho.round(3).astype(str) + p\n",
      "Res_pearson_sig\n",
      "341/20:\n",
      "from scipy.stats import pearsonr\n",
      "rho = df.corr()\n",
      "pval = df.corr(method='pearson' x, y: pearsonr(x, y)[1]) - np.eye(*rho.shape)\n",
      "p = pval.applymap('pearson' x: ''.join(['*' for t in [0.001,0.01,0.05] if x<=t]))\n",
      "Res_pearson_sig = rho.round(3).astype(str) + p\n",
      "Res_pearson_sig\n",
      "341/21:\n",
      "from scipy.stats import pearsonr\n",
      "rho = df.corr()\n",
      "pval = df.corr(method='pearson', x, y: pearsonr(x, y)[1]) - np.eye(*rho.shape)\n",
      "p = pval.applymap('pearson', x: ''.join(['*' for t in [0.001,0.01,0.05] if x<=t]))\n",
      "Res_pearson_sig = rho.round(3).astype(str) + p\n",
      "Res_pearson_sig\n",
      "341/22:\n",
      "from scipy.stats import pearsonr\n",
      "rho = df.corr()\n",
      "pval = df.corr(method=lambda x, y: pearsonr(x, y)[1]) - np.eye(*rho.shape)\n",
      "p = pval.applymap(lambda x: ''.join(['*' for t in [0.001,0.01,0.05] if x<=t]))\n",
      "Res_pearson_sig = rho.round(3).astype(str) + p\n",
      "Res_pearson_sig\n",
      "341/23:\n",
      "Res_pearson = df.corr(method='pearson').round(3)\n",
      "Res_pearson\n",
      "341/24:\n",
      "fig, ax = plt.subplots(figsize=(10,10)) \n",
      "ax = sns.heatmap(Res_pearson, annot=True, cmap='coolwarm', fmt='.2f')\n",
      "plt.tight_layout()\n",
      "341/25:\n",
      "fig, ax = plt.subplots(figsize=(10,10)) \n",
      "ax = sns.heatmap(Res_pearson, annot=True, cmap='coolwarm', fmt='.2f')\n",
      "plt.tight_layout()\n",
      "plt.savefig('corr_pearson_number.pdf', dpi=300)\n",
      "341/26:\n",
      "fig, ax = plt.subplots(figsize=(10,10)) \n",
      "ax = sns.heatmap(Res_pearson, annot=False, cmap='coolwarm', fmt='.2f')\n",
      "plt.tight_layout()\n",
      "plt.savefig('corr_pearson_number.pdf', dpi=300)\n",
      "341/27:\n",
      "fig, ax = plt.subplots(figsize=(10,10)) \n",
      "ax = sns.heatmap(Res_pearson, annot=False, cmap='coolwarm', fmt='.2f')\n",
      "plt.tight_layout()\n",
      "plt.savefig('corr_pearson_no_number.pdf', dpi=300)\n",
      "341/28:\n",
      "fig, ax = plt.subplots(figsize=(10,10)) \n",
      "ax = sns.heatmap(Res_pearson, annot=False, cmap='Spectral', fmt='.2f')\n",
      "plt.tight_layout()\n",
      "plt.savefig('corr_pearson_no_number.pdf', dpi=300)\n",
      "341/29:\n",
      "fig, ax = plt.subplots(figsize=(10,10)) \n",
      "ax = sns.heatmap(Res_pearson, annot=True, cmap='Spectral', fmt='.2f')\n",
      "plt.tight_layout()\n",
      "plt.savefig('corr_pearson_number.pdf', dpi=300)\n",
      "341/30: df.to_csv('df_anthyperglycemia_clean.csv')\n",
      "342/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "342/2: df = pd.read_csv('df_antihyperglycemia_clean.csv')\n",
      "342/3: df\n",
      "341/31: df.to_csv('df_anthyperglycemia_clean.csv', index=True)\n",
      "341/32: df.to_csv('df_antihyperglycemia_clean.csv', index=True)\n",
      "342/4: df = pd.read_csv('df_antihyperglycemia_clean.csv')\n",
      "342/5: df\n",
      "341/33:\n",
      "df.set_index(['Name_Short'])\n",
      ".to_csv('df_antihyperglycemia_clean.csv', index=True)\n",
      "341/34: df1 = df.set_index(['Name_Short'])\n",
      "341/35:\n",
      "df1 = df.set_index(['Name_Short'])\n",
      "df1\n",
      "341/36:\n",
      "df1 = df.set_index(['Name_Short'])\n",
      "df2 = df1.drop(['Name_Sci'])\n",
      "341/37:\n",
      "df1 = df.set_index(['Name_Short'])\n",
      "df2 = df1.drop(['Name_Sci'], axis=1)\n",
      "341/38: df2\n",
      "341/39: df2.to_csv('df_clean.csv')\n",
      "342/6: df = pd.read_csv('df_clean.csv')\n",
      "342/7: df\n",
      "342/8: df_nostring = df.drop(['Bioactivity_class_glucosidase', 'Bioactivity_class_glucosidase'], axis=1)\n",
      "342/9: df_nona_nostr = df_nostring.notna()\n",
      "342/10: df_nona_nostr\n",
      "342/11: df_PCA = df.drop(['IC50_amylase','IC50_glucosidase','Bioactivity_class_glucosidase', 'Bioactivity_class_glucosidase'], axis=1)\n",
      "342/12: df_PCA\n",
      "342/13: df_PCA = df.drop(['IC50_amylase','IC50_glucosidase','Bioactivity_class_amylase', 'Bioactivity_class_glucosidase'], axis=1)\n",
      "342/14: df_PCA\n",
      "342/15: df.head(2)\n",
      "342/16: features = ['Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Sapoonins', 'Tannins', 'Xanthones']\n",
      "342/17: x = df.loc[:, features].values\n",
      "342/18: features = ['Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']\n",
      "342/19: x = df.loc[:, features].values\n",
      "342/20: y = df.loc[:,['Bioactivity_class_amylase']].values\n",
      "342/21: pd.DataFrame(data = x, columns = features).head()\n",
      "342/22: pca = PCA(n_components=2)\n",
      "342/23:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.decomposition import PCA\n",
      "342/24: pca = PCA(n_components=2)\n",
      "342/25: principalComponents = pca.fit_transform(x)\n",
      "342/26:\n",
      "principalDf = pd.DataFrame(data = principalComponents\n",
      "             , columns = ['principal component 1', 'principal component 2'])\n",
      "342/27: principalDf.head(5)\n",
      "342/28: df[['Bioactivity_class_amylase']].head()\n",
      "342/29: pd.DataFrame(data = x, columns = features, index=True).head()\n",
      "342/30: indices = df.index\n",
      "342/31: pd.DataFrame(data = x, columns = features, index=indices).head()\n",
      "342/32: pd.DataFrame(data = x, columns = features, index=indices).head()\n",
      "342/33: indices = df.index\n",
      "342/34: df.head(2)\n",
      "342/35: df = pd.read_csv('df_clean.csv', indices)\n",
      "342/36: df = pd.read_csv('df_clean.csv', index_col=Name_Short)\n",
      "342/37: df = pd.read_csv('df_clean.csv', index_col='Name_Short')\n",
      "342/38: df.head(2)\n",
      "342/39: indices = df.index\n",
      "342/40: pd.DataFrame(data = x, columns = features, index=indices).head()\n",
      "342/41: principalDf\n",
      "342/42:\n",
      "principalDf = pd.DataFrame(data = principalComponents\n",
      "             , columns = ['principal component 1', 'principal component 2'], index=indices)\n",
      "342/43: principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'], index=indices)\n",
      "342/44: principalDf\n",
      "342/45: principalDf.head(3)\n",
      "342/46: df[['Bioactivity_class_amylase'], index=indices].head()\n",
      "342/47: df[['Bioactivity_class_amylase']].head()\n",
      "342/48: Bioactivity_class_amylase = df[['Bioactivity_class_amylase']].head()\n",
      "342/49: finalDf = pd.concat([principalDf, Bioactivity_class_amylase], axis = 1)\n",
      "342/50:\n",
      "finalDf = pd.concat([principalDf, Bioactivity_class_amylase], axis = 1)\n",
      "finalDf.head(5)\n",
      "342/51: principalComponents\n",
      "342/52: principalComponents.explained_variance_ratio_\n",
      "342/53: pca.explained_variance_ratio_\n",
      "342/54:\n",
      "fig = plt.figure(figsize = (8,8))\n",
      "ax = fig.add_subplot(1,1,1) \n",
      "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
      "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
      "ax.set_title('2 Component PCA', fontsize = 20)\n",
      "\n",
      "\n",
      "targets = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n",
      "colors = ['r', 'g', 'b']\n",
      "for target, color in zip(targets,colors):\n",
      "    indicesToKeep = finalDf['target'] == target\n",
      "    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
      "               , finalDf.loc[indicesToKeep, 'principal component 2']\n",
      "               , c = color\n",
      "               , s = 50)\n",
      "ax.legend(targets)\n",
      "ax.grid()\n",
      "342/55:\n",
      "fig = plt.figure(figsize = (8,8))\n",
      "ax = fig.add_subplot(1,1,1) \n",
      "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
      "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
      "ax.set_title('2 Component PCA', fontsize = 20)\n",
      "\n",
      "\n",
      "Bioactivity_class_amylase = ['active', 'inactive']\n",
      "colors = ['r', 'g']\n",
      "for target, color in zip(targets,colors):\n",
      "    indicesToKeep = finalDf['target'] == target\n",
      "    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
      "               , finalDf.loc[indicesToKeep, 'principal component 2']\n",
      "               , c = color\n",
      "               , s = 50)\n",
      "ax.legend(targets)\n",
      "ax.grid()\n",
      "342/56:\n",
      "fig = plt.figure(figsize = (8,8))\n",
      "ax = fig.add_subplot(1,1,1) \n",
      "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
      "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
      "ax.set_title('2 Component PCA', fontsize = 20)\n",
      "\n",
      "\n",
      "Bioactivity_class_amylase = ['active', 'inactive']\n",
      "colors = ['r', 'g']\n",
      "for target, color in zip(targets,colors):\n",
      "    indicesToKeep = finalDf['Bioactivity_class_amylase'] == Bioactivity_class_amylase\n",
      "    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
      "               , finalDf.loc[indicesToKeep, 'principal component 2']\n",
      "               , c = color\n",
      "               , s = 50)\n",
      "ax.legend(targets)\n",
      "ax.grid()\n",
      "342/57:\n",
      "fig = plt.figure(figsize = (8,8))\n",
      "ax = fig.add_subplot(1,1,1) \n",
      "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
      "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
      "ax.set_title('2 Component PCA', fontsize = 20)\n",
      "\n",
      "\n",
      "Bioactivity_class_amylase = ['active', 'inactive']\n",
      "colors = ['r', 'g']\n",
      "for Bioactivity_class_amylase, color in zip(Bioactivity_class_amylase,colors):\n",
      "    indicesToKeep = finalDf['Bioactivity_class_amylase'] == Bioactivity_class_amylase\n",
      "    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
      "               , finalDf.loc[indicesToKeep, 'principal component 2']\n",
      "               , c = color\n",
      "               , s = 50)\n",
      "ax.legend(targets)\n",
      "ax.grid()\n",
      "342/58:\n",
      "fig = plt.figure(figsize = (8,8))\n",
      "ax = fig.add_subplot(1,1,1) \n",
      "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
      "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
      "ax.set_title('2 Component PCA', fontsize = 20)\n",
      "\n",
      "\n",
      "Bioactivity_class_amylase = ['active', 'inactive']\n",
      "colors = ['r', 'g']\n",
      "for Bioactivity_class_amylase, color in zip(Bioactivity_class_amylase,colors):\n",
      "    indicesToKeep = finalDf['Bioactivity_class_amylase'] == Bioactivity_class_amylase\n",
      "    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
      "               , finalDf.loc[indicesToKeep, 'principal component 2']\n",
      "               , c = color\n",
      "               , s = 50)\n",
      "ax.legend(Bioactivity_class_amylase)\n",
      "ax.grid()\n",
      "342/59: pca = PCA(n_components=5)\n",
      "342/60: principalComponents = pca.fit_transform(x)\n",
      "342/61: pca.explained_variance_ratio_\n",
      "342/62: pca = PCA(n_components=7)\n",
      "342/63: principalComponents = pca.fit_transform(x)\n",
      "342/64: pca.explained_variance_ratio_\n",
      "342/65: from mpl_toolkits import mplot3d\n",
      "342/66:\n",
      "fig = plt.figure()\n",
      "ax = plt.axes(projection='3d')\n",
      "342/67: pca = PCA(n_components=5)\n",
      "342/68: principalComponents = pca.fit_transform(x)\n",
      "342/69: pca.explained_variance_ratio_\n",
      "342/70: principalComponents\n",
      "342/71: principalComponents.argmax\n",
      "342/72: principalComponents.cumsum\n",
      "342/73: principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2', 'principal component 3', 'principal component 4', 'principal component 5'], index=indices)\n",
      "342/74: principalDf.head(3)\n",
      "342/75:\n",
      "finalDf = pd.concat([principalDf, Bioactivity_class_amylase], axis = 1)\n",
      "finalDf.head(5)\n",
      "342/76: Bioactivity_class_amylase = df[['Bioactivity_class_amylase']].head()\n",
      "342/77:\n",
      "finalDf = pd.concat([principalDf, Bioactivity_class_amylase], axis = 1)\n",
      "finalDf.head(5)\n",
      "342/78: sns.pairplot(principalDf)\n",
      "342/79: sns.pairplot(finalDf, hue=\"Bioactivity_class_amylase\")\n",
      "342/80:\n",
      "finalDf = pd.concat([principalDf, Bioactivity_class_amylase], axis = 1)\n",
      "finalDf\n",
      "342/81: Bioactivity_class_amylase = df[['Bioactivity_class_amylase']]\n",
      "342/82:\n",
      "finalDf = pd.concat([principalDf, Bioactivity_class_amylase], axis = 1)\n",
      "finalDf\n",
      "342/83: sns.pairplot(finalDf, hue=\"Bioactivity_class_amylase\")\n",
      "342/84: sns.pairplot(finalDf, hue=\"Bioactivity_class_amylase\", height=1.5)\n",
      "342/85: sns.pairplot(finalDf, hue=\"Bioactivity_class_amylase\", height=2)\n",
      "342/86:\n",
      "sns.scatterplot(x=\"principal component 1\", y=\"principal component 2\",\n",
      "                hue=\"Bioactivity_class_amylase\", \n",
      "                sizes=(1, 8), linewidth=0,\n",
      "                data=finalDf, ax=ax)\n",
      "342/87: sns.scatterplot(finalDf, x=\"principal component 1\", y=\"principal component 2\", hue=\"Bioactivity_class_amylase\")\n",
      "342/88: sns.scatterplot(finalDf, x=\"principal component 1\", y=\"principal component 3\", hue=\"Bioactivity_class_amylase\")\n",
      "342/89: sns.pairplot(finalDf, x_vars=['principal component 1'], y_vars=['principal component 2'] hue=\"Bioactivity_class_amylase\", height=2)\n",
      "342/90: sns.pairplot(finalDf, x_vars=['principal component 1'], y_vars=['principal component 2'], hue=\"Bioactivity_class_amylase\", height=2)\n",
      "342/91: sns.pairplot(finalDf, x_vars=['principal component 1'], y_vars=['principal component 2'], hue=\"Bioactivity_class_amylase\")\n",
      "342/92:\n",
      "# Set a / KMeans clustering\n",
      "kmeans = KMeans(n_clusters=2)\n",
      "# Compute cluster centers and predict cluster indices\n",
      "X_clustered = kmeans.fit_predict(principalComponents)\n",
      "\n",
      "# Define our own color map\n",
      "LABEL_COLOR_MAP = {active : 'r',inactive : 'g',2 : 'b'}\n",
      "label_color = [LABEL_COLOR_MAP[l] for l in X_clustered]\n",
      "\n",
      "# Plot the scatter digram\n",
      "plt.figure(figsize = (7,7))\n",
      "plt.scatter(principalComponents[:,0],principalComponents[:,2], c= label_color, alpha=0.5) \n",
      "plt.show()\n",
      "342/93:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cluster import KMeans\n",
      "342/94:\n",
      "# Set a / KMeans clustering\n",
      "kmeans = KMeans(n_clusters=2)\n",
      "# Compute cluster centers and predict cluster indices\n",
      "X_clustered = kmeans.fit_predict(principalComponents)\n",
      "\n",
      "# Define our own color map\n",
      "LABEL_COLOR_MAP = {active : 'r',inactive : 'g',2 : 'b'}\n",
      "label_color = [LABEL_COLOR_MAP[l] for l in X_clustered]\n",
      "\n",
      "# Plot the scatter digram\n",
      "plt.figure(figsize = (7,7))\n",
      "plt.scatter(principalComponents[:,0],principalComponents[:,2], c= label_color, alpha=0.5) \n",
      "plt.show()\n",
      "342/95:\n",
      "# Set a / KMeans clustering\n",
      "kmeans = KMeans(n_clusters=2)\n",
      "# Compute cluster centers and predict cluster indices\n",
      "X_clustered = kmeans.fit_predict(principalComponents)\n",
      "\n",
      "\n",
      "# Plot the scatter digram\n",
      "plt.figure(figsize = (7,7))\n",
      "plt.scatter(principalComponents[:,0],principalComponents[:,2], c= Bioactivity_class_amylase, alpha=0.5) \n",
      "plt.show()\n",
      "341/40:\n",
      "fig, ax = plt.subplots(figsize=(10,10)) \n",
      "ax = sns.heatmap(Res_pearson, annot=True, cmap='Spectral', fmt='.2f', linecolor='black')\n",
      "plt.tight_layout()\n",
      "plt.savefig('corr_pearson_number.pdf', dpi=300)\n",
      "342/96:\n",
      "fig, ax = plt.subplots(figsize=(10,10)) \n",
      "ax = sns.pairplot(finalDf, hue=\"Bioactivity_class_amylase\", height=2)\n",
      "plt.tight_layout()\n",
      "plt.savefig('PCA_pairplot.pdf', dpi=300)\n",
      "342/97:\n",
      "ax = sns.pairplot(finalDf, hue=\"Bioactivity_class_amylase\", height=2)\n",
      "plt.tight_layout()\n",
      "plt.savefig('PCA_pairplot.pdf', dpi=300)\n",
      "342/98: sns.pairplot(finalDf, hue=\"Bioactivity_class_amylase\", height=2)\n",
      "342/99:\n",
      "sns.pairplot(finalDf, hue=\"Bioactivity_class_amylase\", height=2)\n",
      "plt.savefig('PCA_pairplot.pdf', dpi=300)\n",
      "342/100:\n",
      "# Set a / KMeans clustering\n",
      "kmeans = KMeans(n_clusters=2)\n",
      "# Compute cluster centers and predict cluster indices\n",
      "X_clustered = kmeans.fit_predict(principalComponents)\n",
      "\n",
      "# Plot the scatter digram\n",
      "plt.figure(figsize = (7,7))\n",
      "plt.scatter(principalComponents[:,0],principalComponents[:,2], alpha=0.5) \n",
      "plt.show()\n",
      "342/101:\n",
      "# Set a / KMeans clustering\n",
      "kmeans = KMeans(n_clusters=2)\n",
      "# Compute cluster centers and predict cluster indices\n",
      "X_clustered = kmeans.fit_predict(principalComponents)\n",
      "\n",
      "# Plot the scatter digram\n",
      "plt.figure(figsize = (7,7))\n",
      "sns.scatter(principalComponents[:,0],principalComponents[:,2], hue=\"Bioactivity_class_amylase\", alpha=0.5) \n",
      "plt.show()\n",
      "342/102:\n",
      "# Set a / KMeans clustering\n",
      "kmeans = KMeans(n_clusters=2)\n",
      "# Compute cluster centers and predict cluster indices\n",
      "X_clustered = kmeans.fit_predict(principalComponents)\n",
      "\n",
      "# Plot the scatter digram\n",
      "plt.figure(figsize = (7,7))\n",
      "sns.scatterplot(principalComponents[:,0],principalComponents[:,2], hue=finalDf[\"Bioactivity_class_amylase\"], alpha=0.5) \n",
      "plt.show()\n",
      "342/103:\n",
      "# Set a / KMeans clustering\n",
      "kmeans = KMeans(n_clusters=2)\n",
      "# Compute cluster centers and predict cluster indices\n",
      "X_clustered = kmeans.fit_predict(principalComponents)\n",
      "\n",
      "# Plot the scatter digram\n",
      "plt.figure(figsize = (7,7))\n",
      "sns.scatterplot(principalComponents[:,0],principalComponents[:,2], hue=finalDf[\"Bioactivity_class_amylase\"], alpha=1) \n",
      "plt.show()\n",
      "342/104:\n",
      "plt.plot(range(0,5), pca.explained_variance_ratio_)\n",
      "plt.ylabel('Explained Variance')\n",
      "plt.xlabel('Principal Components')\n",
      "plt.xticks(range(0,3),\n",
      "           [\"1st comp\", \"2nd comp\", \"3rd comp\"], rotation=60)\n",
      "plt.title('Explained Variance Ratio')\n",
      "plt.show()\n",
      "342/105:\n",
      "plt.plot(range(0,5), pca.explained_variance_ratio_)\n",
      "plt.ylabel('Explained Variance')\n",
      "plt.xlabel('Principal Components')\n",
      "plt.xticks(range(0,3),\n",
      "           ['principal component 1', 'principal component 2', 'principal component 3', 'principal component 4', 'principal component 5'], rotation=60)\n",
      "plt.title('Explained Variance Ratio')\n",
      "plt.show()\n",
      "342/106:\n",
      "plt.plot(range(0,5), pca.explained_variance_ratio_)\n",
      "plt.ylabel('Explained Variance')\n",
      "plt.xlabel('Principal Components')\n",
      "plt.xticks(range(0,5),\n",
      "           ['principal component 1', 'principal component 2', 'principal component 3', 'principal component 4', 'principal component 5'], rotation=60)\n",
      "plt.title('Explained Variance Ratio')\n",
      "plt.show()\n",
      "342/107:\n",
      "plt.plot(range(0,5), pca.explained_variance_ratio_)\n",
      "plt.ylabel('Explained Variance')\n",
      "plt.xlabel('Principal Components')\n",
      "plt.xticks(range(0,5),\n",
      "           ['principal component 1', 'principal component 2', 'principal component 3', 'principal component 4', 'principal component 5'], rotation=90)\n",
      "plt.title('Explained Variance Ratio')\n",
      "plt.show()\n",
      "342/108:\n",
      "plt.plot(range(0,5), pca.explained_variance_ratio_)\n",
      "plt.ylabel('Explained Variance')\n",
      "plt.xlabel('Principal Components')\n",
      "plt.xticks(range(0,5),\n",
      "           ['PC 1', 'PC 2', 'PC 3', 'PC 4', 'PC 5'], rotation=90)\n",
      "plt.title('Explained Variance Ratio')\n",
      "plt.show()\n",
      "342/109:\n",
      "plt.plot(range(0,5), pca.explained_variance_ratio_)\n",
      "plt.ylabel('Explained Variance')\n",
      "plt.xlabel('Principal Components')\n",
      "plt.xticks(range(0,5),\n",
      "           ['PC 1', 'PC 2', 'PC 3', 'PC 4', 'PC 5'], rotation=0)\n",
      "plt.title('Explained Variance Ratio')\n",
      "plt.show()\n",
      "342/110:\n",
      "# Calculation of Explained Variance from the eigenvalues\n",
      "tot = sum(pca.explained_variance_ratio_)\n",
      "var_exp = [(i/tot)*100 for i in sorted(eig_vals, reverse=True)] # Individual explained variance\n",
      "cum_var_exp = np.cumsum(var_exp) # Cumulative explained variance\n",
      "342/111:\n",
      "# Calculation of Explained Variance from the eigenvalues\n",
      "tot = sum(pca.explained_variance_ratio_)\n",
      "var_exp = [(i/tot)*100 for i in sorted(pca.explained_variance_ratio_, reverse=True)] # Individual explained variance\n",
      "cum_var_exp = np.cumsum(var_exp) # Cumulative explained variance\n",
      "342/112:\n",
      "plt.figure(figsize=(8, 5))\n",
      "plt.bar(range(16), var_exp, alpha=0.3333, align='center', label='individual explained variance', color = 'g')\n",
      "plt.step(range(16), cum_var_exp, where='mid',label='cumulative explained variance')\n",
      "plt.ylabel('Explained variance ratio')\n",
      "plt.xlabel('Principal components')\n",
      "plt.legend(loc='best')\n",
      "plt.show()\n",
      "342/113:\n",
      "plt.figure(figsize=(8, 5))\n",
      "plt.bar(range(5), var_exp, alpha=0.3333, align='center', label='individual explained variance', color = 'g')\n",
      "plt.step(range(5), cum_var_exp, where='mid',label='cumulative explained variance')\n",
      "plt.ylabel('Explained variance ratio')\n",
      "plt.xlabel('Principal components')\n",
      "plt.legend(loc='best')\n",
      "plt.show()\n",
      "342/114:\n",
      "plt.figure(figsize=(8, 5))\n",
      "plt.bar(range(5), var_exp, alpha=0.3333, align='center', label='individual explained variance', color = 'g')\n",
      "plt.step(range(5), cum_var_exp, where='mid',label='cumulative explained variance')\n",
      "plt.ylabel('Explained variance ratio')\n",
      "plt.xlabel('Principal components')\n",
      "plt.xticks(range(0,5),\n",
      "           ['PC 1', 'PC 2', 'PC 3', 'PC 4', 'PC 5'], rotation=0)\n",
      "plt.legend(loc='best')\n",
      "plt.show()\n",
      "342/115:\n",
      "plt.figure(figsize=(8, 5))\n",
      "plt.bar(range(5), var_exp, alpha=0.3333, align='center', label='individual explained variance', color = 'g')\n",
      "plt(range(5), cum_var_exp, where='mid',label='cumulative explained variance')\n",
      "plt.ylabel('Explained variance ratio')\n",
      "plt.xlabel('Principal components')\n",
      "plt.xticks(range(0,5),\n",
      "           ['PC 1', 'PC 2', 'PC 3', 'PC 4', 'PC 5'], rotation=0)\n",
      "plt.legend(loc='best')\n",
      "plt.show()\n",
      "342/116:\n",
      "plt.figure(figsize=(8, 5))\n",
      "plt.bar(range(5), var_exp, alpha=0.3333, align='center', label='individual explained variance', color = 'g')\n",
      "plt.plot(range(5), cum_var_exp, where='mid',label='cumulative explained variance')\n",
      "plt.ylabel('Explained variance ratio')\n",
      "plt.xlabel('Principal components')\n",
      "plt.xticks(range(0,5),\n",
      "           ['PC 1', 'PC 2', 'PC 3', 'PC 4', 'PC 5'], rotation=0)\n",
      "plt.legend(loc='best')\n",
      "plt.show()\n",
      "342/117:\n",
      "plt.figure(figsize=(8, 5))\n",
      "plt.bar(range(5), var_exp, alpha=0.3333, align='center', label='individual explained variance', color = 'g')\n",
      "plt.plot(cum_var_exp, where='mid',label='cumulative explained variance')\n",
      "plt.ylabel('Explained variance ratio')\n",
      "plt.xlabel('Principal components')\n",
      "plt.xticks(range(0,5),\n",
      "           ['PC 1', 'PC 2', 'PC 3', 'PC 4', 'PC 5'], rotation=0)\n",
      "plt.legend(loc='best')\n",
      "plt.show()\n",
      "342/118:\n",
      "plt.figure(figsize=(8, 5))\n",
      "plt.bar(range(5), var_exp, alpha=0.3333, align='center', label='individual explained variance', color = 'g')\n",
      "plt.step(range(5), cum_var_exp, where='mid',label='cumulative explained variance')\n",
      "plt.ylabel('Explained variance ratio')\n",
      "plt.xlabel('Principal components')\n",
      "plt.xticks(range(0,5),\n",
      "           ['PC 1', 'PC 2', 'PC 3', 'PC 4', 'PC 5'], rotation=0)\n",
      "plt.legend(loc='best')\n",
      "plt.show()\n",
      "342/119:\n",
      "plt.figure(figsize=(8, 5))\n",
      "plt.bar(range(5), var_exp, alpha=0.3333, align='center', label='individual explained variance', color = 'g')\n",
      "ax = sns.plot(cum_var_exp, where='mid',label='cumulative explained variance')\n",
      "plt.ylabel('Explained variance ratio')\n",
      "plt.xlabel('Principal components')\n",
      "plt.xticks(range(0,5),\n",
      "           ['PC 1', 'PC 2', 'PC 3', 'PC 4', 'PC 5'], rotation=0)\n",
      "plt.legend(loc='best')\n",
      "plt.show()\n",
      "342/120:\n",
      "plt.figure(figsize=(8, 5))\n",
      "plt.bar(range(5), var_exp, alpha=0.3333, align='center', label='individual explained variance', color = 'g')\n",
      "ax = sns.lineplot(cum_var_exp, where='mid',label='cumulative explained variance')\n",
      "plt.ylabel('Explained variance ratio')\n",
      "plt.xlabel('Principal components')\n",
      "plt.xticks(range(0,5),\n",
      "           ['PC 1', 'PC 2', 'PC 3', 'PC 4', 'PC 5'], rotation=0)\n",
      "plt.legend(loc='best')\n",
      "plt.show()\n",
      "342/121:\n",
      "plt.figure(figsize=(8, 5))\n",
      "plt.bar(range(5), var_exp, alpha=0.3333, align='center', label='individual explained variance', color = 'g')\n",
      "ax = sns.lineplot(cum_var_exp,label='cumulative explained variance')\n",
      "plt.ylabel('Explained variance ratio')\n",
      "plt.xlabel('Principal components')\n",
      "plt.xticks(range(0,5),\n",
      "           ['PC 1', 'PC 2', 'PC 3', 'PC 4', 'PC 5'], rotation=0)\n",
      "plt.legend(loc='best')\n",
      "plt.show()\n",
      "342/122:\n",
      "plt.figure(figsize=(8, 5))\n",
      "plt.bar(range(5), var_exp, alpha=0.3333, align='center', label='individual explained variance', color = 'g')\n",
      "ax = sns.lineplot(range(5), cum_var_exp,label='cumulative explained variance')\n",
      "plt.ylabel('Explained variance ratio')\n",
      "plt.xlabel('Principal components')\n",
      "plt.xticks(range(0,5),\n",
      "           ['PC 1', 'PC 2', 'PC 3', 'PC 4', 'PC 5'], rotation=0)\n",
      "plt.legend(loc='best')\n",
      "plt.show()\n",
      "342/123:\n",
      "plt.figure(figsize=(8, 5))\n",
      "plt.bar(range(5), var_exp, alpha=0.3333, align='center', label='individual explained variance', color = 'g')\n",
      "plt.plot(range(5), cum_var_exp,label='cumulative explained variance')\n",
      "plt.ylabel('Explained variance ratio')\n",
      "plt.xlabel('Principal components')\n",
      "plt.xticks(range(0,5),\n",
      "           ['PC 1', 'PC 2', 'PC 3', 'PC 4', 'PC 5'], rotation=0)\n",
      "plt.legend(loc='best')\n",
      "plt.show()\n",
      "342/124:\n",
      "plt.figure(figsize=(8, 5))\n",
      "plt.bar(range(5), var_exp, alpha=0.5, align='center', label='individual explained variance', color = 'g')\n",
      "plt.plot(range(5), cum_var_exp,label='cumulative explained variance')\n",
      "plt.ylabel('Explained variance ratio')\n",
      "plt.xlabel('Principal components')\n",
      "plt.xticks(range(0,5),\n",
      "           ['PC 1', 'PC 2', 'PC 3', 'PC 4', 'PC 5'], rotation=0)\n",
      "plt.legend(loc='best')\n",
      "plt.show()\n",
      "342/125:\n",
      "# Calculation of Explained Variance from the eigenvalues\n",
      "tot = sum(pca.explained_variance_ratio_)\n",
      "var_exp = [(i/tot)*100 for i in sorted(pca.explained_variance_ratio_, reverse=True)] # Individual explained variance\n",
      "cum_var_exp = np.cumsum(var_exp) # Cumulative explained variance\n",
      "cum_var_exp\n",
      "342/126:\n",
      "# Calculation of Explained Variance from the eigenvalues\n",
      "tot = sum(pca.explained_variance_ratio_)\n",
      "var_exp = [(i/tot)*100 for i in sorted(pca.explained_variance_ratio_, reverse=True)] # Individual explained variance\n",
      "cum_var_exp = np.cumsum(var_exp) # Cumulative explained variance\n",
      "cum_var_exp\n",
      "342/127:\n",
      "# Calculation of Explained Variance from the eigenvalues\n",
      "tot = sum(pca.explained_variance_ratio_)\n",
      "var_exp = [(i/tot)*100 for i in sorted(pca.explained_variance_ratio_, reverse=True)] # Individual explained variance\n",
      "cum_var_exp = np.cumsum(var_exp) # Cumulative explained variance\n",
      "cum_var_exp.T\n",
      "342/128: cum_var_exp.T\n",
      "342/129: cum_var_exp.transpose()\n",
      "342/130: pd.DataFrame(cum_var_exp, columns=['PC'])\n",
      "342/131: pd.DataFrame(cum_var_exp, columns=['PC'], index=['PC 1', 'PC 2', 'PC 3', 'PC 4', 'PC 5'])\n",
      "342/132:\n",
      "SumExplained = pd.DataFrame(cum_var_exp, columns=['PC'], index=['PC 1', 'PC 2', 'PC 3', 'PC 4', 'PC 5'])\n",
      "SumExplained\n",
      "342/133:\n",
      "finalDf = pd.concat([principalDf, Bioactivity_class_amylase], axis = 1)\n",
      "finalDf.head(3)\n",
      "342/134: Bioactivity_class_glucosidase = df[['Bioactivity_class_glucosidase']]\n",
      "342/135:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cluster import KMeans \n",
      "from mpl_toolkits import mplot3d\n",
      "342/136:\n",
      "plt.figure(figsize=(8, 5))\n",
      "plt.bar(range(5), var_exp, alpha=0.5, align='center', label='individual explained variance', color = 'g')\n",
      "plt.plot(range(5), cum_var_exp,label='cumulative explained variance')\n",
      "plt.ylabel('Explained variance ratio')\n",
      "plt.xlabel('Principal components')\n",
      "plt.xticks(range(0,5),\n",
      "           ['PC 1', 'PC 2', 'PC 3', 'PC 4', 'PC 5'], rotation=0)\n",
      "plt.legend(loc='best')\n",
      "plt.savefig('PCA_explained_variance.pdf', dpi=300)\n",
      "plt.show()\n",
      "342/137: sns.pairplot(finalDf, x_vars=['principal component 1'], y_vars=['principal component 2'], hue=\"Index\")\n",
      "342/138: sns.pairplot(finalDf, x_vars=['principal component 1'], y_vars=['principal component 2'], hue=index)\n",
      "342/139: sns.pairplot(finalDf, x_vars=['principal component 1'], y_vars=['principal component 2'], hue=indices)\n",
      "342/140: sns.pairplot(finalDf, x_vars=['principal component 1'], y_vars=['principal component 2'], hue=indices)\n",
      "342/141:\n",
      "# Calculation of Explained Variance from the eigenvalues\n",
      "tot = sum(pca.explained_variance_ratio_)\n",
      "var_exp = [(i/tot)*100 for i in sorted(pca.explained_variance_ratio_, reverse=True)] # Individual explained variance\n",
      "cum_var_exp = np.cumsum(var_exp) # Cumulative explained variance\n",
      "342/142: sns.pairplot(finalDf, x_vars=['principal component 1'], y_vars=['principal component 2'], hue='indices')\n",
      "342/143:\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(projection='3d')\n",
      "ax.scatter(finalDf['principal component 1', 'principal component 2', 'principal component 3', marker=m)\n",
      "ax.set_xlabel('PC 1')\n",
      "ax.set_ylabel('PC 2')\n",
      "ax.set_zlabel('PC 3')\n",
      "\n",
      "plt.show()\n",
      "342/144:\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(projection='3d')\n",
      "ax.scatter(finalDf['principal component 1', 'principal component 2', 'principal component 3')\n",
      "ax.set_xlabel('PC 1')\n",
      "ax.set_ylabel('PC 2')\n",
      "ax.set_zlabel('PC 3')\n",
      "\n",
      "plt.show()\n",
      "342/145:\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(projection='3d')\n",
      "ax.scatter(finalDf['principal component 1', 'principal component 2', 'principal component 3'])\n",
      "ax.set_xlabel('PC 1')\n",
      "ax.set_ylabel('PC 2')\n",
      "ax.set_zlabel('PC 3')\n",
      "\n",
      "plt.show()\n",
      "342/146:\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(projection='3d')\n",
      "ax.scatter(finalDf[['principal component 1', 'principal component 2', 'principal component 3']])\n",
      "ax.set_xlabel('PC 1')\n",
      "ax.set_ylabel('PC 2')\n",
      "ax.set_zlabel('PC 3')\n",
      "\n",
      "plt.show()\n",
      "342/147:\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(projection='3d')\n",
      "ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'])\n",
      "ax.set_xlabel('PC 1')\n",
      "ax.set_ylabel('PC 2')\n",
      "ax.set_zlabel('PC 3')\n",
      "\n",
      "plt.show()\n",
      "342/148:\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(projection='3d')\n",
      "ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=finalDf['Bioactivity_class_amylase'])\n",
      "ax.set_xlabel('PC 1')\n",
      "ax.set_ylabel('PC 2')\n",
      "ax.set_zlabel('PC 3')\n",
      "\n",
      "plt.show()\n",
      "342/149:\n",
      "LABEL_COLOR_MAP = {active : 'blue',inactive : 'orange'}\n",
      "label_color = [LABEL_COLOR_MAP[l] for l in finalDf['Bioactivity_class_amylase']]\n",
      "342/150:\n",
      "LABEL_COLOR_MAP = {'active' : 'blue','inactive' : 'orange'}\n",
      "label_color = [LABEL_COLOR_MAP[l] for l in finalDf['Bioactivity_class_amylase']]\n",
      "342/151:\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(projection='3d')\n",
      "ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color)\n",
      "ax.set_xlabel('PC 1')\n",
      "ax.set_ylabel('PC 2')\n",
      "ax.set_zlabel('PC 3')\n",
      "\n",
      "plt.show()\n",
      "342/152:\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(projection='3d')\n",
      "ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color)\n",
      "ax.set_xlabel('PC 1')\n",
      "ax.set_ylabel('PC 2')\n",
      "ax.set_zlabel('PC 3')\n",
      "plt.figure(figsize=(8, 5))\n",
      "plt.show()\n",
      "342/153:\n",
      "fig = plt.figure(figsize=(8, 8))\n",
      "ax = fig.add_subplot(projection='3d')\n",
      "ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color)\n",
      "ax.set_xlabel('PC 1')\n",
      "ax.set_ylabel('PC 2')\n",
      "ax.set_zlabel('PC 3')\n",
      "\n",
      "plt.show()\n",
      "342/154:\n",
      "fig = plt.figure(figsize=(8, 8))\n",
      "ax = fig.add_subplot(projection='3d')\n",
      "ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s='100')\n",
      "ax.set_xlabel('PC 1')\n",
      "ax.set_ylabel('PC 2')\n",
      "ax.set_zlabel('PC 3')\n",
      "\n",
      "plt.show()\n",
      "342/155:\n",
      "fig = plt.figure(figsize=(8, 8))\n",
      "ax = fig.add_subplot(projection='3d')\n",
      "ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s='2')\n",
      "ax.set_xlabel('PC 1')\n",
      "ax.set_ylabel('PC 2')\n",
      "ax.set_zlabel('PC 3')\n",
      "\n",
      "plt.show()\n",
      "342/156:\n",
      "fig = plt.figure(figsize=(8, 8))\n",
      "ax = fig.add_subplot(projection='3d')\n",
      "ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=3)\n",
      "ax.set_xlabel('PC 1')\n",
      "ax.set_ylabel('PC 2')\n",
      "ax.set_zlabel('PC 3')\n",
      "\n",
      "plt.show()\n",
      "342/157:\n",
      "fig = plt.figure(figsize=(8, 8))\n",
      "ax = fig.add_subplot(projection='3d')\n",
      "ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)\n",
      "ax.set_xlabel('PC 1')\n",
      "ax.set_ylabel('PC 2')\n",
      "ax.set_zlabel('PC 3')\n",
      "\n",
      "plt.show()\n",
      "342/158:\n",
      "fig = plt.figure(figsize=(4, 4))\n",
      "ax = fig.add_subplot(projection='3d')\n",
      "ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)\n",
      "ax.set_xlabel('PC 1')\n",
      "ax.set_ylabel('PC 2')\n",
      "ax.set_zlabel('PC 3')\n",
      "ax.set_title('PCA')\n",
      "plt.show()\n",
      "342/159:\n",
      "fig = plt.figure(figsize=(6, 6))\n",
      "ax = fig.add_subplot(projection='3d')\n",
      "ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)\n",
      "ax.set_xlabel('PC 1')\n",
      "ax.set_ylabel('PC 2')\n",
      "ax.set_zlabel('PC 3')\n",
      "ax.set_title('PCA')\n",
      "plt.show()\n",
      "342/160:\n",
      "fig = plt.figure(figsize=(6, 6))\n",
      "ax = fig.add_subplot(projection='3d')\n",
      "ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)\n",
      "ax.set_xlabel('PC 1')\n",
      "ax.set_ylabel('PC 2')\n",
      "ax.set_zlabel('PC 3')\n",
      "ax.legend(LABEL_COLOR_MAP)\n",
      "ax.set_title('PCA')\n",
      "plt.show()\n",
      "342/161:\n",
      "fig = plt.figure(figsize=(6, 6))\n",
      "ax = fig.add_subplot(projection='3d')\n",
      "ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)\n",
      "ax.set_xlabel('PC 1')\n",
      "ax.set_ylabel('PC 2')\n",
      "ax.set_zlabel('PC 3')\n",
      "ax.legend('active' : 'blue','inactive' : 'orange')\n",
      "ax.set_title('PCA')\n",
      "plt.show()\n",
      "342/162:\n",
      "fig = plt.figure(figsize=(6, 6))\n",
      "ax = fig.add_subplot(projection='3d')\n",
      "ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)\n",
      "ax.set_xlabel('PC 1')\n",
      "ax.set_ylabel('PC 2')\n",
      "ax.set_zlabel('PC 3')\n",
      "ax.legend({'active': 'blue','inactive' : 'orange'})\n",
      "ax.set_title('PCA')\n",
      "plt.show()\n",
      "342/163:\n",
      "fig = plt.figure(figsize=(6, 6))\n",
      "ax = fig.add_subplot(projection='3d')\n",
      "ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)\n",
      "ax.set_xlabel('PC 1')\n",
      "ax.set_ylabel('PC 2')\n",
      "ax.set_zlabel('PC 3')\n",
      "ax.legend()\n",
      "ax.set_title('PCA')\n",
      "plt.show()\n",
      "342/164:\n",
      "fig = plt.figure(figsize=(6, 6))\n",
      "ax = fig.add_subplot(projection='3d')\n",
      "ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100, )\n",
      "ax.set_xlabel('PC 1')\n",
      "ax.set_ylabel('PC 2')\n",
      "ax.set_zlabel('PC 3')\n",
      "ax.legend(label_color)\n",
      "ax.set_title('PCA')\n",
      "plt.show()\n",
      "342/165:\n",
      "fig = plt.figure(figsize=(6, 6))\n",
      "ax = fig.add_subplot(projection='3d')\n",
      "ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)\n",
      "ax.set_xlabel('PC 1')\n",
      "ax.set_ylabel('PC 2')\n",
      "ax.set_zlabel('PC 3')\n",
      "ax.legend(['active' : 'blue','inactive' : 'orange'])\n",
      "ax.legend()\n",
      "ax.set_title('PCA')\n",
      "plt.show()\n",
      "342/166:\n",
      "fig = plt.figure(figsize=(6, 6))\n",
      "ax = fig.add_subplot(projection='3d')\n",
      "ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)\n",
      "ax.set_xlabel('PC 1')\n",
      "ax.set_ylabel('PC 2')\n",
      "ax.set_zlabel('PC 3')\n",
      "ax.legend(['active': 'blue','inactive': 'orange'])\n",
      "ax.set_title('PCA')\n",
      "plt.show()\n",
      "342/167:\n",
      "fig = plt.figure(figsize=(6, 6))\n",
      "ax = fig.add_subplot(projection='3d')\n",
      "ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)\n",
      "ax.set_xlabel('PC 1')\n",
      "ax.set_ylabel('PC 2')\n",
      "ax.set_zlabel('PC 3')\n",
      "ax.legend()\n",
      "ax.set_title('PCA')\n",
      "plt.show()\n",
      "342/168:\n",
      "fig = plt.figure(figsize=(6, 6))\n",
      "ax = fig.add_subplot(projection='3d')\n",
      "ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)\n",
      "ax.set_xlabel('PC 1')\n",
      "ax.set_ylabel('PC 2')\n",
      "ax.set_zlabel('PC 3')\n",
      "ax.legend(active, blue\n",
      "         )\n",
      "ax.set_title('PCA')\n",
      "plt.show()\n",
      "342/169:\n",
      "fig = plt.figure(figsize=(6, 6))\n",
      "ax = fig.add_subplot(projection='3d')\n",
      "ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)\n",
      "ax.set_xlabel('PC 1')\n",
      "ax.set_ylabel('PC 2')\n",
      "ax.set_zlabel('PC 3')\n",
      "ax.legend('active', 'inactive'\n",
      "         )\n",
      "ax.set_title('PCA')\n",
      "plt.show()\n",
      "342/170:\n",
      "fig = plt.figure(figsize=(6, 6))\n",
      "ax = fig.add_subplot(projection='3d')\n",
      "ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)\n",
      "ax.set_xlabel('PC 1')\n",
      "ax.set_ylabel('PC 2')\n",
      "ax.set_zlabel('PC 3')\n",
      "ax.legend()\n",
      "ax.set_title('PCA')\n",
      "plt.show()\n",
      "342/171:\n",
      "fig = plt.figure(figsize=(6, 6))\n",
      "ax = fig.add_subplot(projection='3d')\n",
      "ax = ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)\n",
      "ax.set_xlabel('PC 1')\n",
      "ax.set_ylabel('PC 2')\n",
      "ax.set_zlabel('PC 3')\n",
      "ax.legend(ax.legend_elements)\n",
      "ax.set_title('PCA')\n",
      "plt.show()\n",
      "342/172:\n",
      "fig = plt.figure(figsize=(6, 6))\n",
      "ax = fig.add_subplot(projection='3d')\n",
      "ax = ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)\n",
      "ax.set_xlabel('PC 1')\n",
      "ax.set_ylabel('PC 2')\n",
      "ax.set_zlabel('PC 3')\n",
      "ax.legend(ax.legend_elements{})\n",
      "ax.set_title('PCA')\n",
      "plt.show()\n",
      "342/173:\n",
      "fig = plt.figure(figsize=(6, 6))\n",
      "ax = fig.add_subplot(projection='3d')\n",
      "ax = ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)\n",
      "ax.set_xlabel('PC 1')\n",
      "ax.set_ylabel('PC 2')\n",
      "ax.set_zlabel('PC 3')\n",
      "ax.legend(ax.legend_elements())\n",
      "ax.set_title('PCA')\n",
      "plt.show()\n",
      "342/174:\n",
      "fig = plt.figure(figsize=(6, 6))\n",
      "ax = fig.add_subplot(projection='3d')\n",
      "ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)\n",
      "ax.set_xlabel('PC 1')\n",
      "ax.set_ylabel('PC 2')\n",
      "ax.set_zlabel('PC 3')\n",
      "ax.legend(ax.legend_elements())\n",
      "ax.set_title('PCA')\n",
      "plt.show()\n",
      "342/175:\n",
      "fig = plt.figure(figsize=(6, 6))\n",
      "ax = fig.add_subplot(projection='3d')\n",
      "ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)\n",
      "ax.set_xlabel('PC 1')\n",
      "ax.set_ylabel('PC 2')\n",
      "ax.set_zlabel('PC 3')\n",
      "plt.legend(ax.legend_elements())\n",
      "ax.set_title('PCA')\n",
      "plt.show()\n",
      "342/176: label_color\n",
      "342/177: LABEL_COLOR_MAP\n",
      "342/178:\n",
      "fig = plt.figure(figsize=(6, 6))\n",
      "ax = fig.add_subplot(projection='3d')\n",
      "ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)\n",
      "ax.set_xlabel('PC 1')\n",
      "ax.set_ylabel('PC 2')\n",
      "ax.set_zlabel('PC 3')\n",
      "plt.legend(LABEL_COLOR_MAP )\n",
      "ax.set_title('PCA')\n",
      "plt.show()\n",
      "342/179:\n",
      "fig = plt.figure(figsize=(6, 6))\n",
      "ax = fig.add_subplot(projection='3d')\n",
      "ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)\n",
      "ax.set_xlabel('PC 1')\n",
      "ax.set_ylabel('PC 2')\n",
      "ax.set_zlabel('PC 3')\n",
      "plt.legend(LABEL_COLOR_MAP, nrow = 2)\n",
      "ax.set_title('PCA')\n",
      "plt.show()\n",
      "342/180:\n",
      "fig = plt.figure(figsize=(6, 6))\n",
      "ax = fig.add_subplot(projection='3d')\n",
      "ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)\n",
      "ax.set_xlabel('PC 1')\n",
      "ax.set_ylabel('PC 2')\n",
      "ax.set_zlabel('PC 3')\n",
      "plt.legend(LABEL_COLOR_MAP, ncol = 2)\n",
      "ax.set_title('PCA')\n",
      "plt.show()\n",
      "342/181:\n",
      "fig = plt.figure(figsize=(6, 6))\n",
      "ax = fig.add_subplot(projection='3d')\n",
      "ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)\n",
      "ax.set_xlabel('PC 1')\n",
      "ax.set_ylabel('PC 2')\n",
      "ax.set_zlabel('PC 3')\n",
      "plt.legend(LABEL_COLOR_MAP)\n",
      "plt.legend(LABEL_COLOR_MAP)\n",
      "ax.set_title('PCA')\n",
      "plt.show()\n",
      "342/182:\n",
      "fig = plt.figure(figsize=(6, 6))\n",
      "ax = fig.add_subplot(projection='3d')\n",
      "ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)\n",
      "ax.set_xlabel('PC 1')\n",
      "ax.set_ylabel('PC 2')\n",
      "ax.set_zlabel('PC 3')\n",
      "plt.legend(LABEL_COLOR_MAP)\n",
      "ax.set_title('PCA')\n",
      "plt.show()\n",
      "342/183:\n",
      "fig = plt.figure(figsize=(6, 6))\n",
      "ax = fig.add_subplot(projection='3d')\n",
      "ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)\n",
      "ax.set_xlabel('PC 1')\n",
      "ax.set_ylabel('PC 2')\n",
      "ax.set_zlabel('PC 3')\n",
      "plt.legend(LABEL_COLOR_MAP, LABEL_COLOR_MAP)\n",
      "ax.set_title('PCA')\n",
      "plt.show()\n",
      "342/184:\n",
      "fig = plt.figure(figsize=(6, 6))\n",
      "ax = fig.add_subplot(projection='3d')\n",
      "ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)\n",
      "ax.set_xlabel('PC 1')\n",
      "ax.set_ylabel('PC 2')\n",
      "ax.set_zlabel('PC 3')\n",
      "plt.legend(LABEL_COLOR_MAP)\n",
      "ax.set_title('PCA')\n",
      "plt.show()\n",
      "342/185:\n",
      "fig = plt.figure(figsize=(6, 6))\n",
      "ax = fig.add_subplot(projection='3d')\n",
      "ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)\n",
      "ax.set_xlabel('PC 1')\n",
      "ax.set_ylabel('PC 2')\n",
      "ax.set_zlabel('PC 3')\n",
      "plt.legend(LABEL_COLOR_MAP)\n",
      "ax.set_title('PCA')\n",
      "plt.savefig('PCA_3D_amylase.pdf', dpi=300)\n",
      "plt.show()\n",
      "342/186:\n",
      "sns.pairplot(finalDf, hue=\"Bioactivity_class_amylase\", height=2)\n",
      "plt.savefig('PCA_pairplot_amylase.pdf', dpi=300)\n",
      "342/187:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cluster import KMeans \n",
      "from mpl_toolkits import mplot3d\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "342/188:\n",
      "scaler = StandardScaler()\n",
      "scaler.fit(X)\n",
      "X=scaler.transform(X)\n",
      "342/189:\n",
      "scaler = StandardScaler()\n",
      "scaler.fit(x)\n",
      "x=scaler.transform(x)\n",
      "342/190: pd.DataFrame(data = x, columns = features, index=indices).head()\n",
      "342/191: x = df.loc[:, features].values\n",
      "342/192: pd.DataFrame(data = x, columns = features, index=indices).head()\n",
      "344/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cluster import KMeans \n",
      "from mpl_toolkits import mplot3d\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "344/2: df = pd.read_csv('df_clean.csv', index_col='Name_Short')\n",
      "344/3: df.head(2)\n",
      "344/4: indices = df.index\n",
      "344/5: features = ['Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']\n",
      "344/6: x = df.loc[:, features].values\n",
      "344/7: y = df.loc[:,['Bioactivity_class_amylase']].values\n",
      "344/8: pd.DataFrame(data = x, columns = features, index=indices).head()\n",
      "344/9: pca = PCA(n_components=5)\n",
      "344/10: principalComponents = pca.fit_transform(x)\n",
      "344/11: pca.explained_variance_ratio_\n",
      "344/12: principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2', 'principal component 3', 'principal component 4', 'principal component 5'], index=indices)\n",
      "344/13: principalDf.head(3)\n",
      "344/14: Bioactivity_class_amylase = df[['Bioactivity_class_amylase']]\n",
      "344/15:\n",
      "finalDf = pd.concat([principalDf, Bioactivity_class_amylase], axis = 1)\n",
      "finalDf.head(3)\n",
      "344/16:\n",
      "sns.pairplot(finalDf, hue=\"Bioactivity_class_amylase\", height=2)\n",
      "plt.savefig('PCA_pairplot_amylase.pdf', dpi=300)\n",
      "344/17: sns.pairplot(finalDf, x_vars=['principal component 1'], y_vars=['principal component 2'], hue=\"Bioactivity_class_amylase\")\n",
      "344/18:\n",
      "# Calculation of Explained Variance from the eigenvalues\n",
      "tot = sum(pca.explained_variance_ratio_)\n",
      "var_exp = [(i/tot)*100 for i in sorted(pca.explained_variance_ratio_, reverse=True)] # Individual explained variance\n",
      "cum_var_exp = np.cumsum(var_exp) # Cumulative explained variance\n",
      "344/19:\n",
      "plt.figure(figsize=(8, 5))\n",
      "plt.bar(range(5), var_exp, alpha=0.5, align='center', label='individual explained variance', color = 'g')\n",
      "plt.plot(range(5), cum_var_exp,label='cumulative explained variance')\n",
      "plt.ylabel('Explained variance ratio')\n",
      "plt.xlabel('Principal components')\n",
      "plt.xticks(range(0,5),\n",
      "           ['PC 1', 'PC 2', 'PC 3', 'PC 4', 'PC 5'], rotation=0)\n",
      "plt.legend(loc='best')\n",
      "plt.savefig('PCA_explained_variance.pdf', dpi=300)\n",
      "plt.show()\n",
      "344/20:\n",
      "SumExplained = pd.DataFrame(cum_var_exp, columns=['PC'], index=['PC 1', 'PC 2', 'PC 3', 'PC 4', 'PC 5'])\n",
      "SumExplained\n",
      "344/21:\n",
      "LABEL_COLOR_MAP = {'active' : 'blue','inactive' : 'orange'}\n",
      "label_color = [LABEL_COLOR_MAP[l] for l in finalDf['Bioactivity_class_amylase']]\n",
      "344/22:\n",
      "fig = plt.figure(figsize=(6, 6))\n",
      "ax = fig.add_subplot(projection='3d')\n",
      "ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)\n",
      "ax.set_xlabel('PC 1')\n",
      "ax.set_ylabel('PC 2')\n",
      "ax.set_zlabel('PC 3')\n",
      "plt.legend(LABEL_COLOR_MAP)\n",
      "ax.set_title('PCA')\n",
      "plt.savefig('PCA_3D_amylase.pdf', dpi=300)\n",
      "plt.show()\n",
      "344/23: x = df.loc[:, features].values\n",
      "344/24: y = y = df.loc[:,['Bioactivity_class_glucosidase']].values\n",
      "344/25: y = df.loc[:,['Bioactivity_class_glucosidase']].values\n",
      "344/26: pca = PCA(n_components=5)\n",
      "344/27: principalComponents = pca.fit_transform(x)\n",
      "344/28: pca.explained_variance_ratio_\n",
      "344/29: principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2', 'principal component 3', 'principal component 4', 'principal component 5'], index=indices)\n",
      "344/30: Bioactivity_class_glucosidase = df[['Bioactivity_class_glucosidase']]\n",
      "344/31:\n",
      "finalDf = pd.concat([principalDf, Bioactivity_class_glucosidase], axis = 1)\n",
      "finalDf.head(3)\n",
      "344/32:\n",
      "sns.pairplot(finalDf, hue=\"Bioactivity_class_amylase\", height=2, palette=pal)\n",
      "plt.savefig('PCA_pairplot_amylase.pdf', dpi=300)\n",
      "344/33:\n",
      "sns.pairplot(finalDf, hue=\"Bioactivity_class_amylase\", height=2, palette=\"deep\")\n",
      "plt.savefig('PCA_pairplot_glucosidase.pdf', dpi=300)\n",
      "344/34:\n",
      "sns.pairplot(finalDf, hue=\"Bioactivity_class_glucosidase\", height=2, palette=\"deep\")\n",
      "plt.savefig('PCA_pairplot_glucosidase.pdf', dpi=300)\n",
      "344/35:\n",
      "sns.pairplot(finalDf, hue=\"Bioactivity_class_glucosidase\", height=2)\n",
      "plt.savefig('PCA_pairplot_glucosidase.pdf', dpi=300)\n",
      "344/36:\n",
      "LABEL_COLOR_MAP = {'active' : 'blue','inactive' : 'orange'}\n",
      "label_color = [LABEL_COLOR_MAP[l] for l in finalDf['Bioactivity_class_glucosidase']]\n",
      "344/37:\n",
      "fig = plt.figure(figsize=(6, 6))\n",
      "ax = fig.add_subplot(projection='3d')\n",
      "ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)\n",
      "ax.set_xlabel('PC 1')\n",
      "ax.set_ylabel('PC 2')\n",
      "ax.set_zlabel('PC 3')\n",
      "plt.legend(LABEL_COLOR_MAP)\n",
      "ax.set_title('PCA')\n",
      "plt.savefig('PCA_3D_glucosidase.pdf', dpi=300)\n",
      "plt.show()\n",
      "344/38:\n",
      "fig = plt.figure(figsize=(6, 6))\n",
      "ax = fig.add_subplot(projection='3d')\n",
      "ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)\n",
      "ax.set_xlabel('PC 1')\n",
      "ax.set_ylabel('PC 2')\n",
      "ax.set_zlabel('PC 3')\n",
      "plt.legend(LABEL_COLOR_MAP)\n",
      "ax.set_title('PCA_glucosidase')\n",
      "plt.savefig('PCA_3D_glucosidase.pdf', dpi=300)\n",
      "plt.show()\n",
      "344/39:\n",
      "LABEL_COLOR_MAP = {'active' : 'blue','inactive' : 'orange'}\n",
      "label_color = [LABEL_COLOR_MAP[l] for l in finalDf['Bioactivity_class_amylase']]\n",
      "350/1:\n",
      "import networkx as nx\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "G = nx.Graph()\n",
      "G.add_edge(1, 2)\n",
      "G.add_edge(1, 3)\n",
      "G.add_edge(1, 5)\n",
      "G.add_edge(2, 3)\n",
      "G.add_edge(3, 4)\n",
      "G.add_edge(4, 5)\n",
      "\n",
      "# explicitly set positions\n",
      "pos = {1: (0, 0), 2: (-1, 0.3), 3: (2, 0.17), 4: (4, 0.255), 5: (5, 0.03)}\n",
      "\n",
      "options = {\n",
      "    \"font_size\": 36,\n",
      "    \"node_size\": 3000,\n",
      "    \"node_color\": \"white\",\n",
      "    \"edgecolors\": \"black\",\n",
      "    \"linewidths\": 5,\n",
      "    \"width\": 5,\n",
      "}\n",
      "nx.draw_networkx(G, pos, **options)\n",
      "\n",
      "# Set margins for the axes so that nodes aren't clipped\n",
      "ax = plt.gca()\n",
      "ax.margins(0.20)\n",
      "plt.axis(\"off\")\n",
      "plt.show()\n",
      "350/2:\n",
      "import networkx as nx\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "G = nx.Graph()\n",
      "350/3:\n",
      "import networkx as nx\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "G = nx.Graph()\n",
      "G.add_edge(1, 2)\n",
      "350/4: G\n",
      "350/5: G,nodes\n",
      "350/6: G.nodes\n",
      "350/7:\n",
      "import networkx as nx\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "G = nx.Graph()\n",
      "G.add_edge(1, 2)\n",
      "G.add_edge(1, 3)\n",
      "G.add_edge(1, 5)\n",
      "G.add_edge(2, 3)\n",
      "G.add_edge(3, 4)\n",
      "G.add_edge(4, 5)\n",
      "350/8: nx.draw(G)\n",
      "350/9:\n",
      "import networkx as nx\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "350/10:\n",
      "import networkx as nx\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "350/11: df = pd.read_csv('df_clean.csv', index_col='Name_Short')\n",
      "350/12: df.head(2)\n",
      "350/13: df1 = df[['Name_Short', 'Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']]\n",
      "350/14: df = pd.read_csv('df_clean.csv')\n",
      "350/15: df.head(2)\n",
      "350/16: df1 = df[['Name_Short', 'Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']]\n",
      "350/17: df1\n",
      "350/18: df1 = df[['Emax_amylase_2.75mcgml','Emax_glucosidase_0.67mcgml','Name_Short', 'Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']]\n",
      "350/19: df1\n",
      "350/20: df1 = df[['Name_Short','Emax_amylase_2.75mcgml','Emax_glucosidase_0.67mcgml', 'Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']]\n",
      "350/21: df1\n",
      "350/22: df1 = df[['Name_Short','Emax_amylase_2.75mcgml','Emax_glucosidase_0.67mcgml', 'Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']]\n",
      "350/23: df1\n",
      "350/24: df1.head(3)\n",
      "350/25: G = nx.Graph()\n",
      "350/26: G = nx.from_pandas_edgelist(df1, 'Name_Short', 'Emax_amylase_2.75mcgml', edge_attr=True, edge_key='Emax_glucosidase_0.67mcgml')\n",
      "350/27:\n",
      "from matplotlib.pyplot import figurefigure(figsize=(10, 8))\n",
      "nx.draw_shell(G, with_labels=True)\n",
      "350/28:\n",
      "from matplotlib.pyplot import figure\n",
      "figure(figsize=(10, 8))\n",
      "nx.draw_shell(G, with_labels=True)\n",
      "350/29: G = nx.from_pandas_edgelist(df1, 'Name_Short', edge_attr='Emax_amylase_2.75mcgml', edge_key='Emax_glucosidase_0.67mcgml')\n",
      "350/30: G = nx.from_pandas_edgelist(df1, 'Name_Short', 'Name_Short', edge_attr='Emax_amylase_2.75mcgml', edge_key='Emax_glucosidase_0.67mcgml')\n",
      "350/31:\n",
      "from matplotlib.pyplot import figure\n",
      "figure(figsize=(10, 8))\n",
      "nx.draw_shell(G, with_labels=True)\n",
      "350/32: G = nx.from_pandas_edgelist(df1, 'Name_Short', 'Name_Short', ['Alkaloids', 'Antaquinones'])\n",
      "350/33:\n",
      "from matplotlib.pyplot import figure\n",
      "figure(figsize=(10, 8))\n",
      "nx.draw_shell(G, with_labels=True)\n",
      "350/34: G = nx.from_pandas_edgelist(df1, 'Name_Short', 'Carotenoids', ['Alkaloids', 'Antaquinones'])\n",
      "350/35:\n",
      "from matplotlib.pyplot import figure\n",
      "figure(figsize=(10, 8))\n",
      "nx.draw_shell(G, with_labels=True)\n",
      "350/36: df1 = df[['Name_Short','Bioactivity_class_amylase','Bioactivity_class_glucosidase', 'Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']]\n",
      "350/37: df1.head(3)\n",
      "350/38: G = nx.Graph()\n",
      "350/39: G = nx.from_pandas_edgelist(df1, 'Name_Short', 'Bioactivity_class_glucosidase', ['Alkaloids', 'Antaquinones'])\n",
      "350/40:\n",
      "from matplotlib.pyplot import figure\n",
      "figure(figsize=(10, 8))\n",
      "nx.draw_shell(G, with_labels=True)\n",
      "350/41: G = nx.from_pandas_edgelist(df1, 'Name_Short', 'Bioactivity_class_glucosidase', 'Bioactivity_class_amylase', ['Alkaloids', 'Antaquinones'])\n",
      "350/42: G = nx.from_pandas_edgelist(df1, 'Name_Short', 'Bioactivity_class_glucosidase', ['Alkaloids', 'Antaquinones'])\n",
      "350/43: df1 = df[['Name_Short','Bioactivity_class_amylase','Bioactivity_class_glucosidase', 'Emax_glucosidase_0.67mcgml', 'Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']]\n",
      "350/44: df1.head(3)\n",
      "350/45: G = nx.Graph()\n",
      "350/46: G = nx.from_pandas_edgelist(df1, 'Name_Short', 'Bioactivity_class_glucosidase', edge_attr='Emax_glucosidase_0.67mcgml')\n",
      "350/47:\n",
      "from matplotlib.pyplot import figure\n",
      "figure(figsize=(10, 8))\n",
      "nx.draw_shell(G, with_labels=True)\n",
      "350/48: G = nx.from_pandas_edgelist(df1, 'Name_Short', 'Bioactivity_class_glucosidase', edge_attr='flavonoids')\n",
      "350/49:\n",
      "from matplotlib.pyplot import figure\n",
      "figure(figsize=(10, 8))\n",
      "nx.draw_shell(G, with_labels=True)\n",
      "348/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "348/2: df = pd.read_csv('antihyperglycemia.csv')\n",
      "348/3: df\n",
      "348/4:\n",
      "df = df.drop(['Number', 'Name_Thai'], axis=1)\n",
      "df\n",
      "348/5:\n",
      "from scipy.stats import pearsonr\n",
      "rho = df.corr()\n",
      "pval = df.corr(method=lambda x, y: pearsonr(x, y)[1]) - np.eye(*rho.shape)\n",
      "p = pval.applymap(lambda x: ''.join(['*' for t in [0.001,0.01,0.05] if x<=t]))\n",
      "Res_pearson_sig = rho.round(3).astype(str) + p\n",
      "Res_pearson_sig\n",
      "348/6:\n",
      "Res_pearson = df.corr(method='pearson').round(3)\n",
      "Res_pearson\n",
      "348/7:\n",
      "Res_pearson = df.corr(method='pearson').round(3)\n",
      "Res_pearson.to_csv('Res_pearson.csv')\n",
      "Res_pearson\n",
      "350/50: df = pd.read_csv('Res_pearson.csv')\n",
      "350/51: df\n",
      "350/52: df['Unnamed: 0'] = df['List']\n",
      "350/53: df['Unnamed: 0': 'List']\n",
      "350/54: df\n",
      "350/55:\n",
      "from matplotlib.pyplot import figure\n",
      "figure(figsize=(10, 8))\n",
      "nx.draw_networkx_nodes(G, with_labels=True)\n",
      "350/56:\n",
      "from matplotlib.pyplot import figure\n",
      "figure(figsize=(10, 8))\n",
      "nx.draw_networkx_nodes(G)\n",
      "350/57:\n",
      "from matplotlib.pyplot import figure\n",
      "figure(figsize=(10, 8))\n",
      "nx.draw_shell(G, with_labels=True)\n",
      "350/58:\n",
      "import networkx as nx\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "350/59: networkx.draw(G, with_labels=True, font_weight='bold')\n",
      "350/60: nx.draw(G, with_labels=True, font_weight='bold')\n",
      "350/61: df = pd.read_csv('Res_pearson.csv')\n",
      "350/62: df\n",
      "350/63: df.rename(colums={'Unnamed: 0' : Name})\n",
      "350/64: df.rename(colums={'Unnamed: 0' : 'Name'})\n",
      "350/65: df.rename(columns={'Unnamed: 0' : 'Name'})\n",
      "350/66: G = nx.from_pandas_edgelist(df)\n",
      "348/8: rho\n",
      "348/9: pval\n",
      "348/10: rho\n",
      "348/11: val\n",
      "348/12: pal\n",
      "348/13: pval\n",
      "348/14: rho\n",
      "348/15: edges = rho.stack().reset_index()\n",
      "348/16:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy.stats\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import networkx as nx\n",
      "348/17: edges.columns = ['Features_1','Features_2','correlation']\n",
      "348/18:\n",
      "#convert matrix to list of edges and rename the columns\n",
      "edges = rho.stack().reset_index()\n",
      "edges.columns = ['Features_1','Features_2','correlation']\n",
      "348/19:\n",
      "#remove self correlations\n",
      "edges = edges.loc[edges['Features_1'] != edges['Features_2']].copy()\n",
      "348/20: edges.head()\n",
      "348/21:\n",
      "#create undirected graph with weights corresponding to the correlation magnitude\n",
      "G0 = nx.from_pandas_edgelist(edges, 'Features_1', 'Features_2', edge_attr=['correlation'])\n",
      "\n",
      "#print out the graph info\n",
      "#check number of nodes and degrees are as expected (all should have degree = 38, i.e. average degree = 38)\n",
      "print(nx.info(G0))\n",
      "348/22:\n",
      "fig, ax = plt.subplots(nrows=2, ncols=2,figsize=(20,20))\n",
      "\n",
      "nx.draw(G0, with_labels=True, node_size=700, node_color=\"#e1575c\",\n",
      "        edge_color='#363847',  pos=nx.circular_layout(G0),ax=ax[0,0])\n",
      "ax[0,0].set_title(\"Circular layout\")\n",
      "\n",
      "nx.draw(G0, with_labels=True, node_size=700, node_color=\"#e1575c\",\n",
      "        edge_color='#363847',  pos=nx.random_layout(G0),ax=ax[0,1])\n",
      "ax[0,1].set_title(\"Random layout\")\n",
      "\n",
      "nx.draw(G0, with_labels=True, node_size=700, node_color=\"#e1575c\",\n",
      "        edge_color='#363847',  pos=nx.spring_layout(G0),ax=ax[1,0])\n",
      "ax[1,0].set_title(\"Spring layout\")\n",
      "\n",
      "nx.draw(G0, with_labels=True, node_size=700, node_color=\"#e1575c\",\n",
      "        edge_color='#363847',  pos=nx.spectral_layout(G0),ax=ax[1,1])\n",
      "ax[1,1].set_title(\"Spectral layout\")\n",
      "\n",
      "plt.show()\n",
      "348/23:\n",
      "# 'winner takes all' method - set minium correlation threshold to remove some edges from the diagram\n",
      "threshold = 0.5\n",
      "\n",
      "# create a new graph from edge list\n",
      "Gx = nx.from_pandas_edgelist(edges, 'Features_1', 'Features_2', edge_attr=['correlation'])\n",
      "\n",
      "# list to store edges to remove\n",
      "remove = []\n",
      "# loop through edges in Gx and find correlations which are below the threshold\n",
      "for asset_1, asset_2 in Gx.edges():\n",
      "    corr = Gx[asset_1][asset_2]['correlation']\n",
      "    #add to remove node list if abs(corr) < threshold\n",
      "    if abs(corr) < threshold:\n",
      "        remove.append((asset_1, asset_2))\n",
      "\n",
      "# remove edges contained in the remove list\n",
      "Gx.remove_edges_from(remove)\n",
      "\n",
      "print(str(len(remove)) + \" edges removed\")\n",
      "348/24:\n",
      "ef assign_colour(correlation):\n",
      "    if correlation <= 0:\n",
      "        return \"#ffa09b\"  # red\n",
      "    else:\n",
      "        return \"#9eccb7\"  # green\n",
      "\n",
      "\n",
      "def assign_thickness(correlation, benchmark_thickness=2, scaling_factor=3):\n",
      "    return benchmark_thickness * abs(correlation)**scaling_factor\n",
      "\n",
      "\n",
      "def assign_node_size(degree, scaling_factor=50):\n",
      "    return degree * scaling_factor\n",
      "\n",
      "\n",
      "# assign colours to edges depending on positive or negative correlation\n",
      "# assign edge thickness depending on magnitude of correlation\n",
      "edge_colours = []\n",
      "edge_width = []\n",
      "for key, value in nx.get_edge_attributes(Gx, 'correlation').items():\n",
      "    edge_colours.append(assign_colour(value))\n",
      "    edge_width.append(assign_thickness(value))\n",
      "\n",
      "# assign node size depending on number of connections (degree)\n",
      "node_size = []\n",
      "for key, value in dict(Gx.degree).items():\n",
      "    node_size.append(assign_node_size(value))\n",
      "348/25:\n",
      "def assign_colour(correlation):\n",
      "    if correlation <= 0:\n",
      "        return \"#ffa09b\"  # red\n",
      "    else:\n",
      "        return \"#9eccb7\"  # green\n",
      "\n",
      "\n",
      "def assign_thickness(correlation, benchmark_thickness=2, scaling_factor=3):\n",
      "    return benchmark_thickness * abs(correlation)**scaling_factor\n",
      "\n",
      "\n",
      "def assign_node_size(degree, scaling_factor=50):\n",
      "    return degree * scaling_factor\n",
      "\n",
      "\n",
      "# assign colours to edges depending on positive or negative correlation\n",
      "# assign edge thickness depending on magnitude of correlation\n",
      "edge_colours = []\n",
      "edge_width = []\n",
      "for key, value in nx.get_edge_attributes(Gx, 'correlation').items():\n",
      "    edge_colours.append(assign_colour(value))\n",
      "    edge_width.append(assign_thickness(value))\n",
      "\n",
      "# assign node size depending on number of connections (degree)\n",
      "node_size = []\n",
      "for key, value in dict(Gx.degree).items():\n",
      "    node_size.append(assign_node_size(value))\n",
      "348/26:\n",
      "# draw improved graph\n",
      "sns.set(rc={'figure.figsize': (9, 9)})\n",
      "font_dict = {'fontsize': 18}\n",
      "\n",
      "nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,\n",
      "        node_size=node_size, node_color=\"#e1575c\", edge_color=edge_colours,\n",
      "        width=edge_width)\n",
      "plt.title(\"Correlation_matrix\", fontdict=font_dict)\n",
      "plt.show()\n",
      "348/27:\n",
      "# draw improved graph\n",
      "sns.set(rc={'figure.figsize': (9, 9)})\n",
      "font_dict = {'fontsize': 18}\n",
      "\n",
      "nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,\n",
      "        node_size=node_size, node_color=\"#e1575c\", edge_color=edge_colours,\n",
      "        width=edge_width)\n",
      "plt.title(\"Correlation R>0.5\", fontdict=font_dict)\n",
      "plt.show()\n",
      "348/28:\n",
      "# draw improved graph\n",
      "sns.set(rc={'figure.figsize': (9, 9)})\n",
      "font_dict = {'fontsize': 18}\n",
      "\n",
      "nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,\n",
      "        node_size=node_size, node_color=\"#e1575c\", edge_color=edge_colours,\n",
      "        width=edge_width)\n",
      "plt.title(\"Correlation R > 0.5\", fontdict=font_dict)\n",
      "plt.show()\n",
      "348/29:\n",
      "# draw improved graph\n",
      "sns.set(rc={'figure.figsize': (9, 9)})\n",
      "font_dict = {'fontsize': 30}\n",
      "\n",
      "nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,\n",
      "        node_size=node_size, node_color=\"#e1575c\", edge_color=edge_colours,\n",
      "        width=edge_width)\n",
      "plt.title(\"Correlation R > 0.5\", fontdict=font_dict)\n",
      "plt.show()\n",
      "348/30:\n",
      "# draw improved graph\n",
      "sns.set(rc={'figure.figsize': (9, 9)})\n",
      "font_dict = {'fontsize': 18}\n",
      "\n",
      "nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,\n",
      "        node_size=node_size, node_color=\"#e1575c\", edge_color=edge_colours,\n",
      "        width=edge_width)\n",
      "plt.title(\"Correlation R > 0.5\", fontdict=font_dict)\n",
      "plt.show()\n",
      "348/31:\n",
      "# draw improved graph\n",
      "sns.set(rc={'figure.figsize': (12, 12)})\n",
      "font_dict = {'fontsize': 18}\n",
      "\n",
      "nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,\n",
      "        node_size=node_size, node_color=\"#e1575c\", edge_color=edge_colours,\n",
      "        width=edge_width)\n",
      "plt.title(\"Correlation R > 0.5\", fontdict=font_dict)\n",
      "plt.show()\n",
      "348/32:\n",
      "# 'winner takes all' method - set minium correlation threshold to remove some edges from the diagram\n",
      "threshold = 0.5\n",
      "\n",
      "# create a new graph from edge list\n",
      "Gx = nx.from_pandas_edgelist(edges, 'Features_1', 'Features_2', edge_attr=['correlation'])\n",
      "\n",
      "# list to store edges to remove\n",
      "remove = []\n",
      "# loop through edges in Gx and find correlations which are below the threshold\n",
      "for Features_1, Features_2 in Gx.edges():\n",
      "    corr = Gx[asset_1][asset_2]['correlation']\n",
      "    #add to remove node list if abs(corr) < threshold\n",
      "    if abs(corr) < threshold:\n",
      "        remove.append((Features_1, Features_2))\n",
      "\n",
      "# remove edges contained in the remove list\n",
      "Gx.remove_edges_from(remove)\n",
      "\n",
      "print(str(len(remove)) + \" edges removed\")\n",
      "348/33:\n",
      "def assign_colour(correlation):\n",
      "    if correlation <= 0:\n",
      "        return \"#ffa09b\"  # red\n",
      "    else:\n",
      "        return \"#9eccb7\"  # green\n",
      "\n",
      "\n",
      "def assign_thickness(correlation, benchmark_thickness=2, scaling_factor=3):\n",
      "    return benchmark_thickness * abs(correlation)**scaling_factor\n",
      "\n",
      "\n",
      "def assign_node_size(degree, scaling_factor=50):\n",
      "    return degree * scaling_factor\n",
      "\n",
      "\n",
      "# assign colours to edges depending on positive or negative correlation\n",
      "# assign edge thickness depending on magnitude of correlation\n",
      "edge_colours = []\n",
      "edge_width = []\n",
      "for key, value in nx.get_edge_attributes(Gx, 'correlation').items():\n",
      "    edge_colours.append(assign_colour(value))\n",
      "    edge_width.append(assign_thickness(value))\n",
      "\n",
      "# assign node size depending on number of connections (degree)\n",
      "node_size = []\n",
      "for key, value in dict(Gx.degree).items():\n",
      "    node_size.append(assign_node_size(value))\n",
      "348/34:\n",
      "# draw improved graph\n",
      "sns.set(rc={'figure.figsize': (12, 12)})\n",
      "font_dict = {'fontsize': 18}\n",
      "\n",
      "nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,\n",
      "        node_size=node_size, node_color=\"#e1575c\", edge_color=edge_colours,\n",
      "        width=edge_width)\n",
      "plt.title(\"Correlation R > 0.5\", fontdict=font_dict)\n",
      "plt.show()\n",
      "348/35:\n",
      "# 'winner takes all' method - set minium correlation threshold to remove some edges from the diagram\n",
      "threshold = 0.5\n",
      "\n",
      "# create a new graph from edge list\n",
      "Gx = nx.from_pandas_edgelist(edges, 'Features_1', 'Features_2', edge_attr=['correlation'])\n",
      "\n",
      "# list to store edges to remove\n",
      "remove = []\n",
      "# loop through edges in Gx and find correlations which are below the threshold\n",
      "for Features_1, Features_2 in Gx.edges():\n",
      "    corr = Gx[Features_1][Features_2]['correlation']\n",
      "    #add to remove node list if abs(corr) < threshold\n",
      "    if abs(corr) < threshold:\n",
      "        remove.append((Features_1, Features_2))\n",
      "\n",
      "# remove edges contained in the remove list\n",
      "Gx.remove_edges_from(remove)\n",
      "\n",
      "print(str(len(remove)) + \" edges removed\")\n",
      "348/36:\n",
      "def assign_colour(correlation):\n",
      "    if correlation <= 0:\n",
      "        return \"#ffa09b\"  # red\n",
      "    else:\n",
      "        return \"#9eccb7\"  # green\n",
      "\n",
      "\n",
      "def assign_thickness(correlation, benchmark_thickness=2, scaling_factor=3):\n",
      "    return benchmark_thickness * abs(correlation)**scaling_factor\n",
      "\n",
      "\n",
      "def assign_node_size(degree, scaling_factor=50):\n",
      "    return degree * scaling_factor\n",
      "\n",
      "\n",
      "# assign colours to edges depending on positive or negative correlation\n",
      "# assign edge thickness depending on magnitude of correlation\n",
      "edge_colours = []\n",
      "edge_width = []\n",
      "for key, value in nx.get_edge_attributes(Gx, 'correlation').items():\n",
      "    edge_colours.append(assign_colour(value))\n",
      "    edge_width.append(assign_thickness(value))\n",
      "\n",
      "# assign node size depending on number of connections (degree)\n",
      "node_size = []\n",
      "for key, value in dict(Gx.degree).items():\n",
      "    node_size.append(assign_node_size(value))\n",
      "348/37:\n",
      "# draw improved graph\n",
      "sns.set(rc={'figure.figsize': (12, 12)})\n",
      "font_dict = {'fontsize': 18}\n",
      "\n",
      "nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,\n",
      "        node_size=node_size, node_color=\"#e1575c\", edge_color=edge_colours,\n",
      "        width=edge_width)\n",
      "plt.title(\"Correlation R > 0.5\", fontdict=font_dict)\n",
      "plt.show()\n",
      "348/38:\n",
      "# draw improved graph\n",
      "sns.set(rc={'figure.figsize': (5, 5)})\n",
      "font_dict = {'fontsize': 18}\n",
      "\n",
      "nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,\n",
      "        node_size=node_size, node_color=\"#e1575c\", edge_color=edge_colours,\n",
      "        width=edge_width)\n",
      "plt.title(\"Correlation R > 0.5\", fontdict=font_dict)\n",
      "plt.show()\n",
      "348/39:\n",
      "# 'winner takes all' method - set minium correlation threshold to remove some edges from the diagram\n",
      "threshold = 0.4\n",
      "\n",
      "# create a new graph from edge list\n",
      "Gx = nx.from_pandas_edgelist(edges, 'Features_1', 'Features_2', edge_attr=['correlation'])\n",
      "\n",
      "# list to store edges to remove\n",
      "remove = []\n",
      "# loop through edges in Gx and find correlations which are below the threshold\n",
      "for Features_1, Features_2 in Gx.edges():\n",
      "    corr = Gx[Features_1][Features_2]['correlation']\n",
      "    #add to remove node list if abs(corr) < threshold\n",
      "    if abs(corr) < threshold:\n",
      "        remove.append((Features_1, Features_2))\n",
      "\n",
      "# remove edges contained in the remove list\n",
      "Gx.remove_edges_from(remove)\n",
      "\n",
      "print(str(len(remove)) + \" edges removed\")\n",
      "348/40:\n",
      "def assign_colour(correlation):\n",
      "    if correlation <= 0:\n",
      "        return \"#ffa09b\"  # red\n",
      "    else:\n",
      "        return \"#9eccb7\"  # green\n",
      "\n",
      "\n",
      "def assign_thickness(correlation, benchmark_thickness=2, scaling_factor=3):\n",
      "    return benchmark_thickness * abs(correlation)**scaling_factor\n",
      "\n",
      "\n",
      "def assign_node_size(degree, scaling_factor=50):\n",
      "    return degree * scaling_factor\n",
      "\n",
      "\n",
      "# assign colours to edges depending on positive or negative correlation\n",
      "# assign edge thickness depending on magnitude of correlation\n",
      "edge_colours = []\n",
      "edge_width = []\n",
      "for key, value in nx.get_edge_attributes(Gx, 'correlation').items():\n",
      "    edge_colours.append(assign_colour(value))\n",
      "    edge_width.append(assign_thickness(value))\n",
      "\n",
      "# assign node size depending on number of connections (degree)\n",
      "node_size = []\n",
      "for key, value in dict(Gx.degree).items():\n",
      "    node_size.append(assign_node_size(value))\n",
      "348/41:\n",
      "# draw improved graph\n",
      "sns.set(rc={'figure.figsize': (5, 5)})\n",
      "font_dict = {'fontsize': 18}\n",
      "\n",
      "nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,\n",
      "        node_size=node_size, node_color=\"#e1575c\", edge_color=edge_colours,\n",
      "        width=edge_width)\n",
      "plt.title(\"Correlation R > 0.5\", fontdict=font_dict)\n",
      "plt.show()\n",
      "348/42:\n",
      "# draw improved graph\n",
      "sns.set(rc={'figure.figsize': (5, 5)})\n",
      "font_dict = {'fontsize': 18}\n",
      "\n",
      "nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,\n",
      "        node_size=node_size, node_color=\"#e1575c\", edge_color=edge_colours,\n",
      "        width=edge_width)\n",
      "plt.title(\"Correlation R > 0.4\", fontdict=font_dict)\n",
      "plt.show()\n",
      "348/43:\n",
      "def assign_colour(correlation):\n",
      "    if correlation <= 0:\n",
      "        return \"#ffa09b\"  # red\n",
      "    else:\n",
      "        return \"#9eccb7\"  # green\n",
      "\n",
      "\n",
      "def assign_thickness(correlation, benchmark_thickness=2, scaling_factor=5):\n",
      "    return benchmark_thickness * abs(correlation)**scaling_factor\n",
      "\n",
      "\n",
      "def assign_node_size(degree, scaling_factor=100):\n",
      "    return degree * scaling_factor\n",
      "\n",
      "\n",
      "# assign colours to edges depending on positive or negative correlation\n",
      "# assign edge thickness depending on magnitude of correlation\n",
      "edge_colours = []\n",
      "edge_width = []\n",
      "for key, value in nx.get_edge_attributes(Gx, 'correlation').items():\n",
      "    edge_colours.append(assign_colour(value))\n",
      "    edge_width.append(assign_thickness(value))\n",
      "\n",
      "# assign node size depending on number of connections (degree)\n",
      "node_size = []\n",
      "for key, value in dict(Gx.degree).items():\n",
      "    node_size.append(assign_node_size(value))\n",
      "348/44:\n",
      "# draw improved graph\n",
      "sns.set(rc={'figure.figsize': (5, 5)})\n",
      "font_dict = {'fontsize': 18}\n",
      "\n",
      "nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,\n",
      "        node_size=node_size, node_color=\"#e1575c\", edge_color=edge_colours,\n",
      "        width=edge_width)\n",
      "plt.title(\"Correlation R > 0.4\", fontdict=font_dict)\n",
      "plt.show()\n",
      "348/45:\n",
      "def assign_colour(correlation):\n",
      "    if correlation <= 0:\n",
      "        return \"#ffa09b\"  # red\n",
      "    else:\n",
      "        return \"#9eccb7\"  # green\n",
      "\n",
      "\n",
      "def assign_thickness(correlation, benchmark_thickness=5, scaling_factor=5):\n",
      "    return benchmark_thickness * abs(correlation)**scaling_factor\n",
      "\n",
      "\n",
      "def assign_node_size(degree, scaling_factor=100):\n",
      "    return degree * scaling_factor\n",
      "\n",
      "\n",
      "# assign colours to edges depending on positive or negative correlation\n",
      "# assign edge thickness depending on magnitude of correlation\n",
      "edge_colours = []\n",
      "edge_width = []\n",
      "for key, value in nx.get_edge_attributes(Gx, 'correlation').items():\n",
      "    edge_colours.append(assign_colour(value))\n",
      "    edge_width.append(assign_thickness(value))\n",
      "\n",
      "# assign node size depending on number of connections (degree)\n",
      "node_size = []\n",
      "for key, value in dict(Gx.degree).items():\n",
      "    node_size.append(assign_node_size(value))\n",
      "348/46:\n",
      "# draw improved graph\n",
      "sns.set(rc={'figure.figsize': (5, 5)})\n",
      "font_dict = {'fontsize': 18}\n",
      "\n",
      "nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,\n",
      "        node_size=node_size, node_color=\"#e1575c\", edge_color=edge_colours,\n",
      "        width=edge_width)\n",
      "plt.title(\"Correlation R > 0.4\", fontdict=font_dict)\n",
      "plt.show()\n",
      "348/47:\n",
      "def assign_colour(correlation):\n",
      "    if correlation <= 0:\n",
      "        return \"#ffa09b\"  # red\n",
      "    else:\n",
      "        return \"#9eccb7\"  # green\n",
      "\n",
      "\n",
      "def assign_thickness(correlation, benchmark_thickness=5, scaling_factor=5):\n",
      "    return benchmark_thickness * abs(correlation)**scaling_factor\n",
      "\n",
      "\n",
      "def assign_node_size(degree, scaling_factor=100):\n",
      "    return degree * scaling_factor\n",
      "\n",
      "\n",
      "# assign colours to edges depending on positive or negative correlation\n",
      "# assign edge thickness depending on magnitude of correlation\n",
      "edge_colours = []\n",
      "edge_width = []\n",
      "for key, value in nx.get_edge_attributes(Gx, 'correlation').items():\n",
      "    edge_colours.append(assign_colour(value))\n",
      "    edge_width.append(assign_thickness(value))\n",
      "\n",
      "# assign node size depending on number of connections (degree)\n",
      "node_size = []\n",
      "for key, value in dict(Gx.degree).items():\n",
      "    node_size.append(assign_node_size(value))\n",
      "348/48:\n",
      "# draw improved graph\n",
      "sns.set(rc={'figure.figsize': (5, 5)})\n",
      "font_dict = {'fontsize': 18}\n",
      "\n",
      "nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,\n",
      "        node_size=node_size, node_color=\"#e1575c\", edge_color=edge_colours,\n",
      "        width=edge_width)\n",
      "plt.title(\"Correlation R > 0.4\", fontdict=font_dict)\n",
      "plt.show()\n",
      "348/49:\n",
      "# draw improved graph\n",
      "sns.set(rc={'figure.figsize': (5, 5)})\n",
      "font_dict = {'fontsize': 18}\n",
      "\n",
      "nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,\n",
      "        node_size=node_size, node_color=\"#e1575c\", edge_color=edge_colours,\n",
      "        width=edge_width)\n",
      "plt.title(\"Correlation R > 0.4\", fontdict=font_dict)\n",
      "plt.show()\n",
      "348/50:\n",
      "def assign_colour(correlation):\n",
      "    if correlation <= 0:\n",
      "        return \"#ffa09b\"  # red\n",
      "    else:\n",
      "        return \"#9eccb7\"  # green\n",
      "\n",
      "\n",
      "def assign_thickness(correlation, benchmark_thickness=5, scaling_factor=5):\n",
      "    return benchmark_thickness * abs(correlation)**scaling_factor\n",
      "\n",
      "\n",
      "def assign_node_size(degree, scaling_factor=50):\n",
      "    return degree * scaling_factor\n",
      "\n",
      "\n",
      "# assign colours to edges depending on positive or negative correlation\n",
      "# assign edge thickness depending on magnitude of correlation\n",
      "edge_colours = []\n",
      "edge_width = []\n",
      "for key, value in nx.get_edge_attributes(Gx, 'correlation').items():\n",
      "    edge_colours.append(assign_colour(value))\n",
      "    edge_width.append(assign_thickness(value))\n",
      "\n",
      "# assign node size depending on number of connections (degree)\n",
      "node_size = []\n",
      "for key, value in dict(Gx.degree).items():\n",
      "    node_size.append(assign_node_size(value))\n",
      "348/51:\n",
      "# draw improved graph\n",
      "sns.set(rc={'figure.figsize': (5, 5)})\n",
      "font_dict = {'fontsize': 18}\n",
      "\n",
      "nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,\n",
      "        node_size=node_size, node_color=\"#e1575c\", edge_color=edge_colours,\n",
      "        width=edge_width)\n",
      "plt.title(\"Correlation R > 0.4\", fontdict=font_dict)\n",
      "plt.show()\n",
      "348/52:\n",
      "# 'winner takes all' method - set minium correlation threshold to remove some edges from the diagram\n",
      "threshold = 0.2\n",
      "\n",
      "# create a new graph from edge list\n",
      "Gx = nx.from_pandas_edgelist(edges, 'Features_1', 'Features_2', edge_attr=['correlation'])\n",
      "\n",
      "# list to store edges to remove\n",
      "remove = []\n",
      "# loop through edges in Gx and find correlations which are below the threshold\n",
      "for Features_1, Features_2 in Gx.edges():\n",
      "    corr = Gx[Features_1][Features_2]['correlation']\n",
      "    #add to remove node list if abs(corr) < threshold\n",
      "    if abs(corr) < threshold:\n",
      "        remove.append((Features_1, Features_2))\n",
      "\n",
      "# remove edges contained in the remove list\n",
      "Gx.remove_edges_from(remove)\n",
      "\n",
      "print(str(len(remove)) + \" edges removed\")\n",
      "348/53:\n",
      "def assign_colour(correlation):\n",
      "    if correlation <= 0:\n",
      "        return \"#ffa09b\"  # red\n",
      "    else:\n",
      "        return \"#9eccb7\"  # green\n",
      "\n",
      "\n",
      "def assign_thickness(correlation, benchmark_thickness=5, scaling_factor=5):\n",
      "    return benchmark_thickness * abs(correlation)**scaling_factor\n",
      "\n",
      "\n",
      "def assign_node_size(degree, scaling_factor=50):\n",
      "    return degree * scaling_factor\n",
      "\n",
      "\n",
      "# assign colours to edges depending on positive or negative correlation\n",
      "# assign edge thickness depending on magnitude of correlation\n",
      "edge_colours = []\n",
      "edge_width = []\n",
      "for key, value in nx.get_edge_attributes(Gx, 'correlation').items():\n",
      "    edge_colours.append(assign_colour(value))\n",
      "    edge_width.append(assign_thickness(value))\n",
      "\n",
      "# assign node size depending on number of connections (degree)\n",
      "node_size = []\n",
      "for key, value in dict(Gx.degree).items():\n",
      "    node_size.append(assign_node_size(value))\n",
      "348/54:\n",
      "# draw improved graph\n",
      "sns.set(rc={'figure.figsize': (5, 5)})\n",
      "font_dict = {'fontsize': 18}\n",
      "\n",
      "nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,\n",
      "        node_size=node_size, node_color=\"#e1575c\", edge_color=edge_colours,\n",
      "        width=edge_width)\n",
      "plt.title(\"Correlation R > 0.4\", fontdict=font_dict)\n",
      "plt.show()\n",
      "348/55:\n",
      "# draw improved graph\n",
      "sns.set(rc={'figure.figsize': (10, 10)})\n",
      "font_dict = {'fontsize': 18}\n",
      "\n",
      "nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,\n",
      "        node_size=node_size, node_color=\"#e1575c\", edge_color=edge_colours,\n",
      "        width=edge_width)\n",
      "plt.title(\"Correlation R > 0.4\", fontdict=font_dict)\n",
      "plt.show()\n",
      "348/56:\n",
      "def assign_colour(correlation):\n",
      "    if correlation <= 0:\n",
      "        return \"#ffa09b\"  # red\n",
      "    else:\n",
      "        return \"#9eccb7\"  # green\n",
      "\n",
      "\n",
      "def assign_thickness(correlation, benchmark_thickness=100, scaling_factor=5):\n",
      "    return benchmark_thickness * abs(correlation)**scaling_factor\n",
      "\n",
      "\n",
      "def assign_node_size(degree, scaling_factor=50):\n",
      "    return degree * scaling_factor\n",
      "\n",
      "\n",
      "# assign colours to edges depending on positive or negative correlation\n",
      "# assign edge thickness depending on magnitude of correlation\n",
      "edge_colours = []\n",
      "edge_width = []\n",
      "for key, value in nx.get_edge_attributes(Gx, 'correlation').items():\n",
      "    edge_colours.append(assign_colour(value))\n",
      "    edge_width.append(assign_thickness(value))\n",
      "\n",
      "# assign node size depending on number of connections (degree)\n",
      "node_size = []\n",
      "for key, value in dict(Gx.degree).items():\n",
      "    node_size.append(assign_node_size(value))\n",
      "348/57:\n",
      "# draw improved graph\n",
      "sns.set(rc={'figure.figsize': (10, 10)})\n",
      "font_dict = {'fontsize': 18}\n",
      "\n",
      "nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,\n",
      "        node_size=node_size, node_color=\"#e1575c\", edge_color=edge_colours,\n",
      "        width=edge_width)\n",
      "plt.title(\"Correlation R > 0.4\", fontdict=font_dict)\n",
      "plt.show()\n",
      "348/58:\n",
      "# draw improved graph\n",
      "sns.set(rc={'figure.figsize': (5, 5)})\n",
      "font_dict = {'fontsize': 18}\n",
      "\n",
      "nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,\n",
      "        node_size=node_size, node_color=\"#e1575c\", edge_color=edge_colours,\n",
      "        width=edge_width)\n",
      "plt.title(\"Correlation R > 0.4\", fontdict=font_dict)\n",
      "plt.show()\n",
      "348/59:\n",
      "def assign_colour(correlation):\n",
      "    if correlation <= 0:\n",
      "        return \"blue\"  # red\n",
      "    else:\n",
      "        return \"#9eccb7\"  # green\n",
      "\n",
      "\n",
      "def assign_thickness(correlation, benchmark_thickness=100, scaling_factor=5):\n",
      "    return benchmark_thickness * abs(correlation)**scaling_factor\n",
      "\n",
      "\n",
      "def assign_node_size(degree, scaling_factor=50):\n",
      "    return degree * scaling_factor\n",
      "\n",
      "\n",
      "# assign colours to edges depending on positive or negative correlation\n",
      "# assign edge thickness depending on magnitude of correlation\n",
      "edge_colours = []\n",
      "edge_width = []\n",
      "for key, value in nx.get_edge_attributes(Gx, 'correlation').items():\n",
      "    edge_colours.append(assign_colour(value))\n",
      "    edge_width.append(assign_thickness(value))\n",
      "\n",
      "# assign node size depending on number of connections (degree)\n",
      "node_size = []\n",
      "for key, value in dict(Gx.degree).items():\n",
      "    node_size.append(assign_node_size(value))\n",
      "348/60:\n",
      "# draw improved graph\n",
      "sns.set(rc={'figure.figsize': (5, 5)})\n",
      "font_dict = {'fontsize': 18}\n",
      "\n",
      "nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,\n",
      "        node_size=node_size, node_color=\"#e1575c\", edge_color=edge_colours,\n",
      "        width=edge_width)\n",
      "plt.title(\"Correlation R > 0.4\", fontdict=font_dict)\n",
      "plt.show()\n",
      "348/61:\n",
      "def assign_colour(correlation):\n",
      "    if correlation <= 0:\n",
      "        return \"blue\"  # red\n",
      "    else:\n",
      "        return \"#Green\"  # green\n",
      "\n",
      "\n",
      "def assign_thickness(correlation, benchmark_thickness=100, scaling_factor=5):\n",
      "    return benchmark_thickness * abs(correlation)**scaling_factor\n",
      "\n",
      "\n",
      "def assign_node_size(degree, scaling_factor=50):\n",
      "    return degree * scaling_factor\n",
      "\n",
      "\n",
      "# assign colours to edges depending on positive or negative correlation\n",
      "# assign edge thickness depending on magnitude of correlation\n",
      "edge_colours = []\n",
      "edge_width = []\n",
      "for key, value in nx.get_edge_attributes(Gx, 'correlation').items():\n",
      "    edge_colours.append(assign_colour(value))\n",
      "    edge_width.append(assign_thickness(value))\n",
      "\n",
      "# assign node size depending on number of connections (degree)\n",
      "node_size = []\n",
      "for key, value in dict(Gx.degree).items():\n",
      "    node_size.append(assign_node_size(value))\n",
      "348/62:\n",
      "def assign_colour(correlation):\n",
      "    if correlation <= 0:\n",
      "        return \"blue\"  # red\n",
      "    else:\n",
      "        return \"Green\"  # green\n",
      "\n",
      "\n",
      "def assign_thickness(correlation, benchmark_thickness=100, scaling_factor=5):\n",
      "    return benchmark_thickness * abs(correlation)**scaling_factor\n",
      "\n",
      "\n",
      "def assign_node_size(degree, scaling_factor=50):\n",
      "    return degree * scaling_factor\n",
      "\n",
      "\n",
      "# assign colours to edges depending on positive or negative correlation\n",
      "# assign edge thickness depending on magnitude of correlation\n",
      "edge_colours = []\n",
      "edge_width = []\n",
      "for key, value in nx.get_edge_attributes(Gx, 'correlation').items():\n",
      "    edge_colours.append(assign_colour(value))\n",
      "    edge_width.append(assign_thickness(value))\n",
      "\n",
      "# assign node size depending on number of connections (degree)\n",
      "node_size = []\n",
      "for key, value in dict(Gx.degree).items():\n",
      "    node_size.append(assign_node_size(value))\n",
      "348/63:\n",
      "# draw improved graph\n",
      "sns.set(rc={'figure.figsize': (5, 5)})\n",
      "font_dict = {'fontsize': 18}\n",
      "\n",
      "nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,\n",
      "        node_size=node_size, node_color=\"#e1575c\", edge_color=edge_colours,\n",
      "        width=edge_width)\n",
      "plt.title(\"Correlation R > 0.4\", fontdict=font_dict)\n",
      "plt.show()\n",
      "348/64:\n",
      "def assign_colour(correlation):\n",
      "    if correlation <= 0:\n",
      "        return \"blue\"  # red\n",
      "    else:\n",
      "        return \"Green\"  # green\n",
      "\n",
      "\n",
      "def assign_thickness(correlation, benchmark_thickness=50, scaling_factor=5):\n",
      "    return benchmark_thickness * abs(correlation)**scaling_factor\n",
      "\n",
      "\n",
      "def assign_node_size(degree, scaling_factor=50):\n",
      "    return degree * scaling_factor\n",
      "\n",
      "\n",
      "# assign colours to edges depending on positive or negative correlation\n",
      "# assign edge thickness depending on magnitude of correlation\n",
      "edge_colours = []\n",
      "edge_width = []\n",
      "for key, value in nx.get_edge_attributes(Gx, 'correlation').items():\n",
      "    edge_colours.append(assign_colour(value))\n",
      "    edge_width.append(assign_thickness(value))\n",
      "\n",
      "# assign node size depending on number of connections (degree)\n",
      "node_size = []\n",
      "for key, value in dict(Gx.degree).items():\n",
      "    node_size.append(assign_node_size(value))\n",
      "348/65:\n",
      "# draw improved graph\n",
      "sns.set(rc={'figure.figsize': (5, 5)})\n",
      "font_dict = {'fontsize': 18}\n",
      "\n",
      "nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,\n",
      "        node_size=node_size, node_color=\"#e1575c\", edge_color=edge_colours,\n",
      "        width=edge_width)\n",
      "plt.title(\"Correlation R > 0.4\", fontdict=font_dict)\n",
      "plt.show()\n",
      "348/66:\n",
      "def assign_colour(correlation):\n",
      "    if correlation <= 0:\n",
      "        return \"blue\"  # red\n",
      "    else:\n",
      "        return \"Green\"  # green\n",
      "\n",
      "\n",
      "def assign_thickness(correlation, benchmark_thickness=20, scaling_factor=5):\n",
      "    return benchmark_thickness * abs(correlation)**scaling_factor\n",
      "\n",
      "\n",
      "def assign_node_size(degree, scaling_factor=50):\n",
      "    return degree * scaling_factor\n",
      "\n",
      "\n",
      "# assign colours to edges depending on positive or negative correlation\n",
      "# assign edge thickness depending on magnitude of correlation\n",
      "edge_colours = []\n",
      "edge_width = []\n",
      "for key, value in nx.get_edge_attributes(Gx, 'correlation').items():\n",
      "    edge_colours.append(assign_colour(value))\n",
      "    edge_width.append(assign_thickness(value))\n",
      "\n",
      "# assign node size depending on number of connections (degree)\n",
      "node_size = []\n",
      "for key, value in dict(Gx.degree).items():\n",
      "    node_size.append(assign_node_size(value))\n",
      "348/67:\n",
      "# draw improved graph\n",
      "sns.set(rc={'figure.figsize': (5, 5)})\n",
      "font_dict = {'fontsize': 18}\n",
      "\n",
      "nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,\n",
      "        node_size=node_size, node_color=\"#e1575c\", edge_color=edge_colours,\n",
      "        width=edge_width)\n",
      "plt.title(\"Correlation R > 0.4\", fontdict=font_dict)\n",
      "plt.show()\n",
      "348/68:\n",
      "# 'winner takes all' method - set minium correlation threshold to remove some edges from the diagram\n",
      "threshold = 0.4\n",
      "\n",
      "# create a new graph from edge list\n",
      "Gx = nx.from_pandas_edgelist(edges, 'Features_1', 'Features_2', edge_attr=['correlation'])\n",
      "\n",
      "# list to store edges to remove\n",
      "remove = []\n",
      "# loop through edges in Gx and find correlations which are below the threshold\n",
      "for Features_1, Features_2 in Gx.edges():\n",
      "    corr = Gx[Features_1][Features_2]['correlation']\n",
      "    #add to remove node list if abs(corr) < threshold\n",
      "    if abs(corr) < threshold:\n",
      "        remove.append((Features_1, Features_2))\n",
      "\n",
      "# remove edges contained in the remove list\n",
      "Gx.remove_edges_from(remove)\n",
      "\n",
      "print(str(len(remove)) + \" edges removed\")\n",
      "348/69:\n",
      "def assign_colour(correlation):\n",
      "    if correlation <= 0:\n",
      "        return \"blue\"  # red\n",
      "    else:\n",
      "        return \"Green\"  # green\n",
      "\n",
      "\n",
      "def assign_thickness(correlation, benchmark_thickness=20, scaling_factor=5):\n",
      "    return benchmark_thickness * abs(correlation)**scaling_factor\n",
      "\n",
      "\n",
      "def assign_node_size(degree, scaling_factor=50):\n",
      "    return degree * scaling_factor\n",
      "\n",
      "\n",
      "# assign colours to edges depending on positive or negative correlation\n",
      "# assign edge thickness depending on magnitude of correlation\n",
      "edge_colours = []\n",
      "edge_width = []\n",
      "for key, value in nx.get_edge_attributes(Gx, 'correlation').items():\n",
      "    edge_colours.append(assign_colour(value))\n",
      "    edge_width.append(assign_thickness(value))\n",
      "\n",
      "# assign node size depending on number of connections (degree)\n",
      "node_size = []\n",
      "for key, value in dict(Gx.degree).items():\n",
      "    node_size.append(assign_node_size(value))\n",
      "348/70:\n",
      "# draw improved graph\n",
      "sns.set(rc={'figure.figsize': (5, 5)})\n",
      "font_dict = {'fontsize': 18}\n",
      "\n",
      "nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,\n",
      "        node_size=node_size, node_color=\"#e1575c\", edge_color=edge_colours,\n",
      "        width=edge_width)\n",
      "plt.title(\"Correlation R > 0.4\", fontdict=font_dict)\n",
      "plt.show()\n",
      "352/1:\n",
      "import networkx as nx\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "352/2: df = pd.read_csv('df_clean.csv')\n",
      "352/3: df\n",
      "352/4:\n",
      "bioactivity_class = []\n",
      "for i in df.Bioactivity_class_amylase:\n",
      "    if df['Bioactivity_class_amylase']='active':\n",
      "        bioactivity_class.append(\"active\")\n",
      "352/5:\n",
      "bioactivity_class = []\n",
      "for i in df.Bioactivity_class_amylase:\n",
      "    if i = 'active':\n",
      "        bioactivity_class.append(\"active\")\n",
      "352/6:\n",
      "bioactivity_class = []\n",
      "for i in df.Bioactivity_class_amylase:\n",
      "    if i=='active':\n",
      "        bioactivity_class.append(\"active\")\n",
      "352/7: bioactivity_class\n",
      "352/8:\n",
      "bioactivity_class = []\n",
      "for i in df.Bioactivity_class_amylase&df.Bioactivity_class_amylase:\n",
      "    if i=='active':\n",
      "        bioactivity_class.append(\"active\")\n",
      "    elif\n",
      "352/9: df = pd.read_csv('df_clean.csv')\n",
      "352/10: df\n",
      "352/11: df = pd.read_csv('df_clean.csv')\n",
      "352/12: df\n",
      "352/13: df = pd.read_csv('df_clean_class.csv')\n",
      "352/14: df\n",
      "352/15:\n",
      "bioactivity_class = []\n",
      "for i in df.Bioactivity_class_amylase:\n",
      "    if i=='active':\n",
      "        bioactivity_class.append(\"active\")\n",
      "    elif:\n",
      "        bioactivity_class.append(\"inactive\")\n",
      "352/16:\n",
      "bioactivity_class = []\n",
      "for i in df.Bioactivity_class_amylase:\n",
      "    if i=='active':\n",
      "        bioactivity_class.append(\"active\")\n",
      "    elif:\n",
      "        bioactivity_class.append(\"inactive\")\n",
      "352/17:\n",
      "bioactivity_class = []\n",
      "for i in df.Bioactivity_class_amylase:\n",
      "    if i=='active':\n",
      "        bioactivity_class.append(\"active\")\n",
      "    elif\n",
      "        bioactivity_class.append(\"inactive\")\n",
      "352/18:\n",
      "bioactivity_class = []\n",
      "for i in df.Bioactivity_class_amylase:\n",
      "    if i=='active':\n",
      "        bioactivity_class.append(\"active\")\n",
      "    elif :\n",
      "        bioactivity_class.append(\"inactive\")\n",
      "352/19:\n",
      "bioactivity_class = []\n",
      "for i in df.Bioactivity_class_amylase:\n",
      "    if i=='active':\n",
      "        bioactivity_class.append(\"active\")\n",
      "    else:\n",
      "        bioactivity_class.append(\"inactive\")\n",
      "352/20: bioactivity_class\n",
      "352/21: df = pd.read_csv('df_clean_class.csv', index = 'Name_Short')\n",
      "352/22: df = pd.read_csv('df_clean_class.csv', index_col = 'Name_Short')\n",
      "352/23: df\n",
      "352/24:\n",
      "plant_df = df.copy()\n",
      "fingerprint_to_model = plant_df.[['Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']]\n",
      "label_to_model = compound_df.Bioactivity_class.tolist()\n",
      "352/25:\n",
      "plant_df = df.copy()\n",
      "fingerprint_to_model = plant_df[['Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']]\n",
      "label_to_model = compound_df.Bioactivity_class.tolist()\n",
      "352/26:\n",
      "plant_df = df.copy()\n",
      "fingerprint_to_model = plant_df[['Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']]\n",
      "label_to_model = plant_df.Bioactivity_class.tolist()\n",
      "352/27: indices = fingerprint_to_model.index\n",
      "352/28: fingerprint_to_model.ehad()\n",
      "352/29: fingerprint_to_model.head()\n",
      "352/30: label_to_model\n",
      "352/31: label_to_model.head()\n",
      "352/32: label_to_model\n",
      "352/33:\n",
      "train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "352/34: indices_train, indices_test\n",
      "352/35:\n",
      "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
      "# Shuffle the indices for the k-fold cross-validation\n",
      "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
      "352/36:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "model_RF_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "plt.scatter(test_y, y_ext, c='red', s)\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "352/37:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestClassifier(n_estimators=31, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "model_RF_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "plt.scatter(test_y, y_ext, c='red', s)\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "352/38:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestClassifier(n_estimators=31, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "\n",
      "#calculate R2\n",
      "model_RF_fit = model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "r2_train = r2_score(train_y, y_pred)\n",
      "RMSE_train = mean_squared_error(train_y, y_pred)\n",
      "\n",
      "\n",
      "#CV\n",
      "y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)\n",
      "\n",
      "#Rsqure_CV\n",
      "r2_cv = r2_score(train_y, y_cv)\n",
      "RMSE_cv = mean_squared_error(train_y, y_cv)\n",
      "\n",
      "\n",
      "#Test\n",
      "y_ext = model_RF.predict(test_x)\n",
      "#Rsqure_ext\n",
      "r2_ext = r2_score(test_y, y_ext)\n",
      "RMSE_test = mean_squared_error(test_y, y_ext)\n",
      "\n",
      "print('Results')\n",
      "print('===================================')\n",
      "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
      "print('===================================')\n",
      "print(f'R2 model is {r2_train}')\n",
      "print(f'RMSE model is {RMSE_train}')\n",
      "print('===================================')\n",
      "print(f'Q2 CV is {r2_cv}')\n",
      "print(f'RMSE CV is {RMSE_cv}')\n",
      "print('===================================')\n",
      "print(f'Q2 test is {r2_ext}')\n",
      "print(f'RMSE test is {RMSE_test}')\n",
      "print('===================================')\n",
      "plt.scatter(test_y, y_ext, c='red')\n",
      "plt.scatter(train_y, y_cv, c='blue')\n",
      "plt.scatter(train_y, y_pred, c='black')\n",
      "352/39:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestClassifier(n_estimators=31, random_state=42)\n",
      "352/40:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestClassifier(n_estimators=31, random_state=42)\n",
      "model_RF\n",
      "352/41:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestClassifier(n_estimators=31, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "352/42: scores\n",
      "352/43:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "352/44: scores\n",
      "352/45:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "avg_score = np.mean(score_array)\n",
      "print(avg_score)\n",
      "352/46:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "avg_score = np.mean(scores)\n",
      "print(avg_score)\n",
      "352/47:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "avg_score = np.mean(scores)\n",
      "sd_score = np.sd(scores)\n",
      "print(avg_score)\n",
      "352/48:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "avg_score = np.mean(scores)\n",
      "sd_score = np.std(scores)\n",
      "print(avg_score)\n",
      "352/49:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "avg_score = np.mean(scores)\n",
      "sd_score = np.std(scores)\n",
      "print(avg_score)\n",
      "print(sd_score)\n",
      "352/50:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "model_RF.fit(train_x)\n",
      "\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "avg_score = np.mean(scores)\n",
      "sd_score = np.std(scores)\n",
      "print(avg_score)\n",
      "print(sd_score)\n",
      "352/51:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "model_RF.fit(train_x, train_y)\n",
      "352/52:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "\n",
      "\n",
      "model_RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "y_pred\n",
      "352/53:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "model_RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "y_pred\n",
      "accuracy_score(y_true, y_pred, normalize=False)\n",
      "352/54:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "model_RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "y_pred\n",
      "accuracy_score(train_y, y_pred, normalize=False)\n",
      "352/55:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "model_RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(train_x)\n",
      "y_pred\n",
      "accuracy_score(train_y, y_pred, normalize=True)\n",
      "352/56:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import accuracy_score\n",
      "import scikitplot as scplot\n",
      "\n",
      "model_RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(test_x)\n",
      "y_pred\n",
      "accuracy_score(train_y, y_pred, normalize=True)\n",
      "352/57:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import accuracy_score\n",
      "import scikitplot as scplot\n",
      "\n",
      "model_RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(test_x)\n",
      "y_pred\n",
      "accuracy_score(test_y, y_pred, normalize=True)\n",
      "352/58:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import accuracy_score\n",
      "import scikitplot as skplot\n",
      "\n",
      "model_RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(test_x)\n",
      "y_pred\n",
      "accuracy_score(test_y, y_pred, normalize=True)\n",
      "352/59:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import accuracy_score\n",
      "import scikitplot as skplot\n",
      "\n",
      "model_RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(test_x)\n",
      "y_pred\n",
      "accuracy_score(test_y, y_pred, normalize=True)\n",
      "skplot.metrics.confusion_matrix(test_y, y_pred)\n",
      "352/60:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import accuracy_score\n",
      "import scikitplot as skplot\n",
      "\n",
      "model_RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(test_x)\n",
      "y_pred\n",
      "accuracy_score(test_y, y_pred, normalize=True)\n",
      "skplot.metrics.confusion_matrix(test_y, y_pred)\n",
      "plot.show\n",
      "352/61:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import accuracy_score\n",
      "import scikitplot as skplot\n",
      "\n",
      "model_RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(test_x)\n",
      "y_pred\n",
      "accuracy_score(test_y, y_pred, normalize=True)\n",
      "skplot.metrics.confusion_matrix(test_y, y_pred)\n",
      "plt.show\n",
      "352/62:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import accuracy_score\n",
      "import scikitplot as skplot\n",
      "\n",
      "model_RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(test_x)\n",
      "y_pred\n",
      "accuracy_score(test_y, y_pred, normalize=True)\n",
      "skplot.metrics.plot_confusion_matrixconfusion_matrix(test_y, y_pred)\n",
      "plt.show\n",
      "352/63:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import accuracy_score\n",
      "import scikitplot as skplot\n",
      "\n",
      "model_RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(test_x)\n",
      "y_pred\n",
      "accuracy_score(test_y, y_pred, normalize=True)\n",
      "skplot.metrics.plot_confusion_matrix(test_y, y_pred)\n",
      "plt.show\n",
      "352/64:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import accuracy_score\n",
      "import scikitplot as skplot\n",
      "\n",
      "model_RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(test_x)\n",
      "y_pred\n",
      "accuracy_score = accuracy_score(test_y, y_pred, normalize=True)\n",
      "print(f'accuracy score = {accuracy_score}')\n",
      "skplot.metrics.plot_confusion_matrix(test_y, y_pred)\n",
      "plt.show\n",
      "352/65:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import accuracy_score\n",
      "import scikitplot as skplot\n",
      "\n",
      "model_RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(test_x)\n",
      "y_pred\n",
      "accuracy_score = accuracy_score(test_y, y_pred, normalize=True)\n",
      "print(f'accuracy score = {accuracy_score}')\n",
      "skplot.metrics.plot_confusion_matrix(test_y, y_pred)\n",
      "352/66:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import accuracy_score\n",
      "import scikitplot as skplot\n",
      "\n",
      "model_RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(test_x)\n",
      "y_pred\n",
      "accuracy_score = accuracy_score(test_y, y_pred, normalize=True)\n",
      "print(f'accuracy score = {accuracy_score}')\n",
      "skplot.metrics.plot_confusion_matrix(test_y, y_pred)\n",
      "plt.show()\n",
      "352/67:\n",
      "Bioactivity_class = []\n",
      "for i in df.Bioactivity_class:\n",
      "    if i =='active_both' or 'active_glucosidase' or 'active_amylase':\n",
      "        bioactivity_class.append(\"active\")\n",
      "    else:\n",
      "        bioactivity_class.append(\"inactive\")\n",
      "352/68: Bioactivity_class\n",
      "352/69:\n",
      "Bioactivity_class = []\n",
      "for i in df.Bioactivity_class:\n",
      "    if i =='active_both' or 'active_glucosidase' or 'active_amylase':\n",
      "        Bioactivity_class.append(\"active\")\n",
      "    else:\n",
      "        Bioactivity_class.append(\"inactive\")\n",
      "352/70: Bioactivity_class\n",
      "352/71: len(Bioactivity_class)\n",
      "352/72:\n",
      "Bioactivity_class = []\n",
      "for i in df.Bioactivity_class:\n",
      "    if i = 'active_both' or 'active_glucosidase' or 'active_amylase':\n",
      "        Bioactivity_class.append(\"active\")\n",
      "    else:\n",
      "        Bioactivity_class.append(\"inactive\")\n",
      "352/73: len(Bioactivity_class)\n",
      "352/74:\n",
      "Bioactivity_class = []\n",
      "for i in df.Bioactivity_class:\n",
      "    if i =='active_both' or 'active_glucosidase' or 'active_amylase':\n",
      "        Bioactivity_class.append(\"active\")\n",
      "    else:\n",
      "        Bioactivity_class.append(\"inactive\")\n",
      "352/75: Bioactivity_class\n",
      "352/76:\n",
      "Bioactivity_class = []\n",
      "for i in df.Bioactivity_class:\n",
      "    if i =='active_both' or 'active_glucosidase' or 'active_amylase':\n",
      "        Bioactivity_class.append(\"active\")\n",
      "    else i =='inactive':\n",
      "        Bioactivity_class.append(\"inactive\")\n",
      "352/77:\n",
      "Bioactivity_class = []\n",
      "for i in df.Bioactivity_class:\n",
      "    if i =='active_both' or 'active_glucosidase' or 'active_amylase':\n",
      "        Bioactivity_class.append(\"active\")\n",
      "    else i =='inactive':\n",
      "        Bioactivity_class.append(\"inactive\")\n",
      "352/78:\n",
      "Bioactivity_class = []\n",
      "for i in df.Bioactivity_class:\n",
      "    if i =='active_both' or 'active_glucosidase' or 'active_amylase':\n",
      "        Bioactivity_class.append(\"active\")\n",
      "        else i =='inactive':\n",
      "        Bioactivity_class.append(\"inactive\")\n",
      "352/79:\n",
      "Bioactivity_class = []\n",
      "for i in df.Bioactivity_class:\n",
      "    if i =='active_both' or 'active_glucosidase' or 'active_amylase':\n",
      "        Bioactivity_class.append(\"active\")\n",
      "    else i =='inactive':\n",
      "        Bioactivity_class.append(\"inactive\")\n",
      "352/80:\n",
      "Bioactivity_class = []\n",
      "for i in df.Bioactivity_class:\n",
      "    if i =='active_both' or 'active_glucosidase' or 'active_amylase':\n",
      "        Bioactivity_class.append(\"active\")\n",
      "    else i == 'inactive':\n",
      "        Bioactivity_class.append(\"inactive\")\n",
      "352/81:\n",
      "Bioactivity_class = []\n",
      "for i in df.Bioactivity_class:\n",
      "    if i =='active_both' or 'active_glucosidase' or 'active_amylase':\n",
      "        Bioactivity_class.append(\"active\")\n",
      "    else == 'inactive':\n",
      "        Bioactivity_class.append(\"inactive\")\n",
      "352/82:\n",
      "Bioactivity_class = []\n",
      "for i in df.Bioactivity_class:\n",
      "    if i =='active_both' or 'active_glucosidase' or 'active_amylase':\n",
      "        Bioactivity_class.append(\"active\")\n",
      "    else i== 'inactive':\n",
      "        Bioactivity_class.append(\"inactive\")\n",
      "352/83:\n",
      "Bioactivity_class = []\n",
      "for i in df.Bioactivity_class:\n",
      "    if i in {'active_both' or 'active_glucosidase' or 'active_amylase'}:\n",
      "        Bioactivity_class.append(\"active\")\n",
      "    else i== 'inactive':\n",
      "        Bioactivity_class.append(\"inactive\")\n",
      "352/84:\n",
      "Bioactivity_class = []\n",
      "for i in df.Bioactivity_class:\n",
      "    if i in {'active_both' or 'active_glucosidase' or 'active_amylase'}:\n",
      "        Bioactivity_class.append(\"active\")\n",
      "    else:\n",
      "        Bioactivity_class.append(\"inactive\")\n",
      "352/85: len(Bioactivity_class)\n",
      "352/86: Bioactivity_class\n",
      "352/87:\n",
      "Bioactivity_class = []\n",
      "for i in df.Bioactivity_class:\n",
      "    if i in {'active_both', 'active_glucosidase', 'active_amylase'}:\n",
      "        Bioactivity_class.append(\"active\")\n",
      "    else:\n",
      "        Bioactivity_class.append(\"inactive\")\n",
      "352/88: len(Bioactivity_class)\n",
      "352/89: Bioactivity_class\n",
      "352/90: label_to_model = Bioactivity_class\n",
      "352/91:\n",
      "train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "352/92: indices_train, indices_test\n",
      "352/93:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import accuracy_score\n",
      "import scikitplot as skplot\n",
      "\n",
      "model_RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(test_x)\n",
      "y_pred\n",
      "accuracy_score = accuracy_score(test_y, y_pred, normalize=True)\n",
      "print(f'accuracy score = {accuracy_score}')\n",
      "skplot.metrics.plot_confusion_matrix(test_y, y_pred)\n",
      "plt.show()\n",
      "352/94:\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "avg_score = np.mean(scores)\n",
      "sd_score = np.std(scores)\n",
      "print(avg_score)\n",
      "print(sd_score)\n",
      "352/95:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import accuracy_score\n",
      "import scikitplot as skplot\n",
      "\n",
      "model_RF = RandomForestClassifier(n_estimators=200, random_state=42)\n",
      "model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(test_x)\n",
      "y_pred\n",
      "accuracy_score = accuracy_score(test_y, y_pred, normalize=True)\n",
      "print(f'accuracy score = {accuracy_score}')\n",
      "skplot.metrics.plot_confusion_matrix(test_y, y_pred)\n",
      "plt.show()\n",
      "352/96:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import accuracy_score\n",
      "import scikitplot as skplot\n",
      "\n",
      "model_RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(test_x)\n",
      "y_pred\n",
      "accuracy_score = accuracy_score(test_y, y_pred, normalize=True)\n",
      "print(f'accuracy score = {accuracy_score}')\n",
      "skplot.metrics.plot_confusion_matrix(test_y, y_pred)\n",
      "plt.show()\n",
      "352/97:\n",
      "plant_df = df.copy()\n",
      "fingerprint_to_model = plant_df[['Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']]\n",
      "label_to_model = plant_df.Bioactivity_class.tolist()\n",
      "352/98:\n",
      "plant_df = df.copy()\n",
      "fingerprint_to_model = plant_df[['Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']]\n",
      "label_to_model = plant_df.Bioactivity_class.tolist()\n",
      "352/99:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "model = KNeighborsClassifier(n_neighbors=4)\n",
      "352/100:\n",
      "scores=cross_val_score(model, train_x, train_y, cv=kf)\n",
      "avg_score = np.mean(scores)\n",
      "sd_score = np.std(scores)\n",
      "print(avg_score)\n",
      "print(sd_score)\n",
      "352/101:\n",
      "train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "352/102:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "model = KNeighborsClassifier(n_neighbors=4)\n",
      "352/103:\n",
      "scores=cross_val_score(model, train_x, train_y, cv=kf)\n",
      "avg_score = np.mean(scores)\n",
      "sd_score = np.std(scores)\n",
      "print(avg_score)\n",
      "print(sd_score)\n",
      "352/104:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "model = KNeighborsClassifier(n_neighbors=2)\n",
      "352/105:\n",
      "scores=cross_val_score(model, train_x, train_y, cv=kf)\n",
      "avg_score = np.mean(scores)\n",
      "sd_score = np.std(scores)\n",
      "print(avg_score)\n",
      "print(sd_score)\n",
      "352/106:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "model = KNeighborsClassifier(n_neighbors=3)\n",
      "352/107:\n",
      "scores=cross_val_score(model, train_x, train_y, cv=kf)\n",
      "avg_score = np.mean(scores)\n",
      "sd_score = np.std(scores)\n",
      "print(avg_score)\n",
      "print(sd_score)\n",
      "352/108:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "model = KNeighborsClassifier(n_neighbors=5)\n",
      "352/109:\n",
      "scores=cross_val_score(model, train_x, train_y, cv=kf)\n",
      "avg_score = np.mean(scores)\n",
      "sd_score = np.std(scores)\n",
      "print(avg_score)\n",
      "print(sd_score)\n",
      "352/110:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "model = KNeighborsClassifier(n_neighbors=10)\n",
      "352/111:\n",
      "scores=cross_val_score(model, train_x, train_y, cv=kf)\n",
      "avg_score = np.mean(scores)\n",
      "sd_score = np.std(scores)\n",
      "print(avg_score)\n",
      "print(sd_score)\n",
      "352/112:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "model = KNeighborsClassifier(n_neighbors=4)\n",
      "352/113:\n",
      "scores=cross_val_score(model, train_x, train_y, cv=kf)\n",
      "avg_score = np.mean(scores)\n",
      "sd_score = np.std(scores)\n",
      "print(avg_score)\n",
      "print(sd_score)\n",
      "352/114:\n",
      "scores=cross_val_score(model, train_x, train_y, cv=kf)\n",
      "avg_score = np.mean(scores)\n",
      "sd_score = np.std(scores)\n",
      "print(scores)\n",
      "print(avg_score)\n",
      "print(sd_score)\n",
      "352/115: print(label_to_model.unique())\n",
      "352/116: print(label_to_model)\n",
      "352/117:\n",
      "from sklearn.cluster import KMeans\n",
      "model = KMeans(n_clusters=4)\n",
      "y_pred = model.fit_predict(fingerprint_to_model)\n",
      "352/118: df['cluster'] = y_pred\n",
      "352/119: df\n",
      "352/120: df[df.cluster==0]\n",
      "352/121: df[df.cluster==1]a\n",
      "352/122: df[df.cluster==1]\n",
      "352/123: df[df.cluster==2]\n",
      "352/124: df[df.cluster==3]\n",
      "352/125: fingerprint_to_model_KMean = plant_df[['Emax_amylase_2.75mcgml','Emax_glucosidase_0.67mcgml','Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']]\n",
      "352/126:\n",
      "from sklearn.cluster import KMeans\n",
      "model = KMeans(n_clusters=4)\n",
      "y_pred = model.fit_predict(fingerprint_to_model_KMean)\n",
      "352/127: df['cluster'] = y_pred\n",
      "352/128: df\n",
      "352/129: pvt = df.pivot_table(index='Name_Short', columns=['cluster'], values='Bioactivity_class', fill_value=0, aggfunc='count')\n",
      "352/130: pvt\n",
      "352/131: pvt = df.pivot_table(index='Bioactivity_class', columns=['cluster'], values='Bioactivity_class', fill_value=0, aggfunc='count')\n",
      "352/132: pvt = df.pivot_table(index='Bioactivity_class', columns=['cluster'], values='', fill_value=0, aggfunc='count')\n",
      "352/133: pvt = df.pivot_table(index='Bioactivity_class', columns=['cluster'], fill_value=0, aggfunc='count')\n",
      "352/134: pvt\n",
      "352/135: pvt = df.pivot_table(index='Bioactivity_class', columns=['cluster'], values='Name_short', fill_value=0, aggfunc='count')\n",
      "352/136: pvt = df.pivot_table(index='Bioactivity_class', columns=['cluster'], values='Name_Short', fill_value=0, aggfunc='count')\n",
      "352/137: pvt = df.pivot_table(index='Bioactivity_class', columns=['cluster'], values='cluster', fill_value=0, aggfunc='count')\n",
      "352/138: pvt = df.pivot_table(index='Name_Short', columns=['cluster'], values='Bioactivity_class', fill_value=0, aggfuc='cont')\n",
      "352/139: pvt = df.pivot_table(index='Name_Short', columns=['cluster'], values='Bioactivity_class', fill_value=0, aggfunc='cont')\n",
      "352/140: pvt = df.pivot_table(index='Name_Short', columns=['cluster'], values='Bioactivity_class', fill_value=0, aggfunc='cout')\n",
      "352/141: pvt = df.pivot_table(index='Name_Short', columns=['cluster'], values='Bioactivity_class', fill_value=0, aggfunc='count')\n",
      "352/142: pvt\n",
      "352/143: df['Bioactivity_class_number'] = df['Bioactivity_class'].map({'active_both': 3, 'active_glucosidase': 2, 'active_amylase': 1, 'inactive': 0})\n",
      "352/144:\n",
      "df['Bioactivity_class_number'] = df['Bioactivity_class'].map({'active_both': 3, 'active_glucosidase': 2, 'active_amylase': 1, 'inactive': 0})\n",
      "df\n",
      "352/145:\n",
      "df['Bioactivity_class_number'] = df['Bioactivity_class'].map({'active_both': 3, 'active_glucosidase': 2, 'active_amylase': 1, 'inactive': 0})\n",
      "df.head()\n",
      "352/146:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "model = KNeighborsClassifier(n_neighbors=4)\n",
      "352/147:\n",
      "plant_df = df.copy()\n",
      "fingerprint_to_model = plant_df[['Emax_amylase_2.75mcgml','Emax_glucosidase_0.67mcgml', 'Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']]\n",
      "label_to_model = plant_df.Bioactivity_class_number.tolist()\n",
      "352/148:\n",
      "train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "352/149:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "model = KNeighborsClassifier(n_neighbors=4)\n",
      "scores=cross_val_score(model, train_x, train_y, cv=kf)\n",
      "avg_score = np.mean(scores)\n",
      "sd_score = np.std(scores)\n",
      "print(scores)\n",
      "print(avg_score)\n",
      "print(sd_score)\n",
      "352/150:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "model = KNeighborsClassifier(n_neighbors=4)\n",
      "scores=cross_val_score(model, train_x, train_y, cv=kf)\n",
      "avg_score = np.mean(scores)\n",
      "sd_score = np.std(scores)\n",
      "print(scores)\n",
      "print('mean (%) = {:.3f}'.format(scores.mean(*100))\n",
      "print(sd_score)\n",
      "352/151:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "model = KNeighborsClassifier(n_neighbors=4)\n",
      "scores=cross_val_score(model, train_x, train_y, cv=kf)\n",
      "avg_score = np.mean(scores)\n",
      "sd_score = np.std(scores)\n",
      "print(scores)\n",
      "print('mean (%) = {:.3f}'.format(scores.mean(*100))\n",
      "352/152:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "model = KNeighborsClassifier(n_neighbors=4)\n",
      "scores=cross_val_score(model, train_x, train_y, cv=kf)\n",
      "avg_score = np.mean(scores)\n",
      "sd_score = np.std(scores)\n",
      "print(scores)\n",
      "print('mean (%) = {:.3f}'.format(scores.mean(*100))\n",
      "352/153:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "model = KNeighborsClassifier(n_neighbors=4)\n",
      "scores=cross_val_score(model, train_x, train_y, cv=kf)\n",
      "avg_score = np.mean(scores)\n",
      "sd_score = np.std(scores)\n",
      "print(scores)\n",
      "print('mean (%) = {:.3f}'.format(scores.mean()*100))\n",
      "print(sd_score)\n",
      "352/154:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "model = KNeighborsClassifier(n_neighbors=4)\n",
      "scores=cross_val_score(model, train_x, train_y, cv=kf)\n",
      "avg_score = np.mean(scores)\n",
      "sd_score = np.std(scores)\n",
      "print(scores)\n",
      "print('mean (%) = {:.3f}'.format(avg_score*100))\n",
      "print(sd_score)\n",
      "352/155:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "model = KNeighborsClassifier(n_neighbors=4)\n",
      "scores=cross_val_score(model, train_x, train_y, cv=kf)\n",
      "avg_score = np.mean(scores)\n",
      "sd_score = np.std(scores)\n",
      "print(scores)\n",
      "print('mean (%) = {:.3f}'.format(avg_score*100))\n",
      "print('sd (%) = {:.3f}'.format(sd_score*100))\n",
      "352/156:\n",
      "from sklearn.cluster import KMeans\n",
      "model = KMeans(n_clusters=4)\n",
      "y_pred = model.fit_predict(fingerprint_to_model_KMean)\n",
      "352/157: df['cluster'] = y_pred\n",
      "352/158: df\n",
      "352/159: df.head()\n",
      "352/160: print(label_to_model)\n",
      "352/161: fingerprint_to_model_KMean = plant_df[['Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']]\n",
      "352/162:\n",
      "from sklearn.cluster import KMeans\n",
      "model = KMeans(n_clusters=4)\n",
      "y_pred = model.fit_predict(fingerprint_to_model_KMean)\n",
      "352/163: df['cluster'] = y_pred\n",
      "352/164: df.head()\n",
      "352/165:\n",
      "plant_df = df.copy()\n",
      "fingerprint_to_model = plant_df[['Emax_amylase_2.75mcgml','Emax_glucosidase_0.67mcgml', 'Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']]\n",
      "label_to_model = plant_df.Bioactivity_class_number.tolist()\n",
      "352/166:\n",
      "train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "352/167:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "model = KNeighborsClassifier(n_neighbors=4)\n",
      "scores=cross_val_score(model, train_x, train_y, cv=kf)\n",
      "avg_score = np.mean(scores)\n",
      "sd_score = np.std(scores)\n",
      "print(scores)\n",
      "print('mean (%) = {:.3f}'.format(avg_score*100))\n",
      "print('sd (%) = {:.3f}'.format(sd_score*100))\n",
      "352/168:\n",
      "plant_df = df.copy()\n",
      "fingerprint_to_model = plant_df[['Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']]\n",
      "label_to_model = plant_df.Bioactivity_class_number.tolist()\n",
      "352/169:\n",
      "train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "352/170:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "model = KNeighborsClassifier(n_neighbors=4)\n",
      "scores=cross_val_score(model, train_x, train_y, cv=kf)\n",
      "avg_score = np.mean(scores)\n",
      "sd_score = np.std(scores)\n",
      "print(scores)\n",
      "print('mean (%) = {:.3f}'.format(avg_score*100))\n",
      "print('sd (%) = {:.3f}'.format(sd_score*100))\n",
      "352/171:\n",
      "y_pred = model.predict(test_x)\n",
      "y_pred\n",
      "accuracy_score = accuracy_score(test_y, y_pred, normalize=True)\n",
      "print(f'accuracy score = {accuracy_score}')\n",
      "skplot.metrics.plot_confusion_matrix(test_y, y_pred)\n",
      "plt.show()\n",
      "352/172:\n",
      "model.fit(train_x)\n",
      "y_pred = model.predict(test_x)\n",
      "y_pred\n",
      "accuracy_score = accuracy_score(test_y, y_pred, normalize=True)\n",
      "print(f'accuracy score = {accuracy_score}')\n",
      "skplot.metrics.plot_confusion_matrix(test_y, y_pred)\n",
      "plt.show()\n",
      "352/173:\n",
      "model.fit(train_x, train_y)\n",
      "y_pred = model.predict(test_x)\n",
      "y_pred\n",
      "accuracy_score = accuracy_score(test_y, y_pred, normalize=True)\n",
      "print(f'accuracy score = {accuracy_score}')\n",
      "skplot.metrics.plot_confusion_matrix(test_y, y_pred)\n",
      "plt.show()\n",
      "352/174:\n",
      "model.fit(train_x, train_y)\n",
      "y_pred = model.predict(test_x)\n",
      "y_pred\n",
      "skplot.metrics.plot_confusion_matrix(test_y, y_pred)\n",
      "plt.show()\n",
      "352/175:\n",
      "model.fit(train_x, train_y)\n",
      "y_pred = model.predict(test_x)\n",
      "y_pred\n",
      "accuracy_score = accuracy_score(test_y, y_pred)\n",
      "print(f'accuracy score = {accuracy_score}')\n",
      "skplot.metrics.plot_confusion_matrix(test_y, y_pred)\n",
      "plt.show()\n",
      "352/176:\n",
      "model.fit(train_x, train_y)\n",
      "y_pred = model.predict(test_x)\n",
      "y_pred\n",
      "352/177:\n",
      "model.fit(train_x, train_y)\n",
      "y_pred = model.predict(test_x)\n",
      "y_pred\n",
      "skplot.metrics.plot_confusion_matrix(test_y, y_pred)\n",
      "plt.show()\n",
      "352/178: pvt.head()\n",
      "352/179: accuracy_score(y_true, y_pred)\n",
      "352/180: accuracy_score(train_y, y_pred)\n",
      "352/181:\n",
      "plant_df = df.copy()\n",
      "fingerprint_to_model = plant_df[['Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']]\n",
      "label_to_model = plant_df.Bioactivity_class.tolist()\n",
      "352/182:\n",
      "train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "352/183:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "model = KNeighborsClassifier(n_neighbors=4)\n",
      "scores=cross_val_score(model, train_x, train_y, cv=kf)\n",
      "avg_score = np.mean(scores)\n",
      "sd_score = np.std(scores)\n",
      "print(scores)\n",
      "print('mean (%) = {:.3f}'.format(avg_score*100))\n",
      "print('sd (%) = {:.3f}'.format(sd_score*100))\n",
      "352/184:\n",
      "model.fit(train_x, train_y)\n",
      "y_pred = model.predict(test_x)\n",
      "y_pred\n",
      "skplot.metrics.plot_confusion_matrix(test_y, y_pred)\n",
      "plt.show()ef\n",
      "352/185:\n",
      "model.fit(train_x, train_y)\n",
      "y_pred = model.predict(test_x)\n",
      "y_pred\n",
      "skplot.metrics.plot_confusion_matrix(test_y, y_pred)\n",
      "plt.show()\n",
      "352/186:\n",
      "model.fit(train_x, train_y)\n",
      "y_pred = model.predict(test_x)\n",
      "accuracy_score = accuracy_score(test_y, y_pred, normalize=True)\n",
      "print(f'accuracy score = {accuracy_score}')\n",
      "skplot.metrics.plot_confusion_matrix(test_y, y_pred)\n",
      "plt.show()\n",
      "352/187:\n",
      "model.fit(train_x, train_y)\n",
      "y_pred = model.predict(test_x)\n",
      "accuracy_score = accuracy_score(test_y, y_pred, normalize=True)\n",
      "print(f'accuracy score = {accuracy_score}')\n",
      "skplot.metrics.plot_confusion_matrix(test_y, y_pred)\n",
      "plt.show()\n",
      "352/188:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "model = KNeighborsClassifier(n_neighbors=4)\n",
      "scores=cross_val_score(model, train_x, train_y, cv=kf)\n",
      "avg_score = np.mean(scores)\n",
      "sd_score = np.std(scores)\n",
      "print(scores)\n",
      "print('mean (%) = {:.3f}'.format(avg_score*100))\n",
      "print('sd (%) = {:.3f}'.format(sd_score*100))\n",
      "352/189:\n",
      "model.fit(train_x, train_y)\n",
      "y_pred = model.predict(test_x)\n",
      "accuracy_score = accuracy_score(test_y, y_pred, normalize=True)\n",
      "print(f'accuracy score = {accuracy_score}')\n",
      "skplot.metrics.plot_confusion_matrix(test_y, y_pred)\n",
      "plt.show()\n",
      "352/190:\n",
      "model.fit(train_x, train_y)\n",
      "y_pred = model.predict(test_x)\n",
      "accuracy_score = accuracy_score(test_y, y_pred, normalize=True)\n",
      "print(f'accuracy score = {accuracy_score}')\n",
      "skplot.metrics.plot_confusion_matrix(test_y, y_pred)\n",
      "plt.show()\n",
      "352/191:\n",
      "model.fit(train_x, train_y)\n",
      "y_pred = model.predict(test_x)\n",
      "y_pred\n",
      "352/192:\n",
      "model.fit(train_x, train_y)\n",
      "y_pred = model.predict(test_x)\n",
      "y_pred\n",
      "text_y\n",
      "352/193:\n",
      "model.fit(train_x, train_y)\n",
      "y_pred = model.predict(test_x)\n",
      "y_pred\n",
      "test_y\n",
      "352/194:\n",
      "model.fit(train_x, train_y)\n",
      "y_pred = model.predict(test_x)\n",
      "\n",
      "test_y\n",
      "352/195:\n",
      "model.fit(train_x, train_y)\n",
      "y_pred = model.predict(test_x)\n",
      "accuracy_score = accuracy_score(test_y, y_pred, normalize=True)\n",
      "352/196:\n",
      "model.fit(train_x, train_y)\n",
      "y_pred = model.predict(test_x)\n",
      "accuracy_score = accuracy_score(test_y, y_pred, normalize=False)\n",
      "352/197:\n",
      "model.fit(train_x, train_y)\n",
      "y_pred = model.predict(test_x)\n",
      "accuracy_score = accuracy_score(np.array(test_y), y_pred, normalize=False)\n",
      "352/198:\n",
      "model.fit(train_x, train_y)\n",
      "y_pred = model.predict(test_x)\n",
      "accuracy_score = accuracy_score(np.array(test_y), np.array(y_pred), normalize=False)\n",
      "352/199:\n",
      "model.fit(train_x, train_y)\n",
      "y_pred = model.predict(test_x)\n",
      "accuracy_score(test_y, y_pred)\n",
      "352/200:\n",
      "plant_df = df.copy()\n",
      "fingerprint_to_model = plant_df[['Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']]\n",
      "label_to_model = plant_df.Bioactivity_class.tolist()\n",
      "352/201:\n",
      "train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "352/202:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "model = KNeighborsClassifier(n_neighbors=4)\n",
      "scores=cross_val_score(model, train_x, train_y, cv=kf)\n",
      "avg_score = np.mean(scores)\n",
      "sd_score = np.std(scores)\n",
      "print(scores)\n",
      "print('mean (%) = {:.3f}'.format(avg_score*100))\n",
      "print('sd (%) = {:.3f}'.format(sd_score*100))\n",
      "352/203:\n",
      "model.fit(train_x, train_y)\n",
      "y_pred = model.predict(test_x)\n",
      "accuracy_score(test_y, y_pred)\n",
      "352/204:\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.metrics import accuracy_score\n",
      "import scikitplot as skplot\n",
      "\n",
      "model_RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "model_RF.fit(train_x, train_y)\n",
      "y_pred = model_RF.predict(test_x)\n",
      "y_pred\n",
      "accuracy_score = accuracy_score(test_y, y_pred, normalize=True)\n",
      "print(f'accuracy score = {accuracy_score}')\n",
      "skplot.metrics.plot_confusion_matrix(test_y, y_pred)\n",
      "plt.show()\n",
      "352/205:\n",
      "scores=cross_val_score(model_RF, train_x, train_y, cv=kf)\n",
      "avg_score = np.mean(scores)\n",
      "sd_score = np.std(scores)\n",
      "print(avg_score)\n",
      "print(sd_score)\n",
      "352/206:\n",
      "plant_df = df.copy()\n",
      "fingerprint_to_model = plant_df[['Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']]\n",
      "label_to_model = plant_df.Bioactivity_class.tolist()\n",
      "352/207:\n",
      "df['Bioactivity_class_number'] = df['Bioactivity_class'].map({'active_both': 3, 'active_glucosidase': 2, 'active_amylase': 1, 'inactive': 0})\n",
      "df.head()\n",
      "352/208:\n",
      "plant_df = df.copy()\n",
      "fingerprint_to_model = plant_df[['Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']]\n",
      "label_to_model = plant_df.Bioactivity_class.tolist()\n",
      "352/209:\n",
      "train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)\n",
      "splits = [train_x, test_x, train_y, test_y]\n",
      "# NBVAL_CHECK_OUTPUT\n",
      "print(\"Training data size:\", len(train_x))\n",
      "print(\"Test data size:\", len(test_x))\n",
      "352/210:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "model = KNeighborsClassifier(n_neighbors=4)\n",
      "scores=cross_val_score(model, train_x, train_y, cv=kf)\n",
      "avg_score = np.mean(scores)\n",
      "sd_score = np.std(scores)\n",
      "print(scores)\n",
      "print('mean (%) = {:.3f}'.format(avg_score*100))\n",
      "print('sd (%) = {:.3f}'.format(sd_score*100))\n",
      "352/211:\n",
      "model.fit(train_x, train_y)\n",
      "y_pred = model.predict(test_x)\n",
      "accuracy_score(test_y, y_pred)\n",
      "352/212:\n",
      "model.fit(train_x, train_y)\n",
      "y_pred = model.predict(test_x)\n",
      "model.score(test_x, test_y)\n",
      "352/213:\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "print(classification_report(test_y, y_pred))\n",
      "print(confusion_matrix(test_y, y_pred))\n",
      "352/214:\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "print(classification_report(test_y, y_pred))\n",
      "skplot.metrics.plot_confusion_matrix(test_y, y_pred)\n",
      "plt.show()\n",
      "352/215:\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "print(classification_report(test_y, y_pred))\n",
      "skplot.metrics.plot_confusion_matrix(test_y, y_pred)\n",
      "skplt.metrics.plot_roc(test_y, y_preds)\n",
      "plt.show()\n",
      "352/216:\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "print(classification_report(test_y, y_pred))\n",
      "skplot.metrics.plot_confusion_matrix(test_y, y_pred)\n",
      "skplot.metrics.plot_roc(test_y, y_preds)\n",
      "plt.show()\n",
      "352/217:\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "print(classification_report(test_y, y_pred))\n",
      "skplot.metrics.plot_confusion_matrix(test_y, y_pred)\n",
      "skplot.metrics.plot_roc(test_y, y_pred)\n",
      "plt.show()\n",
      "352/218:\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "print(classification_report(test_y, y_pred))\n",
      "skplot.metrics.plot_confusion_matrix(test_y, y_pred)\n",
      "y_prob = model.predict_proba(test_x)\n",
      "skplot.metrics.plot_roc(test_y, y_prob)\n",
      "plt.show()\n",
      "352/219:\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "print(classification_report(test_y, y_pred))\n",
      "skplot.metrics.plot_confusion_matrix(test_y, y_pred)\n",
      "y_prob = model.predict_proba(test_x)\n",
      "skplot.metrics.plot_precision_recall(test_y, y_prob)\n",
      "plt.show()\n",
      "352/220:\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "print(classification_report(test_y, y_pred))\n",
      "skplot.metrics.plot_confusion_matrix(test_y, y_pred)\n",
      "y_prob = model.predict_proba(test_x)\n",
      "skplot.metrics.plot_roc(test_y, y_prob)\n",
      "plt.show()\n",
      "352/221:\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "print(classification_report(test_y, y_pred))\n",
      "skplot.metrics.plot_confusion_matrix(test_y, y_pred)\n",
      "y_prob = model.predict_proba(test_x)\n",
      "plt.show()\n",
      "352/222:\n",
      "test_prob = model.predict_proba(test_x)[:, 1]\n",
      "# Prediction class on test set\n",
      "test_pred = ml_model.predict(test_x)\n",
      "# Compute False postive rate and True positive rate\n",
      "fpr, tpr, thresholds = metrics.roc_curve(test_y, test_prob)\n",
      "# Calculate Area under the curve to display on the plot\n",
      "auc = roc_auc_score(test_y, test_prob)\n",
      "# Plot the computed values\n",
      "ax.plot(fpr, tpr, label=(f\"{model['label']} AUC area = {auc:.2f}\"))\n",
      "ax.plot([0, 1], [0, 1], \"r--\")\n",
      "ax.set_xlabel(\"False Positive Rate\")\n",
      "ax.set_ylabel(\"True Positive Rate\")\n",
      "ax.set_title(\"Receiver Operating Characteristic\")\n",
      "ax.legend(loc=\"lower right\")\n",
      "352/223:\n",
      "test_prob = model.predict_proba(test_x)[:, 1]\n",
      "# Prediction class on test set\n",
      "test_pred = model.predict(test_x)\n",
      "# Compute False postive rate and True positive rate\n",
      "fpr, tpr, thresholds = metrics.roc_curve(test_y, test_prob)\n",
      "# Calculate Area under the curve to display on the plot\n",
      "auc = roc_auc_score(test_y, test_prob)\n",
      "# Plot the computed values\n",
      "ax.plot(fpr, tpr, label=(f\"{model['label']} AUC area = {auc:.2f}\"))\n",
      "ax.plot([0, 1], [0, 1], \"r--\")\n",
      "ax.set_xlabel(\"False Positive Rate\")\n",
      "ax.set_ylabel(\"True Positive Rate\")\n",
      "ax.set_title(\"Receiver Operating Characteristic\")\n",
      "ax.legend(loc=\"lower right\")\n",
      "352/224:\n",
      "from sklearn.metrics import auc, accuracy_score, recall_score\n",
      "from sklearn.metrics import roc_curve, roc_auc_score\n",
      "\n",
      "test_prob = model.predict_proba(test_x)[:, 1]\n",
      "# Prediction class on test set\n",
      "test_pred = model.predict(test_x)\n",
      "# Compute False postive rate and True positive rate\n",
      "fpr, tpr, thresholds = metrics.roc_curve(test_y, test_prob)\n",
      "# Calculate Area under the curve to display on the plot\n",
      "auc = roc_auc_score(test_y, test_prob)\n",
      "# Plot the computed values\n",
      "ax.plot(fpr, tpr, label=(f\"{model['label']} AUC area = {auc:.2f}\"))\n",
      "ax.plot([0, 1], [0, 1], \"r--\")\n",
      "ax.set_xlabel(\"False Positive Rate\")\n",
      "ax.set_ylabel(\"True Positive Rate\")\n",
      "ax.set_title(\"Receiver Operating Characteristic\")\n",
      "ax.legend(loc=\"lower right\")\n",
      "352/225:\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "352/226:\n",
      "models = []\n",
      "model.append(('kNN', KNeighborsClassier()))\n",
      "model.append(('NB', Gaussian()))\n",
      "model.append(('SVC', SVC()))\n",
      "model.append(('RF', RandomForestClassifier()))\n",
      "model.append(('DT', DecisionTreeClassifier()))\n",
      "\n",
      "results = []\n",
      "names = []\n",
      "for names, model in models\n",
      "    cvs = cross_val_score(model, train_x, train_y, cv=kf)\n",
      "    results.append(cvs)\n",
      "    names.append(name)\n",
      "    print('{:4} {:3f} {:3f}').format(name, cvs.mean(), cvs.std())\n",
      "352/227:\n",
      "models = []\n",
      "model.append(('kNN', KNeighborsClassier()))\n",
      "model.append(('NB', Gaussian()))\n",
      "model.append(('SVC', SVC()))\n",
      "model.append(('RF', RandomForestClassifier()))\n",
      "model.append(('DT', DecisionTreeClassifier()))\n",
      "\n",
      "results = []\n",
      "names = []\n",
      "for names, model in models:\n",
      "    cvs = cross_val_score(model, train_x, train_y, cv=kf)\n",
      "    results.append(cvs)\n",
      "    names.append(name)\n",
      "    print('{:4} {:3f} {:3f}').format(name, cvs.mean(), cvs.std())\n",
      "352/228:\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "352/229:\n",
      "models = []\n",
      "model.append(('kNN', KNeighborsClassier()))\n",
      "model.append(('NB', Gaussian()))\n",
      "model.append(('SVC', SVC()))\n",
      "model.append(('RF', RandomForestClassifier()))\n",
      "model.append(('DT', DecisionTreeClassifier()))\n",
      "\n",
      "results = []\n",
      "names = []\n",
      "for names, model in models:\n",
      "    cvs = cross_val_score(model, train_x, train_y, cv=kf)\n",
      "    results.append(cvs)\n",
      "    names.append(name)\n",
      "    print('{:4} {:3f} {:3f}').format(name, cvs.mean(), cvs.std())\n",
      "352/230:\n",
      "models = []\n",
      "model.append(('kNN', KNeighborsClassifier()))\n",
      "model.append(('NB', Gaussian()))\n",
      "model.append(('SVC', SVC()))\n",
      "model.append(('RF', RandomForestClassifier()))\n",
      "model.append(('DT', DecisionTreeClassifier()))\n",
      "\n",
      "results = []\n",
      "names = []\n",
      "for names, model in models:\n",
      "    cvs = cross_val_score(model, train_x, train_y, cv=kf)\n",
      "    results.append(cvs)\n",
      "    names.append(name)\n",
      "    print('{:4} {:3f} {:3f}').format(name, cvs.mean(), cvs.std())\n",
      "352/231:\n",
      "models = []\n",
      "models.append(('kNN', KNeighborsClassifier()))\n",
      "models.append(('NB', Gaussian()))\n",
      "models.append(('SVC', SVC()))\n",
      "models.append(('RF', RandomForestClassifier()))\n",
      "models.append(('DT', DecisionTreeClassifier()))\n",
      "\n",
      "results = []\n",
      "names = []\n",
      "for names, model in models:\n",
      "    cvs = cross_val_score(model, train_x, train_y, cv=kf)\n",
      "    results.append(cvs)\n",
      "    names.append(name)\n",
      "    print('{:4} {:3f} {:3f}').format(name, cvs.mean(), cvs.std())\n",
      "352/232:\n",
      "models = []\n",
      "models.append(('kNN', KNeighborsClassifier()))\n",
      "models.append(('NB', GaussianNB()))\n",
      "models.append(('SVC', SVC()))\n",
      "models.append(('RF', RandomForestClassifier()))\n",
      "models.append(('DT', DecisionTreeClassifier()))\n",
      "\n",
      "results = []\n",
      "names = []\n",
      "for names, model in models:\n",
      "    cvs = cross_val_score(model, train_x, train_y, cv=kf)\n",
      "    results.append(cvs)\n",
      "    names.append(name)\n",
      "    print('{:4} {:3f} {:3f}').format(name, cvs.mean(), cvs.std())\n",
      "352/233:\n",
      "models = []\n",
      "models.append(('kNN', KNeighborsClassifier()))\n",
      "models.append(('NB', GaussianNB()))\n",
      "models.append(('SVC', SVC()))\n",
      "models.append(('RF', RandomForestClassifier()))\n",
      "models.append(('DT', DecisionTreeClassifier()))\n",
      "\n",
      "results = []\n",
      "names = []\n",
      "for names, model in models:\n",
      "    cvs = cross_val_score(model, train_x, train_y, cv=kf)\n",
      "    results.append(cvs)\n",
      "    names.append(name)\n",
      "    print('{:4} {:3f} {:3f}').format(name, cvs.mean(), cvs.std())\n",
      "352/234:\n",
      "models = []\n",
      "models.append(('kNN', KNeighborsClassifier()))\n",
      "models.append(('NB', GaussianNB()))\n",
      "models.append(('SVC', SVC()))\n",
      "models.append(('RF', RandomForestClassifier()))\n",
      "models.append(('DT', DecisionTreeClassifier()))\n",
      "\n",
      "results = []\n",
      "names = []\n",
      "for name, model in models:\n",
      "    cvs = cross_val_score(model, train_x, train_y, cv=kf)\n",
      "    results.append(cvs)\n",
      "    names.append(name)\n",
      "    print('{:4} {:3f} {:3f}').format(name, cvs.mean(), cvs.std())\n",
      "352/235:\n",
      "models = []\n",
      "models.append(('kNN', KNeighborsClassifier()))\n",
      "models.append(('NB', GaussianNB()))\n",
      "models.append(('SVC', SVC()))\n",
      "models.append(('RF', RandomForestClassifier()))\n",
      "models.append(('DT', DecisionTreeClassifier()))\n",
      "\n",
      "results = []\n",
      "names = []\n",
      "for name, model in models:\n",
      "    cvs = cross_val_score(model, train_x, train_y, cv=kf)\n",
      "    results.append(cvs)\n",
      "    names.append(name)\n",
      "    print('{:4} {:.3f} {:.3f}').format(name, cvs.mean(), cvs.std())\n",
      "352/236:\n",
      "models = []\n",
      "models.append(('kNN', KNeighborsClassifier()))\n",
      "models.append(('NB', GaussianNB()))\n",
      "models.append(('SVC', SVC()))\n",
      "models.append(('RF', RandomForestClassifier()))\n",
      "models.append(('DT', DecisionTreeClassifier()))\n",
      "\n",
      "results = []\n",
      "names = []\n",
      "for name, model in models:\n",
      "    cvs = cross_val_score(model, train_x, train_y, cv=kf)\n",
      "    results.append(cvs)\n",
      "    names.append(name)\n",
      "    print('{:4} {:.3f} ({:.3f})'.format(name, cvs.mean(), cvs.std()))\n",
      "352/237:\n",
      "fig = ply.figure()\n",
      "fig.suptitle('Algorithm Comparison')\n",
      "ax = fig.add_subplot(111)\n",
      "plt.boxplot(results)\n",
      "352/238:\n",
      "fig = plt.figure()\n",
      "fig.suptitle('Algorithm Comparison')\n",
      "ax = fig.add_subplot(111)\n",
      "plt.boxplot(results)\n",
      "ax.set_xticklabels(names)\n",
      "plt.show()\n",
      "352/239:\n",
      "models = []\n",
      "models.append(('kNN', KNeighborsClassifier()))\n",
      "models.append(('NB', GaussianNB()))\n",
      "models.append(('SVC', SVC()))\n",
      "models.append(('RF', RandomForestClassifier()))\n",
      "models.append(('DT', DecisionTreeClassifier()))\n",
      "\n",
      "results = []\n",
      "names = []\n",
      "for name, model in models:\n",
      "    cvs = cross_val_score(model, train_x, train_y, cv=kf)\n",
      "    results.append(cvs)\n",
      "    names.append(name)\n",
      "    print('{:4} {:.3f} ({:.3f})'.format(name, cvs.median(), cvs.std()))\n",
      "352/240:\n",
      "models = []\n",
      "models.append(('kNN', KNeighborsClassifier()))\n",
      "models.append(('NB', GaussianNB()))\n",
      "models.append(('SVC', SVC()))\n",
      "models.append(('RF', RandomForestClassifier()))\n",
      "models.append(('DT', DecisionTreeClassifier()))\n",
      "\n",
      "results = []\n",
      "names = []\n",
      "for name, model in models:\n",
      "    cvs = cross_val_score(model, train_x, train_y, cv=kf)\n",
      "    results.append(cvs)\n",
      "    names.append(name)\n",
      "    print('{:4} {:.3f} ({:.3f})'.format(name, cvs.mean(), cvs.std()))\n",
      "352/241:\n",
      "fig = plt.figure()\n",
      "fig.suptitle('Algorithm Comparison')\n",
      "ax = fig.add_subplot(111)\n",
      "plt.boxplot(results)\n",
      "ax.set_xticklabels(names)\n",
      "ax.set_xticklabels('Accuracy score')\n",
      "plt.show()\n",
      "352/242:\n",
      "fig = plt.figure()\n",
      "fig.suptitle('Algorithm Comparison')\n",
      "ax = fig.add_subplot(111)\n",
      "plt.boxplot(results)\n",
      "ax.set_xticklabels(names)\n",
      "ax.set_ylabels('Accuracy score')\n",
      "plt.show()\n",
      "352/243:\n",
      "fig = plt.figure()\n",
      "fig.suptitle('Algorithm Comparison')\n",
      "ax = fig.add_subplot(111)\n",
      "plt.boxplot(results)\n",
      "ax.set_xticklabels(names)\n",
      "ax.set_ylabel('Accuracy score')\n",
      "plt.show()\n",
      "352/244:\n",
      "fig = plt.figure()\n",
      "fig.suptitle('Algorithm Comparison by 10-fold Cross validation')\n",
      "ax = fig.add_subplot(111)\n",
      "plt.boxplot(results)\n",
      "ax.set_xticklabels(names)\n",
      "ax.set_ylabel('Accuracy score')\n",
      "plt.show()\n",
      "352/245:\n",
      "models = []\n",
      "models.append(('kNN', KNeighborsClassifier(n_neighbors=4)))\n",
      "models.append(('NB', GaussianNB()))\n",
      "models.append(('SVC', SVC()))\n",
      "models.append(('RF', RandomForestClassifier()))\n",
      "models.append(('DT', DecisionTreeClassifier()))\n",
      "\n",
      "results = []\n",
      "names = []\n",
      "for name, model in models:\n",
      "    cvs = cross_val_score(model, train_x, train_y, cv=kf)\n",
      "    results.append(cvs)\n",
      "    names.append(name)\n",
      "    print('{:4} {:.3f} ({:.3f})'.format(name, cvs.mean(), cvs.std()))\n",
      "352/246:\n",
      "models = []\n",
      "models.append(('kNN', KNeighborsClassifier()))\n",
      "models.append(('NB', GaussianNB()))\n",
      "models.append(('SVC', SVC()))\n",
      "models.append(('RF', RandomForestClassifier()))\n",
      "models.append(('DT', DecisionTreeClassifier()))\n",
      "\n",
      "results = []\n",
      "names = []\n",
      "for name, model in models:\n",
      "    cvs = cross_val_score(model, train_x, train_y, cv=kf)\n",
      "    results.append(cvs)\n",
      "    names.append(name)\n",
      "    print('{:4} {:.3f} ({:.3f})'.format(name, cvs.mean(), cvs.std()))\n",
      "352/247:\n",
      "models = []\n",
      "models.append(('kNN', KNeighborsClassifier()))\n",
      "models.append(('NB', GaussianNB()))\n",
      "models.append(('SVC', SVC()))\n",
      "models.append(('RF', RandomForestClassifier(random_state=42)))\n",
      "models.append(('DT', DecisionTreeClassifier()))\n",
      "\n",
      "results = []\n",
      "names = []\n",
      "for name, model in models:\n",
      "    cvs = cross_val_score(model, train_x, train_y, cv=kf)\n",
      "    results.append(cvs)\n",
      "    names.append(name)\n",
      "    print('{:4} {:.3f} ({:.3f})'.format(name, cvs.mean(), cvs.std()))\n",
      "352/248:\n",
      "fig = plt.figure()\n",
      "fig.suptitle('Algorithm Comparison by 10-fold Cross validation')\n",
      "ax = fig.add_subplot(111)\n",
      "plt.boxplot(results)\n",
      "ax.set_xticklabels(names)\n",
      "ax.set_ylabel('Accuracy score')\n",
      "plt.show()\n",
      "352/249:\n",
      "models = []\n",
      "models.append(('kNN', KNeighborsClassifier()))\n",
      "models.append(('NB', GaussianNB()))\n",
      "models.append(('SVC', SVC()))\n",
      "models.append(('RF', RandomForestClassifier(n_estimators=200, random_state=42)))\n",
      "models.append(('DT', DecisionTreeClassifier()))\n",
      "\n",
      "results = []\n",
      "names = []\n",
      "for name, model in models:\n",
      "    cvs = cross_val_score(model, train_x, train_y, cv=kf)\n",
      "    results.append(cvs)\n",
      "    names.append(name)\n",
      "    print('{:4} {:.3f} ({:.3f})'.format(name, cvs.mean(), cvs.std()))\n",
      "352/250:\n",
      "fig = plt.figure()\n",
      "fig.suptitle('Algorithm Comparison by 10-fold Cross validation')\n",
      "ax = fig.add_subplot(111)\n",
      "plt.boxplot(results)\n",
      "ax.set_xticklabels(names)\n",
      "ax.set_ylabel('Accuracy score')\n",
      "plt.show()\n",
      "352/251:\n",
      "models = []\n",
      "models.append(('kNN', KNeighborsClassifier()))\n",
      "models.append(('NB', GaussianNB()))\n",
      "models.append(('SVC', SVC()))\n",
      "models.append(('RF', RandomForestClassifier(random_state=42)))\n",
      "models.append(('DT', DecisionTreeClassifier()))\n",
      "\n",
      "results = []\n",
      "names = []\n",
      "for name, model in models:\n",
      "    cvs = cross_val_score(model, train_x, train_y, cv=kf)\n",
      "    results.append(cvs)\n",
      "    names.append(name)\n",
      "    print('{:4} {:.3f} ({:.3f})'.format(name, cvs.mean(), cvs.std()))\n",
      "352/252:\n",
      "fig = plt.figure()\n",
      "fig.suptitle('Algorithm Comparison by 10-fold Cross validation')\n",
      "ax = fig.add_subplot(111)\n",
      "plt.boxplot(results)\n",
      "ax.set_xticklabels(names)\n",
      "ax.set_ylabel('Accuracy score')\n",
      "plt.show()\n",
      "352/253: model = SVC()\n",
      "352/254: model_SV = SVC()\n",
      "352/255: svc = SVC()\n",
      "352/256: SVR = SVR(kernel='rbf')\n",
      "352/257: SVC = SVC(kernel='rbf')\n",
      "352/258:\n",
      "SVC = SVC(kernel='rbf')\n",
      "SVR_fit = SVR.fit(train_x_selected, train_y)\n",
      "352/259:\n",
      "SVC = SVC(kernel='rbf')\n",
      "SVC.fit(train_x, train_y)\n",
      "352/260:\n",
      "SVC = SVC(kernel='rbf')\n",
      "SVC.fit(train_x, train_y)\n",
      "352/261:\n",
      "svc = SVC(kernel='rbf')\n",
      "Svc.fit(train_x, train_y)\n",
      "352/262:\n",
      "svc = SVC(kernel='rbf')\n",
      "svc.fit(train_x, train_y)\n",
      "352/263:\n",
      "svc = SVC()\n",
      "svc.fit(train_x, train_y)\n",
      "352/264:\n",
      "model_svc = SVC()\n",
      "svc.fit(train_x, train_y)\n",
      "352/265:\n",
      "model_svc = SVC()\n",
      "svc.fit(train_x, train_y)\n",
      "   1: %history -g\n"
     ]
    }
   ],
   "source": [
    "%history -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "572e9c20-75aa-4474-b848-03793df49510",
   "metadata": {},
   "outputs": [],
   "source": [
    "%history -g -f notebook_file.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba742593-a3ff-4536-a053-1e4b6d1815af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
